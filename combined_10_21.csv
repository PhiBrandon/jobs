,company,description,descriptionHTML,externalApplyLink,id,jobType,jobType/0,jobType/1,jobType/2,location,positionName,postedAt,postingDateParsed,rating,reviewsCount,salary,scrapedAt,searchInput/country,searchInput/location,searchInput/position,url
0,OrangePeople,"We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.
Responsibilities:

 Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.
 Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.
 Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.
 Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.
 Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.
 Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.
 Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.
 Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.
 Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.
 Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.
 Is familiar with SOC 2 compliance and its impact on company policies and processes.
 Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.
 Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.

Requirements:

 Bachelor's or Master’s degree in Computer Science, Engineering, or a related field.
 8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.
 Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.
 Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.
 Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.
 Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.
 Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.
 Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.
 Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.
 A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.

Job Type: Contract
Pay: From $60.00 per hour
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Vision insurance

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Python: 5 years (Preferred)
 data storage, processing, and analytics: 6 years (Preferred)
 SQL and NoSQL: 6 years (Preferred)
 healthcare: 5 years (Preferred)
 Airflow: 5 years (Preferred)
 Kubernetes: 6 years (Preferred)
 Docker: 6 years (Preferred)
 ETL: 5 years (Preferred)
 data engineering: 8 years (Preferred)
 Apache Spark & Snowflake: 6 years (Preferred)
 Big data: 6 years (Preferred)
 Databricks: 5 years (Preferred)
 AWS Services: 5 years (Preferred)
 PySpark: 5 years (Preferred)

Work Location: Remote","<p>We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.</li>
 <li>Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.</li>
 <li>Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.</li>
 <li>Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.</li>
 <li>Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.</li>
 <li>Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.</li>
 <li>Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.</li>
 <li>Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.</li>
 <li>Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.</li>
 <li>Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.</li>
 <li>Is familiar with SOC 2 compliance and its impact on company policies and processes.</li>
 <li>Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.</li>
 <li>Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.</li>
</ul>
<p><b>Requirements:</b></p>
<ul>
 <li>Bachelor&apos;s or Master&#x2019;s degree in Computer Science, Engineering, or a related field.</li>
 <li>8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.</li>
 <li>Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.</li>
 <li>Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.</li>
 <li>Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.</li>
 <li>Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.</li>
 <li>Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.</li>
 <li>Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.</li>
 <li>Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.</li>
 <li>A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: From &#x24;60.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Python: 5 years (Preferred)</li>
 <li>data storage, processing, and analytics: 6 years (Preferred)</li>
 <li>SQL and NoSQL: 6 years (Preferred)</li>
 <li>healthcare: 5 years (Preferred)</li>
 <li>Airflow: 5 years (Preferred)</li>
 <li>Kubernetes: 6 years (Preferred)</li>
 <li>Docker: 6 years (Preferred)</li>
 <li>ETL: 5 years (Preferred)</li>
 <li>data engineering: 8 years (Preferred)</li>
 <li>Apache Spark &amp; Snowflake: 6 years (Preferred)</li>
 <li>Big data: 6 years (Preferred)</li>
 <li>Databricks: 5 years (Preferred)</li>
 <li>AWS Services: 5 years (Preferred)</li>
 <li>PySpark: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,2865c3b2b6437bd7,,Contract,,,Remote,Cloud Data Engineer,Today,2023-10-18T13:30:38.652Z,,,From $60 an hour,2023-10-18T13:30:38.749Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2865c3b2b6437bd7&from=jasx&tk=1hd1fm1o6iman800&vjs=3
1,UnitedHealthcare,"At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.
  
  
 Positions in this function are responsible for the management and manipulation of mostly structured data, with a focus on building business intelligence tools, conducting analysis, performing normalization operations, and assuring data quality. Depending on the specific role and business line, example responsibilities in this function could include creating specifications to bring data into a common structure, creating product specifications and models, developing data solutions to support analyses, performing analysis, interpreting results, developing actionable insights and presenting recommendations for use across the company. Roles in this function could partner with stakeholders to understand data requirements and develop tools and models such as segmentation, dashboards, data visualizations, decision aids and business case analysis to support the organization. Other roles involved could include producing and managing the delivery of activity and value analytics to external stakeholders and clients. Team members will typically use business intelligence, data visualization, query, analytic and statistical software to build solutions, perform analysis and interpret data.
  
  
  You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.
  
  
 Primary Responsibilities: 
  General Job Profile 
  
  This position will work directly with stakeholders within our UHC Core Operations and more broadly within UHG as appropriate to deliver quality data solutions. This position provides visibility, analytical, ad hoc and development capabilities leveraging data from different functional areas across the UHG landscape 
  
  Job Scope and Guidelines 
  
  Works in an Agile framework within a matrix environment working in sprints and utilizing agile tools (e.g., RallyDev) 
  Instills an Agile framework within the team and across the matrix environment to operate as applicable and fully utilizing RallyDev tools 
  Build, maintain and/or adhere to a structured data governance process to be used across all datasets with a focus on quality and accuracy 
  Works closely within Quality Management & Insights (QMI) and across UnitedHealthcare 
  Identify and participate in the resolution of data integrity issues and organizational problems 
  
 
 Functional Competencies 
  
  Demonstrate and apply understanding of UnitedHealth Group's business (e.g., specific business capabilities, functions, processes, and business cycles) and knowledge of operations, goals, and policies and procedures of internal business partners (e.g., information contacts) to provide effective support to internal and/or external customers 
  Manage and protect data, adhering to applicable legal/regulatory requirements (e.g., HIPAA, PHI, PII, DOI, state and federal regulations) 
  Propose and/or define long-term strategies for implementing process and/or data and reporting improvements 
  Identify and/or provide opportunities for additional training and learning to support process and report improvements 
  Review and/or identify appropriate data infrastructure to use based on customers' needs in alignment with QMI priorities 
  Develop business context diagrams (e.g., business data flows, process flows) to analyze/confirm the definition of project requirements 
  Demonstrate understanding of the difference between business requirements and technical solutions and define approach for storing and updating business requirements 
  Collaborate with business and technical stakeholders (e.g., business owners, process owners, domain experts) to identify specific business requirements. Perform reviews with all stakeholders to obtain approval/signoff of project requirements documents 
  Update progress to project schedule to track/measure progress one’s progress fulfilling aligned tasks. In addition to supporting ongoing monitoring by keeping project documentation or applications updated (e.g., RallyDev)
 
  
  
  
 You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
  Required Qualifications: 
  
  3+ years of SQL/TSQL development experience 
  3+ years of SSIS package development experience 
  3+ years of experience with Data Modeling, ETL construction with advanced job scheduling 
  3+ years of experience performing data analysis and report development 
  3+ years of experience working in relational databases, database structures and design, systems design, data management, data warehouse
 
  
  
  
 Preferred Qualifications: 
  
  Bachelor’s Degree 
  Experience with MS Access
 
  
  
 
  All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy
 
  
  
 California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The salary range for California/Colorado/Connecticut/Nevada/New Jersey/New York/Rhode Island/Washington residents is $67,800 to $133,100 annually. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
  
  
  At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
  
  
  
 Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
  
  
  UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.","<div>
 <p>At UnitedHealthcare, we&#x2019;re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start <b>Caring. Connecting. Growing together.</b></p>
 <br> 
 <p> </p>
 <p>Positions in this function are responsible for the management and manipulation of mostly structured data, with a focus on building business intelligence tools, conducting analysis, performing normalization operations, and assuring data quality. Depending on the specific role and business line, example responsibilities in this function could include creating specifications to bring data into a common structure, creating product specifications and models, developing data solutions to support analyses, performing analysis, interpreting results, developing actionable insights and presenting recommendations for use across the company. Roles in this function could partner with stakeholders to understand data requirements and develop tools and models such as segmentation, dashboards, data visualizations, decision aids and business case analysis to support the organization. Other roles involved could include producing and managing the delivery of activity and value analytics to external stakeholders and clients. Team members will typically use business intelligence, data visualization, query, analytic and statistical software to build solutions, perform analysis and interpret data.</p>
 <br> 
 <p></p> 
 <p> You&#x2019;ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.</p>
 <br> 
 <p> </p>
 <p><b>Primary Responsibilities:</b></p> 
 <p><b> General Job Profile</b></p> 
 <ul> 
  <li>This position will work directly with stakeholders within our UHC Core Operations and more broadly within UHG as appropriate to deliver quality data solutions. This position provides visibility, analytical, ad hoc and development capabilities leveraging data from different functional areas across the UHG landscape</li> 
 </ul> 
 <p><b> </b><b>Job Scope and Guidelines</b></p> 
 <ul> 
  <li>Works in an Agile framework within a matrix environment working in sprints and utilizing agile tools (e.g., RallyDev)</li> 
  <li>Instills an Agile framework within the team and across the matrix environment to operate as applicable and fully utilizing RallyDev tools</li> 
  <li>Build, maintain and/or adhere to a structured data governance process to be used across all datasets with a focus on quality and accuracy</li> 
  <li>Works closely within Quality Management &amp; Insights (QMI) and across UnitedHealthcare</li> 
  <li>Identify and participate in the resolution of data integrity issues and organizational problems</li> 
 </ul> 
 <p></p>
 <p><b>Functional Competencies</b></p> 
 <ul> 
  <li>Demonstrate and apply understanding of UnitedHealth Group&apos;s business (e.g., specific business capabilities, functions, processes, and business cycles) and knowledge of operations, goals, and policies and procedures of internal business partners (e.g., information contacts) to provide effective support to internal and/or external customers</li> 
  <li>Manage and protect data, adhering to applicable legal/regulatory requirements (e.g., HIPAA, PHI, PII, DOI, state and federal regulations)</li> 
  <li>Propose and/or define long-term strategies for implementing process and/or data and reporting improvements</li> 
  <li>Identify and/or provide opportunities for additional training and learning to support process and report improvements</li> 
  <li>Review and/or identify appropriate data infrastructure to use based on customers&apos; needs in alignment with QMI priorities</li> 
  <li>Develop business context diagrams (e.g., business data flows, process flows) to analyze/confirm the definition of project requirements</li> 
  <li>Demonstrate understanding of the difference between business requirements and technical solutions and define approach for storing and updating business requirements</li> 
  <li>Collaborate with business and technical stakeholders (e.g., business owners, process owners, domain experts) to identify specific business requirements. Perform reviews with all stakeholders to obtain approval/signoff of project requirements documents</li> 
  <li>Update progress to project schedule to track/measure progress one&#x2019;s progress fulfilling aligned tasks. In addition to supporting ongoing monitoring by keeping project documentation or applications updated (e.g., RallyDev)</li>
 </ul>
 <br> 
 <p></p> 
 <p> </p>
 <p>You&#x2019;ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.</p>
 <p><b> Required Qualifications:</b></p> 
 <ul> 
  <li>3+ years of SQL/TSQL development experience</li> 
  <li>3+ years of SSIS package development experience</li> 
  <li>3+ years of experience with Data Modeling, ETL construction with advanced job scheduling</li> 
  <li>3+ years of experience performing data analysis and report development</li> 
  <li>3+ years of experience working in relational databases, database structures and design, systems design, data management, data warehouse</li>
 </ul>
 <br> 
 <p></p> 
 <p> </p>
 <p><b>Preferred Qualifications:</b></p> 
 <ul> 
  <li>Bachelor&#x2019;s Degree</li> 
  <li>Experience with MS Access</li>
 </ul>
 <br> 
 <p></p> 
 <ul>
  <li>All employees working remotely will be required to adhere to UnitedHealth Group&#x2019;s Telecommuter Policy</li>
 </ul>
 <br> 
 <p> </p>
 <p><b>California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: </b>The salary range for California/Colorado/Connecticut/Nevada/New Jersey/New York/Rhode Island/Washington residents is &#x24;67,800 to &#x24;133,100 annually. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you&#x2019;ll find a far-reaching choice of benefits and incentives.</p>
 <br> 
 <p></p> 
 <p><i> At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone&#x2013;of every race, gender, sexuality, age, location and income&#x2013;deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes &#x2014; an enterprise priority reflected in our mission.</i></p>
 <br> 
 <p></p> 
 <p><i> </i></p>
 <p><i>Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.</i></p>
 <br> 
 <p></p> 
 <p><i> UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.</i></p>
</div>",https://careers.unitedhealthgroup.com/job/19297158/data-engineer-business-insights-remote-remote/?src=JB-22473,445662e46d0fc401,,Full-time,,,"Chicago, IL 60601","Data Engineer, Business Insights - Remote",Today,2023-10-18T13:30:38.447Z,3.7,2350.0,"$67,800 - $133,100 a year",2023-10-18T13:30:38.450Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=445662e46d0fc401&from=jasx&tk=1hd1fm1o6iman800&vjs=3
2,SCIGON Solution,"Skills:

 Extensive experience as a Data Engineer or similar role, with a focus on Scala, Java, and Python programming languages.
 Proven expertise in Apache Spark, HBase, and Hive, with the ability to process and manipulate large datasets effectively.
 Familiarity with AWS services (e.g., EMR, S3, Redshift) is a plus.
 Experience with data migration from on-premises to cloud environments is desirable.
 A bachelor's degree in Computer Science, Data Science, or a related field (a master's degree is a plus).
 Strong problem-solving skills and attention to detail.
 Excellent communication and teamwork abilities.

Responsibilities:

 Develop Data Processing Solutions: Leverage your expertise in Scala (60%), Java, and Python to design, develop, and maintain data processing solutions that operate at scale, ensuring the efficiency, reliability, and performance of data workflows.
 Big Data Mastery: Utilize your in-depth knowledge of Big Data technologies, including Apache Spark, HBase, and Hive, to process and manipulate large datasets efficiently.
 Cloud Expertise: If you have experience with AWS, contribute to the migration of data processing systems from on-premises to the cloud, ensuring seamless and secure operations.
 Data Migration: Collaborate on data migration initiatives, playing a pivotal role in transitioning data processing and handling systems to modern cloud-based platforms.
 Data Handling: Employ best practices in data handling, ensuring the integrity, security, and accessibility of data assets while complying with relevant regulations.

Job Type: Contract
Pay: $58.00 - $68.00 per hour
Experience level:

 5 years
 6 years

Schedule:

 Monday to Friday

Application Question(s):

 Due to numerous fraudulent applications, we require candidates to show any valid IDs at the beginning of the video interview stage (We only need to see your name and photo, you can cover the rest of the details). Are you willing to provide it? (Yes/No):

Education:

 Bachelor's (Preferred)

Experience:

 Data Engineering: 5 years (Preferred)
 Scala: 3 years (Preferred)
 Java: 3 years (Preferred)
 Python: 3 years (Preferred)
 Apache Spark: 3 years (Preferred)
 HBase, and Hive: 3 years (Preferred)
 AWS Services: S3, Redshift: 3 years (Preferred)
 data migration from on-premises to cloud environment: 3 years (Preferred)
 AWS EMR: 3 years (Preferred)
 AWS Airflow: 3 years (Preferred)
 Industry: 5 years (Preferred)

Work Location: Remote","<p>Skills:</p>
<ul>
 <li>Extensive experience as a Data Engineer or similar role, with a focus on Scala, Java, and Python programming languages.</li>
 <li>Proven expertise in Apache Spark, HBase, and Hive, with the ability to process and manipulate large datasets effectively.</li>
 <li>Familiarity with AWS services (e.g., EMR, S3, Redshift) is a plus.</li>
 <li>Experience with data migration from on-premises to cloud environments is desirable.</li>
 <li>A bachelor&apos;s degree in Computer Science, Data Science, or a related field (a master&apos;s degree is a plus).</li>
 <li>Strong problem-solving skills and attention to detail.</li>
 <li>Excellent communication and teamwork abilities.</li>
</ul>
<p>Responsibilities:</p>
<ul>
 <li>Develop Data Processing Solutions: Leverage your expertise in Scala (60%), Java, and Python to design, develop, and maintain data processing solutions that operate at scale, ensuring the efficiency, reliability, and performance of data workflows.</li>
 <li>Big Data Mastery: Utilize your in-depth knowledge of Big Data technologies, including Apache Spark, HBase, and Hive, to process and manipulate large datasets efficiently.</li>
 <li>Cloud Expertise: If you have experience with AWS, contribute to the migration of data processing systems from on-premises to the cloud, ensuring seamless and secure operations.</li>
 <li>Data Migration: Collaborate on data migration initiatives, playing a pivotal role in transitioning data processing and handling systems to modern cloud-based platforms.</li>
 <li>Data Handling: Employ best practices in data handling, ensuring the integrity, security, and accessibility of data assets while complying with relevant regulations.</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;58.00 - &#x24;68.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
 <li>6 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Due to numerous fraudulent applications, we require candidates to show any valid IDs at the beginning of the video interview stage (We only need to see your name and photo, you can cover the rest of the details). Are you willing to provide it? (Yes/No):</li>
</ul>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Preferred)</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Engineering: 5 years (Preferred)</li>
 <li>Scala: 3 years (Preferred)</li>
 <li>Java: 3 years (Preferred)</li>
 <li>Python: 3 years (Preferred)</li>
 <li>Apache Spark: 3 years (Preferred)</li>
 <li>HBase, and Hive: 3 years (Preferred)</li>
 <li>AWS Services: S3, Redshift: 3 years (Preferred)</li>
 <li>data migration from on-premises to cloud environment: 3 years (Preferred)</li>
 <li>AWS EMR: 3 years (Preferred)</li>
 <li>AWS Airflow: 3 years (Preferred)</li>
 <li>Industry: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,334af16420b57c82,,Contract,,,Remote,Sr. Data Engineer (Contract/W2 Only/ No Vendor C2C),Today,2023-10-18T13:30:55.953Z,,,$58 - $68 an hour,2023-10-18T13:30:56.047Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=334af16420b57c82&from=jasx&tk=1hd1fm1o6iman800&vjs=3
3,Sleeper,"Position Summary
  The Senior/First Data Engineer will play a crucial role in building, maintaining, and enhancing ETL processes that drive our analytics and machine learning platforms. This individual will be responsible for developing actionable insights from complex data sets, and work closely with various business units to inform strategy and decision-making.
  You will be the first hire in this function.
 
  Location
 
   SF Bay Area, NYC, or Remote
 
 
  Key Responsibilities
 
   ETL & Backend Development:
 
 
   Design and optimize ETL pipelines.
   Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB.
 
 
   Data Architecture:
 
 
   Design scalable and efficient data models for Cassandra and ScyllaDB.
   Ensure data integrity, quality, and security.
 
 
   Data Science Support:
 
 
   Collaborate with data scientists, providing them with clean and reliable datasets.
   Assist in implementing and scaling data science models.
 
 
   Innovation & Research:
 
 
   Stay abreast of latest technologies.
   Recommend technical improvements for data processing and storage.
 
 
  Qualifications
  Required
 
   Bachelor’s or Master’s degree in Computer Science, Engineering, or a related technical field.
   5+ years of experience in backend development, with a strong focus on data engineering.
   
     Technical skills: Expertise in Python, Java, Scala, and Elixer for backend and ETL processes.
     Mastery of ETL tools/frameworks (e.g. Apache Kafka, Apache Airflow).
     Deep knowledge of SQL/NoSQL databases, including Cassandra and ScyllaDB, and data warehousing solutions (e.g., Redshift, BigQueary, Snowflake).
     Proficiency in cloud platforms (AWS, GCP, Azure) and distributed systems.
     Familiarity with data science concepts, tools, and libraries (e.g. Pandas, Scikit-learn).
     Soft Skills: Exceptional problem-solving skills.
     Strong communication for technical and non-technical discussions.
   
 
  Nice-to-have
 
   Experience with cloud platforms like AWS, GCP, or Azure.
   Exceptional communication skills, both verbal and written.
   Expertise in machine learning algorithms and frameworks (e.g., TensorFlow, PyTorch, scikit-learn).
 
 
  Benefits
 
   Competitive salary ($150,000-$225,000/year) and stock options
   Comprehensive health, dental, and vision plans
   401(k)
   Flexible working hours and remote work options
   Regular team building events and activities","<div>
 <h2 class=""jobSectionHeader""><b>Position Summary</b></h2>
 <p> The Senior/First Data Engineer will play a crucial role in building, maintaining, and enhancing ETL processes that drive our analytics and machine learning platforms. This individual will be responsible for developing actionable insights from complex data sets, and work closely with various business units to inform strategy and decision-making.</p>
 <p> You will be the first hire in this function.</p>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Location</b></h2>
 <ul>
  <li><p> SF Bay Area, NYC, or Remote</p></li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Key Responsibilities</b></h2>
 <ul>
  <li><p> ETL &amp; Backend Development:</p></li>
 </ul>
 <ul>
  <li><p> Design and optimize ETL pipelines.</p></li>
  <li><p> Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB.</p></li>
 </ul>
 <ul>
  <li><p> Data Architecture:</p></li>
 </ul>
 <ul>
  <li><p> Design scalable and efficient data models for Cassandra and ScyllaDB.</p></li>
  <li><p> Ensure data integrity, quality, and security.</p></li>
 </ul>
 <ul>
  <li><p> Data Science Support:</p></li>
 </ul>
 <ul>
  <li><p> Collaborate with data scientists, providing them with clean and reliable datasets.</p></li>
  <li><p> Assist in implementing and scaling data science models.</p></li>
 </ul>
 <ul>
  <li><p> Innovation &amp; Research:</p></li>
 </ul>
 <ul>
  <li><p> Stay abreast of latest technologies.</p></li>
  <li><p> Recommend technical improvements for data processing and storage.</p></li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Qualifications</b></h2>
 <h3 class=""jobSectionHeader""><b> Required</b></h3>
 <ul>
  <li><p> Bachelor&#x2019;s or Master&#x2019;s degree in Computer Science, Engineering, or a related technical field.</p></li>
  <li><p> 5+ years of experience in backend development, with a strong focus on data engineering.</p>
   <ul>
    <li><p> Technical skills: Expertise in Python, Java, Scala, and Elixer for backend and ETL processes.</p></li>
    <li><p> Mastery of ETL tools/frameworks (e.g. Apache Kafka, Apache Airflow).</p></li>
    <li><p> Deep knowledge of SQL/NoSQL databases, including Cassandra and ScyllaDB, and data warehousing solutions (e.g., Redshift, BigQueary, Snowflake).</p></li>
    <li><p> Proficiency in cloud platforms (AWS, GCP, Azure) and distributed systems.</p></li>
    <li><p> Familiarity with data science concepts, tools, and libraries (e.g. Pandas, Scikit-learn).</p></li>
    <li><p> Soft Skills: Exceptional problem-solving skills.</p></li>
    <li><p> Strong communication for technical and non-technical discussions.</p></li>
   </ul></li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Nice-to-have</b></h3>
 <ul>
  <li><p> Experience with cloud platforms like AWS, GCP, or Azure.</p></li>
  <li><p> Exceptional communication skills, both verbal and written.</p></li>
  <li><p> Expertise in machine learning algorithms and frameworks (e.g., TensorFlow, PyTorch, scikit-learn).</p></li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Benefits</b></h2>
 <ul>
  <li><p> Competitive salary (&#x24;150,000-&#x24;225,000/year) and stock options</p></li>
  <li><p> Comprehensive health, dental, and vision plans</p></li>
  <li><p> 401(k)</p></li>
  <li><p> Flexible working hours and remote work options</p></li>
  <li><p> Regular team building events and activities</p></li>
 </ul>
</div>
<p></p>",https://www.indeed.com/applystart?jk=8baee8cb684bbc8a&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXh3KzOnQ4B8j-IF7wQPwfQQ5Z_dA0xndw9ylAW4Hz2CAQ&vjfrom=serp&astse=b85fa03b646a3712&assa=8318,8baee8cb684bbc8a,,Full-time,,,"Washington, DC",Senior/First Data Engineer,Today,2023-10-18T13:30:58.745Z,,,"$150,000 - $225,000 a year",2023-10-18T13:30:58.748Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=8baee8cb684bbc8a&from=jasx&tk=1hd1fm1o6iman800&vjs=3
4,The Talent Trove,"We are hiring for one of our major Health Care client and we are looking for a Senior Data Engineer with Snowflake and Data Factory experience in Azure/ any cloud environment.
Roles and responsibilities:

 Data Pipeline Development: Develop and maintain data pipelines that extract, transform, and load (ETL) data from various sources into a centralized data storage system, such as a data warehouse or data lake. Ensure the smooth flow of data from source systems to destination systems while adhering to data quality and integrity standards.
 Data Integration: Integrate data from multiple sources and systems, including databases, APIs, log files, streaming platforms, and external data providers. Handle data ingestion, transformation, and consolidation to create a unified and reliable data foundation for analysis and reporting.
 Data Transformation and Processing: Develop data transformation routines to clean, normalize, and aggregate data. Apply data processing techniques to handle complex data structures, handle missing or inconsistent data, and prepare the data for analysis, reporting, or machine learning tasks.
 Contribute to common frameworks and best practices in code development, deployment, and automation/orchestration of data pipelines.
 Implement data governance in line with company standards
 Partner with Data Analytics and Product leaders to design best practices and standards for developing and productionalizing analytic pipelines.
 Partner with Infrastructure leaders on architecture approaches to advance the data and analytics platform, including exploring new tools and techniques that leverage the cloud environment (Azure, Snowflake, others)
 Monitoring and Support: Monitor data pipelines and data systems to detect and resolve issues promptly. Develop monitoring tools, alerts, and automated error handling mechanisms to ensure data integrity and system reliability.

Qualifications:

 Extensive experience designing data solutions including data modeling.
 Extensive hands-on experience developing data processing jobs (SQL) that demonstrates a strong understanding of software engineering principles.
 Experience orchestrating data pipelines using technology like ADF, Airflow etc
 Experience working with both real-time and batch data, knowing the strengths and weaknesses of both and when to apply one over another.
 Experience building data pipelines on either AWS, Azure or GCP, following best practices in Cloud deployments
 Experience working with Hive /HBase / Presto
 Fluent in SQL (any flavor), with experience using Window functions and more advanced features.
 Understanding of DevOps tools, Git workflow and building CI/CD pipelines
 Ability to work with business and technical audiences on business requirement meetings, technical white boarding exercises, and SQL coding/debugging sessions.
 Experience supporting big data pipelines.
 Experience applying data governance controls within a highly regulated environment.

Preferred Qualifications:

 Bachelor’s Degree or higher in Database Management, Information Technology, Computer Science or similar
 Experience working in projects with agile/scrum methodologies.
 Familiar with Azure Data Factory or Apache Airflow
 Familiar with Azure Databricks or Snowflake
 Experience with shell scripting languages
 Well versed in Python, in fulfilling multiple general-purpose use-cases, and not limited to developing data APIs and pipelines.
 Experience with Apache Spark and related Big Data stack and technologies
 Experience working with Apache Kafka, building appropriate producer/consumer apps.
 Familiarity with production quality ML and/or AI model development and deployment.
 Experience working with Kubernetes and Docker, and knowledgeable about cloud infrastructure automation and management (e.g., Terraform)

Job Type: Contract
Pay: $65.00 per hour
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 10 years

Schedule:

 8 hour shift

Work Location: Remote","<p>We are hiring for one of our major Health Care client and we are looking for a Senior Data Engineer with Snowflake and Data Factory experience in Azure/ any cloud environment.</p>
<p><b>Roles and responsibilities:</b></p>
<ul>
 <li>Data Pipeline Development: Develop and maintain data pipelines that extract, transform, and load (ETL) data from various sources into a centralized data storage system, such as a data warehouse or data lake. Ensure the smooth flow of data from source systems to destination systems while adhering to data quality and integrity standards.</li>
 <li>Data Integration: Integrate data from multiple sources and systems, including databases, APIs, log files, streaming platforms, and external data providers. Handle data ingestion, transformation, and consolidation to create a unified and reliable data foundation for analysis and reporting.</li>
 <li>Data Transformation and Processing: Develop data transformation routines to clean, normalize, and aggregate data. Apply data processing techniques to handle complex data structures, handle missing or inconsistent data, and prepare the data for analysis, reporting, or machine learning tasks.</li>
 <li>Contribute to common frameworks and best practices in code development, deployment, and automation/orchestration of data pipelines.</li>
 <li>Implement data governance in line with company standards</li>
 <li>Partner with Data Analytics and Product leaders to design best practices and standards for developing and productionalizing analytic pipelines.</li>
 <li>Partner with Infrastructure leaders on architecture approaches to advance the data and analytics platform, including exploring new tools and techniques that leverage the cloud environment (Azure, Snowflake, others)</li>
 <li>Monitoring and Support: Monitor data pipelines and data systems to detect and resolve issues promptly. Develop monitoring tools, alerts, and automated error handling mechanisms to ensure data integrity and system reliability.</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Extensive experience designing data solutions including data modeling.</li>
 <li>Extensive hands-on experience developing data processing jobs (SQL) that demonstrates a strong understanding of software engineering principles.</li>
 <li>Experience orchestrating data pipelines using technology like ADF, Airflow etc</li>
 <li>Experience working with both real-time and batch data, knowing the strengths and weaknesses of both and when to apply one over another.</li>
 <li>Experience building data pipelines on either AWS, Azure or GCP, following best practices in Cloud deployments</li>
 <li>Experience working with Hive /HBase / Presto</li>
 <li>Fluent in SQL (any flavor), with experience using Window functions and more advanced features.</li>
 <li>Understanding of DevOps tools, Git workflow and building CI/CD pipelines</li>
 <li>Ability to work with business and technical audiences on business requirement meetings, technical white boarding exercises, and SQL coding/debugging sessions.</li>
 <li>Experience supporting big data pipelines.</li>
 <li>Experience applying data governance controls within a highly regulated environment.</li>
</ul>
<p><b>Preferred Qualifications:</b></p>
<ul>
 <li>Bachelor&#x2019;s Degree or higher in Database Management, Information Technology, Computer Science or similar</li>
 <li>Experience working in projects with agile/scrum methodologies.</li>
 <li>Familiar with Azure Data Factory or Apache Airflow</li>
 <li>Familiar with Azure Databricks or Snowflake</li>
 <li>Experience with shell scripting languages</li>
 <li>Well versed in Python, in fulfilling multiple general-purpose use-cases, and not limited to developing data APIs and pipelines.</li>
 <li>Experience with Apache Spark and related Big Data stack and technologies</li>
 <li>Experience working with Apache Kafka, building appropriate producer/consumer apps.</li>
 <li>Familiarity with production quality ML and/or AI model development and deployment.</li>
 <li>Experience working with Kubernetes and Docker, and knowledgeable about cloud infrastructure automation and management (e.g., Terraform)</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;65.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,7f70c2bb5373b7a9,,Contract,,,Remote,Senior Data Engineer,Today,2023-10-18T13:31:01.959Z,,,$65 an hour,2023-10-18T13:31:01.960Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=7f70c2bb5373b7a9&from=jasx&tk=1hd1fm1o6iman800&vjs=3
5,VUI,"Role: Data Engineer
 Location: Remote

Contract-to-hire (C2H) Only
Job Description:
Looking for a data engineer to design, develop and maintain pipelines and workflows and create analytics to digitally transform the current NA Accident and Health reporting and business decision processes.
Responsibilities:
Development and support of data pipelines that produce data assets for various A&H workstreams including UW, UA, Actuarial, Claims
Handle data pipelines while testing for data curation, parsing, cleaning, transformation and enrichment of data
Work with fundamentals of data processing, data pipeline, data lineage and ETL (Extract-Transform-Load) methodologies
Implement the project according to the Software Development Life Cycle (SDLC) and programming by using fast paced agile methodology, involving task completion, user stories
Utilize knowledge of database management system software, object oriented programming development, system architecture and components and various programming languages
Review and analyze business workflows and user data needs
Design and implement business performance dashboards
Write customized queries/programs to generate automatic periodical reports highlighting all the Key Performance Indicators (KPIs)
Build applications using SQL and/or Python scripts to manipulate data, monitor and help to improve data quality
Design, build and maintain end-to-end data solutions supporting our processes with the right data architecture
Have working knowledge of Apache Spark, big data processing and building products on distributed cluster-computing framework
Construct workflow charts and diagrams and writing specifications.
Documentation of end-to-end data pipeline process.
Documentation of data assets for information management purposes.
Ad hoc team / business support as needed.
Exploration and evaluation of new technologies and platforms.
Requirements:
Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field
5+ years as a data engineer
Sound Python and SQL skills with ability to query and analyze data, understand complexity and data structuresExperience with data and analytics technology, including but not limited to Hadoop, Spark, Java, Python, R, ElasticSearch, and others
Experience with Palantir Foundry.
Familiarity with relational database concepts
Detail-oriented, analytical, and inquisitiveGood communication skills
Highly organized with strong time-management skills
Ability to work independently and collaborate well with others
Ability to affect smooth organizational transformations
Job Type: Contract
Salary: $40.00 - $60.00 per hour
Experience level:

 10 years
 11+ years
 6 years
 7 years
 8 years
 9 years

Schedule:

 Monday to Friday

Application Question(s):

 What is your work authorization?

Experience:

 Palantir Foundry: 2 years (Required)
 Data Engineer: 5 years (Required)
 Data analytics: 2 years (Required)

Work Location: Remote","<ul>
 <li><b>Role: Data Engineer</b></li>
 <li><b>Location: Remote</b></li>
</ul>
<p><b>Contract-to-hire (C2H) Only</b></p>
<p><b>Job Description:</b></p>
<p>Looking for a data engineer to design, develop and maintain pipelines and workflows and create analytics to digitally transform the current NA Accident and Health reporting and business decision processes.</p>
<p><b>Responsibilities:</b></p>
<p>Development and support of data pipelines that produce data assets for various A&amp;H workstreams including UW, UA, Actuarial, Claims</p>
<p>Handle data pipelines while testing for data curation, parsing, cleaning, transformation and enrichment of data</p>
<p>Work with fundamentals of data processing, data pipeline, data lineage and ETL (Extract-Transform-Load) methodologies</p>
<p>Implement the project according to the Software Development Life Cycle (SDLC) and programming by using fast paced agile methodology, involving task completion, user stories</p>
<p>Utilize knowledge of database management system software, object oriented programming development, system architecture and components and various programming languages</p>
<p>Review and analyze business workflows and user data needs</p>
<p>Design and implement business performance dashboards</p>
<p>Write customized queries/programs to generate automatic periodical reports highlighting all the Key Performance Indicators (KPIs)</p>
<p>Build applications using SQL and/or Python scripts to manipulate data, monitor and help to improve data quality</p>
<p>Design, build and maintain end-to-end data solutions supporting our processes with the right data architecture</p>
<p>Have working knowledge of Apache Spark, big data processing and building products on distributed cluster-computing framework</p>
<p>Construct workflow charts and diagrams and writing specifications.</p>
<p>Documentation of end-to-end data pipeline process.</p>
<p>Documentation of data assets for information management purposes.</p>
<p>Ad hoc team / business support as needed.</p>
<p>Exploration and evaluation of new technologies and platforms.</p>
<p><b>Requirements:</b></p>
<p>Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field</p>
<p>5+ years as a data engineer</p>
<p>Sound <b>Python and</b> SQL skills with ability to query and analyze data, understand complexity and data structures<br>Experience with <b>data and analytics technology</b>, including but not limited to <b>Hadoop, Spark, Java, Python, R, ElasticSearch</b>, and others</p>
<p>Experience with <b>Palantir Foundry</b>.</p>
<p>Familiarity with <b>relational database</b> concepts</p>
<p>Detail-oriented, analytical, and inquisitive<br>Good communication skills</p>
<p>Highly organized with strong time-management skills</p>
<p>Ability to work independently and collaborate well with others</p>
<p>Ability to affect smooth organizational transformations</p>
<p>Job Type: Contract</p>
<p>Salary: &#x24;40.00 - &#x24;60.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>What is your work authorization?</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Palantir Foundry: 2 years (Required)</li>
 <li>Data Engineer: 5 years (Required)</li>
 <li>Data analytics: 2 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,5bcbbacaaee9d357,,Contract,,,Remote,Data Engineer - Palantir Foundry,Today,2023-10-18T13:31:02.290Z,,,$40 - $60 an hour,2023-10-18T13:31:02.291Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=5bcbbacaaee9d357&from=jasx&tk=1hd1fm1o6iman800&vjs=3
6,Stratagen,"Stratagen is currently seeking a highly skilled Data Engineer 2 to join our team. In this role, you will play a key part in implementing essential changes to the legacy National Crime Information Center (NCIC) system in the FBI’s Criminal Justice Information Services (CJIS) Division . These changes are critical for enabling the agile development of the NCIC 3rd Generation (N3G) system. As a Data Engineer, you will have diverse responsibilities encompassing data profiling, data design, data management, and the generation of test data.
Required:

 B.A. or B.S. from an accredited institution
 A minimum of twelve (12) years of data management, database administration or equivalent experience
 A minimum of eight (8) years of data engineering, data analysis, or equivalent experience
 Active Top Secret Clearance

Preferred; however, not required:

 Prior work experience with the Federal Government
 Prior work experience with CJIS systems and associated persistent and/or non- persistent data
 Prior work experience with either Amazon web Services (AwS) or Azure
 Familiarity with Agile development
 Prior work experience with the Scaled Agile Framework (SAFe)
 Experience building data visualizations and reports
 Experience working with IT and business stakeholders in designing the data architecture for a system
 Experience synthesizing and analyzing extremely large data sets
 Familiarity with optimization models

Job Type: Full-time
Pay: $100,000.00 - $140,000.00 per year
Benefits:

 Dental insurance
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Retirement plan
 Vision insurance

Experience level:

 11+ years

Schedule:

 Monday to Friday

Security clearance:

 Top Secret (Required)

Work Location: Remote","<p>Stratagen is currently seeking a highly skilled Data Engineer 2 to join our team. In this role, you will play a key part in implementing essential changes to the legacy National Crime Information Center (NCIC) system in the FBI&#x2019;s Criminal Justice Information Services (CJIS) Division . These changes are critical for enabling the agile development of the NCIC 3rd Generation (N3G) system. As a Data Engineer, you will have diverse responsibilities encompassing data profiling, data design, data management, and the generation of test data.</p>
<p>Required:</p>
<ul>
 <li>B.A. or B.S. from an accredited institution</li>
 <li>A minimum of twelve (12) years of data management, database administration or equivalent experience</li>
 <li>A minimum of eight (8) years of data engineering, data analysis, or equivalent experience</li>
 <li><i>Active Top Secret Clearance</i></li>
</ul>
<p>Preferred; however, not required:</p>
<ul>
 <li>Prior work experience with the Federal Government</li>
 <li>Prior work experience with CJIS systems and associated persistent and/or non- persistent data</li>
 <li>Prior work experience with either Amazon web Services (AwS) or Azure</li>
 <li>Familiarity with Agile development</li>
 <li>Prior work experience with the Scaled Agile Framework (SAFe)</li>
 <li>Experience building data visualizations and reports</li>
 <li>Experience working with IT and business stakeholders in designing the data architecture for a system</li>
 <li>Experience synthesizing and analyzing extremely large data sets</li>
 <li>Familiarity with optimization models</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;100,000.00 - &#x24;140,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>11+ years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Security clearance:</p>
<ul>
 <li>Top Secret (Required)</li>
</ul>
<p>Work Location: Remote</p>",,e65490aaaff7f716,,Full-time,,,Remote,Data Engineer 2,1 day ago,2023-10-17T13:31:10.576Z,,,"$100,000 - $140,000 a year",2023-10-18T13:31:10.649Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e65490aaaff7f716&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
7,FACEBOOK APP,"AWS data engineers have a wide range of responsibilities, which can include:
Creating data models that can be used to extract information from various sources and store it in a usable formatMaintaining the integrity of data by designing backup and recovery proceduresIdentifying opportunities to improve performance by improving database structure or indexing methodsConducting research to identify new technologies that can be applied to current projectsAnalyzing data to find patterns or insights that can be used to develop strategies or make business decisionsDeveloping new applications using existing data sets to create new products or improve existing servicesMaintaining existing applications by updating existing code or adding new features to meet new requirementsDesigning and implementing security measures to protect data from unauthorized access or misuseRecommending infrastructure changes to improve storage capacity or performance
Job Types: Contract, Full-time
Salary: $40.34 - $86.70 per hour
Experience level:

 10 years
 11+ years
 7 years
 8 years
 9 years

Experience:

 AWS Data Engineer: 10 years (Preferred)

Work Location: Remote","<p>AWS data engineers have a wide range of responsibilities, which can include:</p>
<p>Creating data models that can be used to extract information from various sources and store it in a usable format<br>Maintaining the integrity of data by designing backup and recovery procedures<br>Identifying opportunities to improve performance by improving database structure or indexing methods<br>Conducting research to identify new technologies that can be applied to current projects<br>Analyzing data to find patterns or insights that can be used to develop strategies or make business decisions<br>Developing new applications using existing data sets to create new products or improve existing services<br>Maintaining existing applications by updating existing code or adding new features to meet new requirements<br>Designing and implementing security measures to protect data from unauthorized access or misuse<br>Recommending infrastructure changes to improve storage capacity or performance</p>
<p>Job Types: Contract, Full-time</p>
<p>Salary: &#x24;40.34 - &#x24;86.70 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Experience:</p>
<ul>
 <li>AWS Data Engineer: 10 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,77f1038f4ded6818,,Full-time,Contract,,Remote,senior AWS Data Engineer,1 day ago,2023-10-17T13:31:11.925Z,,,$40.34 - $86.70 an hour,2023-10-18T13:31:11.928Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=77f1038f4ded6818&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
8,Dama Technology Inc,"About Dama Financial
   At Dama Financial, we use technology to solve problems that critically impact the growth and reputation of the cannabis industry. We offer innovative, compliant, sustainable financial and traceability products, removing the barriers that exclude cannabis businesses from accessing the fundamental solutions required to support a rapidly growing industry. We have a diverse team of professionals with deep expertise in financial services, payments technology, cannabis regulations, and successfully building and growing companies. Throughout the organization, you’ll find people who solve problems, deliver solutions, and deal with uncertainties while building best in class products for the industry.
 
  
  
 
   The Role
   The Data & Analytics team within Dama is growing to help clients scale their retail operations with a world-class point-of-sale system and flexible payment options. The team is also responsible for helping internal teams become more data-driven by helping them to understand, utilize and extract value from the data generated by our systems and available in our warehouse.
 
  
  
  Although we’re a growth-stage company, our environment is typical of a start-up 
 
   We work in small, high-performing teams, are fast-paced, and we all get a lot done by everyone wearing many hats.
  
 
   We are serious about optimizing our time and staying focused on the most important goals and outcomes.
  
 
   We are a 100% remote team meaning we focus on communication to ensure we can stay in sync despite our physical distance.
 
  
  
 
   What you'll do
  
 
   Report to Director of Data & Analytics
  
 
   Build and maintain pipelines to move data from source systems into our cloud data warehouse
  
 
   Transform and model data using SQL & Python
  
 
   Monitor data pipelines for errors/data quality issues and work with Dev Ops Engineering to improve observability and alerting
  
 
   Improve data quality, reliability, efficiency, and performance while optimizing cost
  
 
   Document data models, schemas, business logic, pipelines, and other metadata
  
 
   Collaborate with engineers and product managers to understand data requirements
 
  
  
 
   What we’re looking for
  
 
   You’re a great creative problem solver with an analytical mind who loves to dig in and solve hard problems
  
 
   You see data flowing and you can’t stop yourself from classifying, categorizing, organizing and directing those flows to create efficient/performant, useful and usable datasets for both operations and decision support
  
 
   You’re not just a “data person”, you’re an Engineer who specializes in data and metadata.
  
 
   You love learning new things and have a passion for building, monitoring and improving a well-oiled “data machine”
  
 
   You have the ability to work both independently and collaboratively as part of a remote team
 
  
  
  Required Experience 
 
   1-3 years of experience in a Data Engineer role working with the “Modern Data Stack”
  
 
   SQL. You know SQL. SQL is a friend of yours. You two probably share a secret handshake
  
 
   Ideal candidate will have experience with both BigQuery & dbt (including Python)
 
  
  
  Nice-to-have Experience 
 
   Integration tools (Airbyte, Stitch, Fivetran)
  
 
   Orchestration tools (Dagster, Airflow, dbt Cloud)
  
 
   Metadata tools (DataHub, OpenMetadata)
  
 
   BI/Dashboard tools (Looker, Superset, PowerBI)
 
  
  
 
   Benefits
  
 
   Healthcare
  
 
   401K
  
 
   Generous PTO
  
 
   Collaborative Environment
 
  
  
 
   What we offer
  
 
   A low ego environment where you can give and receive direct feedback.
  
 
   Managers who care about your career development.
 
  
  
 
   Due to the nature of financial systems, you will be required to pass a background check.
 
  
  
 
   Send resumes to 
  jobs@damafinancial.com
 
  
  
  CHR: Jr./Mid-Level Data Engineer
  
  
 
   LI: Jr./Mid Level Data Engineer
 
  
  
  Salary commensurate upon experience
  
  
  Bonus goals based on company goals","<div>
 <div>
  <b>About Dama Financial</b>
  <br> At Dama Financial, we use technology to solve problems that critically impact the growth and reputation of the cannabis industry. We offer innovative, compliant, sustainable financial and traceability products, removing the barriers that exclude cannabis businesses from accessing the fundamental solutions required to support a rapidly growing industry. We have a diverse team of professionals with deep expertise in financial services, payments technology, cannabis regulations, and successfully building and growing companies. Throughout the organization, you&#x2019;ll find people who solve problems, deliver solutions, and deal with uncertainties while building best in class products for the industry.
 </div>
 <br> 
 <div></div> 
 <div>
  <b> The Role</b>
  <br> The Data &amp; Analytics team within Dama is growing to help clients scale their retail operations with a world-class point-of-sale system and flexible payment options. The team is also responsible for helping internal teams become more data-driven by helping them to understand, utilize and extract value from the data generated by our systems and available in our warehouse.
 </div>
 <br> 
 <div></div> 
 <p> Although we&#x2019;re a growth-stage company, our environment is typical of a start-up</p> 
 <ul>
  <li> We work in small, high-performing teams, are fast-paced, and we all get a lot done by everyone wearing many hats.</li>
 </ul> 
 <ul>
  <li> We are serious about optimizing our time and staying focused on the most important goals and outcomes.</li>
 </ul> 
 <ul>
  <li> We are a 100% remote team meaning we focus on communication to ensure we can stay in sync despite our physical distance.</li>
 </ul>
 <br> 
 <p></p> 
 <div>
  <b> What you&apos;ll do</b>
 </div> 
 <ul>
  <li> Report to Director of Data &amp; Analytics</li>
 </ul> 
 <ul>
  <li> Build and maintain pipelines to move data from source systems into our cloud data warehouse</li>
 </ul> 
 <ul>
  <li> Transform and model data using SQL &amp; Python</li>
 </ul> 
 <ul>
  <li> Monitor data pipelines for errors/data quality issues and work with Dev Ops Engineering to improve observability and alerting</li>
 </ul> 
 <ul>
  <li> Improve data quality, reliability, efficiency, and performance while optimizing cost</li>
 </ul> 
 <ul>
  <li> Document data models, schemas, business logic, pipelines, and other metadata</li>
 </ul> 
 <ul>
  <li> Collaborate with engineers and product managers to understand data requirements</li>
 </ul>
 <br> 
 <p></p> 
 <div>
  <b> What we&#x2019;re looking for</b>
 </div> 
 <ul>
  <li> You&#x2019;re a great creative problem solver with an analytical mind who loves to dig in and solve hard problems</li>
 </ul> 
 <ul>
  <li> You see data flowing and you can&#x2019;t stop yourself from classifying, categorizing, organizing and directing those flows to create efficient/performant, useful and usable datasets for both operations and decision support</li>
 </ul> 
 <ul>
  <li> You&#x2019;re not just a &#x201c;data person&#x201d;, you&#x2019;re an Engineer who specializes in data and metadata.</li>
 </ul> 
 <ul>
  <li> You love learning new things and have a passion for building, monitoring and improving a well-oiled &#x201c;data machine&#x201d;</li>
 </ul> 
 <ul>
  <li> You have the ability to work both independently and collaboratively as part of a remote team</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> Required Experience</b></p> 
 <ul>
  <li> 1-3 years of experience in a Data Engineer role working with the &#x201c;Modern Data Stack&#x201d;</li>
 </ul> 
 <ul>
  <li> SQL. You know SQL. SQL is a friend of yours. You two probably share a secret handshake</li>
 </ul> 
 <ul>
  <li> Ideal candidate will have experience with both BigQuery &amp; dbt (including Python)</li>
 </ul>
 <br> 
 <div></div> 
 <p><b> Nice-to-have Experience</b></p> 
 <ul>
  <li> Integration tools (Airbyte, Stitch, Fivetran)</li>
 </ul> 
 <ul>
  <li> Orchestration tools (Dagster, Airflow, dbt Cloud)</li>
 </ul> 
 <ul>
  <li> Metadata tools (DataHub, OpenMetadata)</li>
 </ul> 
 <ul>
  <li> BI/Dashboard tools (Looker, Superset, PowerBI)</li>
 </ul>
 <br> 
 <p></p> 
 <div>
  <b> Benefits</b>
 </div> 
 <ul>
  <li> Healthcare</li>
 </ul> 
 <ul>
  <li> 401K</li>
 </ul> 
 <ul>
  <li> Generous PTO</li>
 </ul> 
 <ul>
  <li> Collaborative Environment</li>
 </ul>
 <br> 
 <div></div> 
 <div>
  <b> What we offer</b>
 </div> 
 <ul>
  <li> A low ego environment where you can give and receive direct feedback.</li>
 </ul> 
 <ul>
  <li> Managers who care about your career development.</li>
 </ul>
 <br> 
 <div></div> 
 <div>
  <b> Due to the nature of financial systems, you will be required to pass a background check.</b>
 </div>
 <br> 
 <div></div> 
 <div>
  <b> Send resumes to </b>
  <b>jobs@damafinancial.com</b>
 </div>
 <br> 
 <div></div> 
 <p> <b>CHR: Jr./Mid-Level Data Engineer</b></p>
 <br> 
 <div></div> 
 <div>
  <b> LI: Jr./Mid Level Data Engineer</b>
 </div>
 <br> 
 <p></p> 
 <p> Salary commensurate upon experience</p>
 <br> 
 <p></p> 
 <p> Bonus goals based on company goals</p>
</div>",https://secure.entertimeonline.com/ta/CBIZ20462.careers?ShowJob=352679041,28162dd96d9b7a0d,,Full-time,,,"South San Francisco, CA 94083",Jr./Mid-Level Data Engineer,Today,2023-10-18T13:31:06.374Z,,,"$70,000 - $125,000 a year",2023-10-18T13:31:06.380Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=28162dd96d9b7a0d&from=jasx&tk=1hd1fm1o6iman800&vjs=3
9,Cognizant Technology Solutions,"We are Cognizant Artificial Intelligence 
  Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them. 
  With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks
  
  
  Role and Responsibilities: 
  
  5+ years of industry experience in software development data engineering or a related field with a solid track record of building services for manipulating processing datasets 
  Hands-on experience and advanced knowledge of AWS DataOps (i.e. IAM Lambda Step Functions EMR/Glue and DynamoDB) 
  Hands-on experience and advanced knowledge of SQL/Non-relational Data Modeling 
  Experience working with data streaming technologies (Kafka Spark Streaming etc.) 
 
 
  Designing and implementing complex ingestion and processing pipelines through orchestration 
  Design and implement API interfaces for engineering teams to interact with ingestion/processing pipelines 
  Design implement and support scalable multi-tenant service and data infrastructure solutions to integrate with multi heterogeneous data sources aggregate and retrieve data in a fast and secure mode curate data that can be used in reporting analysis machine learning models and ad-hoc data requests 
  Interface with other engineering and ML teams to extract transform and load data from a wide variety of data sources 
  Work with business product owners to understand gather and analyze their processing and extraction needs to solve problems
 
  
  
  Salary and Other Compensation 
  The annual salary for this position is between USD ($110kp/a – $120kp/a) depending on experience and other qualifications of the successful candidate. 
  This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans. 
  Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements: 
  
  Medical/Dental/Vision/Life Insurance 
  Paid holidays plus Paid Time Off 
  401(k) plan and contributions 
  Long-term/Short-term Disability 
  Paid Parental Leave 
  Employee Stock Purchase Plan 
  
 Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
  
  
  #LI-JL1 
  #CB 
  #IND123
 
  Employee Status : Full Time Employee
  Shift : Day Job
  Travel : No
  Job Posting : Oct 17 2023
 
 
   About Cognizant
  Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 
  Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
 
  Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
  If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.","<div>
 <p><b>We are Cognizant Artificial Intelligence</b></p> 
 <p> Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them.</p> 
 <p> With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks</p>
 <br> 
 <p></p> 
 <p><b> Role and Responsibilities:</b></p> 
 <ul> 
  <li>5+ years of industry experience in software development data engineering or a related field with a solid track record of building services for manipulating processing datasets</li> 
  <li>Hands-on experience and advanced knowledge of AWS DataOps (i.e. IAM Lambda Step Functions EMR/Glue and DynamoDB)</li> 
  <li>Hands-on experience and advanced knowledge of SQL/Non-relational Data Modeling</li> 
  <li>Experience working with data streaming technologies (Kafka Spark Streaming etc.)</li> 
 </ul>
 <ul>
  <li>Designing and implementing complex ingestion and processing pipelines through orchestration</li> 
  <li>Design and implement API interfaces for engineering teams to interact with ingestion/processing pipelines</li> 
  <li>Design implement and support scalable multi-tenant service and data infrastructure solutions to integrate with multi heterogeneous data sources aggregate and retrieve data in a fast and secure mode curate data that can be used in reporting analysis machine learning models and ad-hoc data requests</li> 
  <li>Interface with other engineering and ML teams to extract transform and load data from a wide variety of data sources</li> 
  <li>Work with business product owners to understand gather and analyze their processing and extraction needs to solve problems</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> Salary and Other Compensation</b></p> 
 <p> The annual salary for this position is between USD (&#x24;110kp/a &#x2013; &#x24;120kp/a) depending on experience and other qualifications of the successful candidate.</p> 
 <p> This position is also eligible for Cognizant&#x2019;s discretionary annual incentive program, based on performance and subject to the terms of Cognizant&#x2019;s applicable plans.</p> 
 <p> Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:</p> 
 <ul> 
  <li>Medical/Dental/Vision/Life Insurance</li> 
  <li>Paid holidays plus Paid Time Off</li> 
  <li>401(k) plan and contributions</li> 
  <li>Long-term/Short-term Disability</li> 
  <li>Paid Parental Leave</li> 
  <li>Employee Stock Purchase Plan</li> 
 </ul> 
 <p>Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.</p>
 <br> 
 <p></p> 
 <p> #LI-JL1</p> 
 <p> #CB</p> 
 <p> #IND123</p>
 <p></p>
 <p><b><br> Employee Status : </b>Full Time Employee</p>
 <p><b> Shift : </b>Day Job</p>
 <p><b> Travel : </b>No</p>
 <p><b> Job Posting : </b>Oct 17 2023</p>
 <p></p>
 <div>
  <b> About Cognizant</b>
 </div> Cognizant (Nasdaq-100: CTSH) is one of the world&apos;s leading professional services companies, transforming clients&apos; business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 <p></p>
 <p> Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.</p>
 <p></p>
 <p> Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.</p>
 <p> If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.</p>
</div>
<p></p>",https://click.appcast.io/track/hra6v3l-org?cs=hqw&jg=6mpd&ittk=LCLTYKNZ66,8c2d8bd464c9e1f8,,Full-time,,,"Chicago, IL 60290",Sr. AWS Data Engineer (Remote),Today,2023-10-18T13:31:09.464Z,3.9,15967.0,"$110,000 - $120,000 a year",2023-10-18T13:31:09.467Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=8c2d8bd464c9e1f8&from=jasx&tk=1hd1fm1o6iman800&vjs=3
10,Circle,"Circle is a financial technology company at the epicenter of the emerging internet of money, where value can finally travel like other digital data — globally, nearly instantly and less expensively than legacy settlement systems. This ground-breaking new internet layer opens up previously unimaginable possibilities for payments, commerce and markets that can help raise global economic prosperity and enhance inclusion. Our infrastructure – including USDC, a blockchain-based dollar – helps businesses, institutions and developers harness these breakthroughs and capitalize on this major turning point in the evolution of money and technology. 
   What you'll be part of: 
   Circle is committed to visibility and stability in everything we do. As we grow as an organization, we're expanding into some of the world's strongest jurisdictions. Speed and efficiency are motivators for our success and our employees live by our company values: Multistakeholder, Mindfulness, Driven by Excellence and High Integrity. Circlers are consistently evolving in a remote world where strength in numbers fuels team success. We have built a flexible and diverse work environment where new ideas are encouraged and everyone is a stakeholder.
 
  What you'll be responsible for: 
  As a member of the Data Engineering - Business ETL team, you own the ETL/ELT pipelines and data warehouse that is used for financial and regulatory reporting. Your work powers Circle business functions, including the Compliance, Finance, Accounting, and Analytics teams for experimentation, operational excellence, and actionable insights, so as to fuel and accelerate business growth. High integrity to data accuracy and quality is crucial. Your work will directly impact Circle's transparency, trust, and accountability needs. 
  What you'll work on: 
  
  Collaborating with business teams on the design, deployment and continuous improvement of the scalable data platform that ingests, stores, and aggregates various datasets, including data pipelines, platforms, and warehouses, and surfacing data to both internal and customer-facing applications. 
  Being a domain expert on data modeling, data pipelines, data quality and data warehousing. 
  Designing, building and maintaining data ETL/ELT pipelines to source and aggregate the required data for various data analyses and reporting needs, as well as to continually improve the operations, monitoring and performance of the data warehouse. 
  Developing integrations with third party systems to source, qualify and ingest various datasets. 
  Providing data analytics and visualization tools to extract valuable insights from the data to enable data-driven decisions. 
  Working closely with across groups, such as the product, engineering, data science, compliance, and security teams, for data modeling, general management of data life cycle, data governance and processes for meeting regulatory and legal requirements. 
  
 You will aspire to our four core values: 
  
  Multistakeholder - you have dedication and commitment to our customers, shareholders, employees and families and local communities. 
  Mindful - you seek to be respectful, an active listener and to pay attention to detail. 
  Driven by Excellence - you are driven by our mission and our passion for customer success which means you relentlessly pursue excellence, that you do not tolerate mediocrity and you work intensely to achieve your goals. 
  High Integrity - you seek open and honest communication, and you hold yourself to very high moral and ethical standards. You reject manipulation, dishonesty and intolerance. 
  
 What you'll bring to Circle: 
  For Senior Data Engineer (III) 
  
  4+ years of professional data engineering experience. 
  Proficient in one or more programming languages (Java, Scala, Python). 
  Advanced experience in SQL in big data warehouse systems such as Snowflake, BigQuery, Databricks, etc. 
  Experience in SQL and NoSQL, such as MySQL, PostgreSQL, Cassandra, HBase, Redis, DynamoDB, Neo4j, etc. 
  Experience with workflow orchestration management engines such as Airflow, Dagster, DBT, etc 
  Experience with Cloud Services (AWS, Google Cloud, Microsoft Azure, etc). 
  Experience in building scalable infrastructure to support batch, micro-batch or stream data processing for large volumes of data. 
  Experience with financial or compliance data, bonus if in similar business domains, such as payment systems, credit cards, bank transfers, blockchains, etc. 
  Experience in data provenance and governance. 
  Internal knowledge of open source or related big data technologies. 
  Ability to tackle complex and ambiguous problems. 
  Self-starter who takes ownership, gets results, and enjoys moving at a fast pace. 
  Excellent communication skills, able to collaborate with across remote teams, share ideas and present concepts effectively. 
  
 For Staff Data Engineer (IV) 
  All the requirements of above and: 
  
  7+ years of professional data engineering experience. 
  Led teams (>5) technically on architecture and system design. 
  Expert in one of the domains of ETL/ELT pipelines, feature engineering, data modeling and architecture, or data ingestion. 
  Experience working autonomously and able to identify large impactful projects to pursue with minimal guidance. 
  Deep understanding/experience with: 
  
   Data warehouse architecture and design 
   Integration of the data stack to other tools/services 
   Extensive knowledge with implementing data quality checks 
   
 
 Additional Information: 
  
  This position is eligible for day-one PERM sponsorship for qualified candidates. 
  
 Circle is on a mission to create an inclusive financial future, with transparency at our core. We consider a wide variety of elements when crafting our compensation ranges and total compensation packages. 
  Starting pay is determined by various factors, including but not limited to: relevant experience, skill set, qualifications, and other business and organizational needs. Please note that compensation ranges may differ for candidates in other locations. 
  Senior Data Engineer (III) 
  Base Pay Range: $147,500 - $195,000 
  Annual Bonus Target: 12.5% 
  Staff Data Engineer (IV) 
  Base Pay Range: $172,500 - $227,500 
  Annual Bonus Target: 15% 
  Also Included: Equity & Benefits (including medical, dental, vision and 401(k)). Circle has a discretionary vacation policy. We also provide 10 days of paid sick leave per year and 11 paid holidays per year in the U.S.
 
   We are an equal opportunity employer and value diversity at Circle. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Additionally, Circle participates in the E-Verify Program in certain locations, as required by law. 
   #LI-Remote","<div>
 <div>
  <p>Circle is a financial technology company at the epicenter of the emerging internet of money, where value can finally travel like other digital data &#x2014; globally, nearly instantly and less expensively than legacy settlement systems. This ground-breaking new internet layer opens up previously unimaginable possibilities for payments, commerce and markets that can help raise global economic prosperity and enhance inclusion. Our infrastructure &#x2013; including USDC, a blockchain-based dollar &#x2013; helps businesses, institutions and developers harness these breakthroughs and capitalize on this major turning point in the evolution of money and technology.</p> 
  <p><b> What you&apos;ll be part of:</b></p> 
  <p> Circle is committed to visibility and stability in everything we do. As we grow as an organization, we&apos;re expanding into some of the world&apos;s strongest jurisdictions. Speed and efficiency are motivators for our success and our employees live by our company values: Multistakeholder, Mindfulness, Driven by Excellence and High Integrity. Circlers are consistently evolving in a remote world where strength in numbers fuels team success. We have built a flexible and diverse work environment where new ideas are encouraged and everyone is a stakeholder.</p>
 </div>
 <p><b> What you&apos;ll be responsible for:</b></p> 
 <p> As a member of the Data Engineering - Business ETL team, you own the ETL/ELT pipelines and data warehouse that is used for financial and regulatory reporting. Your work powers Circle business functions, including the Compliance, Finance, Accounting, and Analytics teams for experimentation, operational excellence, and actionable insights, so as to fuel and accelerate business growth. High integrity to data accuracy and quality is crucial. Your work will directly impact Circle&apos;s transparency, trust, and accountability needs.</p> 
 <p><b> What you&apos;ll work on:</b></p> 
 <ul> 
  <li>Collaborating with business teams on the design, deployment and continuous improvement of the scalable data platform that ingests, stores, and aggregates various datasets, including data pipelines, platforms, and warehouses, and surfacing data to both internal and customer-facing applications.</li> 
  <li>Being a domain expert on data modeling, data pipelines, data quality and data warehousing.</li> 
  <li>Designing, building and maintaining data ETL/ELT pipelines to source and aggregate the required data for various data analyses and reporting needs, as well as to continually improve the operations, monitoring and performance of the data warehouse.</li> 
  <li>Developing integrations with third party systems to source, qualify and ingest various datasets.</li> 
  <li>Providing data analytics and visualization tools to extract valuable insights from the data to enable data-driven decisions.</li> 
  <li>Working closely with across groups, such as the product, engineering, data science, compliance, and security teams, for data modeling, general management of data life cycle, data governance and processes for meeting regulatory and legal requirements.</li> 
 </ul> 
 <p><b>You will aspire to our four core values:</b></p> 
 <ul> 
  <li>Multistakeholder - you have dedication and commitment to our customers, shareholders, employees and families and local communities.</li> 
  <li>Mindful - you seek to be respectful, an active listener and to pay attention to detail.</li> 
  <li>Driven by Excellence - you are driven by our mission and our passion for customer success which means you relentlessly pursue excellence, that you do not tolerate mediocrity and you work intensely to achieve your goals.</li> 
  <li>High Integrity - you seek open and honest communication, and you hold yourself to very high moral and ethical standards. You reject manipulation, dishonesty and intolerance.</li> 
 </ul> 
 <p><b>What you&apos;ll bring to Circle:</b></p> 
 <p> For Senior Data Engineer (III)</p> 
 <ul> 
  <li>4+ years of professional data engineering experience.</li> 
  <li>Proficient in one or more programming languages (Java, Scala, Python).</li> 
  <li>Advanced experience in SQL in big data warehouse systems such as Snowflake, BigQuery, Databricks, etc.</li> 
  <li>Experience in SQL and NoSQL, such as MySQL, PostgreSQL, Cassandra, HBase, Redis, DynamoDB, Neo4j, etc.</li> 
  <li>Experience with workflow orchestration management engines such as Airflow, Dagster, DBT, etc</li> 
  <li>Experience with Cloud Services (AWS, Google Cloud, Microsoft Azure, etc).</li> 
  <li>Experience in building scalable infrastructure to support batch, micro-batch or stream data processing for large volumes of data.</li> 
  <li>Experience with financial or compliance data, bonus if in similar business domains, such as payment systems, credit cards, bank transfers, blockchains, etc.</li> 
  <li>Experience in data provenance and governance.</li> 
  <li>Internal knowledge of open source or related big data technologies.</li> 
  <li>Ability to tackle complex and ambiguous problems.</li> 
  <li>Self-starter who takes ownership, gets results, and enjoys moving at a fast pace.</li> 
  <li>Excellent communication skills, able to collaborate with across remote teams, share ideas and present concepts effectively.</li> 
 </ul> 
 <p>For Staff Data Engineer (IV)</p> 
 <p> All the requirements of above and:</p> 
 <ul> 
  <li>7+ years of professional data engineering experience.</li> 
  <li>Led teams (&gt;5) technically on architecture and system design.</li> 
  <li>Expert in one of the domains of ETL/ELT pipelines, feature engineering, data modeling and architecture, or data ingestion.</li> 
  <li>Experience working autonomously and able to identify large impactful projects to pursue with minimal guidance.</li> 
  <li>Deep understanding/experience with:</li> 
  <ul>
   <li>Data warehouse architecture and design</li> 
   <li>Integration of the data stack to other tools/services</li> 
   <li>Extensive knowledge with implementing data quality checks</li> 
  </ul> 
 </ul>
 <p><b>Additional Information:</b></p> 
 <ul> 
  <li>This position is eligible for day-one PERM sponsorship for qualified candidates.</li> 
 </ul> 
 <p><i>Circle is on a mission to create an inclusive financial future, with transparency at our core. We consider a wide variety of elements when crafting our compensation ranges and total compensation packages.</i></p> 
 <p><i> Starting pay is determined by various factors, including but not limited to: relevant experience, skill set, qualifications, and other business and organizational needs. Please note that compensation ranges may differ for candidates in other locations.</i></p> 
 <p><b><i> Senior Data Engineer (III)</i></b></p> 
 <p><i> Base Pay Range: &#x24;147,500 - &#x24;195,000</i></p> 
 <p><i> Annual Bonus Target: 12.5%</i></p> 
 <p><b><i> Staff Data Engineer (IV)</i></b></p> 
 <p><i> Base Pay Range: &#x24;172,500 - &#x24;227,500</i></p> 
 <p><i> Annual Bonus Target: 15%</i></p> 
 <p><i> Also Included: Equity &amp; Benefits (including medical, dental, vision and 401(k)). Circle has a discretionary vacation policy. We also provide 10 days of paid sick leave per year and 11 paid holidays per year in the U.S.</i></p>
 <div>
  <p> We are an <b>equal opportunity employer</b> and value diversity at Circle. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Additionally, Circle participates in the <b>E-Verify Program</b> in certain locations, as required by law.</p> 
  <h6 class=""jobSectionHeader""> #LI-Remote</h6>
 </div>
</div>",https://boards.greenhouse.io/circle/jobs/6976295002?gh_src=cd1087ea2us,c41c14c5c2c43e9e,,,,,"San Francisco, CA",Senior Data Engineer,Today,2023-10-18T13:31:26.049Z,,,"$147,500 - $195,000 a year",2023-10-18T13:31:26.050Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=c41c14c5c2c43e9e&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
11,INADEV,"******CANDIDATES MUST ANSWER ALL SCREENING QUESTIONS FOR APPLICATIONS TO BE CONSIDERED******
Formed in 2011, INADEV is focused on its founding principle to build innovative customer-centric solutions incredibly fast, secure, and at scale. We deliver world-class digital experiences to some of the largest federal agencies and commercial companies. Our technical expertise and innovations are comprised of codeless automation, identity intelligence, immersive technology, artificial intelligence/machine learning (AI/ML), virtualization, and digital transformation.
POSITION DESCRIPTION:

 Prepare data migration strategy, Integration plans, DB Performance, business sign-off plan, Live Confidence Plan (LCT plan - all data is migrated successfully).
 Provide support for data migration, data engineering, and integration of existing systems.
 Developing and integrating multiple data types across a range of data sets and sources.
 Performing day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner.
 Evaluate current system designs and identify areas for improvement to create a system that is highly available and has low data latency.
 Plan and design the integration of various source systems and the migration of data between systems.
 Build and implement the source system integration and data migration plan.
 Developing, managing, manipulating, storing and parsing data across a data pipeline for variety of target sources and data consumers
 Writing code to ensure the performance and reliability of data extraction and processing
 Supporting continuous process automation for data ingestion
 Assisting with the maintenance of applications and tools that reside on the data driven systems (upgrades, patches, configuration changes, etc.)
 Working with program management and engineers to implement and document complex and evolving requirements
 Actively and collaboratively participating as a member of a cross-functional Agile/Scrum team while following all Agile/Scrum best practices
 Advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
 Helping cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
 Demonstrating significant technical competence and ownership to broad audiences while driving progress on company strategic objectives at multiple levels.
 Generating and articulating technical strategy to diverse audiences, both technical and non-technical.

NON-TECHNICAL REQUIREMENTS:

 Ability to pass a 7 year background check and have the ability to obtain a U.S. Government clearance.
 Must have been a resident of the continental U.S. for at least the last 3 years.
 Must be willing to work in Eastern Standard Business hours.
 Must possess good communication (written/verbal) skills.

MANDATORY REQUIREMENTS:

 Must have a Bachelor's Degree in a technical discipline and 10+ years pertinent experience with the design, management, and solutioning of large, complex data sets and models.
 Must have at least 2+ years of experience working with/in public cloud environments.
 Must have proven experience leading data migration project from on-prem to cloud.
 Must have extensive experience/knowledge of data integration.
 Must have experience with Cloud Native databases
 Experience with Enterprise Architecture & Distributed Systems

DESIRED SKILLS:

 Experience with AWS DMS/MGN
 Technical knowledge of Datalake/DeltaLake/Lakehouse

PHYSICAL DEMANDS:

 Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions

INADEV Corporation does not discriminate against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibits discrimination against all individuals based on their race, color, religion, sex, sexual orientation/gender identity, or national origin.
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:

 401(k) matching
 Dental insurance
 Flexible spending account
 Health insurance
 Referral program
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 10 years
 7 years
 8 years
 9 years

Schedule:

 8 hour shift
 Monday to Friday

Application Question(s):

 This position requires the ability to obtain/maintain a U.S. Government Clearance and pass a 7 year employment background check. Do you meet these mandatory requirements?
 Are you open to working on a Permanent/W2 basis?
 This position requires that all candidates being considered MUST have resided in the Continental United States for at least the last 3 years and be able/willing to work on East Standard Business hours. Do you meet these mandatory requirements?
 Do you have a Bachelor's Degree in a Technical Discipline (i.e. Computer Science, Computer Engineering, Information Technology, Information Systems)?
 Do you have at least 7 years of professional experience as a Data Engineer?
 Do have professional experience leading data migration projects from on-prem to cloud?
 Do you have at least 7 years of professional experience in Data Manipulation working specifically with Python?
 Do you have at least 7 years of professional ETL experience?

Work Location: Remote","<p><b>******CANDIDATES MUST ANSWER ALL SCREENING QUESTIONS FOR APPLICATIONS TO BE CONSIDERED******</b></p>
<p>Formed in 2011, INADEV is focused on its founding principle to build innovative customer-centric solutions incredibly fast, secure, and at scale. We deliver world-class digital experiences to some of the largest federal agencies and commercial companies. Our technical expertise and innovations are comprised of codeless automation, identity intelligence, immersive technology, artificial intelligence/machine learning (AI/ML), virtualization, and digital transformation.</p>
<p><b>POSITION DESCRIPTION:</b></p>
<ul>
 <li>Prepare data migration strategy, Integration plans, DB Performance, business sign-off plan, Live Confidence Plan (LCT plan - all data is migrated successfully).</li>
 <li>Provide support for data migration, data engineering, and integration of existing systems.</li>
 <li>Developing and integrating multiple data types across a range of data sets and sources.</li>
 <li>Performing day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner.</li>
 <li>Evaluate current system designs and identify areas for improvement to create a system that is highly available and has low data latency.</li>
 <li>Plan and design the integration of various source systems and the migration of data between systems.</li>
 <li>Build and implement the source system integration and data migration plan.</li>
 <li>Developing, managing, manipulating, storing and parsing data across a data pipeline for variety of target sources and data consumers</li>
 <li>Writing code to ensure the performance and reliability of data extraction and processing</li>
 <li>Supporting continuous process automation for data ingestion</li>
 <li>Assisting with the maintenance of applications and tools that reside on the data driven systems (upgrades, patches, configuration changes, etc.)</li>
 <li>Working with program management and engineers to implement and document complex and evolving requirements</li>
 <li>Actively and collaboratively participating as a member of a cross-functional Agile/Scrum team while following all Agile/Scrum best practices</li>
 <li>Advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing</li>
 <li>Helping cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork</li>
 <li>Demonstrating significant technical competence and ownership to broad audiences while driving progress on company strategic objectives at multiple levels.</li>
 <li>Generating and articulating technical strategy to diverse audiences, both technical and non-technical.</li>
</ul>
<p><b>NON-TECHNICAL REQUIREMENTS:</b></p>
<ul>
 <li>Ability to pass a 7 year background check and have the ability to obtain a U.S. Government clearance.</li>
 <li>Must have been a resident of the continental U.S. for at least the last 3 years.</li>
 <li>Must be willing to work in Eastern Standard Business hours.</li>
 <li>Must possess good communication (written/verbal) skills.</li>
</ul>
<p><b>MANDATORY REQUIREMENTS:</b></p>
<ul>
 <li>Must have a Bachelor&apos;s Degree in a technical discipline and 10+ years pertinent experience with the design, management, and solutioning of large, complex data sets and models.</li>
 <li>Must have at least 2+ years of experience working with/in public cloud environments.</li>
 <li>Must have proven experience leading data migration project from on-prem to cloud.</li>
 <li>Must have extensive experience/knowledge of data integration.</li>
 <li>Must have experience with Cloud Native databases</li>
 <li>Experience with Enterprise Architecture &amp; Distributed Systems</li>
</ul>
<p><b>DESIRED SKILLS:</b></p>
<ul>
 <li>Experience with AWS DMS/MGN</li>
 <li>Technical knowledge of Datalake/DeltaLake/Lakehouse</li>
</ul>
<p><b>PHYSICAL DEMANDS:</b></p>
<ul>
 <li>Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions</li>
</ul>
<p>INADEV Corporation does not discriminate against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibits discrimination against all individuals based on their race, color, religion, sex, sexual orientation/gender identity, or national origin.</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;125,000.00 - &#x24;150,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Referral program</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>This position requires the ability to obtain/maintain a U.S. Government Clearance and pass a 7 year employment background check. Do you meet these mandatory requirements?</li>
 <li>Are you open to working on a Permanent/W2 basis?</li>
 <li>This position requires that all candidates being considered MUST have resided in the Continental United States for at least the last 3 years and be able/willing to work on East Standard Business hours. Do you meet these mandatory requirements?</li>
 <li>Do you have a Bachelor&apos;s Degree in a Technical Discipline (i.e. Computer Science, Computer Engineering, Information Technology, Information Systems)?</li>
 <li>Do you have at least 7 years of professional experience as a Data Engineer?</li>
 <li>Do have professional experience leading data migration projects from on-prem to cloud?</li>
 <li>Do you have at least 7 years of professional experience in Data Manipulation working specifically with Python?</li>
 <li>Do you have at least 7 years of professional ETL experience?</li>
</ul>
<p>Work Location: Remote</p>",,fdd1d18770845207,,Full-time,,,Remote,Sr. Data Engineer,Today,2023-10-18T13:31:36.878Z,3.0,6.0,"$125,000 - $150,000 a year",2023-10-18T13:31:36.881Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=fdd1d18770845207&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
12,Nascent,"About Nascent… 
 Nascent is a team of builders who back early-stage web3 founders creating products and primitives for an open financial world. Founded in 2020, we've invested in 50+ early-stage teams (https://www.nascent.xyz/portfolio) that we believe have the potential to create substantive change, expand boundaries and find new horizons. Building from a base of permanent capital, we also deploy a sizable liquid portfolio utilizing a range of strategies that ensure we are among the most active users of the open financial system we are helping to build. The fluid structure that enables our team to build, use, and invest in the future of crypto makes Nascent both an ideal early-stage partner and long-term ally. 
 The Opportunity 
 As a Data Engineer at Nascent, you'll play a critical role in building and maintaining our data infrastructure and pipelines across our liquid and investing activities. This includes designing and building ETLs, optimizing research and transaction infrastructure, and implementing APIs for data querying and access. Reporting directly to the Data Engineering Lead you'll support the strategy and execution of data acquisition to enable internal research and externally facing data projects. You'll serve as a key partner across the organization, delivering internal data projects such as dashboarding and visualizations to improve investing performance. 
 The ideal candidate will have experience in designing and building data pipelines and APIs, with a strong foundation in data modeling, database design, and query optimization. Think data engineer with a software engineering lens, you can design and build software beyond data pipelines (you know the difference). You'll have a passion for working with large datasets and a desire to leverage data to drive business insights and support decision-making. As a fast-moving crypto-native firm, we have a lot of activities and both ingest and create a lot of data—working closely with our Data Engineering Lead you will execute high impact work across data-related activities for the firm. This is a full time fully remote position with preference for EST working hours. 
 About you 
 
  You thrive in less structured environments and are at your best when driving and delivering results with the freedom to build and execute your own plan.
   You are as excited by starting a project as completing, maintaining, and continually optimizing it.
   You find the plethora of opportunities to leverage data to drive value for the firm exciting versus overwhelming
  
 Required Experience 
 
  Building and managing data pipelines on bare metal outside of pure cloud infrastructure (AWS, Azure, GCP, etc.)
   Hands-on experience from beginning to end of a shipped working product (i.e., you're an experienced builder)
   Substantive experience building and maintaining enterprise data systems, ETL frameworks, & pipelines
   Excellent Python and SQL skills
   Familiarity with APIs (REST, Websockets, etc)
   Experience beyond testing (e.g. quality processes, verification & validation)
   Experience in configuring and managing cloud infrastructure (preferred AWS)
  
 Our Team & Culture 
 At Nascent, we are an interdisciplinary team of investors, builders & creators, capable of achieving more together than we can as individuals. We offer the opportunity to contribute to building the future global economic system with a world-class team and culture that pairs the freedom to explore, experiment & play with a competitive drive to win. We invest in our people by providing the autonomy to build, coupled with accountability & honest feedback to help learn, grow, perform & win. We're a fully distributed team that understands the value of in-person time—we host two team retreats per year and encourage team members to come together for more frequent in-person work. 
 Principles that drive our team & work 
 
  Build for the long term
   Align incentives
   Be nimble
   Compete to win
   Explore, experiment, play
   Always be building
   Give and embrace real feedback
  
 What We Offer 
 At Nascent, we offer a competitive total compensation package heavily weighted toward bonus, ensuring that when we perform at our best and the firm wins we all win. 
 
  The opportunity to learn, experiment and build in an entrepreneurial environment
   Remote and distributed working environment
   Comprehensive health benefits package including dental, vision, and life
   Generous paid parental leave & supported return to work
   Home Office, coworking space and wellness stipend
   Retirement plan matching contributions
   Open vacation policy as well as flexible work hours and location
   Access to our internal performance coaching, technical experts and support for continuing your skill development and growth
   Team activities and bi-annual in-person team retreats
  
 We are an equal opportunity employer and celebrate diversity and differences of perspectives. We do not discriminate on the basis of any status, inclusive of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","<div>
 <p><b><i>About Nascent&#x2026;</i></b></p> 
 <p>Nascent is a team of builders who back early-stage web3 founders creating products and primitives for an open financial world. Founded in 2020, we&apos;ve invested in 50+ early-stage teams (https://www.nascent.xyz/portfolio) that we believe have the potential to create substantive change, expand boundaries and find new horizons. Building from a base of permanent capital, we also deploy a sizable liquid portfolio utilizing a range of strategies that ensure we are among the most active users of the open financial system we are helping to build. The fluid structure that enables our team to build, use, and invest in the future of crypto makes Nascent both an ideal early-stage partner and long-term ally.</p> 
 <p><b><i>The</i></b> <b><i>Opportunity</i></b></p> 
 <p>As a Data Engineer at Nascent, you&apos;ll play a critical role in building and maintaining our data infrastructure and pipelines across our liquid and investing activities. This includes designing and building ETLs, optimizing research and transaction infrastructure, and implementing APIs for data querying and access. Reporting directly to the Data Engineering Lead you&apos;ll support the strategy and execution of data acquisition to enable internal research and externally facing data projects. You&apos;ll serve as a key partner across the organization, delivering internal data projects such as dashboarding and visualizations to improve investing performance. </p>
 <p>The ideal candidate will have experience in designing and building data pipelines and APIs, with a strong foundation in data modeling, database design, and query optimization. Think data engineer with a software engineering lens, you can design and build software beyond data pipelines (you know the difference). You&apos;ll have a passion for working with large datasets and a desire to leverage data to drive business insights and support decision-making. As a fast-moving crypto-native firm, we have a lot of activities and both ingest and create a lot of data&#x2014;working closely with our Data Engineering Lead you will execute high impact work across data-related activities for the firm. This is a full time fully remote position with preference for EST working hours. </p>
 <p><b><i>About you</i></b></p> 
 <ul>
  <li>You thrive in less structured environments and are at your best when driving and delivering results with the freedom to build and execute your own plan.</li>
  <li> You are as excited by starting a project as completing, maintaining, and continually optimizing it.</li>
  <li> You find the plethora of opportunities to leverage data to drive value for the firm exciting versus overwhelming</li>
 </ul> 
 <p><b><i>Required Experience</i></b></p> 
 <ul>
  <li>Building and managing data pipelines on bare metal outside of pure cloud infrastructure (AWS, Azure, GCP, etc.)</li>
  <li> Hands-on experience from beginning to end of a shipped working product (i.e., you&apos;re an experienced builder)</li>
  <li> Substantive experience building and maintaining enterprise data systems, ETL frameworks, &amp; pipelines</li>
  <li> Excellent Python and SQL skills</li>
  <li> Familiarity with APIs (REST, Websockets, etc)</li>
  <li> Experience beyond testing (e.g. quality processes, verification &amp; validation)</li>
  <li> Experience in configuring and managing cloud infrastructure (preferred AWS)</li>
 </ul> 
 <p><b><i>Our Team &amp; Culture</i></b></p> 
 <p>At Nascent, we are an interdisciplinary team of investors, builders &amp; creators, capable of achieving more together than we can as individuals. We offer the opportunity to contribute to building the future global economic system with a world-class team and culture that pairs the freedom to explore, experiment &amp; play with a competitive drive to win. We invest in our people by providing the autonomy to build, coupled with accountability &amp; honest feedback to help learn, grow, perform &amp; win. We&apos;re a fully distributed team that understands the value of in-person time&#x2014;we host two team retreats per year and encourage team members to come together for more frequent in-person work.</p> 
 <p><b><i>Principles that drive our team &amp; work</i></b></p> 
 <ul>
  <li>Build for the long term</li>
  <li> Align incentives</li>
  <li> Be nimble</li>
  <li> Compete to win</li>
  <li> Explore, experiment, play</li>
  <li> Always be building</li>
  <li> Give and embrace real feedback</li>
 </ul> 
 <p><b><i>What We Offer</i></b></p> 
 <p>At Nascent, we offer a competitive total compensation package heavily weighted toward bonus, ensuring that when we perform at our best and the firm wins we all win.</p> 
 <ul>
  <li>The opportunity to learn, experiment and build in an entrepreneurial environment</li>
  <li> Remote and distributed working environment</li>
  <li> Comprehensive health benefits package including dental, vision, and life</li>
  <li> Generous paid parental leave &amp; supported return to work</li>
  <li> Home Office, coworking space and wellness stipend</li>
  <li> Retirement plan matching contributions</li>
  <li> Open vacation policy as well as flexible work hours and location</li>
  <li> Access to our internal performance coaching, technical experts and support for continuing your skill development and growth</li>
  <li> Team activities and bi-annual in-person team retreats</li>
 </ul> 
 <p><i>We are an equal opportunity employer and celebrate diversity and differences of perspectives. We do not discriminate on the basis of any status, inclusive of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</i></p>
</div>
<p></p>",https://nascent-xyz.breezy.hr/p/cfd0b01e7274-data-engineer?source=indeed&ittk=5TY4HQRALW,713aaea093a693fd,,Full-time,,,Remote,Data Engineer,1 day ago,2023-10-17T13:31:24.498Z,4.3,4.0,"$130,000 - $160,000 a year",2023-10-18T13:31:24.501Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=713aaea093a693fd&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
13,OpenEarth Foundation,"Lead Data Engineer:
Building Climate Solutions for Cities
Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg
Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.
Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.
We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.
We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.
We have funding and a team of experts focused on Earth systems and digital innovation.
Your mission, should you choose to accept it:
As a lead data engineer, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.
As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.
You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.
The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We're working on the planet's problems and we need the planet's best people to fix them.
The following requirements describe our ideal candidate. If you don't meet some of the requirements, you're encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.
Essential Functions and Specific Duties:

 Design, architect, build and maintain data pipeline systems
 Write code for importing and updating large datasets to relational database and search indexes
 Define and maintain database schemas and data file formats
 Collaborate with web developers on optimizing database schemas for APIs and Web applications
 Collaborate with a team of software engineering peers
 Mentor and guide more junior data engineering staff
 Define and maintain data management processes for the organization
 Work with product managers to develop schedules, estimate tasks, and define success criteria
 Collaborate with team members from other disciplines such as web development, design, product management, and devops
 Coordinate with Open Source contributors
 Coordinate with open standards community to define interoperability standards
 Actively participate in team building and culture development activities at Open Earth Foundation
 Other duties as assigned

Required skills:

 Python programming focused on big data management
 PostgreSQL or other relational database
 Docker
 Kubernetes
 Git

Optional skills that will make a candidate stand out:

 Generative AI and large language model (LLM) APIs and data applications
 GIS tools such as ESRI
 Amazon Web Services
 ElasticSearch
 Data pipeline tools, e.g. Pachyderm
 Experience with 100Gb or larger data sets
 Climate action data such as emissions, targets, and action plans
 Physical (lat, lon, alt) and political (city, state, country) geographical data
 Remote-sensing and satellite data
 RESTful Web APIs
 Engineering leadership
 Open Source project maintainership
 Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis

Qualifications:

 Bachelor’s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience
 5 years of experience in software development for data systems
 3 shipped projects

Interpersonal skills:

 Clear communicator with good verbal and written skills in English (additional languages a plus)
 Creative, flexible and efficient with a focus on details
 Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.
 Ability to embrace new challenges, take ownership and initiative as a key team player.

Compensation and benefits

 This position is full-time with compensation of $60,000-$105,000 /year, dependent on experience and location
 Open Earth offers unlimited paid time off, paid holidays and paid sick leave
 You will work remotely within a dynamic and international environment
 We celebrate our achievements during our annual team retreat

OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.
Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.
We look forward to hearing from you!
Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don’t believe you meet all the requirements, please apply and note what’s missing in your cover letter.Lead Data Engineer:
Building Climate Solutions for Cities
Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg
Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.
Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.
We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.
We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.
We have funding and a team of experts focused on Earth systems and digital innovation.
Your mission, should you choose to accept it:
As a lead data engineer, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.
As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.
You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.
The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We're working on the planet's problems and we need the planet's best people to fix them.
The following requirements describe our ideal candidate. If you don't meet some of the requirements, you're encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.
Essential Functions and Specific Duties:

 Design, architect, build and maintain data pipeline systems
 Write code for importing and updating large datasets to relational database and search indexes
 Define and maintain database schemas and data file formats
 Collaborate with web developers on optimizing database schemas for APIs and Web applications
 Collaborate with a team of software engineering peers
 Mentor and guide more junior data engineering staff
 Define and maintain data management processes for the organization
 Work with product managers to develop schedules, estimate tasks, and define success criteria
 Collaborate with team members from other disciplines such as web development, design, product management, and devops
 Coordinate with Open Source contributors
 Coordinate with open standards community to define interoperability standards
 Actively participate in team building and culture development activities at Open Earth Foundation
 Other duties as assigned

Required skills:

 Python programming focused on big data management
 PostgreSQL or other relational database
 Docker
 Kubernetes
 Git

Optional skills that will make a candidate stand out:

 Generative AI and large language model (LLM) APIs and data applications
 GIS tools such as ESRI
 Amazon Web Services
 ElasticSearch
 Data pipeline tools, e.g. Pachyderm
 Experience with 100Gb or larger data sets
 Climate action data such as emissions, targets, and action plans
 Physical (lat, lon, alt) and political (city, state, country) geographical data
 Remote-sensing and satellite data
 RESTful Web APIs
 Engineering leadership
 Open Source project maintainership
 Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis

Qualifications:

 Bachelor’s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience
 5 years of experience in software development for data systems
 3 shipped projects

Interpersonal skills:

 Clear communicator with good verbal and written skills in English (additional languages a plus)
 Creative, flexible and efficient with a focus on details
 Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.
 Ability to embrace new challenges, take ownership and initiative as a key team player.

Compensation and benefits

 This position is full-time with compensation of $60,000-$105,000 /year, dependent on experience and location
 Open Earth offers unlimited paid time off, paid holidays and paid sick leave
 You will work remotely within a dynamic and international environment
 We celebrate our achievements during our annual team retreat

OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.
Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.
We look forward to hearing from you!
Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don’t believe you meet all the requirements, please apply and note what’s missing in your cover letter.
Job Type: Full-time
Pay: $60,000.00 - $105,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Flexible schedule
 Health insurance
 Paid time off
 Vision insurance

Compensation package:

 Bonus opportunities
 Yearly pay

Experience level:

 3 years
 4 years
 5 years
 6 years
 7 years
 8 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p><b>Lead Data Engineer:</b></p>
<p><b>Building Climate Solutions for Cities</b></p>
<p>Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg</p>
<p><i><b>Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.</b></i></p>
<p><i>Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.</i></p>
<p><i>We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.</i></p>
<p><i>We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.</i></p>
<p><i>We have funding and a team of experts focused on Earth systems and digital innovation.</i></p>
<p><i><b>Your mission, should you choose to accept it:</b></i></p>
<p>As a <b>lead data engineer</b>, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.</p>
<p>As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.</p>
<p>You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.</p>
<p>The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We&apos;re working on the planet&apos;s problems and we need the planet&apos;s best people to fix them.</p>
<p><i>The following requirements describe our ideal candidate. If you don&apos;t meet some of the requirements, you&apos;re encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.</i></p>
<p><b>Essential Functions and Specific Duties:</b></p>
<ul>
 <li>Design, architect, build and maintain data pipeline systems</li>
 <li>Write code for importing and updating large datasets to relational database and search indexes</li>
 <li>Define and maintain database schemas and data file formats</li>
 <li>Collaborate with web developers on optimizing database schemas for APIs and Web applications</li>
 <li>Collaborate with a team of software engineering peers</li>
 <li>Mentor and guide more junior data engineering staff</li>
 <li>Define and maintain data management processes for the organization</li>
 <li>Work with product managers to develop schedules, estimate tasks, and define success criteria</li>
 <li>Collaborate with team members from other disciplines such as web development, design, product management, and devops</li>
 <li>Coordinate with Open Source contributors</li>
 <li>Coordinate with open standards community to define interoperability standards</li>
 <li>Actively participate in team building and culture development activities at Open Earth Foundation</li>
 <li>Other duties as assigned</li>
</ul>
<p><b>Required skills:</b></p>
<ul>
 <li>Python programming focused on big data management</li>
 <li>PostgreSQL or other relational database</li>
 <li>Docker</li>
 <li>Kubernetes</li>
 <li>Git</li>
</ul>
<p><b>Optional skills that will make a candidate stand out:</b></p>
<ul>
 <li>Generative AI and large language model (LLM) APIs and data applications</li>
 <li>GIS tools such as ESRI</li>
 <li>Amazon Web Services</li>
 <li>ElasticSearch</li>
 <li>Data pipeline tools, e.g. Pachyderm</li>
 <li>Experience with 100Gb or larger data sets</li>
 <li>Climate action data such as emissions, targets, and action plans</li>
 <li>Physical (lat, lon, alt) and political (city, state, country) geographical data</li>
 <li>Remote-sensing and satellite data</li>
 <li>RESTful Web APIs</li>
 <li>Engineering leadership</li>
 <li>Open Source project maintainership</li>
 <li>Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience</li>
 <li>5 years of experience in software development for data systems</li>
 <li>3 shipped projects</li>
</ul>
<p><b>Interpersonal skills:</b></p>
<ul>
 <li>Clear communicator with good verbal and written skills in English (additional languages a plus)</li>
 <li>Creative, flexible and efficient with a focus on details</li>
 <li>Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.</li>
 <li>Ability to embrace new challenges, take ownership and initiative as a key team player.</li>
</ul>
<p><i><b>Compensation and benefits</b></i></p>
<ul>
 <li>This position is full-time with compensation of &#x24;60,000-&#x24;105,000 /year, dependent on experience and location</li>
 <li>Open Earth offers unlimited paid time off, paid holidays and paid sick leave</li>
 <li>You will work remotely within a dynamic and international environment</li>
 <li>We celebrate our achievements during our annual team retreat</li>
</ul>
<p><i>OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.</i></p>
<p><i>Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.</i></p>
<p><i>We look forward to hearing from you!</i></p>
<p><i>Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don&#x2019;t believe you meet all the requirements, please apply and note what&#x2019;s missing in your cover letter.</i><b>Lead Data Engineer:</b></p>
<p><b>Building Climate Solutions for Cities</b></p>
<p>Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg</p>
<p><i><b>Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.</b></i></p>
<p><i>Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.</i></p>
<p><i>We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.</i></p>
<p><i>We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.</i></p>
<p><i>We have funding and a team of experts focused on Earth systems and digital innovation.</i></p>
<p><i><b>Your mission, should you choose to accept it:</b></i></p>
<p>As a <b>lead data engineer</b>, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.</p>
<p>As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.</p>
<p>You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.</p>
<p>The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We&apos;re working on the planet&apos;s problems and we need the planet&apos;s best people to fix them.</p>
<p><i>The following requirements describe our ideal candidate. If you don&apos;t meet some of the requirements, you&apos;re encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.</i></p>
<p><b>Essential Functions and Specific Duties:</b></p>
<ul>
 <li>Design, architect, build and maintain data pipeline systems</li>
 <li>Write code for importing and updating large datasets to relational database and search indexes</li>
 <li>Define and maintain database schemas and data file formats</li>
 <li>Collaborate with web developers on optimizing database schemas for APIs and Web applications</li>
 <li>Collaborate with a team of software engineering peers</li>
 <li>Mentor and guide more junior data engineering staff</li>
 <li>Define and maintain data management processes for the organization</li>
 <li>Work with product managers to develop schedules, estimate tasks, and define success criteria</li>
 <li>Collaborate with team members from other disciplines such as web development, design, product management, and devops</li>
 <li>Coordinate with Open Source contributors</li>
 <li>Coordinate with open standards community to define interoperability standards</li>
 <li>Actively participate in team building and culture development activities at Open Earth Foundation</li>
 <li>Other duties as assigned</li>
</ul>
<p><b>Required skills:</b></p>
<ul>
 <li>Python programming focused on big data management</li>
 <li>PostgreSQL or other relational database</li>
 <li>Docker</li>
 <li>Kubernetes</li>
 <li>Git</li>
</ul>
<p><b>Optional skills that will make a candidate stand out:</b></p>
<ul>
 <li>Generative AI and large language model (LLM) APIs and data applications</li>
 <li>GIS tools such as ESRI</li>
 <li>Amazon Web Services</li>
 <li>ElasticSearch</li>
 <li>Data pipeline tools, e.g. Pachyderm</li>
 <li>Experience with 100Gb or larger data sets</li>
 <li>Climate action data such as emissions, targets, and action plans</li>
 <li>Physical (lat, lon, alt) and political (city, state, country) geographical data</li>
 <li>Remote-sensing and satellite data</li>
 <li>RESTful Web APIs</li>
 <li>Engineering leadership</li>
 <li>Open Source project maintainership</li>
 <li>Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience</li>
 <li>5 years of experience in software development for data systems</li>
 <li>3 shipped projects</li>
</ul>
<p><b>Interpersonal skills:</b></p>
<ul>
 <li>Clear communicator with good verbal and written skills in English (additional languages a plus)</li>
 <li>Creative, flexible and efficient with a focus on details</li>
 <li>Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.</li>
 <li>Ability to embrace new challenges, take ownership and initiative as a key team player.</li>
</ul>
<p><i><b>Compensation and benefits</b></i></p>
<ul>
 <li>This position is full-time with compensation of &#x24;60,000-&#x24;105,000 /year, dependent on experience and location</li>
 <li>Open Earth offers unlimited paid time off, paid holidays and paid sick leave</li>
 <li>You will work remotely within a dynamic and international environment</li>
 <li>We celebrate our achievements during our annual team retreat</li>
</ul>
<p><i>OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.</i></p>
<p><i>Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.</i></p>
<p><i>We look forward to hearing from you!</i></p>
<p><i>Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don&#x2019;t believe you meet all the requirements, please apply and note what&#x2019;s missing in your cover letter.</i></p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;60,000.00 - &#x24;105,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Bonus opportunities</li>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
 <li>4 years</li>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,37b5edb36e885430,,Full-time,,,Remote,Lead Data Engineer,Today,2023-10-18T13:31:48.180Z,,,"$60,000 - $105,000 a year",2023-10-18T13:31:48.183Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=37b5edb36e885430&from=jasx&tk=1hd1fm1o6iman800&vjs=3
14,Gen4 Dental,"Company Description
At Gen4, we pride ourselves on our commitment to providing an incredible patient experience. We are passionate about our craft and the impact we can have on our patients. We aim to foster a doctor-centric organization that allows our doctors to do more of what they love. Culture, high performance, growth, and development are deeply embedded in our business and we have our sights set on finding individuals who are excited to be a part of a growing, flourishing company like ours. We offer competitive pay and a comprehensive benefits package available on the 1st of the month after 30 days of employment for full-time employees.
To learn more about us, check out our website here: https://gen4dental.com/
Job Description
The Data Engineer is a detail-oriented and innovative engineer with a deep understanding of Microsoft data platforms and proficiency in Python. In this role, you will spearhead the development and maintenance of our data infrastructure to facilitate efficient data processing and analytics, enhancing our dental services through informed, data-driven strategies.
Duties & Responsibilities

 Develop, construct, test, and maintain robust ETL processes primarily using Microsoft SQL Server Integration Services (SSIS) and\or Python.
 Design and implement relational and non-relational database systems utilizing Microsoft SQL Server, ensuring data integrity, availability, and confidentiality.
 Leverage Microsoft Azure data services (Azure Data Factory, Azure SQL Data Warehouse, Azure Data Lake, etc.) for scalable and cost-effective solutions.
 Develop analytical tools and solutions using Microsoft Power BI to empower teams with actionable insights for strategic planning and operational efficiency.
 Optimize and automate data delivery processes and establish routines for database tasks to improve system performance and stability.
 Uphold high standards for data quality by devising and implementing effective data cleaning and validation procedures.
 Collaborate closely with cross-functional teams to align data management and analytics solutions with business objectives.

Qualifications
Required:

 Minimum of 5+ years’ experience as a Data Engineer, ETL Developer, or similar role with a focus on Microsoft data platforms.
 Proficient in SQL with a solid understanding of Microsoft SQL Server and SSIS.
 Experience with Azure data services and Microsoft Power BI is essential.
 Knowledge of data modeling principles, including experience with data warehousing and big data technologies.
 Python programming experience.
 Strong problem-solving skills with an analytical mindset.
 Excellent communication skills, capable of explaining complex technical concepts to non-technical stakeholders.
 Multi-site healthcare experience.

Preferred:

 Bachelor's degree in a technology or engineering field, or equivalent combination of experience
 Dental industry experience
 Google Bigquery experience
 Postgresql experience.

Physical Requirements:

 Prolonged periods of sitting at a desk and working on a computer.
 Ability to lift up to 15 pounds.
 Excellent written, speaking and listening skills, requiring the perception of speech.
 Must have high finger dexterity to perform duties involving work on the computer.
 Able to travel as needed.

Equipment Used:
General office equipment (e.g. computer).
Additional information
Working conditions include those typically seen in an office environment. Prolonged periods of sitting at a desk and working on a computer.
Equal Opportunity Employer
Gen4 Dental Partners provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Job Type: Full-time
Pay: $115,000.00 - $125,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Paid time off
 Vision insurance

Schedule:

 Monday to Friday

Application Question(s):

 Are you at least 18 years of age?

Work Location: Remote","<p><b>Company Description</b></p>
<p>At Gen4, we pride ourselves on our commitment to providing an incredible patient experience. We are passionate about our craft and the impact we can have on our patients. We aim to foster a doctor-centric organization that allows our doctors to do more of what they love. Culture, high performance, growth, and development are deeply embedded in our business and we have our sights set on finding individuals who are excited to be a part of a growing, flourishing company like ours. <b>We offer competitive pay and a comprehensive benefits package available on the 1st of the month after 30 days of employment for full-time employees</b>.</p>
<p>To learn more about us, check out our website here: https://gen4dental.com/</p>
<p><b>Job Description</b></p>
<p>The Data Engineer is a detail-oriented and innovative engineer with a deep understanding of Microsoft data platforms and proficiency in Python. In this role, you will spearhead the development and maintenance of our data infrastructure to facilitate efficient data processing and analytics, enhancing our dental services through informed, data-driven strategies.</p>
<p><b>Duties &amp; Responsibilities</b></p>
<ul>
 <li>Develop, construct, test, and maintain robust ETL processes primarily using Microsoft SQL Server Integration Services (SSIS) and\or Python.</li>
 <li>Design and implement relational and non-relational database systems utilizing Microsoft SQL Server, ensuring data integrity, availability, and confidentiality.</li>
 <li>Leverage Microsoft Azure data services (Azure Data Factory, Azure SQL Data Warehouse, Azure Data Lake, etc.) for scalable and cost-effective solutions.</li>
 <li>Develop analytical tools and solutions using Microsoft Power BI to empower teams with actionable insights for strategic planning and operational efficiency.</li>
 <li>Optimize and automate data delivery processes and establish routines for database tasks to improve system performance and stability.</li>
 <li>Uphold high standards for data quality by devising and implementing effective data cleaning and validation procedures.</li>
 <li>Collaborate closely with cross-functional teams to align data management and analytics solutions with business objectives.</li>
</ul>
<p><b>Qualifications</b></p>
<p><b>Required:</b></p>
<ul>
 <li>Minimum of 5+ years&#x2019; experience as a Data Engineer, ETL Developer, or similar role with a focus on Microsoft data platforms.</li>
 <li>Proficient in SQL with a solid understanding of Microsoft SQL Server and SSIS.</li>
 <li>Experience with Azure data services and Microsoft Power BI is essential.</li>
 <li>Knowledge of data modeling principles, including experience with data warehousing and big data technologies.</li>
 <li>Python programming experience.</li>
 <li>Strong problem-solving skills with an analytical mindset.</li>
 <li>Excellent communication skills, capable of explaining complex technical concepts to non-technical stakeholders.</li>
 <li>Multi-site healthcare experience.</li>
</ul>
<p><b>Preferred:</b></p>
<ul>
 <li>Bachelor&apos;s degree in a technology or engineering field, or equivalent combination of experience</li>
 <li>Dental industry experience</li>
 <li>Google Bigquery experience</li>
 <li>Postgresql experience.</li>
</ul>
<p><b>Physical Requirements:</b></p>
<ul>
 <li>Prolonged periods of sitting at a desk and working on a computer.</li>
 <li>Ability to lift up to 15 pounds.</li>
 <li>Excellent written, speaking and listening skills, requiring the perception of speech.</li>
 <li>Must have high finger dexterity to perform duties involving work on the computer.</li>
 <li>Able to travel as needed.</li>
</ul>
<p><b>Equipment Used:</b></p>
<p>General office equipment (e.g. computer).</p>
<p><b>Additional information</b></p>
<p>Working conditions include those typically seen in an office environment. Prolonged periods of sitting at a desk and working on a computer.</p>
<p><b>Equal Opportunity Employer</b></p>
<p>Gen4 Dental Partners provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;115,000.00 - &#x24;125,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Are you at least 18 years of age?</li>
</ul>
<p>Work Location: Remote</p>",,e1d633e960423d61,,Full-time,,,Remote,Data Engineer,Today,2023-10-18T13:31:48.858Z,2.0,4.0,"$115,000 - $125,000 a year",2023-10-18T13:31:48.861Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e1d633e960423d61&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
15,Agilon Health,"Agilon health is transforming healthcare by empowering community-based physicians with the resources and expertise they need to innovate the payment and delivery of care for seniors.
The agilon health Total Care Model is powered by our purpose-built platform and frees physicians from the constraints of the traditional fee-for-service reimbursement model, all enabled through a growing national network of like-minded physician partners. With agilon health, physicians are able to practice team-based, coordinated care to serve the individual needs of their senior patients and to transition to a sustainable and predictable, long-term business model.
As you might imagine, analytics and insights are the heart of how we support our physician partners and is our special sauce. We have a lot of analytics programs in place already, but we need more! From generating insights related to our risk adjustment programs to helping analyze health plan attribution, health plan and membership trends, building our next generation Cube or analytics surrounding our brand-new data lake, there's a lot of data to process and actionable insights to present, as well as analytics infrastructure to build. You will be executing some of this work individually, and others in tight partnership with other critical teams such as Finance, Clinical Analytics, and Medical Economics.
There’s much more data we can be leveraging and analyzing to generate and test our hypotheses to help improve patient outcomes and reduce medical waste. Come join the team and help make a direct impact on our senior members’ lives!
More about this role:
- Be part of an agile team working collaboratively with Agilon leadership and many different cross-functional teams, including UX, Product, Technology, Operations, and Clinician teams- Leverage your analytical thinking to explore our ever-growing datasets to test hypotheses and bring life to your own insights- Generate insights that help improve patient outcomes and/or reduce medical waste- Continuously learn and share your new-found knowledge with others
Desired Traits:
1. Experience: A minimum of 4 years of experience performing data analysis and generating intuitive dashboards.
2. Healthcare domain expertise:
a. Exposure to the healthcare industry & domain is preferred, with specific knowledge in either risk adjustment, clinical analytics, clinical quality, or health economics.b. Familiarity with healthcare data models.
3. Education: A bachelor's degree or equivalent education is required.
4. Technical skills:
a. should have strong proficiency in SQL, and expertise in Snowflake would be a plus.b. Data modeling skills are also important.c. Familiarity with data visualization tools such as Sigma, Tableau, Power BI, or Excel is necessary to create intuitive dashboards.d. Some experience with coding in Python or other programming languages is nice to have.
5. Analytical and problem-solving skills: Strong analytical and problem-solving abilities are essential for effectively analyzing data and providing meaningful insights.6. Business Acumen: should be able to translate business needs into data analysis requirements, understanding the goals and objectives of the organization.7. Curiosity and exploration: Should have an intellectual curiosity about data and the ability to go beyond immediate problems. Balancing deadlines and exploring new opportunities for analysis and insights is important.8. Ownership and user focus: Takes ownership over the insights the team generates, and the processes used to develop those analyses, keeping our users' needs in clear focus9. Startup environment readiness: Comfortable in a startup environment and ready to “roll up your sleeves!”10. Communication skills: Excellent communication skills are necessary to effectively convey complex findings to both technical and non-technical stakeholders.11. Nimble learner: Should be eager to learn and adapt quickly to new technologies, methodologies, and industry trends.12. Enthusiasm and drive: A proactive and enthusiastic approach to getting things done is valued in a candidate.
Job Type: Full-time
Pay: $100,000.00 - $115,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Parental leave
 Professional development assistance
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 4 years

Schedule:

 Monday to Friday

Work Location: Remote","<p>Agilon health is transforming healthcare by empowering community-based physicians with the resources and expertise they need to innovate the payment and delivery of care for seniors.</p>
<p>The agilon health Total Care Model is powered by our purpose-built platform and frees physicians from the constraints of the traditional fee-for-service reimbursement model, all enabled through a growing national network of like-minded physician partners. With agilon health, physicians are able to practice team-based, coordinated care to serve the individual needs of their senior patients and to transition to a sustainable and predictable, long-term business model.</p>
<p>As you might imagine, analytics and insights are the heart of how we support our physician partners and is our special sauce. We have a lot of analytics programs in place already, but we need more! From generating insights related to our risk adjustment programs to helping analyze health plan attribution, health plan and membership trends, building our next generation Cube or analytics surrounding our brand-new data lake, there&apos;s a lot of data to process and actionable insights to present, as well as analytics infrastructure to build. You will be executing some of this work individually, and others in tight partnership with other critical teams such as Finance, Clinical Analytics, and Medical Economics.</p>
<p>There&#x2019;s much more data we can be leveraging and analyzing to generate and test our hypotheses to help improve patient outcomes and reduce medical waste. Come join the team and help make a direct impact on our senior members&#x2019; lives!</p>
<p><b>More about this role:</b></p>
<p>- Be part of an agile team working collaboratively with Agilon leadership and many different cross-functional teams, including UX, Product, Technology, Operations, and Clinician teams<br>- Leverage your analytical thinking to explore our ever-growing datasets to test hypotheses and bring life to your own insights<br>- Generate insights that help improve patient outcomes and/or reduce medical waste<br>- Continuously learn and share your new-found knowledge with others</p>
<p><b>Desired Traits:</b></p>
<p>1. Experience: A minimum of 4 years of experience performing data analysis and generating intuitive dashboards.</p>
<p>2. Healthcare domain expertise:</p>
<p>a. Exposure to the healthcare industry &amp; domain is preferred, with specific knowledge in either risk adjustment, clinical analytics, clinical quality, or health economics.<br>b. Familiarity with healthcare data models.</p>
<p>3. Education: A bachelor&apos;s degree or equivalent education is required.</p>
<p>4. Technical skills:</p>
<p>a. should have strong proficiency in SQL, and expertise in Snowflake would be a plus.<br>b. Data modeling skills are also important.<br>c. Familiarity with data visualization tools such as Sigma, Tableau, Power BI, or Excel is necessary to create intuitive dashboards.<br>d. Some experience with coding in Python or other programming languages is nice to have.</p>
<p>5. Analytical and problem-solving skills: Strong analytical and problem-solving abilities are essential for effectively analyzing data and providing meaningful insights.<br>6. Business Acumen: should be able to translate business needs into data analysis requirements, understanding the goals and objectives of the organization.<br>7. Curiosity and exploration: Should have an intellectual curiosity about data and the ability to go beyond immediate problems. Balancing deadlines and exploring new opportunities for analysis and insights is important.<br>8. Ownership and user focus: Takes ownership over the insights the team generates, and the processes used to develop those analyses, keeping our users&apos; needs in clear focus<br>9. Startup environment readiness: Comfortable in a startup environment and ready to &#x201c;roll up your sleeves!&#x201d;<br>10. Communication skills: Excellent communication skills are necessary to effectively convey complex findings to both technical and non-technical stakeholders.<br>11. Nimble learner: Should be eager to learn and adapt quickly to new technologies, methodologies, and industry trends.<br>12. Enthusiasm and drive: A proactive and enthusiastic approach to getting things done is valued in a candidate.</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;100,000.00 - &#x24;115,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Parental leave</li>
 <li>Professional development assistance</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>4 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,f32a0eb4fc36bab8,,Full-time,,,Remote,Senior Engineer BOI Insights (Healthcare Data Analytics),1 day ago,2023-10-17T13:31:56.460Z,2.8,62.0,"$100,000 - $115,000 a year",2023-10-18T13:31:56.463Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=f32a0eb4fc36bab8&from=jasx&tk=1hd1foafmk78p800&vjs=3
16,Leidos,"Description 
 Leidos has an immediate opening for a Senior Systems Engineer supporting development of a data Lakehouse at HHS in Washington DC. This position is an excellent opportunity to interface directly with government personnel to help design and guide technical solutions and business processes for critical public health research and response mission spaces.
 
  Primary Responsibilities
 
   Work directly with the customer to capture and formalize ‘Baseline’ and ‘To Be’ business user stories, use cases, requirements, system architectures, and data schema.
   Track and communicate solution and system development, production and maintenance activities.
   Design and execute test plans for solution and system testing.
   Help support the identification and considerations for alternative application solutions to meet business customer needs.
   Work with the broader development, including SQL developers and Azure engineers.
   Consider security and infrastructure implications of requested changes.
 
 
  Basic Qualifications
 
   Bachelor’s Degree in Systems Engineering, Computer Science, Information Technology, Business Science, or related field required
   8+ years’ of combined experience in system engineering, software development or business analysis required.
   Candidate must demonstrate a willing initiative to solicit customer requirements and translate them into formalized structured technical products. Candidate must have strong verbal and written communication skills. Concise writing and ability to communicate technical content to broad audiences are critical candidate abilities.
   Candidate must be a US Citizen and be able to obtain and maintain a high-risk public trust clearance.
   Must have experience or familiarity with: executing Agile, Scrum, and Kanban methodologies. Soliciting and documenting use cases and designs for system requirements (User Stories, Use Cases, Requirements, Specifications, Data Schema, Business Process Workflows). Use of system management tools such as Azure DevOps, Jira, Redmine, or similar system. Developing and executing testing and acceptance plans. Tracking bugs, issues and resolutions. Performing data analysis and reporting. Knowledge of government system security policies (ATO process) Ability to flexibly pivot to varying needs of the project while maintaining situational awareness
 
 
  Preferred Qualifications
 
   The preferred candidate will possess broader Systems Engineering knowledge and can readily execute low-level engineering tasks as well as high-level technical project management tasks.
   The preferred candidate will have a MS in Systems Engineering or a related discipline.
   Preferred candidates will have experience or familiarity with:
   Knowledge of the Microsoft Azure ecosystem
   Experience with Data Lakes and preferably Data Lakehouses
   Experience working with federal IT systems
   Experience working with SQL developers (or knowledge of SQL)
   Prioritizing and communicating requirements and system development activities.
   Communicating impact of requirement changes on active development activities.
   Executing and presenting trade space of alternatives with cost benefit analysis.
   Supporting Authority to Operate (ATO) activities and other production system processes.
   Experience in developing advanced data visualizations.
 
 
  Pay Range: Pay Range $97,500.00 - $176,250.00
 
  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.
  #Remote","<div>
 <p><b>Description</b> </p>
 <p>Leidos has an immediate opening for a Senior Systems Engineer supporting development of a data Lakehouse at HHS in Washington DC. This position is an excellent opportunity to interface directly with government personnel to help design and guide technical solutions and business processes for critical public health research and response mission spaces.</p>
 <p></p>
 <p><b> Primary Responsibilities</b></p>
 <ul>
  <li> Work directly with the customer to capture and formalize &#x2018;Baseline&#x2019; and &#x2018;To Be&#x2019; business user stories, use cases, requirements, system architectures, and data schema.</li>
  <li> Track and communicate solution and system development, production and maintenance activities.</li>
  <li> Design and execute test plans for solution and system testing.</li>
  <li> Help support the identification and considerations for alternative application solutions to meet business customer needs.</li>
  <li> Work with the broader development, including SQL developers and Azure engineers.</li>
  <li> Consider security and infrastructure implications of requested changes.</li>
 </ul>
 <p></p>
 <p><b> Basic Qualifications</b></p>
 <ul>
  <li> Bachelor&#x2019;s Degree in Systems Engineering, Computer Science, Information Technology, Business Science, or related field required</li>
  <li> 8+ years&#x2019; of combined experience in system engineering, software development or business analysis required.</li>
  <li> Candidate must demonstrate a willing initiative to solicit customer requirements and translate them into formalized structured technical products. Candidate must have strong verbal and written communication skills. Concise writing and ability to communicate technical content to broad audiences are critical candidate abilities.</li>
  <li> Candidate must be a US Citizen and be able to obtain and maintain a high-risk public trust clearance.</li>
  <li> Must have experience or familiarity with: executing Agile, Scrum, and Kanban methodologies. Soliciting and documenting use cases and designs for system requirements (User Stories, Use Cases, Requirements, Specifications, Data Schema, Business Process Workflows). Use of system management tools such as Azure DevOps, Jira, Redmine, or similar system. Developing and executing testing and acceptance plans. Tracking bugs, issues and resolutions. Performing data analysis and reporting. Knowledge of government system security policies (ATO process) Ability to flexibly pivot to varying needs of the project while maintaining situational awareness</li>
 </ul>
 <p></p>
 <p><b> Preferred Qualifications</b></p>
 <ul>
  <li> The preferred candidate will possess broader Systems Engineering knowledge and can readily execute low-level engineering tasks as well as high-level technical project management tasks.</li>
  <li> The preferred candidate will have a MS in Systems Engineering or a related discipline.</li>
  <li> Preferred candidates will have experience or familiarity with:</li>
  <li> Knowledge of the Microsoft Azure ecosystem</li>
  <li> Experience with Data Lakes and preferably Data Lakehouses</li>
  <li> Experience working with federal IT systems</li>
  <li> Experience working with SQL developers (or knowledge of SQL)</li>
  <li> Prioritizing and communicating requirements and system development activities.</li>
  <li> Communicating impact of requirement changes on active development activities.</li>
  <li> Executing and presenting trade space of alternatives with cost benefit analysis.</li>
  <li> Supporting Authority to Operate (ATO) activities and other production system processes.</li>
  <li> Experience in developing advanced data visualizations.</li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Pay Range:</b></h2> Pay Range &#x24;97,500.00 - &#x24;176,250.00
 <p></p>
 <p> The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.</p>
 <p> #Remote</p>
</div>",https://careers.leidos.com/jobs/13492238-data-lakehouse-engineer?tm_job=R-00121192&tm_event=view&tm_company=2502,e2317b3154109084,,Full-time,,,Remote,Data Lakehouse Engineer,Today,2023-10-18T13:31:38.939Z,3.7,1699.0,"$97,500 - $176,250 a year",2023-10-18T13:31:38.941Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e2317b3154109084&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
17,Arista Networks,"Company Description
  Arista Networks is an industry leader in Cognitive Cloud Networking for mission critical data center and campus environments. Our award winning open source platforms deliver ultra low latency, high availability, automated analytics and secure network solutions.
  Our culture is one that is founded on our core key values which resonate across all of our employee and include respect, integrity, teamwork, innovation, passion, trust and quality.
 
 

 Job Description
  The Data Center Site Operations Engineer will help customers successfully plan and optimize their Data Center physical space, mechanical, cabling and other site infrastructure to receive Arista equipment and operate it in the highest performing and most efficient manner.
  Arista Professional Services team helps customers throughout the entire technology lifecycle from design to implementation, migration and ongoing operation, to ensure a seamless transition from initial exposure to being able to operate and realize tangible business outcomes from their network infrastructure.
  In this role, you would function as the subject matter expert in this area of the Arista PS team, interfacing with mechanical engineers in the Arista product team to understand detailed requirements, translate these into best practices for field installation, then take a customer facing role in training and advisory.
  Responsibilities will be to provide expert consulting to customers including site design assistance, proactive site prep. for installation, demonstration of best practices, creating training material and delivering training to customers and partiers, site surveys and oversight of physical installation and commissioning. Areas of expertise should include:
  Site requirements for all Arista Data Center products including:
 
   Equipment installation logistics including floorspace and rack preparation, moving and lifting
   Proper handling of all components from receiving, storage, unboxing and assembly
   Size, Weight and Rack mounting considerations for safe operation and serviceability
   Airflow and cooling requirements
   Power requirements including power resiliency
   Cable management considerations
   Copper and Fiber cable types, connector types, pluggable modules
 
  Understand overall DC design and optimization, including:
 
   Floorplan and rack layouts
   Power distribution and resiliency
   Cooling strategies including liquid cooling
   Cable layout strategies for different DC network topologies
   Equipment density and cable length considerations
 
 
 

 Qualifications
  
 
   An engineering, IT or technology focused degree.
   Mechanical Engineering degree a plus.
   10+ years of industry experience designing, constructing and overseeing operations in complex datacenter environments.
   Experience with the handling, installation and servicing of large scale IT equipment in the DC.
   Experience creating technical drawings, documenting methodologies and training DC staff to execute operations.
   Excellent written and verbal skills and ability to effectively communicate with all Arista, partner and customer levels.
   Prior experience with DC site operations as well as with customer facing services or support will be a plus
   This position is remote with a potential for 50% travel globally.
 
  Compensation Information:
  The new hire base pay for this role has a pay range of $103,000 to $158,000 across the US, within California, the base pay range for this role is $124,000 to 158,000.
  Arista offers different pay ranges based on work location, so that we can offer consistent and competitive pay appropriate to the market. The actual base pay offered will be based on a wide range of factors, including skills, qualifications, relevant experience, and work location. The pay range provided reflects base pay only and in addition certain roles may also be eligible for discretionary Arista bonuses and equity. Employees in Sales roles are eligible to participate in Arista’s Sales Incentive Plan, which pays commissions calculated as a percentage of eligible sales. US-based employees are also entitled to benefits including medical, dental, vision, wellbeing, tax savings and income protection. The recruiting team can share more details during the hiring process specific to the role and location.
  Additional Information
  All your information will be kept confidential according to EEO guidelines.","<div>
 Company Description
 <p><br> Arista Networks is an industry leader in Cognitive Cloud Networking for mission critical data center and campus environments. Our award winning open source platforms deliver ultra low latency, high availability, automated analytics and secure network solutions.</p>
 <p> Our culture is one that is founded on our core key values which resonate across all of our employee and include respect, integrity, teamwork, innovation, passion, trust and quality.</p>
</div> 
<br> 
<div>
 Job Description
 <p><br> The Data Center Site Operations Engineer will help customers successfully plan and optimize their Data Center physical space, mechanical, cabling and other site infrastructure to receive Arista equipment and operate it in the highest performing and most efficient manner.</p>
 <p> Arista Professional Services team helps customers throughout the entire technology lifecycle from design to implementation, migration and ongoing operation, to ensure a seamless transition from initial exposure to being able to operate and realize tangible business outcomes from their network infrastructure.</p>
 <p> In this role, you would function as the subject matter expert in this area of the Arista PS team, interfacing with mechanical engineers in the Arista product team to understand detailed requirements, translate these into best practices for field installation, then take a customer facing role in training and advisory.</p>
 <p> Responsibilities will be to provide expert consulting to customers including site design assistance, proactive site prep. for installation, demonstration of best practices, creating training material and delivering training to customers and partiers, site surveys and oversight of physical installation and commissioning. Areas of expertise should include:</p>
 <p><b> Site requirements for all Arista Data Center products including:</b></p>
 <ul>
  <li> Equipment installation logistics including floorspace and rack preparation, moving and lifting</li>
  <li> Proper handling of all components from receiving, storage, unboxing and assembly</li>
  <li> Size, Weight and Rack mounting considerations for safe operation and serviceability</li>
  <li> Airflow and cooling requirements</li>
  <li> Power requirements including power resiliency</li>
  <li> Cable management considerations</li>
  <li> Copper and Fiber cable types, connector types, pluggable modules</li>
 </ul>
 <p><b> Understand overall DC design and optimization, including:</b></p>
 <ul>
  <li> Floorplan and rack layouts</li>
  <li> Power distribution and resiliency</li>
  <li> Cooling strategies including liquid cooling</li>
  <li> Cable layout strategies for different DC network topologies</li>
  <li> Equipment density and cable length considerations</li>
 </ul>
</div> 
<br> 
<div>
 Qualifications
 <br> 
 <ul>
  <li> An engineering, IT or technology focused degree.</li>
  <li> Mechanical Engineering degree a plus.</li>
  <li> 10+ years of industry experience designing, constructing and overseeing operations in complex datacenter environments.</li>
  <li> Experience with the handling, installation and servicing of large scale IT equipment in the DC.</li>
  <li> Experience creating technical drawings, documenting methodologies and training DC staff to execute operations.</li>
  <li> Excellent written and verbal skills and ability to effectively communicate with all Arista, partner and customer levels.</li>
  <li> Prior experience with DC site operations as well as with customer facing services or support will be a plus</li>
  <li> This position is remote with a potential for 50% travel globally.</li>
 </ul>
 <p><b> Compensation Information:</b></p>
 <p> The new hire base pay for this role has a pay range of &#x24;103,000 to &#x24;158,000 across the US, within California, the base pay range for this role is &#x24;124,000 to 158,000.</p>
 <p> Arista offers different pay ranges based on work location, so that we can offer consistent and<br> competitive pay appropriate to the market. The actual base pay offered will be based on a wide range of factors, including skills, qualifications, relevant experience, and work location.<br> The pay range provided reflects base pay only and in addition certain roles may also be eligible for discretionary Arista bonuses and equity. Employees in Sales roles are eligible to participate in Arista&#x2019;s Sales Incentive Plan, which pays commissions calculated as a percentage of eligible sales. US-based employees are also entitled to benefits including medical, dental, vision, wellbeing, tax savings and income protection. The recruiting team can share more details during the hiring process specific to the role and location.</p>
 <br> Additional Information
 <p><br> All your information will be kept confidential according to EEO guidelines.</p>
</div>",https://jobs.smartrecruiters.com/AristaNetworks/743999937616147-data-center-site-operations-engineer-professional-services,6492b9f12e5769ae,,Full-time,,,"5453 Great America Pkwy, Santa Clara, CA 95054",Data Center Site Operations Engineer - Professional Services,1 day ago,2023-10-17T13:31:51.900Z,4.0,40.0,"$103,000 - $158,000 a year",2023-10-18T13:31:51.902Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=6492b9f12e5769ae&from=jasx&tk=1hd1foafmk78p800&vjs=3
18,Canyon Associates,"Role and Responsibilities
Must be a hands on Data Engineer with lead/management experience. Oversee 3 and design, create, test, deploy and support SQL code. Monitor database systems and daily ETL processes. Looking for 5+ years prior experience (hands-on)
AWS, AZURE Cloud, Azure Databricks, Azure SQL Database, Data Structure, Power BI, Snowflake, Relational databases
Job Type: Full-time
Pay: $140,000.00 - $175,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee discount
 Flexible spending account
 Health insurance
 Health savings account
 Paid time off
 Parental leave
 Vision insurance

Schedule:

 Monday to Friday

People with a criminal record are encouraged to apply
Education:

 Bachelor's (Preferred)

Work Location: Remote","<p><b>Role and Responsibilities</b></p>
<p>Must be a hands on Data Engineer with lead/management experience. Oversee 3 and design, create, test, deploy and support SQL code. Monitor database systems and daily ETL processes. Looking for 5+ years prior experience (hands-on)</p>
<p>AWS, AZURE Cloud, Azure Databricks, Azure SQL Database, Data Structure, Power BI, Snowflake, Relational databases</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;140,000.00 - &#x24;175,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee discount</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Paid time off</li>
 <li>Parental leave</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>People with a criminal record are encouraged to apply</p>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,e3a2b78dd10fb62c,,Full-time,,,"New York, NY",Lead Data Engineer,1 day ago,2023-10-17T13:32:19.755Z,,,"$140,000 - $175,000 a year",2023-10-18T13:32:19.758Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e3a2b78dd10fb62c&from=jasx&tk=1hd1foafmk78p800&vjs=3
19,Braintrust,"ABOUT US: 
   Braintrust is a user-owned talent network that connects top-tier professionals with the world's leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality. 
   ABOUT THE HIRING PROCESS: 
   The hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match. 
   Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.
 
  
  
  
  JOB TYPE: Freelance/ Contract Position (no agencies/C2C - see notes below) 
  LOCATION: Hybrid - Chicago, IL 
  SALARY RANGE: $55 - $64 /hr 
  ESTIMATED DURATION: 40hr/week - Long term 
  EXPERIENCE: 3-5 years 
  BRAINTRUST JOB ID: 9746 
  
 THE OPPORTUNITY 
  
  
   
    
     Requirements 
      
      
       ***MUST BE LOCATED IN CHICAGO ARE AND ABLE TO GO TO THE OFFICE 3 DAYS A WEEK***
        
        
        The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data.
        
        
        Job Responsibilities: 
        
        Develop and test data pipelines with ETL and store data in Snowflake, debug errors and make necessary modifications. 
        Modify existing databases and tables and work with data engineers and analysts to make changes. 
        Write code/scripts to validate data between different databases and work on designing new data pipelines. 
        
       Skills: 
        
        Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. 
        Ability to work independently and manage one's time. 
        Good knowledge of data modeling. 
        Expertise with SQL, Python and Snowflake 
        Basic knowledge of computer software, such as Visual Basic, Oracle, etc. 
        
       Education/Experience: 
        
        Associate's degree in computer programming or a relevant field required. Bachelor's/Master degree preferred. 
        2-4 years experience required 
       
      
     
    
   
   
    
     What you'll be working on 
      
      
       Position's Contributions to Work Group: 
        
        The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. 
        A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data. 
        This position is working with a team responsible for managing Customer Value Agreements (CVA) reporting process and providing insights to business partners to increasing sales 
        
       Typical task breakdown: 
        
        Develop and test Airflow DAGs, debug errors and make necessary modifications during our migration from SAS to Airflow and Snowflake. 
        Maintaining and enhancing tables in Snowflake 
        Provide support to business partners to answer questions regarding CVA metrics. 
        Working on migrating Airflow DAGs to AWS 
        
       Interaction with team: 
        
        Working with colleagues in Chicago, Michigan, and Geneva 
        Will be interacting with business stakeholders to explain the CVA logic and metrics impact. 
        
       Technical Skills (Required) 
        
        2+ years exp in Python 
        2+ years exp with SQL 
        2+ years exp in Snowflake 
        
       (Desired) 
        
        Project management exp specifically Agile methodology 
        Airflow experience 
        AWS experience 
        Data analytics 
        Good understanding of Software Development Life Cycle 
        Tableau, Power BI experience 
        
       Soft Skills (Required) 
        
        Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. 
        Ability to work independently and manage one's time. Critical thinking skills a must. 
        Able to work well in a team in a collaborative workspace. 
        Being proactive and take initiative. 
        Able to work off hours when required to support production issues. 
        
       (Desired) 
        
        Being able to create connection with people outside the team and business partners. 
        Organized
       
      
       
      
    
   
  
 
 
 
  Notes: 
   Our employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we'd welcome your application. 
   Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.","<div>
 <div>
  <p><b>ABOUT US</b>:</p> 
  <p> Braintrust is a user-owned talent network that connects top-tier professionals with the world&apos;s leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality.</p> 
  <p><b> ABOUT THE HIRING PROCESS:</b></p> 
  <p> The hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match.</p> 
  <p> Our process isn&apos;t for everyone, that&apos;s intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.</p>
 </div>
 <br> 
 <p></p> 
 <ul> 
  <li><b>JOB TYPE</b>: Freelance/ Contract Position (no agencies/C2C - see notes below)</li> 
  <li><b>LOCATION</b>: Hybrid - Chicago, IL</li> 
  <li><b>SALARY RANGE</b>: &#x24;55 - &#x24;64 /hr</li> 
  <li><b>ESTIMATED DURATION</b>: 40hr/week - Long term</li> 
  <li><b>EXPERIENCE</b>: 3-5 years</li> 
  <li><b>BRAINTRUST JOB ID: </b><b>9746</b></li> 
 </ul> 
 <p><b>THE OPPORTUNITY</b></p> 
 <div> 
  <div>
   <div>
    <div>
     <h2 class=""jobSectionHeader""><b>Requirements</b></h2> 
     <div> 
      <div>
       <p><b>***MUST BE LOCATED IN CHICAGO ARE AND ABLE TO GO TO THE OFFICE 3 DAYS A WEEK***</b></p>
       <br> 
       <p></p> 
       <p> The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data.</p>
       <br> 
       <p></p> 
       <p><b> Job Responsibilities:</b></p> 
       <ul> 
        <li>Develop and test data pipelines with ETL and store data in Snowflake, debug errors and make necessary modifications.</li> 
        <li>Modify existing databases and tables and work with data engineers and analysts to make changes.</li> 
        <li>Write code/scripts to validate data between different databases and work on designing new data pipelines.</li> 
       </ul> 
       <p><b>Skills:</b></p> 
       <ul> 
        <li>Verbal and written communication skills, problem solving skills, customer service and interpersonal skills.</li> 
        <li>Ability to work independently and manage one&apos;s time.</li> 
        <li>Good knowledge of data modeling.</li> 
        <li>Expertise with SQL, Python and Snowflake</li> 
        <li>Basic knowledge of computer software, such as Visual Basic, Oracle, etc.</li> 
       </ul> 
       <p><b>Education/Experience:</b></p> 
       <ul> 
        <li>Associate&apos;s degree in computer programming or a relevant field required. Bachelor&apos;s/Master degree preferred.</li> 
        <li>2-4 years experience required</li> 
       </ul>
      </div>
     </div>
    </div>
   </div>
   <div>
    <div>
     <h2 class=""jobSectionHeader""><b>What you&apos;ll be working on</b></h2> 
     <div> 
      <div>
       <p><b>Position&apos;s Contributions to Work Group:</b></p> 
       <ul> 
        <li>The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal.</li> 
        <li>A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data.</li> 
        <li>This position is working with a team responsible for managing Customer Value Agreements (CVA) reporting process and providing insights to business partners to increasing sales</li> 
       </ul> 
       <p><b>Typical task breakdown:</b></p> 
       <ul> 
        <li>Develop and test Airflow DAGs, debug errors and make necessary modifications during our migration from SAS to Airflow and Snowflake.</li> 
        <li>Maintaining and enhancing tables in Snowflake</li> 
        <li>Provide support to business partners to answer questions regarding CVA metrics.</li> 
        <li>Working on migrating Airflow DAGs to AWS</li> 
       </ul> 
       <p><b>Interaction with team:</b></p> 
       <ul> 
        <li>Working with colleagues in Chicago, Michigan, and Geneva</li> 
        <li>Will be interacting with business stakeholders to explain the CVA logic and metrics impact.</li> 
       </ul> 
       <p><b>Technical Skills</b><br> (Required)</p> 
       <ul> 
        <li>2+ years exp in Python</li> 
        <li>2+ years exp with SQL</li> 
        <li>2+ years exp in Snowflake</li> 
       </ul> 
       <p>(Desired)</p> 
       <ul> 
        <li>Project management exp specifically Agile methodology</li> 
        <li>Airflow experience</li> 
        <li>AWS experience</li> 
        <li>Data analytics</li> 
        <li>Good understanding of Software Development Life Cycle</li> 
        <li>Tableau, Power BI experience</li> 
       </ul> 
       <p><b>Soft Skills</b><br> (Required)</p> 
       <ul> 
        <li>Verbal and written communication skills, problem solving skills, customer service and interpersonal skills.</li> 
        <li>Ability to work independently and manage one&apos;s time. Critical thinking skills a must.</li> 
        <li>Able to work well in a team in a collaborative workspace.</li> 
        <li>Being proactive and take initiative.</li> 
        <li>Able to work off hours when required to support production issues.</li> 
       </ul> 
       <p>(Desired)</p> 
       <ul> 
        <li>Being able to create connection with people outside the team and business partners.</li> 
        <li>Organized</li>
       </ul>
      </div>
      <br> 
     </div> 
    </div>
   </div>
  </div>
 </div>
 <div></div>
 <div>
  <p>Notes:</p> 
  <p> Our employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we&apos;d welcome your application.</p> 
  <p> Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.</p>
 </div>
</div>",https://boards.greenhouse.io/braintrust/jobs/4998692004?gh_src=d894da014us,a500c31bd71fbb3a,,Full-time,Contract,Freelance,"Chicago, IL",Data Engineer 2 (CHICAGO/HYBRID) - Freelance,1 day ago,2023-10-17T13:32:17.825Z,4.5,4.0,$55 - $64 an hour,2023-10-18T13:32:17.828Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=a500c31bd71fbb3a&from=jasx&tk=1hd1foafmk78p800&vjs=3
20,Comfort Keepers,"Job Summary:
   Comfort Keepers is seeking a skilled and experienced Data Engineer to join our data team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to ensure the availability and accessibility of high-quality data for analysis and reporting purposes. The ideal candidate is passionate about data, governance, has strong programming skills, is a creative problem solver and is proficient in building efficient and scalable data solutions.
   Be part of a transformational experience as we meet the challenges of today’s business landscape and lay the foundation for future growth.
   Location: Remote-U.S. or Irvine, CA Expected Salary Range: $100k
   Responsibilities:
  
    Build Azure data factory pipeline, Azure Data Lake, Datawarehouse, Power BI Reports and dashboards.
    Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
    Optimize and fine-tune data processes to ensure high performance and reliability.
    Implement data validation, testing, and quality assurance processes to maintain data accuracy and consistency.
    Work with large and complex datasets, both structured and unstructured, and employ appropriate data storage solutions.
    Develop and maintain documentation for data pipelines, processes, and data dictionaries.
    Monitor and troubleshoot data pipeline issues, resolving them in a timely manner to minimize downtime.
    Collaborate with data scientists, analysts, agency partners and other stakeholders to understand data requirements and ensure data integrity, accuracy, and availability.
    Additional responsibilities will be supporting applications and projects as appropriate.
  
  
   Qualifications:
  
    Microsoft SQL Server Database 2019 and above, including Azure SQL
    Azure Data Factory, Azure Data Lake, Azure Devops , Azure Data warehouse
    Experience with Repository and version control, GitHub
    Microsoft Power BI and Power Query with DAX and M Language
  
 
  
 
 
   Azure Analysis service—Tabular Cube
   Amazon Athena and Redshift
   Development tools (Visual Studio)
   Designing and implementing data models, and data lake solutions
   Working with data scientists and digital teams to meet strategic data needs through project management tools like Microsoft Teams, JIRA, are desired
   Strong problem-solving skills and attention to detail.
   Excellent communication and teamwork skills.
 
  Preferred:
 
   C# Experience
   Python, and/or R applications and languages while managing work using software version control like GitHub and/or Dev Ops
 
 
  Work Environment: Remote
  An Equal Opportunity and Affirmative Action employer, Comfort Keepers considers applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, or any legally protected status. We will make reasonable accommodations for qualified individuals with known disabilities unless doing so would result in an undue hardship.
  
 Xwc2axfoet","<div>
 <div>
  <h1 class=""jobSectionHeader""><b>Job </b><b>Summary:</b></h1>
  <p> Comfort Keepers is seeking a skilled and experienced Data Engineer to join our data team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to ensure the availability and accessibility of high-quality data for analysis and reporting purposes. The ideal candidate is passionate about data, governance, has strong programming skills, is a creative problem solver and is proficient in building efficient and scalable data solutions.</p>
  <p> Be part of a transformational experience as we meet the challenges of today&#x2019;s business landscape and lay the foundation for future growth.</p>
  <p> Location: Remote-U.S. or Irvine, CA<br> Expected Salary Range: &#x24;100k</p>
  <h1 class=""jobSectionHeader""><b> Responsibilities:</b></h1>
  <ul>
   <li> Build Azure data factory pipeline, Azure Data Lake, Datawarehouse, Power BI Reports and dashboards.</li>
   <li> Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.</li>
   <li> Optimize and fine-tune data processes to ensure high performance and reliability.</li>
   <li> Implement data validation, testing, and quality assurance processes to maintain data accuracy and consistency.</li>
   <li> Work with large and complex datasets, both structured and unstructured, and employ appropriate data storage solutions.</li>
   <li> Develop and maintain documentation for data pipelines, processes, and data dictionaries.</li>
   <li> Monitor and troubleshoot data pipeline issues, resolving them in a timely manner to minimize downtime.</li>
   <li> Collaborate with data scientists, analysts, agency partners and other stakeholders to understand data requirements and ensure data integrity, accuracy, and availability.</li>
   <li> Additional responsibilities will be supporting applications and projects as appropriate.</li>
  </ul>
  <p></p>
  <h1 class=""jobSectionHeader""><b><br> Qualifications:</b></h1>
  <ul>
   <li> Microsoft SQL Server Database 2019 and above, including Azure SQL</li>
   <li> Azure Data Factory, Azure Data Lake, Azure Devops , Azure Data warehouse</li>
   <li> Experience with Repository and version control, GitHub</li>
   <li> Microsoft Power BI and Power Query with DAX and M Language</li>
  </ul>
 </div>
 <br> 
 <p></p>
 <ul>
  <li> Azure Analysis service&#x2014;Tabular Cube</li>
  <li> Amazon Athena and Redshift</li>
  <li> Development tools (Visual Studio)</li>
  <li> Designing and implementing data models, and data lake solutions</li>
  <li> Working with data scientists and digital teams to meet strategic data needs through project management tools like Microsoft Teams, JIRA, are desired</li>
  <li> Strong problem-solving skills and attention to detail.</li>
  <li> Excellent communication and teamwork skills.</li>
 </ul>
 <p> Preferred:</p>
 <ul>
  <li> C# Experience</li>
  <li> Python, and/or R applications and languages while managing work using software version control like GitHub and/or Dev Ops</li>
 </ul>
 <p></p>
 <p><b><br> Work Environment: Remote</b></p>
 <p> An Equal Opportunity and Affirmative Action employer, Comfort Keepers considers applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, or any legally protected status. We will make reasonable accommodations for qualified individuals with known disabilities unless doing so would result in an undue hardship.</p>
 <p> </p>
 <p>Xwc2axfoet</p>
</div>",https://sdxhomecareoperationsllc.applytojob.com/apply/Xwc2axfoet/Data-Engineer?source=INDE&utm_source=Indeed&utm_medium=organic&utm_campaign=Indeed,daada383998fad4a,,Full-time,,,Remote,Data Engineer,1 day ago,2023-10-17T13:32:20.583Z,3.6,4807.0,"$100,000 a year",2023-10-18T13:32:20.586Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=daada383998fad4a&from=jasx&tk=1hd1foafmk78p800&vjs=3
21,Booz Allen Hamilton,"Job Description 
  
 
 
  
   
    
     
      
       
        
         Location: 
        
        
         Washington,DC,US 
        
       
       
        
         Remote Work: 
        
        
         Hybrid 
        
       
       
        
         Job Number: 
        
        
         R0181926
        
       
      
     
    
    
    
      
     
       
      
     
    
    
    
     
      
       
        
         Data Engineer
          The Opportunity:
          Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there’s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it’s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions—from fraud detection to cancer research to national intelligence.
         
          As a big data engineer at Booz Allen, you’ll implement data engineering activities on some of the most mission-driven projects in the industry. You’ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you’ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good.
         
          Join us. The world can’t wait.
         
          You Have:
         
           5+ years of experience in application development
           5+ years of experience designing, developing, operationalizing and maintaining complex data applications at enterprise scale
           3+ years of experience creating software for retrieving, parsing and processing structured and unstructured data
           3+ years of experience building scalable ETL/ELT workflows for reporting and analytics
           Experience creating solutions within a collaborative, cross-functional team environment
           Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms
           Ability to obtain and maintain a Public Trust or Suitability/Fitness determination based on client requirements
           Bachelor’s degree
         
         
          Nice If You Have:
         
           Experience with Python, SQL, Scala, or Java
           Experience with Unix/Linux, including basic commands and Shell scripting
           Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud
           Experience with distributed data, computing tools including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka
           Experience working on real-time data and streaming applications
           Experience with NoSQL implementation, including MongoDB or Cassandra
           Experience with data warehousing using AWS Redshift, MySQL, or Snowflake
           Experience with Agile engineering practices
           Experience with HR system data
         
         
          Vetting:
          Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client.
         
          Create Your Career:
          Grow With Us
          Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
         
          A Place Where You Belong
          Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll build your community in no time.
         
          Support Your Well-Being
          Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
         
          Your Candidate Journey
          At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
         
          Compensation
          At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
          Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $73,100.00 to $166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
         
          Work Model Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
         
           If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
           If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
         
         
          EEO Commitment
          We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.","<div>
 <div>
  <div>
   <h2 class=""jobSectionHeader""><b>Job Description</b></h2> 
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Location: 
        </div>
        <div>
         Washington,DC,US 
        </div>
       </div>
       <div>
        <div>
         Remote Work: 
        </div>
        <div>
         Hybrid 
        </div>
       </div>
       <div>
        <div>
         Job Number: 
        </div>
        <div>
         R0181926
        </div>
       </div>
      </div>
     </div>
    </div>
    <p></p>
    <div>
     <br> 
     <div>
      <div> 
      </div>
     </div>
    </div>
    <div></div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Data Engineer
         <p><b> The Opportunity:</b></p>
         <p> Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there&#x2019;s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it&#x2019;s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions&#x2014;from fraud detection to cancer research to national intelligence.</p>
         <p></p>
         <p> As a big data engineer at Booz Allen, you&#x2019;ll implement data engineering activities on some of the most mission-driven projects in the industry. You&#x2019;ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you&#x2019;ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You&#x2019;ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good.</p>
         <p></p>
         <p> Join us. The world can&#x2019;t wait.</p>
         <p></p>
         <p><b> You Have:</b></p>
         <ul>
          <li><p> 5+ years of experience in application development</p></li>
          <li><p> 5+ years of experience designing, developing, operationalizing and maintaining complex data applications at enterprise scale</p></li>
          <li><p> 3+ years of experience creating software for retrieving, parsing and processing structured and unstructured data</p></li>
          <li><p> 3+ years of experience building scalable ETL/ELT workflows for reporting and analytics</p></li>
          <li><p> Experience creating solutions within a collaborative, cross-functional team environment</p></li>
          <li><p> Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms</p></li>
          <li><p> Ability to obtain and maintain a Public Trust or Suitability/Fitness determination based on client requirements</p></li>
          <li><p> Bachelor&#x2019;s degree</p></li>
         </ul>
         <p></p>
         <p><b> Nice If You Have:</b></p>
         <ul>
          <li><p> Experience with Python, SQL, Scala, or Java</p></li>
          <li><p> Experience with Unix/Linux, including basic commands and Shell scripting</p></li>
          <li><p> Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud</p></li>
          <li><p> Experience with distributed data, computing tools including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka</p></li>
          <li><p> Experience working on real-time data and streaming applications</p></li>
          <li><p> Experience with NoSQL implementation, including MongoDB or Cassandra</p></li>
          <li><p> Experience with data warehousing using AWS Redshift, MySQL, or Snowflake</p></li>
          <li><p> Experience with Agile engineering practices</p></li>
          <li><p> Experience with HR system data</p></li>
         </ul>
         <p></p>
         <p><b><br> Vetting:</b></p>
         <p> Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client.</p>
         <p></p>
         <p><b> Create Your Career:</b></p>
         <p><b> Grow With Us</b></p>
         <p> Your growth matters to us&#x2014;that&#x2019;s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.</p>
         <p></p>
         <p><b> A Place Where You Belong</b></p>
         <p> Diverse perspectives cultivate collective ingenuity. Booz Allen&#x2019;s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you&#x2019;ll build your community in no time.</p>
         <p></p>
         <p><b> Support Your Well-Being</b></p>
         <p> Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we&#x2019;ll support you as you pursue a balanced, fulfilling life&#x2014;at work and at home.</p>
         <p></p>
         <p><b> Your Candidate Journey</b></p>
         <p> At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we&#x2019;ve compiled a list of resources so you&#x2019;ll know what to expect as we forge a connection with you during your journey as a candidate with us.</p>
         <p></p>
         <p><b> Compensation</b></p>
         <p> At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen&#x2019;s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.</p>
         <p></p> Salary at Booz Allen is determined by various factors, including but not limited to location, the individual&#x2019;s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is &#x24;73,100.00 to &#x24;166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen&#x2019;s total compensation package for employees.
         <p></p>
         <p><b> Work Model</b><br> Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.</p>
         <ul>
          <li> If this position is listed as remote or hybrid, you&#x2019;ll periodically work from a Booz Allen or client site facility.</li>
          <li> If this position is listed as onsite, you&#x2019;ll work with colleagues and clients in person, as needed for the specific role.</li>
         </ul>
         <p></p>
         <p><b> EEO Commitment</b></p>
         <p> We&#x2019;re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change &#x2013; no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.</p>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<p></p>",https://careers.boozallen.com/jobs/JobDetail/Washington-Data-Engineer-R0181926/86617?source=JB-14400,814ba5de32777d5d,,,,,"Washington, DC",Data Engineer,1 day ago,2023-10-17T13:32:24.125Z,3.9,2512.0,"$73,100 - $166,000 a year",2023-10-18T13:32:24.148Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=814ba5de32777d5d&from=jasx&tk=1hd1fo8m3llq3800&vjs=3
22,eTeam Inc,"Post1
Job Title: Big Data EngineerDuration: 6 Months (Contract to hire)Location: Remote
Job Opportunity:

 This job designs and engineers solutions associated with analytic data for the organization and, working closely with the business, analytic and IT teams, assists with the build and upkeep for these solutions. This includes coding data ingestion, transformation, and delivery programs/locic for analysts to access operational, derived, and external data sets.
 Expected deliverables will include; coding of delivery frameworks to load and transform raw source data into enhanced analytic assets, being a key resource for analytical and big data efforts, working with architects, analysts and data scientists as needed.
 The incumbent is responsible for the operation and execution of projects related to Big Data or other analytic platforms. Leverages experience in analyzing and delivering large data sets by using a variety of delivery tools to perform tasks.
 Works in cross-functional teams from different organizations (both technical and non-technical) on projects. Provides guidance and education to Seniors and Intermediate level staff. Responsible for maintaining customer relationships. Technologies such as, but not limited to: Hadoop, Hive, NoSQL, Spark, Python, SAS, Teradata, Oracle, Informatica.
 Work closely with IT, architect and engineer solutions that provide views for Enterprise Data Objects or other analytic ecosystems. This would include working with the appropriate teams, leading the design and building out the design, and providing upkeep for the solution. Contribute to creating high performance Big Data (and traditional) systems to be used with analytic applications.
 Code, test, process, and maintain data resources for the analytics organizations. This will include working to maintain data sourcing, transformation and delivery, for key analytic platforms throughout the organization. (ETL/ELT)
 Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. Work with data science teams and strategic partners on capabilities of core platform. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams.
 Contribute to large functional efforts for programs across multiple projects. Manage relationships with customers of the function. Attend meetings with customers on a stand-alone basis or with team as needed.
 Follow standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. Keep current with Big Data technologies in order to recommend best tools in order to perform current and future work.

Required:

 Bachelor's Degree in Computer Systems Analysis, Computer Engineering, Data Processing, Healthcare Informatics or Management Information Systems

Preferred:

 Master's Degree in Management Information Systems, Healthcare Informatics or Computer Engineering .

Experience:Required -

 2 years in Data platform development, data engineering, software development, or data science
 2 years in Big data or cloud data platform

Preferred -

 2 years in the Healthcare Industry
 2 years in Data Warehousing
 2 years in Database Administration

Licenses And Certifications:

 Cloud certification (GCP, Azure, AWS)

Required Skills:

 Demonstrated skills in SQL
 Data Warehousing
 Problem-Solving
 Communication Skills
 Analytical Skills
 Demonstrated skills in Spark or Python or related tool
 Demonstrated skills in Google Cloud Technologies
 Databricks

Job Type: Contract
Salary: $50.00 - $55.00 per hour
Benefits:

 Referral program

Experience level:

 6 years

Schedule:

 8 hour shift

Experience:

 Data science: 5 years (Preferred)
 Big data: 5 years (Required)
 Data warehouse: 5 years (Required)
 Healthcare: 5 years (Required)
 SQL: 5 years (Required)

Work Location: Remote","<p>Post1</p>
<p><b>Job Title: Big Data Engineer</b><br><b>Duration: 6 Months (Contract to hire)</b><br><b>Location: Remote</b></p>
<p><b>Job Opportunity:</b></p>
<ul>
 <li>This job designs and engineers solutions associated with analytic data for the organization and, working closely with the business, analytic and IT teams, assists with the build and upkeep for these solutions. This includes coding data ingestion, transformation, and delivery programs/locic for analysts to access operational, derived, and external data sets.</li>
 <li>Expected deliverables will include; coding of delivery frameworks to load and transform raw source data into enhanced analytic assets, being a key resource for analytical and big data efforts, working with architects, analysts and data scientists as needed.</li>
 <li>The incumbent is responsible for the operation and execution of projects related to Big Data or other analytic platforms. Leverages experience in analyzing and delivering large data sets by using a variety of delivery tools to perform tasks.</li>
 <li>Works in cross-functional teams from different organizations (both technical and non-technical) on projects. Provides guidance and education to Seniors and Intermediate level staff. Responsible for maintaining customer relationships. Technologies such as, but not limited to: Hadoop, Hive, NoSQL, Spark, Python, SAS, Teradata, Oracle, Informatica.</li>
 <li>Work closely with IT, architect and engineer solutions that provide views for Enterprise Data Objects or other analytic ecosystems. This would include working with the appropriate teams, leading the design and building out the design, and providing upkeep for the solution. Contribute to creating high performance Big Data (and traditional) systems to be used with analytic applications.</li>
 <li>Code, test, process, and maintain data resources for the analytics organizations. This will include working to maintain data sourcing, transformation and delivery, for key analytic platforms throughout the organization. (ETL/ELT)</li>
 <li>Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. Work with data science teams and strategic partners on capabilities of core platform. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams.</li>
 <li>Contribute to large functional efforts for programs across multiple projects. Manage relationships with customers of the function. Attend meetings with customers on a stand-alone basis or with team as needed.</li>
 <li>Follow standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. Keep current with Big Data technologies in order to recommend best tools in order to perform current and future work.</li>
</ul>
<p><b>Required:</b></p>
<ul>
 <li>Bachelor&apos;s Degree in Computer Systems Analysis, Computer Engineering, Data Processing, Healthcare Informatics or Management Information Systems</li>
</ul>
<p><b>Preferred:</b></p>
<ul>
 <li>Master&apos;s Degree in Management Information Systems, Healthcare Informatics or Computer Engineering .</li>
</ul>
<p><b>Experience:</b><br><b>Required -</b></p>
<ul>
 <li>2 years in Data platform development, data engineering, software development, or data science</li>
 <li>2 years in Big data or cloud data platform</li>
</ul>
<p><b>Preferred </b>-</p>
<ul>
 <li>2 years in the Healthcare Industry</li>
 <li>2 years in Data Warehousing</li>
 <li>2 years in Database Administration</li>
</ul>
<p><b>Licenses And Certifications:</b></p>
<ul>
 <li>Cloud certification (GCP, Azure, AWS)</li>
</ul>
<p><b>Required Skills:</b></p>
<ul>
 <li>Demonstrated skills in SQL</li>
 <li>Data Warehousing</li>
 <li>Problem-Solving</li>
 <li>Communication Skills</li>
 <li>Analytical Skills</li>
 <li>Demonstrated skills in Spark or Python or related tool</li>
 <li>Demonstrated skills in Google Cloud Technologies</li>
 <li>Databricks</li>
</ul>
<p>Job Type: Contract</p>
<p>Salary: &#x24;50.00 - &#x24;55.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Referral program</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>6 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data science: 5 years (Preferred)</li>
 <li>Big data: 5 years (Required)</li>
 <li>Data warehouse: 5 years (Required)</li>
 <li>Healthcare: 5 years (Required)</li>
 <li>SQL: 5 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,66298b9c49650d80,,Contract,,,Remote,Big Data Engineer,1 day ago,2023-10-17T13:32:35.659Z,2.8,99.0,$50 - $55 an hour,2023-10-18T13:32:35.662Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dtmpfj98iB4C0jJJOWen3Era3IQfJzNZ4PFwBIKpo80CvlYmJYyffHOwy26mz3iNMuQaWfydZNaRB4htGwveySDwsdOtMP0srpXKAxJfXqhZah1I5V6HTeCm_4CjUcUkWhOaQhoZKcFqsUbmE_W_ayfXwg4C-EWaJkU9g2Kfk9G3u7iBrFylhZqkga7RYx9ahLi0QvkLMWN_gRZIwjTjhIeNJ8IlmHnFowBIayWeA93U4Rc7Yf15lcS_KFTVIcCzvdiFRWHQWgi0ytqgMQrajhE_T4WGznOiRaE79uVDEBEEDtNmz5ZWCy0L7uZDh9mrYXciIs2ySd29L-pb9GPtcUlh-1EhGrznwEpoCbmA-K72FOEAwr00CyV4SOGmTVdxZTYK539qXOhuQSpQTNbKhrXpMy4TD0J8ZOQE9iuCwqsaH6lEgeUo0wpAxwlsWNy776Fd0hl_k14YnoxDJI3acxiERcR32PMC1wvkNORwrZjiOV0Ehtx0saHbtstN95PhYj3W7gP6ZcxlQBh3_kcWDqQ4SF_E0ELLY01D_U6Y-U_hsnZOhXSoku7mKn6Qu-DvhxiCU9zolWfrOBvpNxowTcXoRxdGY8TNm-i7sJIyFg_w%3D%3D&xkcb=SoC8-_M3JhXpNEW-cB0LbzkdCdPP&p=0&fvj=1&vjs=3&jsa=9925&tk=1hd1fpnavjm7b800&from=jasx&wvign=1
23,Verizon,"When you join Verizon
  Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect around the world. We’re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together—lifting up our communities and striving to make an impact to move the world forward. If you’re fueled by purpose, and powered by persistence, explore a career with us. Here, you’ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.
 
  What you’ll be doing...
  As a part of our Big Data Product team, the Data Solutions Engineer will be responsible for developing and validating Big data products and applications which runs on the large Hadoop cluster and Cloud. The qualified engineer will be developing and testing ETL process, Migrating different applications to cloud , developing Data validation tools used for performing quality assessments and measurements on different data sets that feed data & AI products.
  Building big data and batch/real-time analytical solutions that leverage emerging technologies.
 
   Performing data migration and conversion activities on different applications and platforms.
   Designing, developing and testing of data ingestion pipelines, perform end to end automation of ETL process for various datasets that are being ingested into the big data platform.
   Performing data profiling/analysis, discovery, analysis, suitability and coverage of data, and identifying the various data types, formats, and data quality issues which exist within a given data source.
   Developing transformation logic, interfaces and reports as needed to meet project requirements.
   Participating in discussion for technical architecture, data modeling and ETL standards, collaborate with Product Managers, Architects and Senior Developers to establish the physical application framework (e.g. libraries, modules, execution environments).
   Improving and performance-tuning the optimization of data pipelines.
   Developing unit and integrated automated test suites to validate end to end data pipeline flow, data transformation rules, and data integrity.
   Developing tools to measure the data quality and visualize the anomaly pattern in source and processed data.
   Integrating automated processes into continuous integration workflows.
   Contributing to data quality assurance standards and procedures.
 
 
  What we’re looking for...
  You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems. You thrive in a fast-paced, innovative environment working as a phenomenal teammate to drive the best results and business outcomes.
 
  You'll need to have:
 
   Bachelor's degree or four or more years of work experience.
   Four or more years of relevant work experience.
   Programming experience in Scala, Java or Python
   Experience in data engineering, preferably in Google Cloud Platform with BigQuery
   Hands-on experience in designing, building, testing and deploying data pipelines in Teradata and Hadoop platform with experience in, HDFS, Hive, Spark, Streaming, HBase, Kafka, Oozie etc.
   Good organizational skills and strong written and verbal communication skills.
 
 
  Even better if you have one or more of the following:
 
   Bachelor’s degree in Computer Science or equivalent education/training
   Five or more years of Software development and Testing experience.
   Experience with development and automated testing in a CI/CD environment.
   Hands on experience in dashboard development using Looker/Tableau
   Knowledge of GIT/Jenkins and pipeline automation is a must.
   Experience with developing and testing real-time data-processing and Analytics Application Systems.
   Strong knowledge in SQL development on Database and/or BI/DW
   Strong knowledge in shell scripting Experience in Web Services - API development and testing.
   A solid understanding of common software development practices and tools.
   Strong analytical skills with a methodical approach to problem solving applied to the Big Data/AI domain.
   Experience with incident management tools (Opsgenie, PagerDuty etc) is a plus
 
 
  If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.
 
  This role is eligible to be considered for the Department of Defense SkillBridge Program.
 
 
  
   
    
     
      
       
         Where you’ll be working
       
      
     
    
   
  
  In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
 
  Scheduled Weekly Hours 40
 
  Equal Employment Opportunity
  We’re proud to be an equal opportunity employer - and celebrate our employees’ differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.
  Our benefits are designed to help you move forward in your career, and in areas of your life outside of Verizon. From health and wellness benefits, short term incentives, 401(k) Savings Plan, stock incentive programs, paid time off, parental leave, adoption assistance and tuition assistance, plus other incentives, we’ve got you covered with our award-winning total rewards package. For part-timers, your coverage will vary as you may be eligible for some of these benefits depending on your individual circumstances.
  If you are hired into a California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington work location, the compensation range for this position is between $113,000.00 and $210,000.00 annually based on a full-time schedule. The salary will vary depending on your location and confirmed job-related skills and experience. This is an incentive based position with the potential to earn more. For part time roles, your compensation will be adjusted to reflect your hours.","<div>
 <h3 class=""jobSectionHeader""><b>When you join Verizon</b></h3>
 <p> Verizon is one of the world&#x2019;s leading providers of technology and communications services, transforming the way we connect around the world. We&#x2019;re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together&#x2014;lifting up our communities and striving to make an impact to move the world forward. If you&#x2019;re fueled by purpose, and powered by persistence, explore a career with us. Here, you&#x2019;ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.</p>
 <p></p>
 <h3 class=""jobSectionHeader""><b> What you&#x2019;ll be doing...</b></h3>
 <p> As a part of our Big Data Product team, the Data Solutions Engineer will be responsible for developing and validating Big data products and applications which runs on the large Hadoop cluster and Cloud. The qualified engineer will be developing and testing ETL process, Migrating different applications to cloud , developing Data validation tools used for performing quality assessments and measurements on different data sets that feed data &amp; AI products.</p>
 <p> Building big data and batch/real-time analytical solutions that leverage emerging technologies.</p>
 <ul>
  <li><p> Performing data migration and conversion activities on different applications and platforms.</p></li>
  <li><p> Designing, developing and testing of data ingestion pipelines, perform end to end automation of ETL process for various datasets that are being ingested into the big data platform.</p></li>
  <li><p> Performing data profiling/analysis, discovery, analysis, suitability and coverage of data, and identifying the various data types, formats, and data quality issues which exist within a given data source.</p></li>
  <li><p> Developing transformation logic, interfaces and reports as needed to meet project requirements.</p></li>
  <li><p> Participating in discussion for technical architecture, data modeling and ETL standards, collaborate with Product Managers, Architects and Senior Developers to establish the physical application framework (e.g. libraries, modules, execution environments).</p></li>
  <li><p> Improving and performance-tuning the optimization of data pipelines.</p></li>
  <li><p> Developing unit and integrated automated test suites to validate end to end data pipeline flow, data transformation rules, and data integrity.</p></li>
  <li><p> Developing tools to measure the data quality and visualize the anomaly pattern in source and processed data.</p></li>
  <li><p> Integrating automated processes into continuous integration workflows.</p></li>
  <li><p> Contributing to data quality assurance standards and procedures.</p></li>
 </ul>
 <p></p>
 <h3 class=""jobSectionHeader""><b> What we&#x2019;re looking for...</b></h3>
 <p> You&#x2019;re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems. You thrive in a fast-paced, innovative environment working as a phenomenal teammate to drive the best results and business outcomes.</p>
 <p></p>
 <p> You&apos;ll need to have:</p>
 <ul>
  <li><p> Bachelor&apos;s degree or four or more years of work experience.</p></li>
  <li><p> Four or more years of relevant work experience.</p></li>
  <li><p> Programming experience in Scala, Java or Python</p></li>
  <li><p> Experience in data engineering, preferably in Google Cloud Platform with BigQuery</p></li>
  <li><p> Hands-on experience in designing, building, testing and deploying data pipelines in Teradata and Hadoop platform with experience in, HDFS, Hive, Spark, Streaming, HBase, Kafka, Oozie etc.</p></li>
  <li><p> Good organizational skills and strong written and verbal communication skills.</p></li>
 </ul>
 <p></p>
 <p> Even better if you have one or more of the following:</p>
 <ul>
  <li><p> Bachelor&#x2019;s degree in Computer Science or equivalent education/training</p></li>
  <li><p> Five or more years of Software development and Testing experience.</p></li>
  <li><p> Experience with development and automated testing in a CI/CD environment.</p></li>
  <li><p> Hands on experience in dashboard development using Looker/Tableau</p></li>
  <li><p> Knowledge of GIT/Jenkins and pipeline automation is a must.</p></li>
  <li><p> Experience with developing and testing real-time data-processing and Analytics Application Systems.</p></li>
  <li><p> Strong knowledge in SQL development on Database and/or BI/DW</p></li>
  <li><p> Strong knowledge in shell scripting Experience in Web Services - API development and testing.</p></li>
  <li><p> A solid understanding of common software development practices and tools.</p></li>
  <li><p> Strong analytical skills with a methodical approach to problem solving applied to the Big Data/AI domain.</p></li>
  <li><p> Experience with incident management tools (Opsgenie, PagerDuty etc) is a plus</p></li>
 </ul>
 <p></p>
 <p> If Verizon and this role sound like a fit for you, we encourage you to apply even if you don&#x2019;t meet every &#x201c;even better&#x201d; qualification listed above.</p>
 <p></p>
 <p> This role is eligible to be considered for the Department of Defense SkillBridge Program.</p>
 <p></p>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <h3 class=""jobSectionHeader""><b> Where you&#x2019;ll be working</b></h3>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div> In this hybrid role, you&apos;ll have a defined work location that includes work from home and assigned office days set by your manager.
 <p></p>
 <h3 class=""jobSectionHeader""><b> Scheduled Weekly Hours</b></h3> 40
 <p></p>
 <h3 class=""jobSectionHeader""><b> Equal Employment Opportunity</b></h3>
 <p> We&#x2019;re proud to be an equal opportunity employer - and celebrate our employees&#x2019; differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.</p>
 <p></p> Our benefits are designed to help you move forward in your career, and in areas of your life outside of Verizon. From health and wellness benefits, short term incentives, 401(k) Savings Plan, stock incentive programs, paid time off, parental leave, adoption assistance and tuition assistance, plus other incentives, we&#x2019;ve got you covered with our award-winning total rewards package. For part-timers, your coverage will vary as you may be eligible for some of these benefits depending on your individual circumstances.
 <p></p> If you are hired into a California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington work location, the compensation range for this position is between &#x24;113,000.00 and &#x24;210,000.00 annually based on a full-time schedule. The salary will vary depending on your location and confirmed job-related skills and experience. This is an incentive based position with the potential to earn more. For part time roles, your compensation will be adjusted to reflect your hours.
</div>
<p></p>",https://mycareer.verizon.com/jobs/r-1018721/senior-data-engineer/?source=jb-indeed&dclid=CL2MoeTa_4EDFYbMwAodcIQDxQ,b15ec8b380fddd07,,Full-time,,,"340 Washington St, Boston, MA 02108",Senior Data Engineer,1 day ago,2023-10-17T13:32:36.095Z,3.8,32072.0,"$113,000 - $210,000 a year",2023-10-18T13:32:36.098Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=b15ec8b380fddd07&from=jasx&tk=1hd1fpkd42j42000&vjs=3
24,Reality Defender,"** No firms - we cannot work with firms due to regulatory reasons.**
 
 
 
   Reality Defender seeks a forward deployed data engineer to join the Data Engineering team. You would work on product-oriented data infrastructure development for in-the-wild deepfake media detection, with an emphasis on engaging directly with clients and facilitating communication between technical and non-technical teams.
 
 
 
   #LI-Remote
  
 Responsibilities
 
   Building scalable robust infrastructure for data ingestion, storage, and sampling.
   Communicate complex technical concepts effectively to client executives, ensuring alignment between technical implementations and organizational objectives.
   Develop custom data tools tailored to meet specific client requirements, including adapting internally developed solutions to meet clients' needs.
   Create and maintain comprehensive technical documentation, including APIs, algorithms, and system architecture.
   Provide technical support to resolve complex issues escalated from customer support teams. Collaborate with cross-functional teams to diagnose and troubleshoot production incidents, and report results back to the customer in clear non-technical language.
 
  Requirements
 
   We encourage candidates who may not meet all the specified requirements to still apply. We value diverse perspectives and skills, and believe that unique experiences can contribute significantly to our team. If you are passionate about the role and confident in your ability to make a meaningful impact, we welcome your application. Your enthusiasm, adaptability, and potential for growth are equally important to us. Please use your cover letter to elaborate on how your background and experience make you an ideal fit for this role!
 
 
 
   Required
 
 
   3+ years of professional experience in software/data science and a bachelor's or master's degree in computer science, engineering, math, or STEM discipline.
   Strong communication skills.
   Proficiency in Python, NodeJs, Typescript, with a strong emphasis on adapting scalable software solutions to customer needs.
   Database experience, particularly NoSQL databases (MongoDB, DynamoDB, etc).
 
 
 
   Nice to have
 
 
   Interest in data exploration, visualization, cleaning, and analytics for real-world data modeling.
   Solid understanding of linear algebra, statistics and deep learning concepts.
   Experience working with audio, visual, and/or text datasets and models.
   Experience with AWS, Google Cloud, Azure, and On-Premises.
   Experience working with very large databases and deep learning APIs, including Pandas, PyTorch, PySpark, etc. 
  Highly organized, detail-oriented, and possess a proven ability to thrive under deadline pressure.
 
 
 
   Additional Requirements
 
 
   Willing to work extended hours when needed.
   Willing to occasionally work from or travel to client’s location.","<div>
 <div>
  <b>** No firms - we cannot work with firms due to regulatory reasons.**</b>
 </div>
 <div></div>
 <div>
  <br> Reality Defender seeks a forward deployed data engineer to join the Data Engineering team. You would work on product-oriented data infrastructure development for in-the-wild deepfake media detection, with an emphasis on engaging directly with clients and facilitating communication between technical and non-technical teams.
 </div>
 <div></div>
 <div>
  <br> #LI-Remote
 </div> 
 <h3 class=""jobSectionHeader""><b>Responsibilities</b></h3>
 <ul>
  <li> Building scalable robust infrastructure for data ingestion, storage, and sampling.</li>
  <li> Communicate complex technical concepts effectively to client executives, ensuring alignment between technical implementations and organizational objectives.</li>
  <li> Develop custom data tools tailored to meet specific client requirements, including adapting internally developed solutions to meet clients&apos; needs.</li>
  <li> Create and maintain comprehensive technical documentation, including APIs, algorithms, and system architecture.</li>
  <li> Provide technical support to resolve complex issues escalated from customer support teams. Collaborate with cross-functional teams to diagnose and troubleshoot production incidents, and report results back to the customer in clear non-technical language.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Requirements</b></h3>
 <ul>
  <li> We encourage candidates who may not meet all the specified requirements to still apply. We value diverse perspectives and skills, and believe that unique experiences can contribute significantly to our team. If you are passionate about the role and confident in your ability to make a meaningful impact, we welcome your application. Your enthusiasm, adaptability, and potential for growth are equally important to us. Please use your cover letter to elaborate on how your background and experience make you an ideal fit for this role!</li>
 </ul>
 <div></div>
 <div>
  <b><br> Required</b>
 </div>
 <ul>
  <li> 3+ years of professional experience in software/data science and a bachelor&apos;s or master&apos;s degree in computer science, engineering, math, or STEM discipline.</li>
  <li> Strong communication skills.</li>
  <li> Proficiency in Python, NodeJs, Typescript, with a strong emphasis on adapting scalable software solutions to customer needs.</li>
  <li> Database experience, particularly NoSQL databases (MongoDB, DynamoDB, etc).</li>
 </ul>
 <div></div>
 <div>
  <b><br> Nice to have</b>
 </div>
 <ul>
  <li> Interest in data exploration, visualization, cleaning, and analytics for real-world data modeling.</li>
  <li> Solid understanding of linear algebra, statistics and deep learning concepts.</li>
  <li> Experience working with audio, visual, and/or text datasets and models.</li>
  <li> Experience with AWS, Google Cloud, Azure, and On-Premises.</li>
  <li> Experience working with very large databases and deep learning APIs, including Pandas, PyTorch, PySpark, etc. </li>
  <li>Highly organized, detail-oriented, and possess a proven ability to thrive under deadline pressure.</li>
 </ul>
 <div></div>
 <div>
  <b><br> Additional Requirements</b>
 </div>
 <ul>
  <li> Willing to work extended hours when needed.</li>
  <li> Willing to occasionally work from or travel to client&#x2019;s location.</li>
 </ul>
</div>",https://jobs.lever.co/realitydefender/73112d98-7a9a-4800-9921-6576fdd1f0df?lever-source=Indeed,4eb3da80458c4c7f,,Full-time,,,"New York, NY",Forward Deployed Data Engineer - Remote,1 day ago,2023-10-17T13:32:38.660Z,,,"$100,000 - $200,000 a year",2023-10-18T13:32:38.748Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=4eb3da80458c4c7f&from=jasx&tk=1hd1fpkd42j42000&vjs=3
25,ServiceNow,"Company Description
  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. 
 With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. 
 Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. 
 Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.
  Job Description
  As a Senior Staff Data and Software Engineer, you will be responsible for developing and implementing cutting-edge technical solutions that align with our organization's business objectives. You will work closely with stakeholders to understand their needs, assess existing systems and infrastructure, and design robust and scalable data and mircroservices solutions that drive innovation and efficiency. Your role will require a combination of technical expertise, strategic thinking, and effective communication to bridge the gap between business and technology.
 
  Key Responsibilities:
 
   Solution Design: Collaborate with business leaders, project managers, and technical teams to understand requirements and design holistic technical solutions that address current and future needs.
   Architecture Planning: Develop and maintain technology roadmaps, ensuring alignment with organizational goals and industry best practices.
   Technical Leadership: Provide technical leadership and guidance to development teams, ensuring adherence to architectural standards and best practices.
   Risk Assessment: Identify and evaluate technical risks and propose mitigation strategies to ensure project success and data security.
   Documentation: Create and maintain comprehensive architecture documentation, including diagrams, guidelines, and standards for development teams to follow.
   Vendor Evaluation: Assess and recommend third-party tools, products, and services that can enhance our technical solutions.
   Prototyping: Develop proof-of-concept and prototype solutions to validate architectural decisions and demonstrate feasibility.
   Performance Optimization: Continuously monitor and analyze system performance, identifying areas for improvement and optimizing existing solutions.
   Security and Compliance: Ensure that solutions comply with industry regulations and security standards, and proactively address security vulnerabilities.
   Collaboration: Foster collaboration and effective communication between cross-functional teams, promoting a culture of innovation and excellence.
 
  
  Qualifications
  Qualifications: 
 
  Bachelor's degree in Computer Science, Information Technology, or related field (Master's degree preferred). 
  Proven experience as a Lead Engineer and Solution Architect or a similar role. 
  Strong knowledge of enterprise architecture principles and best practices. 
  Proficiency in designing and implementing solutions using various technologies and platforms. 
  Excellent problem-solving and analytical skills. 
  Outstanding communication and interpersonal abilities. 
  Project management skills and experience in managing complex technical projects. 
  Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus. 
 
 Preferred Skills: 
 
  Cloud computing expertise (e.g., AWS, Azure, Google Cloud Platform). 
  Knowledge of DevOps practices and tools. 
  Familiarity with microservices architecture, expertise a plus. 
  Familiarity with graph databases, expertise a plus. 
  Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes). 
  Strong understanding of data architecture and database technologies. 
  Knowledge of cybersecurity best practices. 
  Excellent presentation and facilitation skills. 
 For positions in the Bay Area, we offer a base pay of $184,700 - $323,300, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.
  
 Additional Information
  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. 
 At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. 
 If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. 
 For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. 
 Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.
  
 From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license. 
 Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.","<div>
 <b>Company Description</b>
 <p><br> At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can&#x2019;t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.</p> 
 <p>With more than 7,700+ customers, we serve approximately 85% of the Fortune 500&#xae;, and we&apos;re proud to be one of FORTUNE 100 Best Companies to Work For&#xae; and World&apos;s Most Admired Companies&#x2122;.</p> 
 <p>Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.</p> 
 <p>Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.</p>
 <b><br> Job Description</b>
 <p><br> As a Senior Staff Data and Software Engineer, you will be responsible for developing and implementing cutting-edge technical solutions that align with our organization&apos;s business objectives. You will work closely with stakeholders to understand their needs, assess existing systems and infrastructure, and design robust and scalable data and mircroservices solutions that drive innovation and efficiency. Your role will require a combination of technical expertise, strategic thinking, and effective communication to bridge the gap between business and technology.</p>
 <p></p>
 <p><b><br> Key Responsibilities:</b></p>
 <ul>
  <li><b> Solution Design:</b> Collaborate with business leaders, project managers, and technical teams to understand requirements and design holistic technical solutions that address current and future needs.</li>
  <li><b> Architecture Planning:</b> Develop and maintain technology roadmaps, ensuring alignment with organizational goals and industry best practices.</li>
  <li><b> Technical Leadership:</b> Provide technical leadership and guidance to development teams, ensuring adherence to architectural standards and best practices.</li>
  <li><b> Risk Assessment:</b> Identify and evaluate technical risks and propose mitigation strategies to ensure project success and data security.</li>
  <li><b> Documentation:</b> Create and maintain comprehensive architecture documentation, including diagrams, guidelines, and standards for development teams to follow.</li>
  <li><b> Vendor Evaluation:</b> Assess and recommend third-party tools, products, and services that can enhance our technical solutions.</li>
  <li><b> Prototyping:</b> Develop proof-of-concept and prototype solutions to validate architectural decisions and demonstrate feasibility.</li>
  <li><b> Performance Optimization:</b> Continuously monitor and analyze system performance, identifying areas for improvement and optimizing existing solutions.</li>
  <li><b> Security and Compliance:</b> Ensure that solutions comply with industry regulations and security standards, and proactively address security vulnerabilities.</li>
  <li><b> Collaboration:</b> Foster collaboration and effective communication between cross-functional teams, promoting a culture of innovation and excellence.</li>
 </ul>
 <br> 
 <b> Qualifications</b>
 <p><b><br> Qualifications:</b></p> 
 <ul>
  <li>Bachelor&apos;s degree in Computer Science, Information Technology, or related field (Master&apos;s degree preferred).</li> 
  <li>Proven experience as a Lead Engineer and Solution Architect or a similar role.</li> 
  <li>Strong knowledge of enterprise architecture principles and best practices.</li> 
  <li>Proficiency in designing and implementing solutions using various technologies and platforms.</li> 
  <li>Excellent problem-solving and analytical skills.</li> 
  <li>Outstanding communication and interpersonal abilities.</li> 
  <li>Project management skills and experience in managing complex technical projects.</li> 
  <li>Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus.</li> 
 </ul>
 <p><b>Preferred Skills:</b></p> 
 <ul>
  <li>Cloud computing expertise (e.g., AWS, Azure, Google Cloud Platform).</li> 
  <li>Knowledge of DevOps practices and tools.</li> 
  <li>Familiarity with microservices architecture, expertise a plus.</li> 
  <li>Familiarity with graph databases, expertise a plus.</li> 
  <li>Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes).</li> 
  <li>Strong understanding of data architecture and database technologies.</li> 
  <li>Knowledge of cybersecurity best practices.</li> 
  <li>Excellent presentation and facilitation skills.</li> 
 </ul>For positions in the Bay Area, we offer a base pay of &#x24;184,700 - &#x24;323,300, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.
 <br> 
 <b>Additional Information</b>
 <p><br> ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.</p> 
 <p>At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.</p> 
 <p>If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.</p> 
 <p>For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.</p> 
 <p>Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.</p>
 <p><br> </p>
 <p>From Fortune. &#xa9; 2022 Fortune Media IP Limited All rights reserved. Used under license.</p> 
 <p>Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.</p>
</div>",https://careers.servicenow.com/careers/jobs/743999937606270EXT?lang=en-us&trid=35ab2906-b356-4a56-8472-f60d30d2e2f0,c9f696550a62823e,,Full-time,,,"Chicago, IL 60607",Senior Staff Data and Services Software Engineer,1 day ago,2023-10-17T13:32:41.640Z,3.7,239.0,"$184,700 - $323,300 a year",2023-10-18T13:32:41.642Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=c9f696550a62823e&from=jasx&tk=1hd1fpkd42j42000&vjs=3
26,LaTronic Solutions LLC,"DATA ENGINEER
***Candidate Must have an Active DoD Secret clearance***
Responsible for the engineering of big data solutions and multi-tiered data environments. Experience with large scale big data deployments in Government and large commercial environments preferred.
Key requirements include:

 Leads initiatives utilizing big data solutions to provide actionable insights for addressing strategic and tactical mission objectives.
 Experience with Hadoop-based technologies (Cloudera, Hortonworks, MapReduce, Hive, HDFS).
 Experience with NoSQL technologies (e.g., MongoDB, Cassandra) (d) Ability to design data models using Enterprise Data modeling tools (ERWin, Visio).
 Builds high-performance algorithms, prototypes, and data models using required programing languages (e.g., Python, C/C++, Java, Perl, Scala).
 Analyzes and develops data set processes for data ingestion, modeling, mining. Experience integrating Big Data solutions with SAP technologies.

Minimum Experience:
Senior Level: 5+ years of relevant experience.
Must be experienced in the following or equivalent technologies:

 Python
 Flask, FastAPI, or other microweb framework equivalent
 An Object Relational Mapping toolkit like SQLAlchemy or equivalent
 Github
 Data analysis libraries such as Pandas.
 AWS S3 - Used as code triggers and intermediate result storage
 CloudTrail - Used to analyze logs
 Lambda - Used as serverless compute for small functions
 ECS Fargate - Used as serverless compute for large functions. Infrastructure:
 Terraform - Used to manage AWS infrastructure.

Minimum Education:
Undergraduate degree required.
Required:
Must have DoD Secret clearance
Must possess IT-II security clearance or have a current National Agency Check with Local Agency Check and Credit Check (NACLC
Job Types: Full-time, Contract
Pay: From $130,000.00 per year
Compensation package:

 1099 contract

Experience level:

 5 years

Schedule:

 8 hour shift

Experience:

 ETL: 4 years (Preferred)
 Big data: 4 years (Preferred)
 Data science: 4 years (Preferred)

Security clearance:

 Secret (Required)

Ability to Commute:

 Remote (Preferred)

Work Location: Remote","<p><b>DATA ENGINEER</b></p>
<p><b>***Candidate Must have an Active DoD Secret clearance***</b></p>
<p>Responsible for the engineering of big data solutions and multi-tiered data environments. Experience with large scale big data deployments in Government and large commercial environments preferred.</p>
<p>Key requirements include:</p>
<ul>
 <li>Leads initiatives utilizing big data solutions to provide actionable insights for addressing strategic and tactical mission objectives.</li>
 <li>Experience with Hadoop-based technologies (Cloudera, Hortonworks, MapReduce, Hive, HDFS).</li>
 <li>Experience with NoSQL technologies (e.g., MongoDB, Cassandra) (d) Ability to design data models using Enterprise Data modeling tools (ERWin, Visio).</li>
 <li>Builds high-performance algorithms, prototypes, and data models using required programing languages (e.g., Python, C/C++, Java, Perl, Scala).</li>
 <li>Analyzes and develops data set processes for data ingestion, modeling, mining. Experience integrating Big Data solutions with SAP technologies.</li>
</ul>
<p>Minimum Experience:</p>
<p>Senior Level: 5+ years of relevant experience.</p>
<p>Must be experienced in the following or equivalent technologies:</p>
<ul>
 <li>Python</li>
 <li>Flask, FastAPI, or other microweb framework equivalent</li>
 <li>An Object Relational Mapping toolkit like SQLAlchemy or equivalent</li>
 <li>Github</li>
 <li>Data analysis libraries such as Pandas.</li>
 <li>AWS S3 - Used as code triggers and intermediate result storage</li>
 <li>CloudTrail - Used to analyze logs</li>
 <li>Lambda - Used as serverless compute for small functions</li>
 <li>ECS Fargate - Used as serverless compute for large functions. Infrastructure:</li>
 <li>Terraform - Used to manage AWS infrastructure.</li>
</ul>
<p>Minimum Education:</p>
<p>Undergraduate degree required.</p>
<p>Required:</p>
<p>Must have DoD Secret clearance</p>
<p>Must possess IT-II security clearance or have a current National Agency Check with Local Agency Check and Credit Check (NACLC</p>
<p>Job Types: Full-time, Contract</p>
<p>Pay: From &#x24;130,000.00 per year</p>
<p>Compensation package:</p>
<ul>
 <li>1099 contract</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>ETL: 4 years (Preferred)</li>
 <li>Big data: 4 years (Preferred)</li>
 <li>Data science: 4 years (Preferred)</li>
</ul>
<p>Security clearance:</p>
<ul>
 <li>Secret (Required)</li>
</ul>
<p>Ability to Commute:</p>
<ul>
 <li>Remote (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,d9235c7fdf0bf5f3,,Full-time,Contract,,Remote,Data Engineer,4 days ago,2023-10-14T13:32:55.697Z,,,"From $130,000 a year",2023-10-18T13:32:55.700Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d9235c7fdf0bf5f3&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
27,INTEL,"Job Description
  Intel is on a multi-year transformational journey to unlock the full potential of enterprise data to strengthen and grow the business. The Core Data Solutions (CDS) group within the Intel IT business segment is chartered with delivering and maintaining world-class data and analytics capabilities that equip the business to capture value through consistent, accurate, reliable, and high-quality data and cutting-edge data, reporting, and analytical solutions.
 
  Key focus areas for the Data and Analytics Group include: maturing enterprise data governance, leveraging master data management, establishing an enterprise data foundation, and delivering strategic data assets and insights.
 
  We are seeking a passionate senior MDM software engineer to focus on designing Enterprise Master Data systems using Reltio, SAP and middle layer technologies consistent with the overall design of the organization's information systems architecture including:
 
 
   Collecting and analyzing information from users to formulate the scope and objectives of the system
   Preparing flow charts, models, and procedures and conducting feasibility studies to design possible system solutions
   Preparing and maintaining technical documentation to guide system users and to assist with the ongoing operation, maintenance, and development of the system
   Leverages Agile methodology to design and configure solutions. Works closely with the business to develop required training, change management, and operations processes
   Develop use cases, customer scenarios, and-or demos, plan and coordinate testing of the newly developed or enhanced applications and provide support
   Looks for opportunities to automate business processes with technology, and may provide consultation to users in the area of automated systems
   Information transformation engagements
   Presenting technical or new concepts across multiple levels of a global organization
   Working with and collaborating with all IT and biz teams toward developing a best practice data creation maintenance governance quality and efficiencies
 
  Qualifications
  You must possess the below minimum qualifications to be initially considered for this position. Preferred qualifications are in addition to the minimum requirements and are considered a plus factor in identifying top candidates.
 
  Minimum Qualifications
 
  The candidate must have a Bachelor's Degree in Computer Science, Information Management, or any other related field with 6+ years of experience -OR- a Master's Degree in Computer Science, Information Management, or any other related field and 4+ years of experience -OR- a PhD in Computer Science, Information Management, or any other related field and 2+ years of experience in: 
 
  System configuration and / or development on Cloud MDM, Reltio or SAP
   SaaS, Cloud Software, Software Configuration, Software Development, SAP MDG, Master Data Management
 
 
  Preferred Qualifications
 
   Managing or executing data cleansing data mapping and data governance areas as well as integration across complex ERP landscapes
   Assisting in developing normalizing and maintaining data standards and definitions.
   Developing processes tools and integration for Master Data Processes including data cleansing and data validation
   Mapping of Master data and integration from the legacy environment to the target environment
   Configuring and managing MDM entities for one or more master data domain
   Denodo and / or Mulesoft
 
  Inside this Business Group
  Intel's Information Technology Group (IT) designs, deploys and supports the information technology architecture and hardware/software applications for Intel. This includes the LAN, WAN, telephony, data centers, client PCs, backup and restore, and enterprise applications. IT is also responsible for e-Commerce development, data hosting and delivery of Web content and services.
  
  Covid Statement
  Intel strongly encourages employees to be vaccinated against COVID-19. Intel aligns to federal, state, and local laws and as a contractor to the U.S. Government is subject to government mandates that may be issued. Intel policies for COVID-19 including guidance about testing and vaccination are subject to change over time.
  
  Posting Statement
  All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.
  
  Benefits
  We offer a total compensation package that ranks among the best in the industry. It consists of competitive pay, stock, bonuses, as well as, benefit programs which include health, retirement, and vacation. Find more information about all of our Amazing Benefits here: https://www.intel.com/content/www/us/en/jobs/benefits.html
  
  Annual Salary Range for jobs which could be performed in US, Colorado, New York, Washington, California: $139,480.00-$209,760.00
  
 
  Salary range dependent on a number of factors including location and experience
 
  Working Model
  This role is available as a fully home-based and generally would require you to attend Intel sites only occasionally based on business need. This role may also be available as our hybrid work model which allows employees to split their time between working on-site at their assigned Intel site and off-site. 
 In certain circumstances the work model may change to accommodate business needs.
  JobType 
 Fully Remote","<p></p>
<div>
 <h2 class=""jobSectionHeader""><b>Job Description</b></h2>
 <p><br> Intel is on a multi-year transformational journey to unlock the full potential of enterprise data to strengthen and grow the business. The Core Data Solutions (CDS) group within the Intel IT business segment is chartered with delivering and maintaining world-class data and analytics capabilities that equip the business to capture value through consistent, accurate, reliable, and high-quality data and cutting-edge data, reporting, and analytical solutions.</p>
 <p></p>
 <p> Key focus areas for the Data and Analytics Group include: maturing enterprise data governance, leveraging master data management, establishing an enterprise data foundation, and delivering strategic data assets and insights.</p>
 <p></p>
 <p> We are seeking a passionate senior MDM software engineer to focus on designing Enterprise Master Data systems using Reltio, SAP and middle layer technologies consistent with the overall design of the organization&apos;s information systems architecture including:</p>
 <p></p>
 <ul>
  <li><p> Collecting and analyzing information from users to formulate the scope and objectives of the system</p></li>
  <li><p> Preparing flow charts, models, and procedures and conducting feasibility studies to design possible system solutions</p></li>
  <li><p> Preparing and maintaining technical documentation to guide system users and to assist with the ongoing operation, maintenance, and development of the system</p></li>
  <li><p> Leverages Agile methodology to design and configure solutions. Works closely with the business to develop required training, change management, and operations processes</p></li>
  <li><p> Develop use cases, customer scenarios, and-or demos, plan and coordinate testing of the newly developed or enhanced applications and provide support</p></li>
  <li><p> Looks for opportunities to automate business processes with technology, and may provide consultation to users in the area of automated systems</p></li>
  <li><p> Information transformation engagements</p></li>
  <li><p> Presenting technical or new concepts across multiple levels of a global organization</p></li>
  <li><p> Working with and collaborating with all IT and biz teams toward developing a best practice data creation maintenance governance quality and efficiencies</p></li>
 </ul>
 <h2 class=""jobSectionHeader""><b><br> Qualifications</b></h2>
 <p><br> You must possess the below minimum qualifications to be initially considered for this position. Preferred qualifications are in addition to the minimum requirements and are considered a plus factor in identifying top candidates.</p>
 <p></p>
 <p><b><br> Minimum Qualifications</b></p>
 <p></p>
 <p><br> The candidate must have a Bachelor&apos;s Degree in Computer Science, Information Management, or any other related field with 6+ years of experience -OR- a Master&apos;s Degree in Computer Science, Information Management, or any other related field and 4+ years of experience -OR- a PhD in Computer Science, Information Management, or any other related field and 2+ years of experience in: </p>
 <ul>
  <li><p>System configuration and / or development on Cloud MDM, Reltio or SAP</p></li>
  <li><p> SaaS, Cloud Software, Software Configuration, Software Development, SAP MDG, Master Data Management</p></li>
 </ul>
 <p></p>
 <p><b> Preferred Qualifications</b></p>
 <ul>
  <li><p> Managing or executing data cleansing data mapping and data governance areas as well as integration across complex ERP landscapes</p></li>
  <li><p> Assisting in developing normalizing and maintaining data standards and definitions.</p></li>
  <li><p> Developing processes tools and integration for Master Data Processes including data cleansing and data validation</p></li>
  <li><p> Mapping of Master data and integration from the legacy environment to the target environment</p></li>
  <li><p> Configuring and managing MDM entities for one or more master data domain</p></li>
  <li><p> Denodo and / or Mulesoft</p></li>
 </ul>
 <h2 class=""jobSectionHeader""><b><br> Inside this Business Group</b></h2>
 <br> Intel&apos;s Information Technology Group (IT) designs, deploys and supports the information technology architecture and hardware/software applications for Intel. This includes the LAN, WAN, telephony, data centers, client PCs, backup and restore, and enterprise applications. IT is also responsible for e-Commerce development, data hosting and delivery of Web content and services.
 <br> 
 <h2 class=""jobSectionHeader""><b> Covid Statement</b></h2>
 <br> Intel strongly encourages employees to be vaccinated against COVID-19. Intel aligns to federal, state, and local laws and as a contractor to the U.S. Government is subject to government mandates that may be issued. Intel policies for COVID-19 including guidance about testing and vaccination are subject to change over time.
 <br> 
 <h2 class=""jobSectionHeader""><b> Posting Statement</b></h2>
 <br> All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.
 <br> 
 <h2 class=""jobSectionHeader""><b> Benefits</b></h2>
 <br> We offer a total compensation package that ranks among the best in the industry. It consists of competitive pay, stock, bonuses, as well as, benefit programs which include health, retirement, and vacation. Find more information about all of our Amazing Benefits here: https://www.intel.com/content/www/us/en/jobs/benefits.html
 <br> 
 <br> Annual Salary Range for jobs which could be performed in US, Colorado, New York, Washington, California: &#x24;139,480.00-&#x24;209,760.00
 <br> 
 <ul>
  <li>Salary range dependent on a number of factors including location and experience</li>
 </ul>
 <h2 class=""jobSectionHeader""><b><br> Working Model</b></h2>
 <br> This role is available as a fully home-based and generally would require you to attend Intel sites only occasionally based on business need. This role may also be available as our hybrid work model which allows employees to split their time between working on-site at their assigned Intel site and off-site. 
 <b>In certain circumstances the work model may change to accommodate business needs.</b>
 <p><br> JobType </p>
 <p>Fully Remote</p>
</div>",https://jobs.intel.com/en/job/-/-/599/55687681968,705fe8793dda2171,,Full-time,,,Remote,Customer and Supplier Master Data Software Engineer,4 days ago,2023-10-14T13:32:52.075Z,4.1,6140.0,"$139,480 - $209,760 a year",2023-10-18T13:32:52.078Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=705fe8793dda2171&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
28,Epsilon Inc.,"Big Data Engineer   Who is Epsilon:  Epsilon is an IT Services company that was founded in 2009 and has become an established leader in providing Information Technology services to both Federal Government and Commercial businesses across the United States. Epsilon is known for its solution-focused and innovative approach, aligning technology systems, tools, and processes with the missions and objectives of its customers.  Epsilon’s headquarters are in Weaverville, NC with other corporate offices in Greenville, SC, Crystal City, VA, and Denver, CO. We have employees in 30+ States across the U.S.  Why work for Epsilon: In joining Epsilon’s team, you will have the opportunity to contribute to Epsilon’s business and customer initiatives, as well as influence our brand culture through people interaction and technology advancements.  Epsilon invests in our employees by promoting from within and enabling employees to elevate their knowledge and skill set in their profession by allocating $3,000 annually in Professional Development funds. We also offer competitive pay, comprehensive benefits through one of the largest national carriers, Paid Time Off (PTO) that increases with tenure and has a generous rollover, 11 company paid Holidays, and 401(k) with immediate contribution.  Where you’ll work: This fully remote opportunity allows you the flexibility to work from home in support of the USDA ENS program.  Our Customer’s Mission: At USDA Enterprise Network Services (ENS), our customers mission is to drive the modernization of telecommunication systems throughout the USDA, redefining the delivery of services, enhancing capabilities, and evolving into a world-class digital and business intelligence organization. ENS is committed to building strategic partnerships that connect the USDA with the global community, while providing adaptable solutions for the strategic delivery of support services. ENS ultimate goal is to deliver high-quality results that support the ongoing IT modernization efforts within the USDA, paving the way for a more efficient and innovative future.  An average day:  As Big Data Engineer, you will develop Agile environments leveraging advanced engineering practices to deliver architecturally-aligned, scalable solutions. You will develop generic data frameworks and data products that maintain the highest availability, performance, and strive for simplicity. Additionally, you will implement data engineering solutions using: AWS, Spark, Scala, Python, Airflow, EMR, Redshift, Athena, Snowflake, ECS, DevOps Automation, Integration, Docker, Build and Deployment Tools. In this position you will: 
 
  Create data warehouse architecture and associated diagrams and maintain the data, in collaboration with SMVB, to support the approval process and documentation requirements for the ENS Data Warehouse and associated dashboards. 
  Deliver a business requirements document (BRD) for each request business requirements document includes planning and documenting the government’s requirements of BI&A technical work, meeting with subject matter experts to determine business justification, project risks, constraints, assumptions, dependencies, estimated completion time, a list of user stories and acceptance criteria, and architecture diagrams. 
  Analyze and document efficiency improvements on problematic, long running data warehouse queries, to ensure improvement to query return time. 
  Shall implement cost effective, efficient, and innovative methods to migrate the existing data analytics platform to any newly identified platform [such as Amazon Web Services (AWS)]. 
  Develop and deliver a plan of action and milestones (POAM) to migrate existing siloed systems to a software as a service cloud architecture, in collaboration with SMVB. 
  Migrate or implement the services/system as described in any applicable federally approved POAM. 
  Configure BI&A continuous monitoring / report systems that track all events such as errors, ingestion / batch run reports, and unit / integration test results in a single location. 
  Maintain separate development and production environments for ETL and other analytics development and ensure the two environments are as similar as possible, as well as documenting a process in the Team Wiki (currently Confluence) for proper separation of development lifecycle activities. 
  Develop and integrate government-approved Extract, Transform, and Load (ETL) data engineering pipelines. 
  Create, execute, and document technical test plans consisting of test cases describing steps taken and what is being tested. Incorporate automated testing processes, as applicable, to eliminate manual steps. 
  Perform data governance functions to assess quality and risk; Track data steward roles over specific data sets of interest, Maintenance of metadata in the Enterprise Data Catalogue (Informatica), Assign data stewards to meet with internal customers to assess quality standards in technical work (ETL, Dashboards), Track all of ENS' data sources in a collaborative ENS-wide data source tracking list, Assess risks on data sources, Assess quality controls on data sets (such as formats for columns known only to product owners), Consider the appropriate level of technical testing required per the risk assessment. 
  Create user acceptance test plans and cases for user acceptance testing for each developed product and each feature of the developed product. 
  Create knowledge base articles and store the articles on the BI&A Team wiki (currently Confluence) to capture repeatable processes and SOPs related to BI&A development. 
  Develop, test, and integrate data visualizations (such as dashboards) using approved industry standard tools. 
  Maintain ENS BI&A’s version control (currently Bitbucket/git); Perform administration of the repositories using git, Host weekly or ad-hoc code review/pull request meetings. 
  Participate in planning and execution of tasks, in collaboration with SMVB and ENS’ teams, to develop BI&A capabilities and accommodate evolving requirements. 
  Administer Alteryx Server, Tableau Server, and analytics tools used in Amazon Web Services (AWS), to include the following tasks: Provide user administration, Create usage and performance monitoring, Schedule ETL batch runs and keep current. 
  Compile and deliver a quarterly report of the data product that uses data science methodology to identify problems and potential solutions to problems, to provide improvement opportunities to ENS. 
  Maintain an inventory listing of the BI&A tool licenses (currently Alteryx and Tableau) across ENS and its branches to track usage trends, forecast future licensing needs and ensure ENS does not overprovision licenses. 
 
 Basic Qualifications: 
 
  As a requirement of this position, all candidates must be a U.S. Citizen. In accordance with 8 U.S.C. 1324b(a)(2)(C), Epsilon will not consider candidates for this position who do not meet the aforementioned conditions. 
  Strong written and oral communication skills in the English language. All contractor employees must be able to read, write, speak, and understand English. 
  Contractor personnel performing in a leadership capacity shall be capable of directing contractor personnel and interfacing with the Government and customers. 
  Exceptional customer service skills. 
  Strong time-management and prioritization skills. 
  Ability to communicate applicable technical subject matter expertise to management and others. 
  ITIL v4 foundation knowledge 
  Ability to apply and provide feedback on service operation model and practices. 
 
 Other Requirements: 
 
  Must be able to pass federal background investigation and obtain a Public Trust
 
   
 Epsilon is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applications will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. EEO/AA: Minorities/Females/Disabled/Vets.  Please click here to review your rights under EEO policy.  If you are an individual with a disability and need special assistance or reasonable accommodation in applying for employment with Epsilon, Inc., please contact our Recruiting department by phone 828-398-5414 or by email careers@epsilon-inc.com.
    #LI-DNI","<div>
 <p><b>Big Data Engineer </b><br> <br> <b>Who is Epsilon: </b><br> Epsilon is an IT Services company that was founded in 2009 and has become an established leader in providing Information Technology services to both Federal Government and Commercial businesses across the United States. Epsilon is known for its solution-focused and innovative approach, aligning technology systems, tools, and processes with the missions and objectives of its customers.<br> <br> Epsilon&#x2019;s headquarters are in Weaverville, NC with other corporate offices in Greenville, SC, Crystal City, VA, and Denver, CO. We have employees in 30+ States across the U.S.<br> <br> <b>Why work for Epsilon:</b><br> In joining Epsilon&#x2019;s team, you will have the opportunity to contribute to Epsilon&#x2019;s business and customer initiatives, as well as influence our brand culture through people interaction and technology advancements.<br> <br> Epsilon invests in our employees by promoting from within and enabling employees to elevate their knowledge and skill set in their profession by allocating &#x24;3,000 annually in Professional Development funds. We also offer competitive pay, comprehensive benefits through one of the largest national carriers, Paid Time Off (PTO) that increases with tenure and has a generous rollover, 11 company paid Holidays, and 401(k) with immediate contribution.<br> <br> <b>Where you&#x2019;ll work:</b><br> This fully remote opportunity allows you the flexibility to work from home in support of the USDA ENS program.<br> <br> <b>Our Customer&#x2019;s Mission</b>:<br> At USDA Enterprise Network Services (ENS), our customers mission is to drive the modernization of telecommunication systems throughout the USDA, redefining the delivery of services, enhancing capabilities, and evolving into a world-class digital and business intelligence organization. ENS is committed to building strategic partnerships that connect the USDA with the global community, while providing adaptable solutions for the strategic delivery of support services. ENS ultimate goal is to deliver high-quality results that support the ongoing IT modernization efforts within the USDA, paving the way for a more efficient and innovative future.<br> <br> <b>An average day: </b><br> As Big Data Engineer, you will develop Agile environments leveraging advanced engineering practices to deliver architecturally-aligned, scalable solutions. You will develop generic data frameworks and data products that maintain the highest availability, performance, and strive for simplicity. Additionally, you will implement data engineering solutions using: AWS, Spark, Scala, Python, Airflow, EMR, Redshift, Athena, Snowflake, ECS, DevOps Automation, Integration, Docker, Build and Deployment Tools. In this position you will:</p> 
 <ul>
  <li>Create data warehouse architecture and associated diagrams and maintain the data, in collaboration with SMVB, to support the approval process and documentation requirements for the ENS Data Warehouse and associated dashboards.</li> 
  <li>Deliver a business requirements document (BRD) for each request business requirements document includes planning and documenting the government&#x2019;s requirements of BI&amp;A technical work, meeting with subject matter experts to determine business justification, project risks, constraints, assumptions, dependencies, estimated completion time, a list of user stories and acceptance criteria, and architecture diagrams.</li> 
  <li>Analyze and document efficiency improvements on problematic, long running data warehouse queries, to ensure improvement to query return time. </li>
  <li>Shall implement cost effective, efficient, and innovative methods to migrate the existing data analytics platform to any newly identified platform [such as Amazon Web Services (AWS)].</li> 
  <li>Develop and deliver a plan of action and milestones (POAM) to migrate existing siloed systems to a software as a service cloud architecture, in collaboration with SMVB.</li> 
  <li>Migrate or implement the services/system as described in any applicable federally approved POAM.</li> 
  <li>Configure BI&amp;A continuous monitoring / report systems that track all events such as errors, ingestion / batch run reports, and unit / integration test results in a single location.</li> 
  <li>Maintain separate development and production environments for ETL and other analytics development and ensure the two environments are as similar as possible, as well as documenting a process in the Team Wiki (currently Confluence) for proper separation of development lifecycle activities.</li> 
  <li>Develop and integrate government-approved Extract, Transform, and Load (ETL) data engineering pipelines.</li> 
  <li>Create, execute, and document technical test plans consisting of test cases describing steps taken and what is being tested. Incorporate automated testing processes, as applicable, to eliminate manual steps.</li> 
  <li>Perform data governance functions to assess quality and risk; Track data steward roles over specific data sets of interest, Maintenance of metadata in the Enterprise Data Catalogue (Informatica), Assign data stewards to meet with internal customers to assess quality standards in technical work (ETL, Dashboards), Track all of ENS&apos; data sources in a collaborative ENS-wide data source tracking list, Assess risks on data sources, Assess quality controls on data sets (such as formats for columns known only to product owners), Consider the appropriate level of technical testing required per the risk assessment.</li> 
  <li>Create user acceptance test plans and cases for user acceptance testing for each developed product and each feature of the developed product.</li> 
  <li>Create knowledge base articles and store the articles on the BI&amp;A Team wiki (currently Confluence) to capture repeatable processes and SOPs related to BI&amp;A development.</li> 
  <li>Develop, test, and integrate data visualizations (such as dashboards) using approved industry standard tools.</li> 
  <li>Maintain ENS BI&amp;A&#x2019;s version control (currently Bitbucket/git); Perform administration of the repositories using git, Host weekly or ad-hoc code review/pull request meetings.</li> 
  <li>Participate in planning and execution of tasks, in collaboration with SMVB and ENS&#x2019; teams, to develop BI&amp;A capabilities and accommodate evolving requirements.</li> 
  <li>Administer Alteryx Server, Tableau Server, and analytics tools used in Amazon Web Services (AWS), to include the following tasks: Provide user administration, Create usage and performance monitoring, Schedule ETL batch runs and keep current.</li> 
  <li>Compile and deliver a quarterly report of the data product that uses data science methodology to identify problems and potential solutions to problems, to provide improvement opportunities to ENS.</li> 
  <li>Maintain an inventory listing of the BI&amp;A tool licenses (currently Alteryx and Tableau) across ENS and its branches to track usage trends, forecast future licensing needs and ensure ENS does not overprovision licenses.</li> 
 </ul>
 <b>Basic Qualifications: </b>
 <ul>
  <li>As a requirement of this position, all candidates must be a U.S. Citizen. In accordance with 8 U.S.C. 1324b(a)(2)(C), Epsilon will not consider candidates for this position who do not meet the aforementioned conditions.</li> 
  <li>Strong written and oral communication skills in the English language. All contractor employees must be able to read, write, speak, and understand English.</li> 
  <li>Contractor personnel performing in a leadership capacity shall be capable of directing contractor personnel and interfacing with the Government and customers.</li> 
  <li>Exceptional customer service skills.</li> 
  <li>Strong time-management and prioritization skills.</li> 
  <li>Ability to communicate applicable technical subject matter expertise to management and others.</li> 
  <li>ITIL v4 foundation knowledge</li> 
  <li>Ability to apply and provide feedback on service operation model and practices.</li> 
 </ul>
 <b>Other Requirements: </b>
 <ul>
  <li>Must be able to pass federal background investigation and obtain a Public Trust</li>
 </ul>
 <p><br> <br> </p>
 <p>Epsilon is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applications will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. EEO/AA: Minorities/Females/Disabled/Vets.<br> <br> Please click here to review your rights under EEO policy.<br> <br> If you are an individual with a disability and need special assistance or reasonable accommodation in applying for employment with Epsilon, Inc., please contact our Recruiting department by phone 828-398-5414 or by email careers@epsilon-inc.com.</p>
 <p><br> <br> <br> #LI-DNI</p>
</div>",https://www2.jobdiva.com/portal/?a=etjdnw1kw4ry751omqcryv2jkphzlh03e5juym948sy1kv7hh74xv9c2ccsuyfmh&jobid=20657059#/jobs/20657059?compid=0&SearchString=&StatesString=&id=20657059&source=indeed.com,979d7c241a393d79,,,,,"363 W Drake Rd Ste 5, Fort Collins, CO 80526",Big Data Engineer,4 days ago,2023-10-14T13:32:54.594Z,4.2,10.0,"$110,000 - $125,000 a year",2023-10-18T13:32:54.599Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=979d7c241a393d79&from=jasx&tk=1hd1fq96gk6pu800&vjs=3
29,Medidata Solutions,"Requisition ID 
   533295
  
 
 
  
    
  
 
 
  Medidata: Powering Smarter Treatments and Healthier People 
   Medidata, a Dassault Systèmes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world's most trusted platform for clinical development, commercial, and real-world data. Known for its ground-breaking technological innovations, Medidata has supported more than 30,000 clinical trials and 9 million study participants. And Medidata’s ongoing commitment to infusing the patient voice into trial designs and solutions is helping to create a better and more inclusive experience for all participants in clinical studies. Medidata is involved in nearly 40% of company-initiated trial starts globally, with studies conducted in more than 140 countries. More than 70% of novel drugs approved by the Food and Drug Administration (FDA) in 2022 were developed with Medidata software. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us @medidata. 
   Our Team: 
   Medidata is looking for individuals who will help us tackle some of the most complex questions facing the industry today using our proprietary platform and advanced analytics. At Medidata, we never work alone. This role will partner heavily with all of the key stakeholder functions including product, delivery, data science, engineering, partnerships, and biostatistics. Successful Medidata AI candidates will be skilled in analytical/quantitative thinking, structured communication, and excited about building the next horizon of Medidata’s mission to power smarter treatments and healthier people. 
   Who We're Looking For: 
   
   Advanced skills in modern data architecture, data science engineering, data modeling and data quality using state-of-art cloud computing technologies (AWS). 
   Hands-on experience in the latest breed of data ETL, automation and 
   CICD technologies including Python, SQL and Git in a cloud setting. 
   2+ years of experience with cloud-native data warehouse technologies like Snowflake. 
   Skills in data analysis, insight generation and manipulation of structured and unstructured data sources. Experience with automated data quality frameworks. 
   Collaborate with all levels of data science engineering technology personnel and senior leadership. 
   Document and present work to all levels of technical and non-technical audiences. 
   Commitment to creating rigorous, high-quality insights from data, at scale. 
   You should be flexible / willing to work across matrixed delivery landscape which includes and not limited to Agile Applications 
   Development, Support and Deployment. 
   Design and implement secure data pipelines into a Snowflake data warehouse from on premise and cloud data sources. 
   Guide and review off-shore development team work providing coaching and coding feedback aligning to best practices set by the Data Science Engineering team. 
   
  Requirements (Education & Experience): 
   
   Undergraduate degree in a technical or scientific field, such as Statistics, Data Science, Computer Science, or similar  
   8+ years professional experience as a data scientist, data engineer, data analyst, or related role  
   Experience with clinical trial data is not required, but interest to learn and understand how these data improve medical research is paramount  
  
  Medidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation. Nine of the top 10 best-selling drugs in 2017 were developed on the Medidata platform. 
   Medidata Solutions have powered close to 30,000 clinical trials giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner. 
   As with all roles, Medidata sets ranges based on a number of factors including function, level, candidate expertise and experience, and geographic location. 
   The salary range for positions that will be physically based in the NYC Metro Area is $135,000 - $180,000 
   Base pay is one part of the Total Rewards that Medidata provides to compensate and recognize employees for their work. Most sales positions are eligible for a commission on the terms of applicable plan documents, and many of Medidata’s non-sales positions are eligible for annual bonuses. Medidata believes that benefits should connect you to the support you need when it matters most and provides best-in-class benefits, including medical, dental, life and disability insurance; 401(k) matching; unlimited paid time off; and 10 paid holidays per year. 
   #LI-ME1 #LI-Remote 
  
  Equal Employment Opportunity In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Medidata are based on merit, qualifications and abilities. Medidata is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age, disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. Medidata will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.","<p></p>
<div>
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>Requisition ID </b></h3>
   <p>533295</p>
  </div>
 </div>
 <div>
  <div>
   <br> 
  </div>
 </div>
 <div>
  <p>Medidata: Powering Smarter Treatments and Healthier People</p> 
  <p> Medidata, a Dassault Syst&#xe8;mes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world&apos;s most trusted platform for clinical development, commercial, and real-world data. Known for its ground-breaking technological innovations, Medidata has supported more than 30,000 clinical trials and 9 million study participants. And Medidata&#x2019;s ongoing commitment to infusing the patient voice into trial designs and solutions is helping to create a better and more inclusive experience for all participants in clinical studies. Medidata is involved in nearly 40% of company-initiated trial starts globally, with studies conducted in more than 140 countries. More than 70% of novel drugs approved by the Food and Drug Administration (FDA) in 2022 were developed with Medidata software. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us @medidata.</p> 
  <p><b> Our Team:</b></p> 
  <p> Medidata is looking for individuals who will help us tackle some of the most complex questions facing the industry today using our proprietary platform and advanced analytics. At Medidata, we never work alone. This role will partner heavily with all of the key stakeholder functions including product, delivery, data science, engineering, partnerships, and biostatistics. Successful Medidata AI candidates will be skilled in analytical/quantitative thinking, structured communication, and excited about building the next horizon of Medidata&#x2019;s mission to power smarter treatments and healthier people.</p> 
  <p><b> Who We&apos;re Looking For:</b></p> 
  <ul> 
   <li>Advanced skills in modern data architecture, data science engineering, data modeling and data quality using state-of-art cloud computing technologies (AWS).</li> 
   <li>Hands-on experience in the latest breed of data ETL, automation and</li> 
   <li>CICD technologies including Python, SQL and Git in a cloud setting.</li> 
   <li>2+ years of experience with cloud-native data warehouse technologies like Snowflake.</li> 
   <li>Skills in data analysis, insight generation and manipulation of structured and unstructured data sources. Experience with automated data quality frameworks.</li> 
   <li>Collaborate with all levels of data science engineering technology personnel and senior leadership.</li> 
   <li>Document and present work to all levels of technical and non-technical audiences.</li> 
   <li>Commitment to creating rigorous, high-quality insights from data, at scale.</li> 
   <li>You should be flexible / willing to work across matrixed delivery landscape which includes and not limited to Agile Applications</li> 
   <li>Development, Support and Deployment.</li> 
   <li>Design and implement secure data pipelines into a Snowflake data warehouse from on premise and cloud data sources.</li> 
   <li>Guide and review off-shore development team work providing coaching and coding feedback aligning to best practices set by the Data Science Engineering team.</li> 
  </ul> 
  <p><b>Requirements (Education &amp; Experience):</b></p> 
  <ul> 
   <li><p>Undergraduate degree in a technical or scientific field, such as Statistics, Data Science, Computer Science, or similar</p> </li> 
   <li><p>8+ years professional experience as a data scientist, data engineer, data analyst, or related role</p> </li> 
   <li><p>Experience with clinical trial data is not required, but interest to learn and understand how these data improve medical research is paramount</p> </li> 
  </ul>
  <p>Medidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation. Nine of the top 10 best-selling drugs in 2017 were developed on the Medidata platform.</p> 
  <p> Medidata Solutions have powered close to 30,000 clinical trials giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner.</p> 
  <p> As with all roles, Medidata sets ranges based on a number of factors including function, level, candidate expertise and experience, and geographic location.</p> 
  <p> The salary range for positions that will be physically based in the NYC Metro Area is &#x24;135,000 - &#x24;180,000</p> 
  <p> Base pay is one part of the Total Rewards that Medidata provides to compensate and recognize employees for their work. Most sales positions are eligible for a commission on the terms of applicable plan documents, and many of Medidata&#x2019;s non-sales positions are eligible for annual bonuses. Medidata believes that benefits should connect you to the support you need when it matters most and provides best-in-class benefits, including medical, dental, life and disability insurance; 401(k) matching; unlimited paid time off; and 10 paid holidays per year.</p> 
  <p> #LI-ME1<br> #LI-Remote</p> 
  <p></p>
  <h3 class=""jobSectionHeader""><b>Equal Employment Opportunity</b></h3> In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Medidata are based on merit, qualifications and abilities. Medidata is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age, disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. Medidata will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.
 </div>
</div>
<p></p>",https://www.indeed.com/rc/clk?jk=dbf9b007f625f71b&atk=&xpse=SoBP67I3JhXlT9yU8J0LbzkdCdPP,dbf9b007f625f71b,,,,,"New York, NY",Lead Data Science Engineer,3 days ago,2023-10-15T13:32:58.592Z,3.6,94.0,"$135,000 - $180,000 a year",2023-10-18T13:32:58.595Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=dbf9b007f625f71b&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
30,Amaze systems,"Location – Alpharetta, GA/Armonk, NY/Boston, MA/Fort Wayne, IN/Miami, FL/NYC, NY/Windsor, CT/ Washington, DC 
Job Title - Data Engineer
6 months C2h with client
Role Description:
Looking for a data engineer to design, develop and maintain pipelines and workflows and create analytics to digitally transform the current NA Accident and Health reporting and business decision processes.
Responsibilities
Development and support of data pipelines that produce data assets for various A&H workstreams including UW, UA, Actuarial, Claims
Handle data pipelines while testing for data curation, parsing, cleaning, transformation and enrichment of data
Work with fundamentals of data processing, data pipeline, data lineage and ETL (Extract-Transform-Load) methodologies
Implement the project according to the Software Development Life Cycle (SDLC) and programming by using fast paced agile methodology, involving task completion, user stories
Utilize knowledge of database management system software, object oriented programming development, system architecture and components and various programming languages
Review and analyze business workflows and user data needs
Design and implement business performance dashboards
Write customized queries/programs to generate automatic periodical reports highlighting all the Key Performance Indicators (KPIs)
Build applications using SQL and/or Python scripts to manipulate data, monitor and help to improve data quality
Design, build and maintain end-to-end data solutions supporting our processes with the right data architecture
Have working knowledge of Apache Spark, big data processing and building products on distributed cluster-computing framework
Construct workflow charts and diagrams and writing specifications.
Documentation of end-to-end data pipeline process.
Documentation of data assets for information management purposes.
Ad hoc team / business support as needed.
Exploration and evaluation of new technologies and platforms.
Requirements
Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field
5+ years as a data engineer
Sound Python and SQL skills with ability to query and analyze data, understand complexity and data structuresExperience with data and analytics technology, including but not limited to Hadoop, Spark, Java, Python, R, ElasticSearch, and others
Familiarity with relational database concepts
Detail-oriented, analytical, and inquisitiveGood communication skills
Highly organized with strong time-management skills
Ability to work independently and collaborate well with others
Ability to affect smooth organizational transformations
Job Type: Contract
Salary: From $60.00 per hour
Experience level:

 11+ years

Experience:

 Data Engineer: 10 years (Required)
 Python: 7 years (Required)
 Insurance Domain: 5 years (Required)

Work Location: Remote","<p><b>Location &#x2013; Alpharetta, GA/Armonk, NY/Boston, MA/Fort Wayne, IN/Miami, FL/NYC, NY/Windsor, CT/ Washington, DC </b></p>
<p><b>Job Title - Data Engineer</b></p>
<p><b>6 months C2h with client</b></p>
<p><b>Role Description:</b></p>
<p>Looking for a data engineer to design, develop and maintain pipelines and workflows and create analytics to digitally transform the current NA Accident and Health reporting and business decision processes.</p>
<p>Responsibilities</p>
<p>Development and support of data pipelines that produce data assets for various A&amp;H workstreams including UW, UA, Actuarial, Claims</p>
<p>Handle data pipelines while testing for data curation, parsing, cleaning, transformation and enrichment of data</p>
<p>Work with fundamentals of data processing, data pipeline, data lineage and ETL (Extract-Transform-Load) methodologies</p>
<p>Implement the project according to the Software Development Life Cycle (SDLC) and programming by using fast paced agile methodology, involving task completion, user stories</p>
<p>Utilize knowledge of database management system software, object oriented programming development, system architecture and components and various programming languages</p>
<p>Review and analyze business workflows and user data needs</p>
<p>Design and implement business performance dashboards</p>
<p>Write customized queries/programs to generate automatic periodical reports highlighting all the Key Performance Indicators (KPIs)</p>
<p>Build applications using SQL and/or Python scripts to manipulate data, monitor and help to improve data quality</p>
<p>Design, build and maintain end-to-end data solutions supporting our processes with the right data architecture</p>
<p>Have working knowledge of Apache Spark, big data processing and building products on distributed cluster-computing framework</p>
<p>Construct workflow charts and diagrams and writing specifications.</p>
<p>Documentation of end-to-end data pipeline process.</p>
<p>Documentation of data assets for information management purposes.</p>
<p>Ad hoc team / business support as needed.</p>
<p>Exploration and evaluation of new technologies and platforms.</p>
<p>Requirements</p>
<p>Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field</p>
<p>5+ years as a data engineer</p>
<p>Sound Python and SQL skills with ability to query and analyze data, understand complexity and data structures<br>Experience with data and analytics technology, including but not limited to Hadoop, Spark, Java, Python, R, ElasticSearch, and others</p>
<p>Familiarity with relational database concepts</p>
<p>Detail-oriented, analytical, and inquisitive<br>Good communication skills</p>
<p>Highly organized with strong time-management skills</p>
<p>Ability to work independently and collaborate well with others</p>
<p>Ability to affect smooth organizational transformations</p>
<p>Job Type: Contract</p>
<p>Salary: From &#x24;60.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>11+ years</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Engineer: 10 years (Required)</li>
 <li>Python: 7 years (Required)</li>
 <li>Insurance Domain: 5 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,22cd2b66172d9b52,,Contract,,,Remote,Data Engineer,3 days ago,2023-10-15T13:33:04.365Z,,,From $60 an hour,2023-10-18T13:33:04.373Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=22cd2b66172d9b52&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
31,Redcloud Consulting,"RedCloud Consulting is a business and IT consulting company with local Puget Sound Enterprise and Mid-sized clients. RedCloud seeks a 
 Senior Engineer – Data Center Infrastructure to support immediate client operations. Seattle Business Magazine has recognized us, ranked #1 on their ""Best Companies to Work for in Washington"" for Mid-Sized Businesses list, awarded #1 Fastest Growing Company in Washington by Puget Sound Business Journal, and named on the Inc. 500/5000 list.
  
  
 Job Description:
  We are seeking a highly skilled and experienced Senior Engineer specializing in Data Center Infrastructure to join our dynamic team. The ideal candidate will possess a strong background in Data Center Infrastructure Management (DCIM) with a focus on Sunbird DC Track software. This individual will play a crucial role in managing, maintaining, and optimizing our data center operations.
  
  
 Responsibilities include but are not limited to: 
 
  Serve as the primary owner and administrator of the DCIM platform, responsible for its day-to-day operations, maintenance, and periodic audits. 
  Collaborate with cross-functional teams to ensure seamless integration and operation of data center infrastructure. 
  Stay updated with industry best practices and emerging technologies to recommend and implement improvements in data center operations. 
  
  
 Required Knowledge, Skills, and Abilities: 
 
  Demonstrated expertise in Data Center Infrastructure Management (DCIM) with a deep understanding of data center operations, including power and cooling systems, rack and power configuration, and device management. 
  Proficient in using DCIM software, with a specific emphasis on Sunbird DC Track, to monitor and manage data center resources effectively. 
  Experience with ServiceNow is highly desirable, as it will be advantageous in streamlining operational processes. 
  Expertise in Subird DC Track DCIM Software is a critical factor for this role. The ability to leverage this specialization will be a substantial advantage in the hiring process. 
  Familiarity with other DCIM software solutions, such as PowerIQ and TigerEyes, is considered a plus. 
  
  
 Qualifications: 
 
  Bachelor's degree in data science, IT infrastructure, computer science, or other related field. 
  8+ years of proven experience in Data Center Infrastructure Management. 
  Extensive experience with Sunbird DC Track software is essential. 
  Strong proficiency in DCIM software applications. 
  Knowledge of physical hardware components and their integration within a data center environment. 
  Excellent problem-solving skills and attention to detail. 
  Effective communication and collaboration skills.
 
  
  
  Compensation range for position is $135,000 – 170,500 DOE.
  Benefits and bonus information can be found at 
 https://www.redcloudconsulting.com/careers.html
  
  RedCloud requires employees maintain permanent residency within the United States during their employment period. During onboarding, proof of eligibility to work in the United States will be requested. RedCloud does not provide visa sponsorship.
  
  
 About Us:
  RedCloud is a boutique, business and technology consulting firm providing local companies with expert-level support for over two decades. Whether it’s to solve a specific business challenge or to provide additional support for an ambitious project, we can help bring even the most visionary endeavors to fruition.
  
  Anchored by a foundation of ""integrity-based consulting"", the RedCloud team of subject matter experts collaborate closely with clients to develop and implement high-level solutions, bringing stability, growth, and innovation together for long-term success. We provide a broad array of business and technology consulting services through RedCloud’s core services: Empower Operations, Empower Sales and Marketing, Empower Customers, Empower Security and Privacy. 
  
  Visit 
 http://www.redcloudconsulting.com/ for more info. 
  #LI-Remote","<div>
 RedCloud Consulting is a business and IT consulting company with local Puget Sound Enterprise and Mid-sized clients. RedCloud seeks a 
 <b>Senior Engineer &#x2013; Data Center Infrastructure</b> to support immediate client operations. Seattle Business Magazine has recognized us, ranked #1 on their &quot;Best Companies to Work for in Washington&quot; for Mid-Sized Businesses list, awarded #1 Fastest Growing Company in Washington by Puget Sound Business Journal, and named on the Inc. 500/5000 list.
 <br> 
 <br> 
 <b>Job Description:</b>
 <br> We are seeking a highly skilled and experienced Senior Engineer specializing in Data Center Infrastructure to join our dynamic team. The ideal candidate will possess a strong background in Data Center Infrastructure Management (DCIM) with a focus on Sunbird DC Track software. This individual will play a crucial role in managing, maintaining, and optimizing our data center operations.
 <br> 
 <br> 
 <b>Responsibilities include but are not limited to:</b> 
 <ul>
  <li>Serve as the primary owner and administrator of the DCIM platform, responsible for its day-to-day operations, maintenance, and periodic audits. </li>
  <li>Collaborate with cross-functional teams to ensure seamless integration and operation of data center infrastructure. </li>
  <li>Stay updated with industry best practices and emerging technologies to recommend and implement improvements in data center operations. </li>
 </ul> 
 <br> 
 <b>Required Knowledge, Skills, and Abilities:</b> 
 <ul>
  <li>Demonstrated expertise in Data Center Infrastructure Management (DCIM) with a deep understanding of data center operations, including power and cooling systems, rack and power configuration, and device management. </li>
  <li>Proficient in using DCIM software, with a specific emphasis on Sunbird DC Track, to monitor and manage data center resources effectively. </li>
  <li>Experience with ServiceNow is highly desirable, as it will be advantageous in streamlining operational processes. </li>
  <li>Expertise in Subird DC Track DCIM Software is a critical factor for this role. The ability to leverage this specialization will be a substantial advantage in the hiring process. </li>
  <li>Familiarity with other DCIM software solutions, such as PowerIQ and TigerEyes, is considered a plus. </li>
 </ul> 
 <br> 
 <b>Qualifications:</b> 
 <ul>
  <li>Bachelor&apos;s degree in data science, IT infrastructure, computer science, or other related field. </li>
  <li>8+ years of proven experience in Data Center Infrastructure Management. </li>
  <li>Extensive experience with Sunbird DC Track software is essential. </li>
  <li>Strong proficiency in DCIM software applications. </li>
  <li>Knowledge of physical hardware components and their integration within a data center environment. </li>
  <li>Excellent problem-solving skills and attention to detail. </li>
  <li>Effective communication and collaboration skills.</li>
 </ul>
 <br> 
 <br> 
 <br> Compensation range for position is &#x24;135,000 &#x2013; 170,500 DOE.
 <br> Benefits and bonus information can be found at 
 <b>https://www.redcloudconsulting.com/careers.html</b>
 <br> 
 <br> RedCloud requires employees maintain permanent residency within the United States during their employment period. During onboarding, proof of eligibility to work in the United States will be requested. RedCloud does not provide visa sponsorship.
 <br> 
 <br> 
 <b>About Us:</b>
 <br> RedCloud is a boutique, business and technology consulting firm providing local companies with expert-level support for over two decades. Whether it&#x2019;s to solve a specific business challenge or to provide additional support for an ambitious project, we can help bring even the most visionary endeavors to fruition.
 <br> 
 <br> Anchored by a foundation of &quot;integrity-based consulting&quot;, the RedCloud team of subject matter experts collaborate closely with clients to develop and implement high-level solutions, bringing stability, growth, and innovation together for long-term success. We provide a broad array of business and technology consulting services through RedCloud&#x2019;s core services: Empower Operations, Empower Sales and Marketing, Empower Customers, Empower Security and Privacy. 
 <br> 
 <br> Visit 
 <b>http://www.redcloudconsulting.com/</b> for more info. 
 <br> #LI-Remote
</div>",http://careerportal.redcloudconsulting.com/#/jobs/1517,15a24cf7d36918c8,,,,,"Seattle, WA","Senior Engineer, Data Center Infrastructure",3 days ago,2023-10-15T13:33:02.827Z,4.7,6.0,"$135,000 - $170,500 a year",2023-10-18T13:33:02.829Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=15a24cf7d36918c8&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
32,Clari,"Clari’s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here…are you ready to achieve remarkable with us?
 
 
 
   About the Team
 
 
   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari’s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can’t wait to meet you.
 
 
 
   About the Role
 
 
   We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari’s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.
 
 
 
   You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari’s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don’t believe us? Hear what our customers have to say
   
 
 
  
 
  Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.
   
 
 
  
 
  This is a fully remote opportunity and can be worked from any location in the United States.
  
 Responsibilities
 
   Design and evolve the architecture for the query layer that powers Clari’s product suite and platform
   Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion
   Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers
   Write scalable, robust, and fully tested software for deployment in mission-critical production environments
   Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization
   Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent
 
  Qualifications
 
   10+ years of software development experience using Java or similar object-oriented languages for backend development
   Deep expertise with relational database skills and concepts
   Experience having led multiple projects from inception through deployment, maintenance, and support
   Experience with Postgres and non-relational databases like MongoDB
   Experience with AWS
   Experience building scalable systems and architectures
   Experience with database performance tuning
 
  Perks and Benefits @ Clari 
 
  Remote-first with opportunities to work and celebrate in person
   Medical, dental, vision, short & long-term disability, Life insurance, and EAP
   Mental health support provided by Modern Health
   Pre-IPO stock options
   Well-being and professional development funds
   Retirement 401(k) plan
   100% paid parental leave, plus fertility and family planning support provided by Maven
   Discretionary paid time off, monthly ‘take a break’ days, and Focus Fridays
   Focus on culture: Charitable giving match, plus in-person and virtual events
 
 
 
   It is Clari’s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari’s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.
 
 
 
   Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.
 
 
 
   The salary range for this position is $191,300 to $286,900. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.
 
 
   #BI-Remote #LI-Remote
 
 
 
   You’ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We’d love to have you join us on our journey to remarkable!
 
 
 
   If you feel you don’t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you.
 
 
 
   Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you’ll bring your whole self to the job.","<div>
 <div>
  Clari&#x2019;s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here&#x2026;are you ready to achieve remarkable with us?
 </div>
 <div></div>
 <div>
  <b><br> About the Team</b>
 </div>
 <div>
   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari&#x2019;s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can&#x2019;t wait to meet you.
 </div>
 <div></div>
 <div>
  <b><br> About the Role</b>
 </div>
 <div>
   We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari&#x2019;s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.
 </div>
 <div></div>
 <div>
  <br> You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari&#x2019;s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don&#x2019;t believe us? Hear what our customers have to say
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  <i>This is a fully remote opportunity and can be worked from any location in the United States.</i>
 </div> 
 <h3 class=""jobSectionHeader""><b>Responsibilities</b></h3>
 <ul>
  <li> Design and evolve the architecture for the query layer that powers Clari&#x2019;s product suite and platform</li>
  <li> Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion</li>
  <li> Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers</li>
  <li> Write scalable, robust, and fully tested software for deployment in mission-critical production environments</li>
  <li> Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization</li>
  <li> Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Qualifications</b></h3>
 <ul>
  <li> 10+ years of software development experience using Java or similar object-oriented languages for backend development</li>
  <li> Deep expertise with relational database skills and concepts</li>
  <li> Experience having led multiple projects from inception through deployment, maintenance, and support</li>
  <li> Experience with Postgres and non-relational databases like MongoDB</li>
  <li> Experience with AWS</li>
  <li> Experience building scalable systems and architectures</li>
  <li> Experience with database performance tuning</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Perks and Benefits @ Clari </b></h3>
 <ul>
  <li>Remote-first with opportunities to work and celebrate in person</li>
  <li> Medical, dental, vision, short &amp; long-term disability, Life insurance, and EAP</li>
  <li> Mental health support provided by Modern Health</li>
  <li> Pre-IPO stock options</li>
  <li> Well-being and professional development funds</li>
  <li> Retirement 401(k) plan</li>
  <li> 100% paid parental leave, plus fertility and family planning support provided by Maven</li>
  <li> Discretionary paid time off, monthly &#x2018;take a break&#x2019; days, and Focus Fridays</li>
  <li> Focus on culture: Charitable giving match, plus in-person and virtual events</li>
 </ul>
 <div></div>
 <div>
  <br> It is Clari&#x2019;s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari&#x2019;s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.
 </div>
 <div></div>
 <div>
  <br> Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.
 </div>
 <div></div>
 <div>
  <br> The salary range for this position is &#x24;191,300 to &#x24;286,900. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.
 </div>
 <div>
   #BI-Remote #LI-Remote
 </div>
 <div></div>
 <div>
  <br> You&#x2019;ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We&#x2019;d love to have you join us on our journey to remarkable!
 </div>
 <div></div>
 <div>
  <b><br> If you feel you don&#x2019;t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you.</b>
 </div>
 <div></div>
 <div>
  <br> Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you&#x2019;ll bring your whole self to the job.
 </div>
</div>",https://jobs.lever.co/clari/3b2a2738-2090-42de-b748-acfc28b25609?lever-source=Indeed,416b51eb49b3f599,,Full-time,,,"Seattle, WA","Principal Software Engineer, Data Platform - Remote",1 day ago,2023-10-17T13:33:04.910Z,4.0,3.0,"$191,300 - $286,900 a year",2023-10-18T13:33:04.928Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=416b51eb49b3f599&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
33,Internet Archive,"Interested in a mission-driven job ensuring perpetual open access to information for a global audience? Enjoy helping scale the use of services and products critical to hundreds of national and international non-profits, libraries, universities, cultural heritage institutions, and mission-driven organizations? If so, the Internet Archive is seeking a Software Engineer for our Archiving & Data Services team.
  Internet Archive (IA) is a non-profit digital library, top 200 website at archive.org, and an archive of over 99 petabytes of digital information running in many self-owned and operated data centers. Internet Archive also provides mission-aligned services to thousands of organizations working collaboratively to advance our shared goal of “Universal Access to All Knowledge.” The Archiving & Data Services group provides a suite of paid, SaaS, and free products, as well as community programs, focused on the archiving, management, analysis, and accessibility of digital information. Its services are used by over 1,500 organizations around the world.
  We are looking for a motivated, detail-oriented Software Engineer to join our team. The role will focus on Archive-It (archive-it.org), our platform for building, sharing, and preserving web archive collections. This position offers the opportunity to work with a range of technologies and gain deep knowledge about web crawling, archival replay, and large-scale distributed systems. Our services work with petabytes of archived data and facilitate the discovery and use of large-scale digital collections. The Software Engineer will have the unique opportunity to build things that further open access to information and advance the public good.
  Key Responsibilities:
 
   Collaborate with team members to understand user needs, design new features, support web crawling and preservation, and improve the performance and reliability of Archive-It and other department products.
  
   
     Implement, test, and maintain software across our stack (Python, Elasticsearch, Postgres, Temporal, HTML/CSS/JS/TS).
   
  
   
     Develop, monitor, and maintain the Archive-It partner application, where web crawls are configured, scheduled, and reported.
   
  
   
     Improve a distributed system orchestrating web crawls and post-processing them for long term preservation, indexing for retrieval, deduplication, and reporting.
   
  
   
     Participate in code reviews to ensure the quality and stability of our software and diffusion of knowledge across the team.
   
   Document architecture, software, and features for internal and external users.
 
  Qualification and Skills:
 
   Degree in Computer Science or a related field, or equivalent experience, strongly preferred.
  
   
     Proficiency in Python, with familiarity in Postgres, Elasticsearch, and HTML/CSS/JS preferred.
   
  
   
     A strong understanding of web services and distributed systems.
   
  
   
     Excellent problem-solving skills, attention to detail, and ability to work both independently and collaboratively.
   
  
   
     Experience with web crawling, Django, workflow systems (e.g. Temporal, Airflow), distributed databases (e.g. Cassandra, Scylla), Hadoop, and Ansible are a plus
   
  
   
     GitLab, GitHub, Sentry, Grafana, JIRA, are other tools we use.
   
  
   
     Our independently operated data centers run Ubuntu Linux VMs and our department runs everything from the VM up, so Linux experience is preferred.
   
   An interest in the Internet Archive’s mission to provide Universal Access to All Knowledge is expected.
 
  Job Details:
  Remote applicants preferred. We have headquarters in San Francisco and Vancouver and candidates in those locations will have the option for hybrid remote/in-office arrangements. Candidates will need to have some time overlap with primarily North America (and largely Pacific Time) based colleagues. Compensation and title will be commensurate with experience and the role is open to candidates of varying seniority with a general, but negotiable, salary range of $90,000 to $115,000 based on living in the San Francisco, CA region. Compensation may be adjusted based on the geographic location of the finalist.
  Benefits & Perks:
  The Internet Archive is a remote first workplace and provides a comprehensive benefits package including; PTO, paid holidays, and medical benefits. Depending on where you live, we also provide these additional benefits; dental, vision, health savings accounts, flex spending accounts, commuter benefits, short term disability, long term disability and retirement programs.
  At the Internet Archive, we believe we do our best work when our employees bring together diverse ideas. Members of all groups under represented in the tech industry and library world are strongly encouraged to apply. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law.","<p></p>
<div>
 <p>Interested in a mission-driven job ensuring perpetual open access to information for a global audience? Enjoy helping scale the use of services and products critical to hundreds of national and international non-profits, libraries, universities, cultural heritage institutions, and mission-driven organizations? If so, the Internet Archive is seeking a Software Engineer for our Archiving &amp; Data Services team.</p>
 <p> Internet Archive (IA) is a non-profit digital library, top 200 website at archive.org, and an archive of over 99 petabytes of digital information running in many self-owned and operated data centers. Internet Archive also provides mission-aligned services to thousands of organizations working collaboratively to advance our shared goal of &#x201c;Universal Access to All Knowledge.&#x201d; The Archiving &amp; Data Services group provides a suite of paid, SaaS, and free products, as well as community programs, focused on the archiving, management, analysis, and accessibility of digital information. Its services are used by over 1,500 organizations around the world.</p>
 <p> We are looking for a motivated, detail-oriented Software Engineer to join our team. The role will focus on Archive-It (archive-it.org), our platform for building, sharing, and preserving web archive collections. This position offers the opportunity to work with a range of technologies and gain deep knowledge about web crawling, archival replay, and large-scale distributed systems. Our services work with petabytes of archived data and facilitate the discovery and use of large-scale digital collections. The Software Engineer will have the unique opportunity to build things that further open access to information and advance the public good.</p>
 <p> Key Responsibilities:</p>
 <ul>
  <li><p> Collaborate with team members to understand user needs, design new features, support web crawling and preservation, and improve the performance and reliability of Archive-It and other department products.</p></li>
  <li>
   <div>
     Implement, test, and maintain software across our stack (Python, Elasticsearch, Postgres, Temporal, HTML/CSS/JS/TS).
   </div></li>
  <li>
   <div>
     Develop, monitor, and maintain the Archive-It partner application, where web crawls are configured, scheduled, and reported.
   </div></li>
  <li>
   <div>
     Improve a distributed system orchestrating web crawls and post-processing them for long term preservation, indexing for retrieval, deduplication, and reporting.
   </div></li>
  <li>
   <div>
     Participate in code reviews to ensure the quality and stability of our software and diffusion of knowledge across the team.
   </div></li>
  <li><p> Document architecture, software, and features for internal and external users.</p></li>
 </ul>
 <p> Qualification and Skills:</p>
 <ul>
  <li><p> Degree in Computer Science or a related field, or equivalent experience, strongly preferred.</p></li>
  <li>
   <div>
     Proficiency in Python, with familiarity in Postgres, Elasticsearch, and HTML/CSS/JS preferred.
   </div></li>
  <li>
   <div>
     A strong understanding of web services and distributed systems.
   </div></li>
  <li>
   <div>
     Excellent problem-solving skills, attention to detail, and ability to work both independently and collaboratively.
   </div></li>
  <li>
   <div>
     Experience with web crawling, Django, workflow systems (e.g. Temporal, Airflow), distributed databases (e.g. Cassandra, Scylla), Hadoop, and Ansible are a plus
   </div></li>
  <li>
   <div>
     GitLab, GitHub, Sentry, Grafana, JIRA, are other tools we use.
   </div></li>
  <li>
   <div>
     Our independently operated data centers run Ubuntu Linux VMs and our department runs everything from the VM up, so Linux experience is preferred.
   </div></li>
  <li><p> An interest in the Internet Archive&#x2019;s mission to provide Universal Access to All Knowledge is expected.</p></li>
 </ul>
 <p> Job Details:</p>
 <p> Remote applicants preferred. We have headquarters in San Francisco and Vancouver and candidates in those locations will have the option for hybrid remote/in-office arrangements. Candidates will need to have some time overlap with primarily North America (and largely Pacific Time) based colleagues. Compensation and title will be commensurate with experience and the role is open to candidates of varying seniority with a general, but negotiable, salary range of &#x24;90,000 to &#x24;115,000 based on living in the San Francisco, CA region. Compensation may be adjusted based on the geographic location of the finalist.</p>
 <p> Benefits &amp; Perks:</p>
 <p> The Internet Archive is a remote first workplace and provides a comprehensive benefits package including; PTO, paid holidays, and medical benefits. Depending on where you live, we also provide these additional benefits; dental, vision, health savings accounts, flex spending accounts, commuter benefits, short term disability, long term disability and retirement programs.</p>
 <p> At the Internet Archive, we believe we do our best work when our employees bring together diverse ideas. Members of all groups under represented in the tech industry and library world are strongly encouraged to apply. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law.</p>
</div>
<p></p>",https://app.trinethire.com/companies/32967-internet-archive/jobs/83694-software-engineer-archiving-and-data-services?source=indeed,438a072845b56b63,,Full-time,,,"San Francisco, CA",Software Engineer - Archiving and Data Services,1 day ago,2023-10-17T13:33:09.063Z,3.6,27.0,"$90,000 - $115,000 a year",2023-10-18T13:33:09.067Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=438a072845b56b63&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
34,Accrete.AI,"The U.S. Government agencies we work with have contracts that require all personnel working on their corresponding contracts to have U.S. citizenship – do you meet this requirement?
 
  
  Accrete is looking for a Data Platform Engineer who will play a critical role in designing, building, and maintaining our data platform to support the organization's data processing and analytics needs. You will collaborate with data engineers, data scientists, and other cross-functional teams to create scalable and efficient data solutions. The ideal candidate should have a strong background in data engineering, cloud technologies, and data architecture, along with a passion for innovation and problem-solving.
  
 Accrete is an AI prime defense contractor with the U.S. government that creates AI software, enabling its customers to make better decisions, faster. Accrete is on a mission to create AI so powerful it amplifies human reasoning and enables enterprises to grow in previously unimaginable ways. Prior to launching Accrete in 2017, Prashant Bhuyan, Accrete’s Founder and CEO, spent over a decade in high-frequency trading where he and a core team experimented with and developed AI technology that ultimately became the early underpinnings of Accrete.
  Accrete’s solutions enable the Department of Defense to predict covert behavior from foreign adversaries seeking to influence the supply chain; the U.S. Air Force to identify vulnerabilities in microprocessor firmware; major music labels to identify superstars before competitors; auto dealers to automatically generate marketing content from vehicle feature lists; employee benefits brokers to identify the shortest path to the hottest leads; and more. 
  To learn more about Accrete, please visit our website: Accrete.ai 
  
 Responsibilities: 
 
  Lead the design and development of our data platform to support the ingestion, organization, and retrieval of data. 
  Work closely with stakeholders to ensure the timely arrival of data that is prepared for downstream use.
   Operate and maintain technologies associated with the platform.
   Optimize data storage and retrieval strategies, ensuring data availability, consistency, and reliability.
   Implement data governance and data security practices to protect sensitive data and ensure compliance with regulatory requirements.
   Develop and maintain data platform documentation, including data models, data dictionaries, and system architecture.
   Conduct performance tuning and optimization of data processing workflows for enhanced efficiency and scalability.
   Stay up-to-date with emerging data technologies, industry trends, and best practices, and evaluate their potential to enhance our data platform.
   Mentor and provide guidance to junior data engineers, fostering a culture of knowledge sharing and skill development.
  
  
 Requirements: 
 
   Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
   2-3+ years of experience in data engineering and data platform development.
   Proven expertise in designing and building scalable data platforms, data warehouses, and data lakes.
   Strong proficiency in data modeling, ETL processes, and data integration techniques.
   Extensive experience with cloud platforms such as AWS, Azure, or Google Cloud, and cloud-based data services.
   Proficiency in programming languages commonly used in data engineering, such as Python, Java, Rust, C, or Go.
   Familiarity with data governance, data security, and data compliance practices.
   Operational knowledge of common data systems like: Kafka, Airflow, Spark, Trino, Ranger.
   Knowledge of containerization technologies, such as Docker and Kubernetes, for data platform deployment.
   Strong problem-solving and analytical skills, with the ability to troubleshoot and resolve complex data issues.
   Excellent communication and collaboration skills to work effectively with cross-functional teams.
   A proactive and self-driven approach to learning and staying updated with evolving data technologies and tools.
  
 
 The base salary range for this role is $145,000 to 160,000. 
  
 Benefits: 
 We offer a competitive salary, benefits package, and opportunities for growth and advancement within the company. If you are an innovative and results-driven leader, we encourage you to apply for this exciting opportunity. 
  Accrete is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.","<div>
 <ul>
  <li><i>The U.S. Government agencies we work with have contracts that require all personnel working on their corresponding contracts to have U.S. citizenship &#x2013; do you meet this requirement?</i></li>
 </ul>
 <p></p> 
 <p> Accrete is looking for a Data Platform Engineer who will play a critical role in designing, building, and maintaining our data platform to support the organization&apos;s data processing and analytics needs. You will collaborate with data engineers, data scientists, and other cross-functional teams to create scalable and efficient data solutions. The ideal candidate should have a strong background in data engineering, cloud technologies, and data architecture, along with a passion for innovation and problem-solving.</p>
 <p></p> 
 <p>Accrete is an AI prime defense contractor with the U.S. government that creates AI software, enabling its customers to make better decisions, faster. Accrete is on a mission to create AI so powerful it amplifies human reasoning and enables enterprises to grow in previously unimaginable ways. Prior to launching Accrete in 2017, Prashant Bhuyan, Accrete&#x2019;s Founder and CEO, spent over a decade in high-frequency trading where he and a core team experimented with and developed AI technology that ultimately became the early underpinnings of Accrete.</p>
 <p><br> Accrete&#x2019;s solutions enable the Department of Defense to predict covert behavior from foreign adversaries seeking to influence the supply chain; the U.S. Air Force to identify vulnerabilities in microprocessor firmware; major music labels to identify superstars before competitors; auto dealers to automatically generate marketing content from vehicle feature lists; employee benefits brokers to identify the shortest path to the hottest leads; and more.</p> 
 <p> To learn more about Accrete, please visit our website: Accrete.ai</p> 
 <p></p> 
 <p><b>Responsibilities: </b></p>
 <ul>
  <li>Lead the design and development of our data platform to support the ingestion, organization, and retrieval of data. </li>
  <li>Work closely with stakeholders to ensure the timely arrival of data that is prepared for downstream use.</li>
  <li> Operate and maintain technologies associated with the platform.</li>
  <li> Optimize data storage and retrieval strategies, ensuring data availability, consistency, and reliability.</li>
  <li> Implement data governance and data security practices to protect sensitive data and ensure compliance with regulatory requirements.</li>
  <li> Develop and maintain data platform documentation, including data models, data dictionaries, and system architecture.</li>
  <li> Conduct performance tuning and optimization of data processing workflows for enhanced efficiency and scalability.</li>
  <li> Stay up-to-date with emerging data technologies, industry trends, and best practices, and evaluate their potential to enhance our data platform.</li>
  <li> Mentor and provide guidance to junior data engineers, fostering a culture of knowledge sharing and skill development.</li>
 </ul> 
 <p></p> 
 <p><b>Requirements:</b></p> 
 <ul>
  <li> Bachelor&apos;s or Master&apos;s degree in Computer Science, Data Engineering, or a related field.</li>
  <li> 2-3+ years of experience in data engineering and data platform development.</li>
  <li> Proven expertise in designing and building scalable data platforms, data warehouses, and data lakes.</li>
  <li> Strong proficiency in data modeling, ETL processes, and data integration techniques.</li>
  <li> Extensive experience with cloud platforms such as AWS, Azure, or Google Cloud, and cloud-based data services.</li>
  <li> Proficiency in programming languages commonly used in data engineering, such as Python, Java, Rust, C, or Go.</li>
  <li> Familiarity with data governance, data security, and data compliance practices.</li>
  <li> Operational knowledge of common data systems like: Kafka, Airflow, Spark, Trino, Ranger.</li>
  <li> Knowledge of containerization technologies, such as Docker and Kubernetes, for data platform deployment.</li>
  <li> Strong problem-solving and analytical skills, with the ability to troubleshoot and resolve complex data issues.</li>
  <li> Excellent communication and collaboration skills to work effectively with cross-functional teams.</li>
  <li> A proactive and self-driven approach to learning and staying updated with evolving data technologies and tools.</li>
 </ul> 
 <p></p>
 <p>The base salary range for this role is &#x24;145,000 to 160,000.</p> 
 <p></p> 
 <p><b>Benefits: </b></p>
 <p><i>We offer a competitive salary, benefits package, and opportunities for growth and advancement within the company. If you are an innovative and results-driven leader, we encourage you to apply for this exciting opportunity.</i></p> 
 <p><i> Accrete is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.</i></p>
</div>",https://www.indeed.com/applystart?jk=77845bc5ebc6aae6&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXjPLcA7DAVOeBgupB7PgHT0-BhdrQogdzP3xc9-PmOQTQ&vjfrom=serp&astse=7845a5fa38e75c67&assa=6120,77845bc5ebc6aae6,,Full-time,,,Remote,Data Platform Engineer,20 days ago,2023-09-28T13:33:16.463Z,,,"$145,000 - $160,000 a year",2023-10-18T13:33:16.465Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=77845bc5ebc6aae6&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
35,ServiceNow,"Company Description
  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. 
 With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. 
 Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. 
 Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.
  Job Description
  Team:
  The Data Streaming group has teams that provide streaming API for higher-layer applications and/or work to scale our application platforms. Depending on the nature of the data, the storage systems include data in motion, such as time-series databases or message bus systems. Our largest customers are constantly pushing the limits of the backend storage in terms of size of the data, speed of IO, and the number of concurrent transactions. Performance, reliability, and scalability are always at the core of our work.
  Our largest customers are constantly pushing the limits of the backend storage in terms of size of the data, speed of IO, and several concurrent transactions. Performance, reliability, and scalability are always at the core of our work.
   What you get to do in this role:
 
   Design and build highly innovative interactive high performant solutions with scalability and quality.
   Design software that is simple to use to allow customers to extend and customize the functionality to meet their specific needs.
   Build high-quality, clean, scalable, and reusable code by enforcing best practices around software engineering architecture and processes (Code Reviews, Unit testing, etc.)
   Design and develop tools, libraries, and frameworks with long term platform mindset thinking for high modularity, extensibility, configurability, and maintainability.
   Lead and coordinate work cross functionally to improve architecture, developing process and mentoring junior members in the team.
   Work with the product owners to understand detailed requirements and own your code from design, implementation, test automation and delivery of high-quality product to our users.
   Contribute to the design and implementation of new products and features while also enhancing the existing product suite.
   Manage projects with material technical risk at a team level.
   Explorer and evaluate new technology and innovation to continuously improve platform capability and functionality.
   Be a mentor for colleagues and help promote knowledge-sharing.
 
  To be successful in the role:
 
   Advanced knowledge and experience with fundamentals in distributed systems design and development
   Strong fluency with Java programming as well as good understanding of Java memory model and garbage collection
   Experience with JVM performance tuning and optimization as well as experience in diagnosing performance bottlenecks.
   Advanced working knowledge of concurrency, sockets, networking, operating systems, memory management, runtimes, portability, etc
   Has the experience and ability to diagnose issues and troubleshoot.
 
  Nice to have:
 
   Experience with streaming systems (Kafka, Pulsar, etc.)
   Experience working with Kafka.
   Experience working in a DevOps environment.
   Experience with relational databases: Oracle, MySQL, MariaDB, MS SQLServer
 
  
  Qualifications
  
 
   10+ years of experience with Java or a similar OO language
   Experience building and operating large-scale systems.
   Experience with data structures, algorithms, object-oriented design, design patterns, and performance/scale considerations
 
  For positions in California (outside of the Bay Area), we offer a base pay of $166,230 - $290,970, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location. For individuals who will be working in the Bay Area, there is a pay enhancement for positions located in that geographical area; please contact your recruiter for additional information.
  Additional Information
  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.
  At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.
  If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.
  For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.
  Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.
  From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license.
  Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.","<div>
 <b>Company Description</b>
 <p><br> At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can&#x2019;t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.</p> 
 <p>With more than 7,700+ customers, we serve approximately 85% of the Fortune 500&#xae;, and we&apos;re proud to be one of FORTUNE 100 Best Companies to Work For&#xae; and World&apos;s Most Admired Companies&#x2122;.</p> 
 <p>Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.</p> 
 <p>Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.</p>
 <b><br> Job Description</b>
 <p><b><br> Team:</b></p>
 <p> The Data Streaming group has teams that provide streaming API for higher-layer applications and/or work to scale our application platforms. Depending on the nature of the data, the storage systems include data in motion, such as time-series databases or message bus systems. Our largest customers are constantly pushing the limits of the backend storage in terms of size of the data, speed of IO, and the number of concurrent transactions. Performance, reliability, and scalability are always at the core of our work.</p>
 <p> Our largest customers are constantly pushing the limits of the backend storage in terms of size of the data, speed of IO, and several concurrent transactions. Performance, reliability, and scalability are always at the core of our work.</p>
 <p><br> <b> What you get to do in this role:</b></p>
 <ul>
  <li> Design and build highly innovative interactive high performant solutions with scalability and quality.</li>
  <li> Design software that is simple to use to allow customers to extend and customize the functionality to meet their specific needs.</li>
  <li> Build high-quality, clean, scalable, and reusable code by enforcing best practices around software engineering architecture and processes (Code Reviews, Unit testing, etc.)</li>
  <li> Design and develop tools, libraries, and frameworks with long term platform mindset thinking for high modularity, extensibility, configurability, and maintainability.</li>
  <li> Lead and coordinate work cross functionally to improve architecture, developing process and mentoring junior members in the team.</li>
  <li> Work with the product owners to understand detailed requirements and own your code from design, implementation, test automation and delivery of high-quality product to our users.</li>
  <li> Contribute to the design and implementation of new products and features while also enhancing the existing product suite.</li>
  <li> Manage projects with material technical risk at a team level.</li>
  <li> Explorer and evaluate new technology and innovation to continuously improve platform capability and functionality.</li>
  <li> Be a mentor for colleagues and help promote knowledge-sharing.</li>
 </ul>
 <p><b> To be successful in the role:</b></p>
 <ul>
  <li> Advanced knowledge and experience with fundamentals in distributed systems design and development</li>
  <li> Strong fluency with Java programming as well as good understanding of Java memory model and garbage collection</li>
  <li> Experience with JVM performance tuning and optimization as well as experience in diagnosing performance bottlenecks.</li>
  <li> Advanced working knowledge of concurrency, sockets, networking, operating systems, memory management, runtimes, portability, etc</li>
  <li> Has the experience and ability to diagnose issues and troubleshoot.</li>
 </ul>
 <p><b> Nice to have:</b></p>
 <ul>
  <li> Experience with streaming systems (Kafka, Pulsar, etc.)</li>
  <li> Experience working with Kafka.</li>
  <li> Experience working in a DevOps environment.</li>
  <li> Experience with relational databases: Oracle, MySQL, MariaDB, MS SQLServer</li>
 </ul>
 <br> 
 <b> Qualifications</b>
 <br> 
 <ul>
  <li> 10+ years of experience with Java or a similar OO language</li>
  <li> Experience building and operating large-scale systems.</li>
  <li> Experience with data structures, algorithms, object-oriented design, design patterns, and performance/scale considerations</li>
 </ul>
 <p> For positions in California (outside of the Bay Area), we offer a base pay of &#x24;166,230 - &#x24;290,970, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location. For individuals who will be working in the Bay Area, there is a pay enhancement for positions located in that geographical area; please contact your recruiter for additional information.</p>
 <b><br> Additional Information</b>
 <p><br> ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.</p>
 <p> At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.</p>
 <p> If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.</p>
 <p> For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.</p>
 <p> Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.</p>
 <p> From Fortune. &#xa9; 2022 Fortune Media IP Limited All rights reserved. Used under license.</p>
 <p> Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.</p>
</div>",https://careers.servicenow.com/careers/jobs/743999935084528EXT?lang=en-us&trid=35ab2906-b356-4a56-8472-f60d30d2e2f0,e56b7ba5eba022d3,,Full-time,,,"4810 Eastgate Mall, San Diego, CA 92121",Sr Staff Data Platform Software Engineer - Data Stream,12 days ago,2023-10-06T13:33:21.665Z,3.7,239.0,"$166,230 - $290,970 a year",2023-10-18T13:33:21.668Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e56b7ba5eba022d3&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
36,Jobscan,"At Jobscan, we’re passionate about empowering job seekers to land more interviews with AI technology. We have helped over 2 million job seekers get more interviews in 100+ countries. Jobscan’s platform benefits job seekers, employers, universities, and communities. We're a fast-growing remote startup. We are completely customer-funded, profitable, and growing exponentially!
  
  
  
    We handle vast amounts of data to help job seekers succeed, and we need an experienced Data Engineer to optimize our pipelines for reliability, efficiency, and quality. As part of our distributed engineering team, you will play a crucial role in shaping the future of our data assets.
  
  
 
  
   What you'll be doing
   
    
      Diagnose and Resolve Issues: Troubleshoot and fix data issues within our existing pipeline, which is built on Segment.
      ETL Development: Design, implement, and maintain ETL processes tailored for BigQuery and MySQL while adhering to privacy and governance principles.
      Data Cleansing: Develop and implement data validation and transformation solutions as an integral part of our ETL workflows.
      Data Integration: Utilize Segment for optimized data collection, integration, and management.
      Stakeholder Collaboration: Work closely with stakeholders to tackle specific data integrity and quality issues.
      Teamwork: Collaborate with our Senior Data Analyst and engineering team to refine data models and architectures.
      SQL Optimization: Write and fine-tune SQL queries for performance and scalability in BigQuery and MySQL environments.
      Documentation: Maintain meticulous documentation for all data processes and updates.
    
   
  
  
 
  
   What you'll need
   
    
      Bachelor’s degree in Computer Science, Engineering, or a related field.
      7+ years of relevant experience in data engineering, especially in data pipeline cleanup and ETL processes.
      Direct experience with Customer Data Platforms (CDP) such as Segment, Rudderstack, or Treasure Data.
      Mastery of SQL with hands-on experience in BigQuery and MySQL.
      Proficient in Google Cloud Platform services, particularly BigQuery and Google Analytics 4.
      Experience with modern programming languages like Python, R, JavaScript, and PHP.
      Exceptional problem-solving and communication skills.
      Proven expertise in data schemas and data cleaning principles.
    
   
  
  
 
  
   Preferred qualifications
   
    
      Specific prior experience with Segment for data integration is a strong plus.
      Capability to read and understand PHP and JavaScript code to collaborate effectively with our engineering team.
      Proven track record in tackling data quality and integrity issues in team settings.
    
   
  
  
 
  
   $140,000 - $175,000 a year
  
  
 
  
   Benefits
  
  
    - Remote work - we trust you to get your work done and make it to your meetings‍‍
  
  
    - Competitive salary + stock options - you should have a piece of what we're building here️
  
  
    - Flexible schedule - we make it easy to take care of the important things, like your family and health‍ ️
  
  
    - Unlimited PTO + 14 Paid Holidays + Paid Sick Days - we want our employees to have time to care for their personal wellness and mental health‍ ️
  
  
    - Paid maternal/parental leave - enjoy time with your family's new addition‍‍‍
  
  
    401(k) + employer match
   Medical, dental, vision, and life insurance with generous employer contributions
   Health savings accounts
   Life insurance
   $1000 office stipend; monthly education and internet stipend
  
  
    - Wellness stipend - use for yoga class, gym membership, or anything that improves your personal wellness‍ ️
  
  
   Apple computer or PC of your choice
   Bi-annual company retreats
  
  
  
    Jobscan is committed to equal pay; diversity, equity, and inclusion; and As we continue to grow, we are always adding more benefits and perks for our team.
  
  
  
    Jobscan provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.","<div>
 <div>
  <div>
   At Jobscan, we&#x2019;re passionate about empowering job seekers to land more interviews with AI technology. We have helped over 2 million job seekers get more interviews in 100+ countries. Jobscan&#x2019;s platform benefits job seekers, employers, universities, and communities. We&apos;re a fast-growing remote startup. We are completely customer-funded, profitable, and growing exponentially!
  </div>
  <div></div>
  <div>
   <br> We handle vast amounts of data to help job seekers succeed, and we need an experienced Data Engineer to optimize our pipelines for reliability, efficiency, and quality. As part of our distributed engineering team, you will play a crucial role in shaping the future of our data assets.
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>What you&apos;ll be doing</b></h3>
   <ul>
    <ul>
     <li><b> Diagnose and Resolve Issues:</b> Troubleshoot and fix data issues within our existing pipeline, which is built on Segment.</li>
     <li><b> ETL Development:</b> Design, implement, and maintain ETL processes tailored for BigQuery and MySQL while adhering to privacy and governance principles.</li>
     <li><b> Data Cleansing:</b> Develop and implement data validation and transformation solutions as an integral part of our ETL workflows.</li>
     <li><b> Data Integration:</b> Utilize Segment for optimized data collection, integration, and management.</li>
     <li><b> Stakeholder Collaboration:</b> Work closely with stakeholders to tackle specific data integrity and quality issues.</li>
     <li><b> Teamwork:</b> Collaborate with our Senior Data Analyst and engineering team to refine data models and architectures.</li>
     <li><b> SQL Optimization:</b> Write and fine-tune SQL queries for performance and scalability in BigQuery and MySQL environments.</li>
     <li><b> Documentation:</b> Maintain meticulous documentation for all data processes and updates.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>What you&apos;ll need</b></h3>
   <ul>
    <ul>
     <li> Bachelor&#x2019;s degree in Computer Science, Engineering, or a related field.</li>
     <li> 7+ years of relevant experience in data engineering, especially in data pipeline cleanup and ETL processes.</li>
     <li> Direct experience with Customer Data Platforms (CDP) such as Segment, Rudderstack, or Treasure Data.</li>
     <li> Mastery of SQL with hands-on experience in BigQuery and MySQL.</li>
     <li> Proficient in Google Cloud Platform services, particularly BigQuery and Google Analytics 4.</li>
     <li> Experience with modern programming languages like Python, R, JavaScript, and PHP.</li>
     <li> Exceptional problem-solving and communication skills.</li>
     <li> Proven expertise in data schemas and data cleaning principles.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>Preferred qualifications</b></h3>
   <ul>
    <ul>
     <li> Specific prior experience with Segment for data integration is a strong plus.</li>
     <li> Capability to read and understand PHP and JavaScript code to collaborate effectively with our engineering team.</li>
     <li> Proven track record in tackling data quality and integrity issues in team settings.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   &#x24;140,000 - &#x24;175,000 a year
  </div>
 </div> 
 <div>
  <div>
   <b>Benefits</b>
  </div>
  <div>
   <b> - Remote work</b> - we trust you to get your work done and make it to your meetings&#x200d;&#x200d;
  </div>
  <div>
   <b> - Competitive salary + stock options</b> - you should have a piece of what we&apos;re building here&#xfe0f;
  </div>
  <div>
   <b> - Flexible schedule</b> - we make it easy to take care of the important things, like your family and health&#x200d; &#xfe0f;
  </div>
  <div>
   <b> - Unlimited PTO + 14 Paid Holidays + Paid Sick Days </b>- we want our employees to have time to care for their personal wellness and mental health&#x200d; &#xfe0f;
  </div>
  <div>
   <b> - Paid maternal/parental leave</b> - enjoy time with your family&apos;s new addition&#x200d;&#x200d;&#x200d;
  </div>
  <ul>
   <li> <b>401(k) + employer match</b></li>
   <li><b>Medical, dental, vision, and life insurance </b>with generous employer contributions</li>
   <li><b>Health savings accounts</b></li>
   <li><b>Life insurance</b></li>
   <li><b>&#x24;1000 office stipend; monthly education and internet stipend</b></li>
  </ul>
  <div>
   <b> - Wellness stipend </b>- use for yoga class, gym membership, or anything that improves your personal wellness&#x200d; &#xfe0f;
  </div>
  <ul>
   <li><b>Apple computer or PC of your choice</b></li>
   <li><b>Bi-annual company retreats</b></li>
  </ul>
  <div></div>
  <div>
   <br> Jobscan is committed to equal pay; diversity, equity, and inclusion; and As we continue to grow, we are always adding more benefits and perks for our team.
  </div>
  <div></div>
  <div>
   <i><br> Jobscan provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws</i>.
  </div>
 </div>
</div>",https://jobs.lever.co/jobscan-2/88ce689e-7f61-4027-bfc7-e727038e90f5,0213c2c18b4e7a68,,Full-time,,,Remote,Senior Data Engineer,19 days ago,2023-09-29T13:33:18.821Z,,,"$140,000 - $175,000 a year",2023-10-18T13:33:18.824Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=0213c2c18b4e7a68&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
37,iRhythm Technologies,"About Us: 
  iRhythm is a leading digital healthcare company focused on the way cardiac arrhythmias are clinically diagnosed by combining our wearable biosensing technology with powerful cloud-based data analytics and machine-learning capabilities. Our goal is to be the leading provider of first-line ambulatory ECG monitoring for patients at risk for arrhythmias. iRhythm's continuous ambulatory monitoring has already put over 6 million patients and their doctors on a shorter path to what they both need – answers. 
  About this Role: 
  As a Sr Research Data Engineer within the iRhythm Research & Systems Engineering team, you will be exposed to a diverse set of engineering problems spanning Machine Learning, Big Data, Multi-Modal data, Data Privacy, AWS, Batch Processing and Mobile technologies in the context of addressing unmet clinical needs. You will have the opportunity to work with a talented team to develop a deep understanding of our end-to-end systems, helping to drive successful delivery of software projects throughout its development life cycle. 
  Note: This role will work remote from a US based home office. We are unable to offer any sort of sponsorship for this role. 
  About You 
  
  You are a quality-minded individual with ability to go detail-oriented and scientific while having the big picture in mind 
  You enjoy investigative and troubleshooting processes using your excellent analytical and problem-solving skills, along with a strong sense of urgency 
  You are self-motivated and demonstrate initiative in helping others 
  You are independent and thrive in the face of ambiguous, open-ended challenges 
  You have effective presentation and interpersonal skills communicating technical information to a variety of stakeholders 
  You enjoy collaborating with cross functional groups 
  
 Responsibilities 
  
  Work with engineering and cross-functional stakeholders to research, define, and write technical requirements, figure out data or operational workflows, and plan for V&V/release activities 
  Design and code pipelines that efficiently transform our raw data into formats and structures that best serve our researchers (Python and algorithm design) 
  Perform exploratory research with physiological data, with good software practices in mind 
  Develop, test, and document proof-of-concept algorithms and/or software tools 
  Participate and support project management, software development, and software QA in the software development life cycle and regulatory submissions as system expert 
  Update and maintain Design History File, participate in risk management/hazards analysis activities 
  Develop, document, and monitor testing regimes that ensure performant code and data integrity 
  Maintain a clean and well-documented code base and audit trail (git, Bitbucket, Confluence, JIRA) 
  
 Basic Qualifications 
  
  Combined 5+ years of algorithm/software development and systems engineering experience in the Healthcare, Medical Device, or other regulated software industry. 
  Software development experience with one or more of Python, Java, C#, or C/C++ 
  Understanding of statistics and proficiency with data analysis. Proficiency in Linux/Unix/Windows environments 
  BA/BS in science or technical field 
  Experience processing and integrating developmental data, clinical study data, EHR, and claims data. 
  Experience selecting and organizing data for training and validating algorithms, managing constraints and requirements. 
  Demonstrated ability to work with structured and unstructured data across multiple modalities. 
  Hands on experience with technologies leveraging multiple servers for data intensive tasks. 
  Experience with software development in Python, including libraries such as pandas and scikit-learn 
  Experience creating data pipelines that transform raw data into insights. 
  Hands on Experience on Data Analytics Services including Athena, Glue Data Catalog and Quicksight 
  Accessing and parsing data from S3 through python API calls 
  Experience and familiarity with AWS Sagemaker 
  Extract, transform and load data from different formats like JSON, databases and integrate results for algorithm training and validation 
  Database familiarity including Oracle, MySQL, Redshift, DynamoDB and Elastic Cache 
  
 Desired/Preferred Technical Qualifications 
  
  MS/PhD in science or technical field 
  Experience with FDA 21 CFR part 820 and IEC standards 62304, 14971, and 62366 
  Experience working with large amounts of data and tools to handle them e.g. AWS, Spark, SQL 
  Working knowledge of signal processing 
  Experience using or managing Splunk, Tableau 
  
 
  What's in it for you: 
   This is a full-time position with competitive compensation package, excellent benefits including medical, dental, and vision insurance (all of which start on your first day), paid holidays, and PTO! 
   iRhythm also provides additional benefits including 401K (with company match), an Employee Stock Purchase Plan, paid parental leave, pet insurance discount, Cultural Committee/Charity events, and so much more! 
   
   FLSA Status: Exempt 
    As a part of our core values, we ensure a diverse and inclusive workforce. We welcome and celebrate people of all backgrounds, experiences, skills, and perspectives. iRhythm Technologies, Inc. is an Equal Opportunity Employer (M/F/V/D). We will consider for employment all qualified applicants with arrest and conviction records in accordance with all applicable laws. 
    Make iRhythm your path forward. 
    #LI-AR1 
    #LI-Remote 
   
 
 
 
  
   Actual compensation may vary depending on job-related factors including knowledge, skills, experience, and work location.
  
  
   Estimated Pay Range
  
    $119,800—$174,500 USD","<div>
 <p><b>About Us:</b></p> 
 <p> iRhythm is a leading digital healthcare company focused on the way cardiac arrhythmias are clinically diagnosed by combining our wearable biosensing technology with powerful cloud-based data analytics and machine-learning capabilities. Our goal is to be the leading provider of first-line ambulatory ECG monitoring for patients at risk for arrhythmias. iRhythm&apos;s continuous ambulatory monitoring has already put over 6 million patients and their doctors on a shorter path to what they both need &#x2013; answers.</p> 
 <p><b> About this Role:</b></p> 
 <p> As a<b> Sr Research Data Engineer </b>within the iRhythm Research &amp; Systems Engineering team, you will be exposed to a diverse set of engineering problems spanning Machine Learning, Big Data, Multi-Modal data, Data Privacy, AWS, Batch Processing and Mobile technologies in the context of addressing unmet clinical needs. You will have the opportunity to work with a talented team to develop a deep understanding of our end-to-end systems, helping to drive successful delivery of software projects throughout its development life cycle.</p> 
 <p> Note: This role will work remote from a US based home office. We are unable to offer any sort of sponsorship for this role.</p> 
 <p><b> About You</b></p> 
 <ul> 
  <li>You are a quality-minded individual with ability to go detail-oriented and scientific while having the big picture in mind</li> 
  <li>You enjoy investigative and troubleshooting processes using your excellent analytical and problem-solving skills, along with a strong sense of urgency</li> 
  <li>You are self-motivated and demonstrate initiative in helping others</li> 
  <li>You are independent and thrive in the face of ambiguous, open-ended challenges</li> 
  <li>You have effective presentation and interpersonal skills communicating technical information to a variety of stakeholders</li> 
  <li>You enjoy collaborating with cross functional groups</li> 
 </ul> 
 <p><b>Responsibilities</b></p> 
 <ul> 
  <li>Work with engineering and cross-functional stakeholders to research, define, and write technical requirements, figure out data or operational workflows, and plan for V&amp;V/release activities</li> 
  <li>Design and code pipelines that efficiently transform our raw data into formats and structures that best serve our researchers (Python and algorithm design)</li> 
  <li>Perform exploratory research with physiological data, with good software practices in mind</li> 
  <li>Develop, test, and document proof-of-concept algorithms and/or software tools</li> 
  <li>Participate and support project management, software development, and software QA in the software development life cycle and regulatory submissions as system expert</li> 
  <li>Update and maintain Design History File, participate in risk management/hazards analysis activities</li> 
  <li>Develop, document, and monitor testing regimes that ensure performant code and data integrity</li> 
  <li>Maintain a clean and well-documented code base and audit trail (git, Bitbucket, Confluence, JIRA)</li> 
 </ul> 
 <p><b>Basic Qualifications</b></p> 
 <ul> 
  <li>Combined 5+ years of algorithm/software development and systems engineering experience in the Healthcare, Medical Device, or other regulated software industry.</li> 
  <li>Software development experience with one or more of Python, Java, C#, or C/C++</li> 
  <li>Understanding of statistics and proficiency with data analysis. Proficiency in Linux/Unix/Windows environments</li> 
  <li>BA/BS in science or technical field</li> 
  <li>Experience processing and integrating developmental data, clinical study data, EHR, and claims data.</li> 
  <li>Experience selecting and organizing data for training and validating algorithms, managing constraints and requirements.</li> 
  <li>Demonstrated ability to work with structured and unstructured data across multiple modalities.</li> 
  <li>Hands on experience with technologies leveraging multiple servers for data intensive tasks.</li> 
  <li>Experience with software development in Python, including libraries such as pandas and scikit-learn</li> 
  <li>Experience creating data pipelines that transform raw data into insights.</li> 
  <li>Hands on Experience on Data Analytics Services including Athena, Glue Data Catalog and Quicksight</li> 
  <li>Accessing and parsing data from S3 through python API calls</li> 
  <li>Experience and familiarity with AWS Sagemaker</li> 
  <li>Extract, transform and load data from different formats like JSON, databases and integrate results for algorithm training and validation</li> 
  <li>Database familiarity including Oracle, MySQL, Redshift, DynamoDB and Elastic Cache</li> 
 </ul> 
 <p><b>Desired/Preferred Technical Qualifications</b></p> 
 <ul> 
  <li>MS/PhD in science or technical field</li> 
  <li>Experience with FDA 21 CFR part 820 and IEC standards 62304, 14971, and 62366</li> 
  <li>Experience working with large amounts of data and tools to handle them e.g. AWS, Spark, SQL</li> 
  <li>Working knowledge of signal processing</li> 
  <li>Experience using or managing Splunk, Tableau</li> 
 </ul> 
 <div>
  <p><b>What&apos;s in it for you:</b></p> 
  <p> This is a full-time position with competitive compensation package, excellent benefits including medical, dental, and vision insurance (all of which start on your first day), paid holidays, and PTO!</p> 
  <p> iRhythm also provides additional benefits including 401K (with company match), an Employee Stock Purchase Plan, paid parental leave, pet insurance discount, Cultural Committee/Charity events, and so much more!</p> 
  <div> 
   <p><b>FLSA Status: </b>Exempt</p> 
   <p><i> As a part of our core values, we ensure a diverse and inclusive workforce. We welcome and celebrate people of all backgrounds, experiences, skills, and perspectives. iRhythm Technologies, Inc. is an Equal Opportunity Employer (M/F/V/D). We will consider for employment all qualified applicants with arrest and conviction records in accordance with all applicable laws.</i></p> 
   <p> Make iRhythm your path forward.</p> 
   <p> #LI-AR1</p> 
   <p> #LI-Remote</p> 
  </div> 
 </div>
 <p></p>
 <div>
  <div>
   <p>Actual compensation may vary depending on job-related factors including knowledge, skills, experience, and work location.</p>
  </div>
  <p></p>
  <p><b><br> Estimated Pay Range</b></p>
  <div>
    &#x24;119,800&#x2014;&#x24;174,500 USD
  </div>
 </div>
</div>",https://www.irhythmtech.com/company/job-openings?gh_jid=5400674&gh_src=c3f698341us,700c104561e30304,,Full-time,,,"Deerfield, IL",Senior Research Data Engineer,12 days ago,2023-10-06T13:33:23.359Z,3.2,95.0,"$119,800 - $174,500 a year",2023-10-18T13:33:23.362Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=700c104561e30304&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
38,CrowdStrike,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We work on large scale distributed systems, processing over 1 trillion events a day with a petabyte of RAM deployed in our Cassandra clusters - and this traffic is growing daily. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to developing and shaping our cybersecurity platform. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
 
 
 
   About the Role
   CrowdStrike is looking to hire a Senior Engineer to the Data Services team to help us take our database systems to the next level. We’re looking for a highly-technical, hands-on engineer, who loves to work with data plane services like Cassandra, ElasticSearch, and Kafka, and is comfortable building automation around large-scale cloud-based critical systems. We’ll be looking at candidate CVs with an eye on achievement - what you’ve accomplished in the past tells us the most about what you can do for us in the future.
 
 
   What You'll Do:
 
 
  
   
     Maintain a deep understanding of the data components - including Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark, and use that understanding to operate and automate properly configured clusters.
   
  
   
     Work with Engineering to roll out new products and features.
   
  
   
     Develop infrastructure services to support the CrowdStrike engineering team’s pursuit of a full devops model.
   
  
   
     Work closely with Engineering and Customer Support to troubleshoot time-sensitive production issues, regardless of when they happen.
   
  
   
     Keep petabytes of critical business data safe, secure, and available.
   
 
 
 
   What You’ll Need:
 
 
  
   
     Experience with large scale datastores using technologies like Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark.
   
  
   
     Experience with large-scale, business-critical Linux environments
   
  
   
     Experience operating within the cloud, preferably Amazon Web Services
   
  
   
     Proven ability to work effectively with both local and remote teams
   
  
   
     Track record of making great decisions, particularly when it matters most
   
  
   
     Rock solid communication skills, verbal and written
   
  
   
     A combination of confidence and independence... with the prudence to know when to ask for help from the rest of the team
   
  
   
     Experience in the information security industry preferred, but not required
   
  
   
     Bachelor’s degree in an applicable field, such as CS, CIS or Engineering
   
 
 
   #LI-SS1
 
 
   #LI-MW1
 
 
   #LI-Remote
 
 
   #HTF
 
 
 
   Benefits of Working at CrowdStrike:
 
 
  
   
     Remote-first culture
   
  
   
     Market leader in compensation and equity awards
   
  
   
     Competitive vacation and flexible working arrangements
   
  
   
     Comprehensive and inclusive health benefits
   
  
   
     Physical and mental wellness programs
   
  
   
     Paid parental leave, including adoption
   
  
   
     A variety of professional development and mentorship opportunities
   
  
   
     Offices with stocked kitchens when you need to fuel innovation and collaboration
   
 
 
 
  
    We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
  
 
 
 
   CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact 
  
   Recruiting@crowdstrike.com
   for further assistance.
 
 
 
   CrowdStrike participates in the E-Verify program.
 
 
  
    Notice of E-Verify Participation
  
 
 
  
    Right to Work
  
  CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $105,000 - $195,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.","<div>
 <div>
  #WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We work on large scale distributed systems, processing over 1 trillion events a day with a petabyte of RAM deployed in our Cassandra clusters - and this traffic is growing daily. We&#x2019;re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to developing and shaping our cybersecurity platform. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
 </div>
 <div></div>
 <div>
   About the Role
  <br> CrowdStrike is looking to hire a Senior Engineer to the Data Services team to help us take our database systems to the next level. We&#x2019;re looking for a highly-technical, hands-on engineer, who loves to work with data plane services like Cassandra, ElasticSearch, and Kafka, and is comfortable building automation around large-scale cloud-based critical systems. We&#x2019;ll be looking at candidate CVs with an eye on achievement - what you&#x2019;ve accomplished in the past tells us the most about what you can do for us in the future.
 </div>
 <div>
  <br> What You&apos;ll Do:
 </div>
 <ul>
  <li>
   <div>
     Maintain a deep understanding of the data components - including Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark, and use that understanding to operate and automate properly configured clusters.
   </div></li>
  <li>
   <div>
     Work with Engineering to roll out new products and features.
   </div></li>
  <li>
   <div>
     Develop infrastructure services to support the CrowdStrike engineering team&#x2019;s pursuit of a full devops model.
   </div></li>
  <li>
   <div>
     Work closely with Engineering and Customer Support to troubleshoot time-sensitive production issues, regardless of when they happen.
   </div></li>
  <li>
   <div>
     Keep petabytes of critical business data safe, secure, and available.
   </div></li>
 </ul>
 <div></div>
 <div>
   What You&#x2019;ll Need:
 </div>
 <ul>
  <li>
   <div>
     Experience with large scale datastores using technologies like Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark.
   </div></li>
  <li>
   <div>
     Experience with large-scale, business-critical Linux environments
   </div></li>
  <li>
   <div>
     Experience operating within the cloud, preferably Amazon Web Services
   </div></li>
  <li>
   <div>
     Proven ability to work effectively with both local and remote teams
   </div></li>
  <li>
   <div>
     Track record of making great decisions, particularly when it matters most
   </div></li>
  <li>
   <div>
     Rock solid communication skills, verbal and written
   </div></li>
  <li>
   <div>
     A combination of confidence and independence... with the prudence to know when to ask for help from the rest of the team
   </div></li>
  <li>
   <div>
     Experience in the information security industry preferred, but not required
   </div></li>
  <li>
   <div>
     Bachelor&#x2019;s degree in an applicable field, such as CS, CIS or Engineering
   </div></li>
 </ul>
 <div>
  <br> #LI-SS1
 </div>
 <div>
   #LI-MW1
 </div>
 <div>
   #LI-Remote
 </div>
 <div>
   #HTF
 </div>
 <div></div>
 <div>
   Benefits of Working at CrowdStrike:
 </div>
 <ul>
  <li>
   <div>
     Remote-first culture
   </div></li>
  <li>
   <div>
     Market leader in compensation and equity awards
   </div></li>
  <li>
   <div>
     Competitive vacation and flexible working arrangements
   </div></li>
  <li>
   <div>
     Comprehensive and inclusive health benefits
   </div></li>
  <li>
   <div>
     Physical and mental wellness programs
   </div></li>
  <li>
   <div>
     Paid parental leave, including adoption
   </div></li>
  <li>
   <div>
     A variety of professional development and mentorship opportunities
   </div></li>
  <li>
   <div>
     Offices with stocked kitchens when you need to fuel innovation and collaboration
   </div></li>
 </ul>
 <div></div>
 <div>
  <div>
    We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
  </div>
 </div>
 <div></div>
 <div>
   CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact 
  <div>
   Recruiting@crowdstrike.com
  </div> for further assistance.
 </div>
 <div></div>
 <div>
   CrowdStrike participates in the E-Verify program.
 </div>
 <div>
  <div>
    Notice of E-Verify Participation
  </div>
 </div>
 <div>
  <div>
    Right to Work
  </div>
 </div> CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is &#x24;105,000 - &#x24;195,000 per year + variable/incentive compensation + equity + benefits. A candidate&#x2019;s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
</div>",https://crowdstrike.wd5.myworkdayjobs.com/en-US/crowdstrikecareers/job/USA---Remote/Sr-Systems-Engineer---Data-Services--Remote-_R14626?indeed_organic,86616a549b160eb2,,Full-time,,,Remote,Sr. Systems Engineer - Data Services (Remote),11 days ago,2023-10-07T13:33:32.605Z,3.6,36.0,"$105,000 - $195,000 a year",2023-10-18T13:33:32.608Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=86616a549b160eb2&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
39,Cypress Consulting,"Sr. Data Center Engineer/Network Security Consultant
We are seeking an Advanced Services Engineer to provide advanced support, guidance and assistance to address specific customer needs. In this position, you will be working as a technology expert in the Routing & Switching space to design, implement, and support (troubleshoot) the deployments within a customer infrastructure. The ideal candidate will also have a level of comfort communicating across all functions within the organization, as well as with clients and partners. Previous Service Provider experience is a huge plus.
Essential Functions of the Job:

 You will provide advanced post-sales support for a large Data Center networking deployment for a large customer.
 Review customer network designs for an EVPN, VxLAN, leaf-spine architecture and make recommendations for deployment
 Migrate or interconnect to/from Cisco, Juniper and other vendors to Arista infrastructure
 Assist with configuration build outs including creating network provisioning automation using Python and tools such as Chef or Ansible
 Assist with implementation and change controls
 You will assist with proof of concepts (POC) and in-depth testing to validate design scenario
 Provide bug scrubs and code recommendations
 Provide interface to TAC and internal development teams and the customer
 You will provide customer advice regarding architectural questions, product prerequisites, product features, etc.
 Translate complex business requirements into Leaf-Spine Network solutions
 Designing Network solutions utilizing extensive experience with routing protocols including advanced BGP design as well as extensive EVPN/VXLAN experience.
 Establish and maintaining strong relationships with key partners
 Attend key partner events, training sessions, and provide ongoing training with the customer teams globally
 Continue training to maintain expertise
 Ability to understand the client’s business objectives and technical needs
 Ability to meet Service Level Agreements (SLAs) for sales and clients
 Regularly exercises discretion and independent judgment
 Maintain professional relationships with teammates, partners, and clients
 Some travel may be required within assigned territory

Qualifications
Required Skills and Experience

 Bachelor’s degree in Computer Science or equivalent years of experience
 Network Industry Certification preferred (e.g. CCIE (R&S), JNCIE)
 5+ years’ working experience with network technologies including network design and deployments of Campus and Data Center networks. Knowledge of leaf-spine architectures highly desired.
 5+ years’ minimum experience with Cisco-based technologies focusing on infrastructure and voice
 Demonstrated experience in technical post-sales, as either a Network Consulting Engineer or as an Advanced Systems (AS) Engineer
 Experience with Cisco enterprise routing/switching within large data center enterprise customers (Catalyst, Nexus, ASR)
 Expert knowledge in the following areas: Ethernet, VLANs, VxLAN, EVPN, IP Routing, TCP/IP, OSPF, BGP, eBGP, Multicast, QoS
 Expertise in at least one area of Data Center related technologies - Openstack, SDN, NFV, Load Balancers, Virtualization, Linux tools
 Expert level knowledge of industry-standard CLI
 Ability to write white papers a plus
 Background in Perl, Python, Scripting for creating network automation is highly desired
 Excellent customer service and verbal communication skills
 Excellent written skills and the ability to do related documentation and ticket tracking of opportunities/meeting follow-up

Job Type: Full-time
Pay: $115.00 - $125.00 per hour
Benefits:

 Dental insurance
 Health insurance
 Paid time off
 Professional development assistance
 Referral program
 Retirement plan
 Vision insurance

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Experience:

 Data Center Design: 5 years (Required)
 BGP, SD-WAN, VXLAN: 5 years (Required)
 Consultative/Customer-facing: 3 years (Required)
 Layer 2/3 switching and routing: 2 years (Required)
 Service Provider: 2 years (Preferred)

Work Location: Remote","<p><b>Sr. Data Center Engineer/Network Security Consultant</b></p>
<p>We are seeking an Advanced Services Engineer to provide advanced support, guidance and assistance to address specific customer needs. In this position, you will be working as a technology expert in the Routing &amp; Switching space to design, implement, and support (troubleshoot) the deployments within a customer infrastructure. The ideal candidate will also have a level of comfort communicating across all functions within the organization, as well as with clients and partners. Previous Service Provider experience is a huge plus.</p>
<p><b>Essential Functions of the Job:</b></p>
<ul>
 <li>You will provide advanced post-sales support for a large Data Center networking deployment for a large customer.</li>
 <li>Review customer network designs for an EVPN, VxLAN, leaf-spine architecture and make recommendations for deployment</li>
 <li>Migrate or interconnect to/from Cisco, Juniper and other vendors to Arista infrastructure</li>
 <li>Assist with configuration build outs including creating network provisioning automation using Python and tools such as Chef or Ansible</li>
 <li>Assist with implementation and change controls</li>
 <li>You will assist with proof of concepts (POC) and in-depth testing to validate design scenario</li>
 <li>Provide bug scrubs and code recommendations</li>
 <li>Provide interface to TAC and internal development teams and the customer</li>
 <li>You will provide customer advice regarding architectural questions, product prerequisites, product features, etc.</li>
 <li>Translate complex business requirements into Leaf-Spine Network solutions</li>
 <li>Designing Network solutions utilizing extensive experience with routing protocols including advanced BGP design as well as extensive EVPN/VXLAN experience.</li>
 <li>Establish and maintaining strong relationships with key partners</li>
 <li>Attend key partner events, training sessions, and provide ongoing training with the customer teams globally</li>
 <li>Continue training to maintain expertise</li>
 <li>Ability to understand the client&#x2019;s business objectives and technical needs</li>
 <li>Ability to meet Service Level Agreements (SLAs) for sales and clients</li>
 <li>Regularly exercises discretion and independent judgment</li>
 <li>Maintain professional relationships with teammates, partners, and clients</li>
 <li>Some travel may be required within assigned territory</li>
</ul>
<p><b>Qualifications</b></p>
<p><b>Required Skills and Experience</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in Computer Science or equivalent years of experience</li>
 <li>Network Industry Certification preferred (e.g. CCIE (R&amp;S), JNCIE)</li>
 <li>5+ years&#x2019; working experience with network technologies including network design and deployments of Campus and Data Center networks. Knowledge of leaf-spine architectures highly desired.</li>
 <li>5+ years&#x2019; minimum experience with Cisco-based technologies focusing on infrastructure and voice</li>
 <li>Demonstrated experience in technical post-sales, as either a Network Consulting Engineer or as an Advanced Systems (AS) Engineer</li>
 <li>Experience with Cisco enterprise routing/switching within large data center enterprise customers (Catalyst, Nexus, ASR)</li>
 <li>Expert knowledge in the following areas: Ethernet, VLANs, VxLAN, EVPN, IP Routing, TCP/IP, OSPF, BGP, eBGP, Multicast, QoS</li>
 <li>Expertise in at least one area of Data Center related technologies - Openstack, SDN, NFV, Load Balancers, Virtualization, Linux tools</li>
 <li>Expert level knowledge of industry-standard CLI</li>
 <li>Ability to write white papers a plus</li>
 <li>Background in Perl, Python, Scripting for creating network automation is highly desired</li>
 <li>Excellent customer service and verbal communication skills</li>
 <li>Excellent written skills and the ability to do related documentation and ticket tracking of opportunities/meeting follow-up</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;115.00 - &#x24;125.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Referral program</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Center Design: 5 years (Required)</li>
 <li>BGP, SD-WAN, VXLAN: 5 years (Required)</li>
 <li>Consultative/Customer-facing: 3 years (Required)</li>
 <li>Layer 2/3 switching and routing: 2 years (Required)</li>
 <li>Service Provider: 2 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,94a5ae6b3646efe1,,Full-time,,,Remote,Sr. Data Center Design Engineer/Consultant -100% Remote,11 days ago,2023-10-07T13:33:40.341Z,,,$115 - $125 an hour,2023-10-18T13:33:40.343Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=94a5ae6b3646efe1&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
40,Cypress Consulting,"Sr. Data Center Engineer/Deployment Engineer
We are seeking a Deployment Engineer to provide advanced support, guidance and assistance to address specific customer needs. In this position, you will be working as a technology expert in the Routing & Switching space to design, implement, and support (troubleshoot) the deployments within a customer infrastructure. The ideal candidate will also have a level of comfort communicating across all functions within the organization, as well as with clients and partners. Arista configuration and CloudVision usage experience is ideal. Should be familiar with EVPN/VXLAN and proven deployment models. Should also be comfortable with documentation and tools like MS Excel/Viso and word.
Essential Functions of the Job:

 You will provide advanced post-sales support for a large Data Center networking deployment for a large customer.
 Review customer network designs for an EVPN, VxLAN, leaf-spine architecture and make recommendations for deployment
 Migrate or interconnect to/from Cisco, Juniper and other vendors to Arista infrastructure
 Assist with implementation and change controls
 You will assist with proof of concepts (POC) and in-depth testing to validate design scenario
 Provide interface to TAC and internal development teams and the customer
 Designing Network solutions utilizing extensive experience with routing protocols including advanced BGP design as well as extensive EVPN/VXLAN experience.
 Establish and maintaining strong relationships with key partners
 Continue training to maintain expertise
 Ability to understand the client’s business objectives and technical needs
 Ability to meet Service Level Agreements (SLAs) for sales and clients
 Regularly exercises discretion and independent judgment
 Maintain professional relationships with teammates, partners, and clients
 Some travel may be required within assigned territory

Qualifications
Required Skills and Experience

 Bachelor’s degree in Computer Science or equivalent years of experience
 5+ years’ working experience with network technologies including network design and deployments of Campus and Data Center networks. Knowledge of leaf-spine architectures highly desired.
 5+ Years’ minimum experience with Cisco-based technologies focusing on infrastructure and voice
 Experience with Cisco enterprise routing/switching within large data center enterprise customers (Catalyst, Nexus, ASR)
 Expert knowledge in the following areas: Ethernet, VLANs, VxLAN, EVPN, IP Routing, TCP/IP, OSPF, BGP, eBGP, Multicast, QoS
 Background in Perl, Python, Scripting for creating network automation is highly desired
 Excellent customer service and verbal communication skills
 Excellent written skills and the ability to do related documentation and ticket tracking of opportunities/meeting follow-up

Job Type: Full-time
Pay: $115.00 - $125.00 per hour
Benefits:

 Dental insurance
 Health insurance
 Paid time off
 Professional development assistance
 Referral program
 Retirement plan
 Vision insurance

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Experience:

 Layer 2/3 switching and routing: 2 years (Required)
 Data Center Design: 3 years (Required)
 BGP, SD-WAN, VXLAN: 3 years (Required)
 Consultative/Customer-facing: 2 years (Required)

Work Location: Remote","<p><b>Sr. Data Center Engineer/Deployment Engineer</b></p>
<p>We are seeking a Deployment Engineer to provide advanced support, guidance and assistance to address specific customer needs. In this position, you will be working as a technology expert in the Routing &amp; Switching space to design, implement, and support (troubleshoot) the deployments within a customer infrastructure. The ideal candidate will also have a level of comfort communicating across all functions within the organization, as well as with clients and partners. Arista configuration and CloudVision usage experience is ideal. Should be familiar with EVPN/VXLAN and proven deployment models. Should also be comfortable with documentation and tools like MS Excel/Viso and word.</p>
<p><b>Essential Functions of the Job:</b></p>
<ul>
 <li>You will provide advanced post-sales support for a large Data Center networking deployment for a large customer.</li>
 <li>Review customer network designs for an EVPN, VxLAN, leaf-spine architecture and make recommendations for deployment</li>
 <li>Migrate or interconnect to/from Cisco, Juniper and other vendors to Arista infrastructure</li>
 <li>Assist with implementation and change controls</li>
 <li>You will assist with proof of concepts (POC) and in-depth testing to validate design scenario</li>
 <li>Provide interface to TAC and internal development teams and the customer</li>
 <li>Designing Network solutions utilizing extensive experience with routing protocols including advanced BGP design as well as extensive EVPN/VXLAN experience.</li>
 <li>Establish and maintaining strong relationships with key partners</li>
 <li>Continue training to maintain expertise</li>
 <li>Ability to understand the client&#x2019;s business objectives and technical needs</li>
 <li>Ability to meet Service Level Agreements (SLAs) for sales and clients</li>
 <li>Regularly exercises discretion and independent judgment</li>
 <li>Maintain professional relationships with teammates, partners, and clients</li>
 <li>Some travel may be required within assigned territory</li>
</ul>
<p><b>Qualifications</b></p>
<p><b>Required Skills and Experience</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in Computer Science or equivalent years of experience</li>
 <li>5+ years&#x2019; working experience with network technologies including network design and deployments of Campus and Data Center networks. Knowledge of leaf-spine architectures highly desired.</li>
 <li>5+ Years&#x2019; minimum experience with Cisco-based technologies focusing on infrastructure and voice</li>
 <li>Experience with Cisco enterprise routing/switching within large data center enterprise customers (Catalyst, Nexus, ASR)</li>
 <li>Expert knowledge in the following areas: Ethernet, VLANs, VxLAN, EVPN, IP Routing, TCP/IP, OSPF, BGP, eBGP, Multicast, QoS</li>
 <li>Background in Perl, Python, Scripting for creating network automation is highly desired</li>
 <li>Excellent customer service and verbal communication skills</li>
 <li>Excellent written skills and the ability to do related documentation and ticket tracking of opportunities/meeting follow-up</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;115.00 - &#x24;125.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Referral program</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Layer 2/3 switching and routing: 2 years (Required)</li>
 <li>Data Center Design: 3 years (Required)</li>
 <li>BGP, SD-WAN, VXLAN: 3 years (Required)</li>
 <li>Consultative/Customer-facing: 2 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,019fe648ac2dfa54,,Full-time,,,Remote,Sr. Data Center Design Engineer/Deployment Engineer -100% Remote,11 days ago,2023-10-07T13:33:40.403Z,,,$115 - $125 an hour,2023-10-18T13:33:40.405Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=019fe648ac2dfa54&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
41,Blackhawk Network,"About Blackhawk Network: 
 
   Blackhawk Network (BHN) is the leader in global branded payment technologies. We strengthen relationships between brands and their customers, employees, and partners by transforming transactions into connections. BHN’s portfolio includes: Gift Card & eGift products, promotions and distribution that grow revenue faster; Rewards & Incentives that build loyalty and acquisition and are integrated into today’s leading platforms; and Payments that enable businesses and customers to access and disburse funds in convenient and innovative ways. BHN’s network spans across the globe with over 400,000 consumer touchpoints. Learn more at BHN.com.
 
 
 
   This position may be performed remotely anywhere within the United States except for the State of Alaska, North Dakota, or South Dakota.
  Overview: 
 
   We are looking for an experienced data scientist to lead the initiatives we’re taking, to improve the product offerings of Blackhawk Network, from the perspective of risk modelling and business forecasting (prescriptive & predictive).
 
 
 
   As a team member in core data science team, you will own the research charter for data and decision science to enable the business stakeholders to be data-driven and deterministic, by providing insights into the decisions at-hand and also roadmap planning.
 
 
 
   You will be working in a team of Data Scientists to enable a culture of 360-degree analysis of business, with ownership of the modelling environments and ML models. You will get the support to evangelize sound practices for prototyping of concepts, to fail-fast and/or maintain continuum of persistent research.
 
 
 
   You will collaborate with multi-disciplinary teams of engineers, product owners & business stakeholders to solve complex & ambiguous problems, in the domains of:
 
 
   Gift Cards E-Commerce (B2B & B2C) 
  Forecasting of Inventory, Traffic & Breakage 
  Risk Modelling (Scorecards) 
  Fraud Detection & Prevention 
  Loyalty & Rewards Programs Modelling 
 Responsibilities: 
 
   You will:
 
 
   Partner closely with Product, Engineering, Marketing, Sales, Finance and Data Science teams to shape product strategy using rigorous scientific solutions
   Apply statistical, machine learning and econometric models on large datasets to: i) measure results and outcomes of our current models and product strategies, ii) optimize user experience while minimizing fraud/risks
   Design, conduct, and analyze experiments to quantify the impact of product and operation changes
   Develop metrics to guide product development, and create dashboards for key performance indicators and deep dive to understand the drivers
   Drive the collection of new data and the refinement of existing data sources
  Qualifications: 
 
  Masters (or equivalent) degree in a quantitative discipline (Statistics, Operations Research, Data Science, Mathematics, Physics, Engineering etc.). 
  A strong background in advanced mathematics, in particular statistics & probability theory, data mining, and machine learning. 
  5+ years of overall professional experience with 3+ years in data science, doing exploratory data analysis, testing hypotheses, and building prescriptive & predictive models.
   Demonstrated experience in creation, deployment and performance evaluation of supervised and unsupervised machine learning models.
   Proficiency in Python. 
  Experience handling terabyte size datasets, diving into data to discover hidden patterns, using data visualization tools, writing SQL.
   Strong Communication: ability to articulate clearly, navigate & adapt across different seniority levels. 
  Ability to use statistical, algorithmic, data mining, and visualization techniques to model complex problems, find opportunities, discover solutions, and deliver actionable business insights. 
  Be passionate about collaborating daily with your team and other groups while working via a distributed global operating model. 
  Be eager to help your teammates, share your knowledge with them, and learn from them. 
 
 
  Preferred Qualifications
 
 
   Experience with MLOPs frameworks
   Experience with creating explainable ML model
   Experience with source code control systems like Git
   Experience with data visualization and business intelligence tools like PowerBI
   Knowledge of experimental design and A/B testing
   Experience in an Agile development environment
   Experience with AWS cloud environment
   Proven ability to quickly learn and apply new techniques
   Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment 
  Must be a team-player and capable of handling multi-tasks in a dynamic environment
   Excellent business writing, verbal communication, and presentation skills
  Benefits: 
 
   Salary Range for all U.S. Residents (excluding Alaska, California, North Dakota, South Dakota): $125,430.00 to $149,000.00
 
 
   Salary Range for California Residents Only: $99,530.00 to $149,000.00
 
 
 
   Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Blackhawk Network offers benefits including 401k with employer match, medical, dental, vision, 12 paid holidays in the year 2023, sick pay accrual according to state law, parental leave, life insurance, disability insurance, accident and illness insurance, health and dependent care flexible spending accounts, wellness benefits, and flexible time off for all full-time employees. 
 EEO Statement: 
 
   Blackhawk Network provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Blackhawk Network believes that diversity leads to strength. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
 
 
 
   Blackhawk Network encourages applicants with previous criminal records to apply to all positions and, pursuant to the San Francisco and Los Angeles Fair Chance Acts (and other “Fair Chance” laws), Blackhawk Network will consider for employment qualified applicants with arrest and conviction records. For Philadelphia applicants or jobs, please see a copy of Philadelphia’s ordinance on this topic by clicking this link: https://codelibrary.amlegal.com/codes/philadelphia/latest/philadelphia_pa/0-0-0-280104.","<div>
 About Blackhawk Network: 
 <div>
   Blackhawk Network (BHN) is the leader in global branded payment technologies. We strengthen relationships between brands and their customers, employees, and partners by transforming transactions into connections. BHN&#x2019;s portfolio includes: Gift Card &amp; eGift products, promotions and distribution that grow revenue faster; Rewards &amp; Incentives that build loyalty and acquisition and are integrated into today&#x2019;s leading platforms; and Payments that enable businesses and customers to access and disburse funds in convenient and innovative ways. BHN&#x2019;s network spans across the globe with over 400,000 consumer touchpoints. Learn more at BHN.com.
 </div>
 <div></div>
 <div>
  <br> This position may be performed remotely anywhere within the United States except for the State of Alaska, North Dakota, or South Dakota.
 </div> Overview: 
 <div>
   We are looking for an experienced data scientist to lead the initiatives we&#x2019;re taking, to improve the product offerings of Blackhawk Network, from the perspective of risk modelling and business forecasting (prescriptive &amp; predictive).
 </div>
 <div></div>
 <div>
  <br> As a team member in core data science team, you will own the research charter for data and decision science to enable the business stakeholders to be data-driven and deterministic, by providing insights into the decisions at-hand and also roadmap planning.
 </div>
 <div></div>
 <div>
  <br> You will be working in a team of Data Scientists to enable a culture of 360-degree analysis of business, with ownership of the modelling environments and ML models. You will get the support to evangelize sound practices for prototyping of concepts, to fail-fast and/or maintain continuum of persistent research.
 </div>
 <div></div>
 <div>
  <br> You will collaborate with multi-disciplinary teams of engineers, product owners &amp; business stakeholders to solve complex &amp; ambiguous problems, in the domains of:
 </div>
 <ul>
  <li> Gift Cards E-Commerce (B2B &amp; B2C) </li>
  <li>Forecasting of Inventory, Traffic &amp; Breakage </li>
  <li>Risk Modelling (Scorecards) </li>
  <li>Fraud Detection &amp; Prevention </li>
  <li>Loyalty &amp; Rewards Programs Modelling </li>
 </ul>Responsibilities: 
 <div>
   You will:
 </div>
 <ul>
  <li> Partner closely with Product, Engineering, Marketing, Sales, Finance and Data Science teams to shape product strategy using rigorous scientific solutions</li>
  <li> Apply statistical, machine learning and econometric models on large datasets to: i) measure results and outcomes of our current models and product strategies, ii) optimize user experience while minimizing fraud/risks</li>
  <li> Design, conduct, and analyze experiments to quantify the impact of product and operation changes</li>
  <li> Develop metrics to guide product development, and create dashboards for key performance indicators and deep dive to understand the drivers</li>
  <li> Drive the collection of new data and the refinement of existing data sources</li>
 </ul> Qualifications: 
 <ul>
  <li>Masters (or equivalent) degree in a quantitative discipline (Statistics, Operations Research, Data Science, Mathematics, Physics, Engineering etc.). </li>
  <li>A strong background in advanced mathematics, in particular statistics &amp; probability theory, data mining, and machine learning. </li>
  <li>5+ years of overall professional experience with 3+ years in data science, doing exploratory data analysis, testing hypotheses, and building prescriptive &amp; predictive models.</li>
  <li> Demonstrated experience in creation, deployment and performance evaluation of supervised and unsupervised machine learning models.</li>
  <li> Proficiency in Python. </li>
  <li>Experience handling terabyte size datasets, diving into data to discover hidden patterns, using data visualization tools, writing SQL.</li>
  <li> Strong Communication: ability to articulate clearly, navigate &amp; adapt across different seniority levels. </li>
  <li>Ability to use statistical, algorithmic, data mining, and visualization techniques to model complex problems, find opportunities, discover solutions, and deliver actionable business insights. </li>
  <li>Be passionate about collaborating daily with your team and other groups while working via a distributed global operating model. </li>
  <li>Be eager to help your teammates, share your knowledge with them, and learn from them. </li>
 </ul>
 <div>
  <b>Preferred Qualifications</b>
 </div>
 <ul>
  <li> Experience with MLOPs frameworks</li>
  <li> Experience with creating explainable ML model</li>
  <li> Experience with source code control systems like Git</li>
  <li> Experience with data visualization and business intelligence tools like PowerBI</li>
  <li> Knowledge of experimental design and A/B testing</li>
  <li> Experience in an Agile development environment</li>
  <li> Experience with AWS cloud environment</li>
  <li> Proven ability to quickly learn and apply new techniques</li>
  <li> Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment </li>
  <li>Must be a team-player and capable of handling multi-tasks in a dynamic environment</li>
  <li> Excellent business writing, verbal communication, and presentation skills</li>
 </ul> Benefits: 
 <div>
   Salary Range for all U.S. Residents (excluding Alaska, California, North Dakota, South Dakota): &#x24;125,430.00 to &#x24;149,000.00
 </div>
 <div>
   Salary Range for California Residents Only: &#x24;99,530.00 to &#x24;149,000.00
 </div>
 <div></div>
 <div>
  <br> Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Blackhawk Network offers benefits including 401k with employer match, medical, dental, vision, 12 paid holidays in the year 2023, sick pay accrual according to state law, parental leave, life insurance, disability insurance, accident and illness insurance, health and dependent care flexible spending accounts, wellness benefits, and flexible time off for all full-time employees. 
 </div>EEO Statement: 
 <div>
   Blackhawk Network provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Blackhawk Network believes that diversity leads to strength. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
 </div>
 <div></div>
 <div>
  <br> Blackhawk Network encourages applicants with previous criminal records to apply to all positions and, pursuant to the San Francisco and Los Angeles Fair Chance Acts (and other &#x201c;Fair Chance&#x201d; laws), Blackhawk Network will consider for employment qualified applicants with arrest and conviction records. For Philadelphia applicants or jobs, please see a copy of Philadelphia&#x2019;s ordinance on this topic by clicking this link: https://codelibrary.amlegal.com/codes/philadelphia/latest/philadelphia_pa/0-0-0-280104.
 </div>
</div>",https://careers-blackhawknetwork.icims.com/jobs/20300/job?utm_source=indeed_integration&iis=Job%20Board&iisn=Indeed&indeed-apply-token=73a2d2b2a8d6d5c0a62696875eaebd669103652d3f0c2cd5445d3e66b1592b0f,8205e7f76522b355,,Full-time,,,Remote,Staff Software Engineer - Data science and Machine Learning (Remote),12 days ago,2023-10-06T13:33:37.343Z,3.5,206.0,"$99,530 - $149,000 a year",2023-10-18T13:33:37.345Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=8205e7f76522b355&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
42,Baylor Scott & White Health,"The Lead Data Protection Engineer will be responsible for overseeing and implementing CIS Control 3 (Data Protection) strategies and solutions to safeguard our organization’s critical data assets. This role will play a pivotal role in enhancing data protection measures across the organization, developing information security policies and standards, introducing security best practices, and supporting the implementation and refinement of selected technologies to support a continuous growth model. This role will also be responsible for building and maintaining strong relationships with service line leaders, vendors, and other departments to help collectively further BSWH’s strategic initiatives. They will assist respective IS Directors and/or leadership with understanding the role the data protection applications play in relation to the IS Application environment and managing architecture of the technology as needed to support the data protection controls. 
  SALARY 
  The pay range for this position is $116,521 (entry-level qualifications) - $209,560 (highly experienced) The specific rate will depend upon the successful candidate’s specific qualifications and prior experience. 
  ESSENTIAL FUNCTIONS OF THE ROLE 
  Responsible for the day-to-day operational activities of the Data Loss Prevention team that includes implementation, support and maintenance of the technology and management of the managed service provider. 
  Ensures that all system platforms are functional and secure. 
  Ensures fulfillment of legal and contractual information security and privacy mandates. 
  Creates and drives long term planning and strategic vision creation for continuous improvement to the information management program, with oversight from Director as needed. 
  Establishes budget and tracks to ensure budget is on track and aligned with strategic goals. 
  Understanding of interdependencies of healthcare landscape and its influence on portfolio. 
  Owns assorted departmental responsibilities as assigned (ie. contract management staffing) 
  Partners with and leads team towards the identification of problems opportunities from a digital innovation perspective, oversee and perform the development and documentation of business requirements, objectives, deliverables, and specifications, and collaboration with customers employees, and support staff. 
  Actively maintains a pulse on digital disruption, innovation, and healthcare. 
  Provides recommendations for improvements that meet team objectives. 
  Presents and explains findings to leadership team and individuals. 
  KEY SUCCESS FACTORS 
  
  Superior leadership, problem solving, team building, and decision-making skills. 
  Skilled project manager with ability to articulate business needs. 
  Excellent written, verbal, and interpersonal communication skills. 
  Proficient computer software and database skills, including Microsoft Products (Excel, SharePoint, Teams, Forms, etc.). 
  Ability to focus and prioritize strategic objectives and work in a growing and challenging environment. 
  Maintains a broad knowledge of state-of-the-art technology equipment and systems. 
  
 LOCATION: Remote 
  SCHEDULE: Full Time 
  BENEFITS 
  Our competitive benefits package includes the following 
  
  Immediate eligibility for health and welfare benefits 
  401(k) savings plan with dollar-for-dollar match up to 5% 
  Tuition Reimbursement 
  PTO accrual beginning Day 1 
  
 Note: Benefits may vary based upon position type and/or level
  QUALIFICATIONS 
 
  EDUCATION - Bachelor's or 4 years of work experience above the minimum qualification 
  EXPERIENCE - 7 Years of Experience","<div>
 <p>The <b>Lead Data Protection Engineer</b> will be responsible for overseeing and implementing CIS Control 3 (Data Protection) strategies and solutions to safeguard our organization&#x2019;s critical data assets. This role will play a pivotal role in enhancing data protection measures across the organization, developing information security policies and standards, introducing security best practices, and supporting the implementation and refinement of selected technologies to support a continuous growth model. This role will also be responsible for building and maintaining strong relationships with service line leaders, vendors, and other departments to help collectively further BSWH&#x2019;s strategic initiatives. They will assist respective IS Directors and/or leadership with understanding the role the data protection applications play in relation to the IS Application environment and managing architecture of the technology as needed to support the data protection controls.</p> 
 <p><b> SALARY</b></p> 
 <p> The pay range for this position is &#x24;116,521 (entry-level qualifications) - &#x24;209,560 (highly experienced) The specific rate will depend upon the successful candidate&#x2019;s specific qualifications and prior experience.</p> 
 <p><b> ESSENTIAL FUNCTIONS OF THE ROLE</b></p> 
 <p> Responsible for the day-to-day operational activities of the Data Loss Prevention team that includes implementation, support and maintenance of the technology and management of the managed service provider.</p> 
 <p> Ensures that all system platforms are functional and secure.</p> 
 <p> Ensures fulfillment of legal and contractual information security and privacy mandates.</p> 
 <p> Creates and drives long term planning and strategic vision creation for continuous improvement to the information management program, with oversight from Director as needed.</p> 
 <p> Establishes budget and tracks to ensure budget is on track and aligned with strategic goals.</p> 
 <p> Understanding of interdependencies of healthcare landscape and its influence on portfolio.</p> 
 <p> Owns assorted departmental responsibilities as assigned (ie. contract management staffing)</p> 
 <p> Partners with and leads team towards the identification of problems opportunities from a digital innovation perspective, oversee and perform the development and documentation of business requirements, objectives, deliverables, and specifications, and collaboration with customers employees, and support staff.</p> 
 <p> Actively maintains a pulse on digital disruption, innovation, and healthcare.</p> 
 <p> Provides recommendations for improvements that meet team objectives.</p> 
 <p> Presents and explains findings to leadership team and individuals.</p> 
 <p><b> KEY SUCCESS FACTORS</b></p> 
 <ul> 
  <li>Superior leadership, problem solving, team building, and decision-making skills.</li> 
  <li>Skilled project manager with ability to articulate business needs.</li> 
  <li>Excellent written, verbal, and interpersonal communication skills.</li> 
  <li>Proficient computer software and database skills, including Microsoft Products (Excel, SharePoint, Teams, Forms, etc.).</li> 
  <li>Ability to focus and prioritize strategic objectives and work in a growing and challenging environment.</li> 
  <li>Maintains a broad knowledge of state-of-the-art technology equipment and systems.</li> 
 </ul> 
 <p><b>LOCATION: Remote</b></p> 
 <p><b> SCHEDULE: Full Time</b></p> 
 <p><b> BENEFITS</b></p> 
 <p> Our competitive benefits package includes the following</p> 
 <ul> 
  <li>Immediate eligibility for health and welfare benefits</li> 
  <li>401(k) savings plan with dollar-for-dollar match up to 5%</li> 
  <li>Tuition Reimbursement</li> 
  <li>PTO accrual beginning Day 1</li> 
 </ul> 
 <p>Note: Benefits may vary based upon position type and/or level</p>
 <p><b> QUALIFICATIONS</b></p> 
 <ul>
  <li>EDUCATION - Bachelor&apos;s or 4 years of work experience above the minimum qualification</li> 
  <li>EXPERIENCE - 7 Years of Experience</li>
 </ul>
</div>",https://jobs.bswhealth.com/us/en/job/23018360/Lead-Data-Protection-Engineer?rx_campaign=indeed0&rx_ch=jobp4p&rx_group=119123&rx_job=23018360&rx_r=none&rx_source=Indeed&rx_ts=20231018T120047Z&rx_vp=cpc&utm_source=indeedorganic&rx_p=N1RFXMVXVV&rx_viewer=f260f8ff6dba11ee904659a2e7370c6b44634e17d02e43f4b49efd1fe7534128,780bbe8f6d47e19d,,Full-time,,,Remote,Lead Data Protection Engineer,11 days ago,2023-10-07T13:33:34.053Z,3.8,4045.0,"$116,521 a year",2023-10-18T13:33:34.055Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=780bbe8f6d47e19d&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
43,Emerson United Inc,"Overall 7-8 years of experience
Must Have :
Databricks/py-spark – 4+ years of experience
Cloud experience – 4+ years of cloud experience
Nice to have:
Preferable Software engineering background
Preferable AWS experience
Experience with Typescript
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:

 7 years

Schedule:

 8 hour shift

Experience:

 AWS: 6 years (Required)
 Databricks: 4 years (Required)
 Pyspark: 4 years (Required)

Work Location: Remote","<p>Overall 7-8 years of experience</p>
<p>Must Have :</p>
<p>Databricks/py-spark &#x2013; 4+ years of experience</p>
<p>Cloud experience &#x2013; 4+ years of cloud experience</p>
<p>Nice to have:</p>
<p>Preferable Software engineering background</p>
<p>Preferable AWS experience</p>
<p>Experience with Typescript</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;75.00 - &#x24;80.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>AWS: 6 years (Required)</li>
 <li>Databricks: 4 years (Required)</li>
 <li>Pyspark: 4 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,5f5de83e70b15c74,,Contract,,,Remote,AWS Data Engineer,5 days ago,2023-10-13T13:33:48.811Z,,,$75 - $80 an hour,2023-10-18T13:33:48.812Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BSRaD9lfi1M4GEnG4pnFsPi92N_zg6XQTKM2RXCnXf0V5919SIZyeemH_4jgA_bdpI9HgUwgut0jGaaF2iDLdiYuuvWAuwswCdYF4kR-bJDD_O6Q2L82HD6YXZ3q7Qi33t3scop2VRFHBIB2Q10usrg63hXNqu_Ee38ReI34fzeiOOjPfK-lIfTud6ULu-M1hDvbwRx5LGmO8zabL5QD2y6M-SwEPH3A7dTqmoVZLiQwolSnrYfX0hOehpP07eWDwM4a9EzJXpyhrD_6xsSO3f221I5IZDW1apUJzIKikQbvPlYBg38zrKVHIAwTmvv4jbpS1QrqiOGb3LWHTFceJ4S_lEladPHH6nNb5c91RkYSQC6-suCChSGhszn-k_cAKswpOf_tp2T6MiFHzp5h0gTZISaPmJLHuHzodQqntJ1W6si_Krb-Ag2KTJCYpf4l7yC5FymY4yzrRgMdzKXrQBBvakSyNGiGeQPTzq7qqA_FKTSkpw7WUd0513akBX6lFmemaFun-hor6VrpjjZDHUiKogEx0l_LpWd1MH6q48xQiGhlw1iPdM-XVc0S0-0uEuyekRngLW7Or46HuJNo5n&xkcb=SoBs-_M3JhX_1MWas50FbzkdCdPP&p=14&fvj=1&vjs=3&jsa=4707&tk=1hd1fs0bnk7b3800&from=jasx&wvign=1
44,Lucayan Technology Solutions,"Description:
10 Months Contract
Required skills:Data Engineering – They need to be stronger.Azure – high need – should have experience here.Kubernetes – have been very stringent with here.Machine Learning.Python – must have.Spark - also required.Jupterhub/ mlflow/ databricks/ kubeflow – Have been deployed and worked on any of this tool.
Secondary Skills - Nice to Haves:HELMArgo Workflow
Job Description:The Engineer is responsible for developing, implementing, and maintaining technical software applications and provides a combination of technical and business leadership while being the primary trusted and capable owner of one or more high priority, high visibility, complex initiatives.This Engineer will typically lead and coach a small number of team members and provide guidance to team members and multiple.You will design, develop, test, deploy, maintain, and enhance Machine Learning Pipelines using K8s/AKS based Argo Workflow Orchestration solutions.Participate and contribute to design reviews with platform engineering team to decide the design, technologies, project priorities, deadlines, and deliverables.You will work closely with Data Lake and Data Science team to understand their data structure and machine learning algorithms.Understanding of ETL pipelines, and ingress / egress methodologies and design patternsImplement real time argo workflow pipelines, integrate pipelines with machine learning models, and translate data and model results into business stakeholders Data LakeDevelop distributed Machine Learning Pipeline for training & inferencing using Argo, Spark & AKSBuild highly scalable backend REST APIs to collect data from Data Lake and other use-cases / scenarios.Deploy Application in Azure Kubernetes Service using GitLab CICD, Jenkins, Docker, Kubectl, Helm and MainfestExperience in branching, tagging, and maintaining the versions across the different environments in GitLab.Review code developed by other developers and provided feedback to ensure best practices (e.g., checking code in, accuracy, testability, and efficiency)Debug/track/resolve by analyzing the sources of issues and the impact on application, network, or service operations and quality.Functional, benchmark & performance testing and tuning for the built workflows.Assess, design & optimize the resources capacities (e.g. Memory, GPU etc.) for ML based resource intensive workloads.
Education: Bachelors Degree
Job Type: Contract
Pay: $48.00 per hour
Experience level:

 7 years

Work Location: Remote","<p><b>Description:</b></p>
<p>10 Months Contract</p>
<p><b>Required skills:</b><br>Data Engineering &#x2013; They need to be stronger.<br>Azure &#x2013; high need &#x2013; should have experience here.<br>Kubernetes &#x2013; have been very stringent with here.<br>Machine Learning.<br>Python &#x2013; must have.<br>Spark - also required.<br>Jupterhub/ mlflow/ databricks/ kubeflow &#x2013; Have been deployed and worked on any of this tool.</p>
<p><b>Secondary Skills - Nice to Haves:</b><br>HELM<br>Argo Workflow</p>
<p><b>Job Description:</b><br>The Engineer is responsible for developing, implementing, and maintaining technical software applications and provides a combination of technical and business leadership while being the primary trusted and capable owner of one or more high priority, high visibility, complex initiatives.<br>This Engineer will typically lead and coach a small number of team members and provide guidance to team members and multiple.<br>You will design, develop, test, deploy, maintain, and enhance Machine Learning Pipelines using K8s/AKS based Argo Workflow Orchestration solutions.<br>Participate and contribute to design reviews with platform engineering team to decide the design, technologies, project priorities, deadlines, and deliverables.<br>You will work closely with Data Lake and Data Science team to understand their data structure and machine learning algorithms.<br>Understanding of ETL pipelines, and ingress / egress methodologies and design patterns<br>Implement real time argo workflow pipelines, integrate pipelines with machine learning models, and translate data and model results into business stakeholders Data Lake<br>Develop distributed Machine Learning Pipeline for training &amp; inferencing using Argo, Spark &amp; AKS<br>Build highly scalable backend REST APIs to collect data from Data Lake and other use-cases / scenarios.<br>Deploy Application in Azure Kubernetes Service using GitLab CICD, Jenkins, Docker, Kubectl, Helm and Mainfest<br>Experience in branching, tagging, and maintaining the versions across the different environments in GitLab.<br>Review code developed by other developers and provided feedback to ensure best practices (e.g., checking code in, accuracy, testability, and efficiency)<br>Debug/track/resolve by analyzing the sources of issues and the impact on application, network, or service operations and quality.<br>Functional, benchmark &amp; performance testing and tuning for the built workflows.<br>Assess, design &amp; optimize the resources capacities (e.g. Memory, GPU etc.) for ML based resource intensive workloads.</p>
<p><b>Education:</b> Bachelors Degree</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;48.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Work Location: Remote</p>",,84bd4ff5bed36cd7,,Contract,,,Remote,Data Engineer (Remote),5 days ago,2023-10-13T13:33:51.293Z,,,$48 an hour,2023-10-18T13:33:51.295Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=84bd4ff5bed36cd7&from=jasx&tk=1hd1fru0j2f35000&vjs=3
47,ICF,"Senior Data Engineer at ICF
**This role can be completely remote, sitting anywhere within the US**
We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!
You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians’ work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.
Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.
Tools & Technology:
o Spark, Hadoop, Scala, Python, and AWS EMR
o Airflow, Jenkins
o AWS Redshift and Teradata
o Git and Github
o Confluence
Key Responsibilities:
o Write complex unit and integration tests for all data processing code
o Work with DevOps engineers on CI, CD, and IaC
o Read specs and translate them into test designs and test automation
o Perform code reviews and develop processes for improving code quality
Basic Qualifications:
o Bachelor’s degree
o 5+ years of high-volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API
o 2+ years of experience with Agile methodology
o 2+ years of experience performing data pipeline and data validation
o Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain a Public Trust Clearance.
Preferred Qualifications:
o MS and 3+ years of Data Engineering experience
o Experience working in the healthcare industry with PHI/PII
o Federal Government contracting work experience
o Expertise working as part of a dynamic, interactive Agile team
o Strong written and verbal communication skills
o Demonstrated time management skills.
o Strong organizational skills with attention to detail
o Curiosity about how things work, ability to look out for potential risks
Job Type: Full-time
Pay: $100,000.00 - $160,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Flexible schedule
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Tuition reimbursement
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 10 years
 8 years
 9 years

Schedule:

 8 hour shift

Education:

 Bachelor's (Required)

Experience:

 Informatica: 1 year (Preferred)
 Spark: 2 years (Required)
 SQL: 3 years (Preferred)
 Cloud: 1 year (Required)
 Scala: 3 years (Required)

Work Location: Remote","<p><b>Senior Data Engineer at ICF</b></p>
<p>**This role can be completely remote, sitting anywhere within the US**</p>
<p>We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!</p>
<p>You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians&#x2019; work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.</p>
<p>Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.</p>
<p><b>Tools &amp; Technology:</b></p>
<p>o Spark, Hadoop, Scala, Python, and AWS EMR</p>
<p>o Airflow, Jenkins</p>
<p>o AWS Redshift and Teradata</p>
<p>o Git and Github</p>
<p>o Confluence</p>
<p><b>Key Responsibilities:</b></p>
<p>o Write complex unit and integration tests for all data processing code</p>
<p>o Work with DevOps engineers on CI, CD, and IaC</p>
<p>o Read specs and translate them into test designs and test automation</p>
<p>o Perform code reviews and develop processes for improving code quality</p>
<p><b>Basic Qualifications:</b></p>
<p>o Bachelor&#x2019;s degree</p>
<p>o <b>5+ years of high-volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API</b></p>
<p>o 2+ years of experience with Agile methodology</p>
<p>o 2+ years of experience performing data pipeline and data validation</p>
<p>o Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain a Public Trust Clearance.</p>
<p><b>Preferred Qualifications:</b></p>
<p>o MS and 3+ years of Data Engineering experience</p>
<p>o Experience working in the healthcare industry with PHI/PII</p>
<p>o Federal Government contracting work experience</p>
<p>o Expertise working as part of a dynamic, interactive Agile team</p>
<p>o Strong written and verbal communication skills</p>
<p>o Demonstrated time management skills.</p>
<p>o Strong organizational skills with attention to detail</p>
<p>o Curiosity about how things work, ability to look out for potential risks</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;100,000.00 - &#x24;160,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Tuition reimbursement</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Required)</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>Spark: 2 years (Required)</li>
 <li>SQL: 3 years (Preferred)</li>
 <li>Cloud: 1 year (Required)</li>
 <li>Scala: 3 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,032e33bd776de85e,,Full-time,,,Remote,Senior Data Engineer - Scala/Spark,4 days ago,2023-10-14T13:34:00.445Z,3.4,666.0,"$100,000 - $160,000 a year",2023-10-18T13:34:00.447Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=032e33bd776de85e&from=jasx&tk=1hd1fs0bnk7b3800&vjs=3
48,hire IT people,"Job Summary:
We are seeking an experienced Azure Cloud Solutions Architect to design, develop and implement cloud solutions based on the Azure platform. The ideal candidate will have a deep understanding of cloud computing principles and be able to design scalable, secure, and highly available cloud-based solutions. As an Azure Cloud Solutions Architect, you will work closely with clients, project managers, and other technical teams to deliver cloud solutions that meet business needs.
Here is the feedback on the position for the customer. Do you have someone who fits the description? They need about 60% development and 40% operations
For this position we need more of a data developer/engineer. Skills I’d like to see (in order of importance): SQL, ELT/ETL experience, Azure generalist, Azure Snapse, Powershell/bash, Python. SQL is a must have –
I would ask them to outline a CTE (alternatively a subquery) as an interview question. That’s indicative of a reasonable amount of DB experience. Experience directly with Azure and the MS DB stack is great too, but less important than being able to write a SQL statement.
Responsibilities:

 Design and architect cloud-based solutions on the Azure platform, including infrastructure, networking, security, and application layers
 Collaborate with clients and project managers to understand business requirements and translate them into technical solutions
 Develop technical architecture and design documents that outline the proposed solution, including diagrams, specifications, and cost estimates
 Evaluate existing systems and applications to determine if they can be migrated to the cloud, and design migration plans as needed
 Develop and implement cloud automation scripts using PowerShell or other scripting tools
 Configure and manage Azure services such as Virtual Machines, Storage Accounts, Virtual Networks, and App Services
 Implement and manage Azure security measures, including Azure Active Directory, Azure Security Center, and Network Security Groups
 Monitor and optimize Azure resources to ensure maximum performance, availability, and scalability
 Provide guidance and mentorship to other technical team members on Azure technologies and best practices
 Stay up-to-date with the latest cloud technologies and trends, and evaluate their potential use in our solutions

Requirements:

 Bachelor's degree in Computer Science, Information Technology, or related field
 5+ years of experience in Azure cloud architecture and design
 Strong understanding of Azure Container solutions like Azure Kubernetes Service, Azure Container Apps, and Azure Container Registry
 Experience with implementing Azure SQL Database
 String understanding
 Strong understanding of cloud computing principles and architectures, including IaaS, PaaS, and SaaS models
 Experience with Azure services such as Virtual Machines, Storage Accounts, Databases, Virtual Networks, and App Services
 Proficiency in cloud automation tools and scripting languages, such as Terraform, Ansible,PowerShell, Python, or Azure CLI
 Experience with Azure DevOps, including Continuous Integration and Continuous Deployment
 Understanding of cloud security principles and best practices, including Azure Security Center and Network Security Groups
 Strong communication and collaboration skills, and ability to work in a team environment
 Ability to adapt to new technologies and learn quickly
 Azure certifications such as AZ-303 and AZ-304 are a plus

Job Type: Full-time
Salary: From $75.00 per hour
Experience level:

 10 years
 11+ years

Schedule:

 8 hour shift

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p><b>Job Summary:</b></p>
<p>We are seeking an experienced Azure Cloud Solutions Architect to design, develop and implement cloud solutions based on the Azure platform. The ideal candidate will have a deep understanding of cloud computing principles and be able to design scalable, secure, and highly available cloud-based solutions. As an Azure Cloud Solutions Architect, you will work closely with clients, project managers, and other technical teams to deliver cloud solutions that meet business needs.</p>
<p>Here is the feedback on the position for the customer. Do you have someone who fits the description? They need about 60% development and 40% operations</p>
<p>For this position we need more of a data developer/engineer. Skills I&#x2019;d like to see (in order of importance): SQL, ELT/ETL experience, Azure generalist, Azure Snapse, Powershell/bash, Python. SQL is a must have &#x2013;</p>
<p>I would ask them to outline a CTE (alternatively a subquery) as an interview question. That&#x2019;s indicative of a reasonable amount of DB experience. Experience directly with Azure and the MS DB stack is great too, but less important than being able to write a SQL statement.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Design and architect cloud-based solutions on the Azure platform, including infrastructure, networking, security, and application layers</li>
 <li>Collaborate with clients and project managers to understand business requirements and translate them into technical solutions</li>
 <li>Develop technical architecture and design documents that outline the proposed solution, including diagrams, specifications, and cost estimates</li>
 <li>Evaluate existing systems and applications to determine if they can be migrated to the cloud, and design migration plans as needed</li>
 <li>Develop and implement cloud automation scripts using PowerShell or other scripting tools</li>
 <li>Configure and manage Azure services such as Virtual Machines, Storage Accounts, Virtual Networks, and App Services</li>
 <li>Implement and manage Azure security measures, including Azure Active Directory, Azure Security Center, and Network Security Groups</li>
 <li>Monitor and optimize Azure resources to ensure maximum performance, availability, and scalability</li>
 <li>Provide guidance and mentorship to other technical team members on Azure technologies and best practices</li>
 <li>Stay up-to-date with the latest cloud technologies and trends, and evaluate their potential use in our solutions</li>
</ul>
<p><b>Requirements:</b></p>
<ul>
 <li>Bachelor&apos;s degree in Computer Science, Information Technology, or related field</li>
 <li>5+ years of experience in Azure cloud architecture and design</li>
 <li><b>Strong understanding of Azure Container solutions like Azure Kubernetes Service, Azure Container Apps, and Azure Container Registry</b></li>
 <li><b>Experience with implementing Azure SQL Database</b></li>
 <li><b>String understanding</b></li>
 <li>Strong understanding of cloud computing principles and architectures, including IaaS, PaaS, and SaaS models</li>
 <li>Experience with Azure services such as Virtual Machines, Storage Accounts, Databases, Virtual Networks, and App Services</li>
 <li>Proficiency in cloud automation tools and scripting languages, such as <b>Terraform, Ansible,</b>PowerShell, Python, or Azure CLI</li>
 <li>Experience with Azure DevOps, including Continuous Integration and Continuous Deployment</li>
 <li>Understanding of cloud security principles and best practices, including Azure Security Center and Network Security Groups</li>
 <li><b>Strong communication and collaboration skills, and ability to work in a team environment</b></li>
 <li>Ability to adapt to new technologies and learn quickly</li>
 <li>Azure certifications such as AZ-303 and AZ-304 are a plus</li>
</ul>
<p>Job Type: Full-time</p>
<p>Salary: From &#x24;75.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,88983c081a8a400b,,Full-time,,,Remote,Sr. Azure Data Engineer,4 days ago,2023-10-14T13:34:07.595Z,,,From $75 an hour,2023-10-18T13:34:07.597Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=88983c081a8a400b&from=jasx&tk=1hd1fru0j2f35000&vjs=3
49,ICF,"Senior Data Engineer - Scala
 
 
 
  
   
    
     
       Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth.
     
     
     
       **This role can be completely remote, sitting anywhere within the US**
     
     
     
       We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!
     
     
     
       You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians’ work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.
     
     
     
       Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.
     
     
     
       Tools & Technology:
     
     
       Spark, Hadoop, Scala, Python, and AWS EMR
       Airflow, Jenkins
       AWS Redshift and Teradata
       Git and Github
       Confluence
     
     
     
       Key Responsibilities:
     
     
       Write complex unit and integration tests for all data processing code
       Work with DevOps engineers on CI, CD, and IaC
       Read specs and translate them into test designs and test automation
       Perform code reviews and develop processes for improving code quality
     
     
     
       Basic Qualifications:
     
     
       Bachelor’s Degree
       5+ years of high volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API
       2+ years of experience with Agile methodology
       2+ years of experience performing data pipeline and data validation
       Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain and maintain a Public Trust Clearance.
     
     
     
       Preferred Qualifications:
     
     
       MS and 3+ years of technical experience
       Experience working in the healthcare industry with PHI/PII
       Federal Government contracting work experience
       Expertise working as part of a dynamic, interactive Agile team
       Strong written and verbal communication skills
       Demonstrated time management skills.
       Strong organizational skills with attention to detail
       Curiosity about how things work, ability to look out for potential risks
     
     
     
       #Indeed
     
     
       #LI-CC1
     
     
       #DMX
     
    
   
  
 
 
 
   Working at ICF
 
  ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
 
 
   We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our 
  
   EEO & AA policy
  .
 
 
 
   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email 
  
   icfcareercenter@icf.com
   and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: 
  
   Know Your Rights
   and 
  
   Pay Transparency Statement.
  
 
 
 
   Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
  $82,673.00 - $140,544.00
  Nationwide Remote Office (US99)","<div>
 <div>
  Senior Data Engineer - Scala
 </div>
 <div></div>
 <div>
  <div>
   <div>
    <div>
     <div>
       Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth.
     </div>
     <div></div>
     <div>
       **This role can be completely remote, sitting anywhere within the US**
     </div>
     <div></div>
     <div>
       We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!
     </div>
     <div></div>
     <div>
       You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians&#x2019; work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.
     </div>
     <div></div>
     <div>
       Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.
     </div>
     <div></div>
     <div>
       Tools &amp; Technology:
     </div>
     <ul>
      <li> Spark, Hadoop, Scala, Python, and AWS EMR</li>
      <li> Airflow, Jenkins</li>
      <li> AWS Redshift and Teradata</li>
      <li> Git and Github</li>
      <li> Confluence</li>
     </ul>
     <div></div>
     <div>
       Key Responsibilities:
     </div>
     <ul>
      <li> Write complex unit and integration tests for all data processing code</li>
      <li> Work with DevOps engineers on CI, CD, and IaC</li>
      <li> Read specs and translate them into test designs and test automation</li>
      <li> Perform code reviews and develop processes for improving code quality</li>
     </ul>
     <div></div>
     <div>
       Basic Qualifications:
     </div>
     <ul>
      <li> Bachelor&#x2019;s Degree</li>
      <li> 5+ years of high volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API</li>
      <li> 2+ years of experience with Agile methodology</li>
      <li> 2+ years of experience performing data pipeline and data validation</li>
      <li> Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain and maintain a Public Trust Clearance.</li>
     </ul>
     <div></div>
     <div>
       Preferred Qualifications:
     </div>
     <ul>
      <li> MS and 3+ years of technical experience</li>
      <li> Experience working in the healthcare industry with PHI/PII</li>
      <li> Federal Government contracting work experience</li>
      <li> Expertise working as part of a dynamic, interactive Agile team</li>
      <li> Strong written and verbal communication skills</li>
      <li> Demonstrated time management skills.</li>
      <li> Strong organizational skills with attention to detail</li>
      <li> Curiosity about how things work, ability to look out for potential risks</li>
     </ul>
     <div></div>
     <div>
       #Indeed
     </div>
     <div>
       #LI-CC1
     </div>
     <div>
       #DMX
     </div>
    </div>
   </div>
  </div>
 </div>
 <div></div>
 <div>
   Working at ICF
 </div>
 <div></div> ICF is a global advisory and technology services provider, but we&#x2019;re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
 <div></div>
 <div>
   We can only solve the world&apos;s toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our 
  <div>
   EEO &amp; AA policy
  </div>.
 </div>
 <div></div>
 <div>
   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email 
  <div>
   icfcareercenter@icf.com
  </div> and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: 
  <div>
   Know Your Rights
  </div> and 
  <div>
   Pay Transparency Statement.
  </div>
 </div>
 <div></div>
 <div>
  <br> Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
 </div> &#x24;82,673.00 - &#x24;140,544.00
 <div></div> Nationwide Remote Office (US99)
</div>",https://icf.wd5.myworkdayjobs.com/en-US/ICFExternal_Career_Site/job/Reston-VA/Senior-Data-Engineer---Scala--Remote-_R2304300?source=indeed&source=indeed,dc6d198d707948d7,,Full-time,,,"Reston, VA",Senior Data Engineer - Scala (Remote),4 days ago,2023-10-14T13:34:18.376Z,3.4,666.0,"$82,673 - $140,544 a year",2023-10-18T13:34:18.378Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=dc6d198d707948d7&from=jasx&tk=1hd1fsp38k6pu800&vjs=3
50,Manrco Inc,"Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment?
We are seeking Data Engineers who are passionate about marrying data with emerging technologies to join our team. A
What You’ll Do:

 Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies


 Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems


 Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake


 Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community


 Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment


 Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Basic Qualifications:

 Bachelor’s Degree


 At least 2 years of experience in application development


 At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)

Preferred Qualifications:

 Master's Degree


 3+ years of experience in application development


 1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink


 1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)


 1+ years of experience with Ansible / Terraform


 2+ years of experience with Agile engineering practices


 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)


 2+ years of experience with NoSQL implementation (Mongo, Cassandra)


 2+ years of experience developing Java based software solutions


 2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)


 2+ years of experience developing software solutions to solve complex business problems


 2+ years of experience with UNIX/Linux including basic commands and shell scripting

Job Type: Contract
Pay: $80.00 per hour
Expected hours: 40 per week
Benefits:

 Health insurance

Compensation package:

 Hourly pay

Experience level:

 5 years
 6 years
 7 years
 8 years

Schedule:

 12 hour shift
 Monday to Friday

Experience:

 Data warehouse: 1 year (Preferred)
 Informatica: 4 years (Preferred)
 Snowflake: 3 years (Preferred)

Work Location: Remote","<p>Data Engineer</p>
<p>Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment?</p>
<p>We are seeking Data Engineers who are passionate about marrying data with emerging technologies to join our team. A</p>
<p>What You&#x2019;ll Do:</p>
<ul>
 <li>Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies</li>
</ul>
<ul>
 <li>Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems</li>
</ul>
<ul>
 <li>Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake</li>
</ul>
<ul>
 <li>Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal &amp; external technology communities, and mentoring other members of the engineering community</li>
</ul>
<ul>
 <li>Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment</li>
</ul>
<ul>
 <li>Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance</li>
</ul>
<p>Basic Qualifications:</p>
<ul>
 <li>Bachelor&#x2019;s Degree</li>
</ul>
<ul>
 <li>At least 2 years of experience in application development</li>
</ul>
<ul>
 <li>At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)</li>
</ul>
<p>Preferred Qualifications:</p>
<ul>
 <li>Master&apos;s Degree</li>
</ul>
<ul>
 <li>3+ years of experience in application development</li>
</ul>
<ul>
 <li>1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink</li>
</ul>
<ul>
 <li>1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)</li>
</ul>
<ul>
 <li>1+ years of experience with Ansible / Terraform</li>
</ul>
<ul>
 <li>2+ years of experience with Agile engineering practices</li>
</ul>
<ul>
 <li>2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)</li>
</ul>
<ul>
 <li>2+ years of experience with NoSQL implementation (Mongo, Cassandra)</li>
</ul>
<ul>
 <li>2+ years of experience developing Java based software solutions</li>
</ul>
<ul>
 <li>2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)</li>
</ul>
<ul>
 <li>2+ years of experience developing software solutions to solve complex business problems</li>
</ul>
<ul>
 <li>2+ years of experience with UNIX/Linux including basic commands and shell scripting</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;80.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Hourly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>12 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data warehouse: 1 year (Preferred)</li>
 <li>Informatica: 4 years (Preferred)</li>
 <li>Snowflake: 3 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,7df0841eea60802f,,Contract,,,Remote,Data Engineer,5 days ago,2023-10-13T13:34:29.848Z,,,$80 an hour,2023-10-18T13:34:29.853Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=7df0841eea60802f&from=jasx&tk=1hd1ft1i8joou800&vjs=3
51,E Source,"Are you an experienced data engineer with a passion for data and innovation? Do you thrive on a cross-functional team of professionals?
 
  E Source is a leading provider of software solutions and consulting services for utilities and their customers. We use data science and machine learning to help utilities optimize their operations and achieve their sustainability goals. We process large amounts of data from our clients and uncover insights and patterns that they didn’t see before.
 
  Data engineering is a key function enabling a robust data science environment for our consulting services and as middleware in preparing our client’s data for processing within our software as a service (SaaS) solutions.
 
  We are seeking a Senior Data engineer to join our Machine Learning Engineering team and help us design and build scalable, reliable, and secure data pipelines, infrastructure, and systems for our consulting services and software as a service solutions. The Senior Data Engineer will work with a cross-functional team of Machine Learning engineers, software engineers, data scientists, and data analysts to deliver data products and solutions for our clients while also contributing to our internal data practices.
 
  Responsibilities:
  
 
 
   Design, develop and maintain data pipelines, infrastructure, and systems to support data products and solutions using technologies such as AWS, Python, Databricks, Spark, and SQL databases like PostgreSQL.
   Work with cross-functional teams to translate business problems into technical solutions and provide technical guidance and mentorship to junior data engineers.
   Develop and implement data engineering strategies and best practices that align with business objectives and customer needs.
   Monitor and troubleshoot data pipelines and systems to ensure data quality, integrity, and availability.
   Conduct research on industry trends and best practices to improve data engineering capabilities and evaluate new data technologies and tools.
   Build and maintain data lakes to support business intelligence needs.
   Manage the software development lifecycle and DevOps aspects of the code.
   Collaborate with data scientists and analysts to understand their data requirements and provide them with optimal data solutions.
   Create new data validation methods and data analysis tools.
 
 
  Requirements:
  
 
 
   Bachelor's degree in computer science, information technology, or a related field.
   4-7 years of experience in data engineering or a similar role.
   Expert-level skills in Python, SQL databases such as PostgreSQL, and big data technologies such as Databricks and Spark.
   Hands-on experience building cloud resident data pipelines in AWS.
   Strong understanding of data governance, security, privacy, and retention policies and procedures.
   Strong communication, collaboration, and problem-solving skills.
   High proficiency using agile software tools like Jira and following mature DevOps practices using GIT, Docker, and CI servers like Jenkins.
   Passion for data and innovation.
 
 
  Preferred Qualifications:
 
   Demonstrated capacity to work autonomously and proactively, with a proven track record of achieving results without constant supervision.
   Experience with ETL optimization, designing, coding, and tuning big data processes in Databricks.
   Sound knowledge of data lineage and data quality techniques.
   Experience in working with data science and machine learning models and frameworks
   Previous experience in the Energy or Utility industry in an analytic role.
   MS Degree in management information systems, computer programming, software engineering, data science, or an equivalent STEM field.
 
 
  A little about E Source
 
  Since 1986, E Source has focused on partnering with utilities to help their customers save electricity. That novel approach defined the art of electric end-use efficiency—better known as energy efficiency. We’ve expanded that concept to include a broader perspective of sustainability for utilities that deliver electricity, natural gas, and water.
 
  We work to enhance relationships with the people utilities serve, achieve the next generation of savings, and lead the carbon-reduction effort. We help utilities think differently, make data useful, and learn from the best strategies across the industry. Our people, our insights, and our network help our clients and give them the assurance that the programs they implement are the most effective.
 
  Benefits
  
 
 
   We offer excellent insurance packages, including medical, dental, and vision plans; company-paid life insurance; company-paid long- and short-term disability insurance; and medical and dependent-care flexible spending plans.
   We provide a flexible time off (FTO) program; E Source employees can take as many paid days off per year as they need, with manager approval, while fulfilling their work obligations and ensuring proper coverage of their responsibilities.
   We offer flexible schedules, flexible work locations, and a paid parental leave benefit.
   We provide a 401(k) plan with a 3% employer match.
 
 
  The budgeted salary for this position is $99,750 to $135,450 (includes base + annual bonus). Actual pay will be adjusted based on experience.
 
  This person can work remote, a hybrid schedule, or from one of our physical office locations.
 
  Applicants must be authorized to work for any employer in the US. We’re unable to sponsor or take over sponsorship of employment visas at this time.
 
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
 
  #LI-Remote
 
  #LI-AP1","<div>
 <p>Are you an experienced data engineer with a passion for data and innovation? Do you thrive on a cross-functional team of professionals?</p>
 <p></p>
 <p><br> E Source is a leading provider of software solutions and consulting services for utilities and their customers. We use data science and machine learning to help utilities optimize their operations and achieve their sustainability goals. We process large amounts of data from our clients and uncover insights and patterns that they didn&#x2019;t see before.</p>
 <p></p>
 <p><br> Data engineering is a key function enabling a robust data science environment for our consulting services and as middleware in preparing our client&#x2019;s data for processing within our software as a service (SaaS) solutions.</p>
 <p></p>
 <p><br> We are seeking a Senior Data engineer to join our Machine Learning Engineering team and help us design and build scalable, reliable, and secure data pipelines, infrastructure, and systems for our consulting services and software as a service solutions. The Senior Data Engineer will work with a cross-functional team of Machine Learning engineers, software engineers, data scientists, and data analysts to deliver data products and solutions for our clients while also contributing to our internal data practices.</p>
 <p></p>
 <p><b><br> Responsibilities:</b></p>
 <br> 
 <p></p>
 <ul>
  <li> Design, develop and maintain data pipelines, infrastructure, and systems to support data products and solutions using technologies such as AWS, Python, Databricks, Spark, and SQL databases like PostgreSQL.</li>
  <li> Work with cross-functional teams to translate business problems into technical solutions and provide technical guidance and mentorship to junior data engineers.</li>
  <li> Develop and implement data engineering strategies and best practices that align with business objectives and customer needs.</li>
  <li> Monitor and troubleshoot data pipelines and systems to ensure data quality, integrity, and availability.</li>
  <li> Conduct research on industry trends and best practices to improve data engineering capabilities and evaluate new data technologies and tools.</li>
  <li> Build and maintain data lakes to support business intelligence needs.</li>
  <li> Manage the software development lifecycle and DevOps aspects of the code.</li>
  <li> Collaborate with data scientists and analysts to understand their data requirements and provide them with optimal data solutions.</li>
  <li> Create new data validation methods and data analysis tools.</li>
 </ul>
 <p></p>
 <p><b><br> Requirements:</b></p>
 <br> 
 <p></p>
 <ul>
  <li> Bachelor&apos;s degree in computer science, information technology, or a related field.</li>
  <li> 4-7 years of experience in data engineering or a similar role.</li>
  <li> Expert-level skills in Python, SQL databases such as PostgreSQL, and big data technologies such as Databricks and Spark.</li>
  <li> Hands-on experience building cloud resident data pipelines in AWS.</li>
  <li> Strong understanding of data governance, security, privacy, and retention policies and procedures.</li>
  <li> Strong communication, collaboration, and problem-solving skills.</li>
  <li> High proficiency using agile software tools like Jira and following mature DevOps practices using GIT, Docker, and CI servers like Jenkins.</li>
  <li> Passion for data and innovation.</li>
 </ul>
 <p></p>
 <p><b><br> Preferred Qualifications:</b></p>
 <ul>
  <li> Demonstrated capacity to work autonomously and proactively, with a proven track record of achieving results without constant supervision.</li>
  <li> Experience with ETL optimization, designing, coding, and tuning big data processes in Databricks.</li>
  <li> Sound knowledge of data lineage and data quality techniques.</li>
  <li> Experience in working with data science and machine learning models and frameworks</li>
  <li> Previous experience in the Energy or Utility industry in an analytic role.</li>
  <li> MS Degree in management information systems, computer programming, software engineering, data science, or an equivalent STEM field.</li>
 </ul>
 <p></p>
 <p><b><br> A little about E Source</b></p>
 <p></p>
 <p><br> Since 1986, E Source has focused on partnering with utilities to help their customers save electricity. That novel approach defined the art of electric end-use efficiency&#x2014;better known as energy efficiency. We&#x2019;ve expanded that concept to include a broader perspective of sustainability for utilities that deliver electricity, natural gas, and water.</p>
 <p></p>
 <p><br> We work to enhance relationships with the people utilities serve, achieve the next generation of savings, and lead the carbon-reduction effort. We help utilities think differently, make data useful, and learn from the best strategies across the industry. Our people, our insights, and our network help our clients and give them the assurance that the programs they implement are the most effective.</p>
 <p></p>
 <p><b><br> Benefits</b></p>
 <br> 
 <p></p>
 <ul>
  <li> We offer excellent insurance packages, including medical, dental, and vision plans; company-paid life insurance; company-paid long- and short-term disability insurance; and medical and dependent-care flexible spending plans.</li>
  <li> We provide a flexible time off (FTO) program; E Source employees can take as many paid days off per year as they need, with manager approval, while fulfilling their work obligations and ensuring proper coverage of their responsibilities.</li>
  <li> We offer flexible schedules, flexible work locations, and a paid parental leave benefit.</li>
  <li> We provide a 401(k) plan with a 3% employer match.</li>
 </ul>
 <p></p>
 <p><br> The budgeted salary for this position is <b>&#x24;99,750 to &#x24;135,450 (includes base + annual bonus)</b>. Actual pay will be adjusted based on experience.</p>
 <p></p>
 <p><br> This person can work remote, a hybrid schedule, or from one of our physical office locations.</p>
 <p></p>
 <p><br> Applicants must be authorized to work for any employer in the US. We&#x2019;re unable to sponsor or take over sponsorship of employment visas at this time.</p>
 <p></p>
 <p><br> All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.</p>
 <p></p>
 <p><br> #LI-Remote</p>
 <p></p>
 <p><br> #LI-AP1</p>
</div>",https://www.paycomonline.net/v4/ats/web.php/jobs/ViewJobDetails?job=174420&clientkey=D091157AE56F3B54E83C2790C96E526E&source=Indeed&source=Indeed.com,cf276d4ea3c5fdcb,,Full-time,,,Hybrid remote,Data Engineer III - REMOTE,5 days ago,2023-10-13T13:34:25.387Z,4.0,10.0,"$99,750 - $135,450 a year",2023-10-18T13:34:25.389Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=cf276d4ea3c5fdcb&from=jasx&tk=1hd1ft1i8joou800&vjs=3
52,Enigma,"The Opportunity: 
  Join Enigma at a pivotal moment as we continue to provide valuable solutions for small businesses. We're seeking an experienced Data Product Engineer to join our team and help develop and build the iteration of small business data products . Your work will directly impact the accuracy of small business profiles, which influence decisions for companies that employ half the U.S. workforce! 
  The Role: 
  As a Data Product Engineer, you will design and develop data products that solve critical customer pain points. Your impact will be measured by your ability to deliver scalable, high-quality data products that customers love. To succeed in this role, you will bring together three distinct capabilities 
  
  Understand acute customer needs and extract common problem structures across customers 
  Analyze and extract value from data at scale 
  Build efficient, maintainable production-grade data pipelines 
  
 We are looking for someone who: 
  
  Operates with a bias for action and knows how to deliver value in the short, medium and long term 
  Loves talking to customers and works hard to solve their problems in a repeatable way 
  Adopts a principled, metrics-driven approach to difficult data problems and demonstrates excellence in their analytics and engineering craft 
  Operates transparently, collaboratively and with low ego—loves learning from others and having their ideas questioned and challenged 
  
 What Makes This Job Exciting: 
  
  Impact: Develop products that take an innovative data-first approach to solving high-value customer problems. 
  Technical Challenge: Tackle complex data and engineering problems while balancing customer impact, reliability, scalability, data quality, and an ambitious forward development plan. 
  Ownership: You'll work directly with customers. You and your teammates will design and build products based on your learnings . 
  
 Bonus Points If You: 
  
  Have experience building data products at scale. 
  Bring prior experience in Databricks or Spark 
  Have worked on data products in the marketing, kyb or credit underwriting space. 
  
 About Us: 
  At Enigma, we're building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values – generosity, curiosity, ingenuity, & drive – guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We're a team of curious, driven individuals with diverse backgrounds and skills, but we're all passionate about engineering deeper understanding through data—together. If this resonates, we would love to hear from you! 
  We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. 
  Salary Range: $160,000-$210,000 
  A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we're creating together","<div>
 <p><b>The Opportunity:</b></p> 
 <p> Join Enigma at a pivotal moment as we continue to provide valuable solutions for small businesses. We&apos;re seeking an experienced Data Product Engineer to join our team and help develop and build the iteration of small business data products . Your work will directly impact the accuracy of small business profiles, which influence decisions for companies that employ half the U.S. workforce!</p> 
 <p><b> The Role:</b></p> 
 <p> As a Data Product Engineer, you will design and develop data products that solve critical customer pain points. Your impact will be measured by your ability to deliver scalable, high-quality data products that customers love. To succeed in this role, you will bring together three distinct capabilities</p> 
 <ul> 
  <li>Understand acute customer needs and extract common problem structures across customers</li> 
  <li>Analyze and extract value from data at scale</li> 
  <li>Build efficient, maintainable production-grade data pipelines</li> 
 </ul> 
 <p><b>We are looking for someone who:</b></p> 
 <ul> 
  <li>Operates with a bias for action and knows how to deliver value in the short, medium and long term</li> 
  <li>Loves talking to customers and works hard to solve their problems in a repeatable way</li> 
  <li>Adopts a principled, metrics-driven approach to difficult data problems and demonstrates excellence in their analytics and engineering craft</li> 
  <li>Operates transparently, collaboratively and with low ego&#x2014;loves learning from others and having their ideas questioned and challenged</li> 
 </ul> 
 <p><b>What Makes This Job Exciting:</b></p> 
 <ul> 
  <li>Impact: Develop products that take an innovative data-first approach to solving high-value customer problems.</li> 
  <li>Technical Challenge: Tackle complex data and engineering problems while balancing customer impact, reliability, scalability, data quality, and an ambitious forward development plan.</li> 
  <li>Ownership: You&apos;ll work directly with customers. You and your teammates will design and build products based on your learnings .</li> 
 </ul> 
 <p><b>Bonus Points If You:</b></p> 
 <ul> 
  <li>Have experience building data products at scale.</li> 
  <li>Bring prior experience in Databricks or Spark</li> 
  <li>Have worked on data products in the marketing, kyb or credit underwriting space.</li> 
 </ul> 
 <p><b>About Us:</b></p> 
 <p> At Enigma, we&apos;re building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values &#x2013; generosity, curiosity, ingenuity, &amp; drive &#x2013; guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We&apos;re a team of curious, driven individuals with diverse backgrounds and skills, but we&apos;re all passionate about engineering deeper understanding through data&#x2014;together. If this resonates, we would love to hear from you!</p> 
 <p> We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.</p> 
 <p><i> Salary Range: &#x24;160,000-&#x24;210,000</i></p> 
 <p><i> A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we&apos;re creating together</i></p>
</div>",https://www.indeed.com/rc/clk?jk=5e030fc53b9ee4f1&atk=&xpse=SoBm67I3JhX6bryKLx0LbzkdCdPP,5e030fc53b9ee4f1,,,,,Remote,"Senior Software Engineer, Data Product",6 days ago,2023-10-12T13:34:29.530Z,,,"$160,000 - $210,000 a year",2023-10-18T13:34:29.531Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=5e030fc53b9ee4f1&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
53,United Talent Agency (UTA),"UTA is committed to building high performance data and application platforms by utilizing the most effective cutting-edge technology we can. UTA's Engineering Team's mission is to use Continuous Integration and Continuous Delivery methods to create a sustainable and secure pipeline in delivering solutions to its business stakeholders. We do this by continuously analyzing areas of improvement and identifying areas of opportunity to automate, secure and codify our environment.
 
 
 
   As a Data Infrastructure Engineer at UTA, you will play a crucial role in optimizing workload, query performance, and distributed query execution. You'll work in a fast-paced development and operational environment, contributing to world-class tooling, automation, and infrastructure for our SaaS platform.
 
 
 
   The salary range for this role is $170,000 to $200,000 commensurate with experience and skills. This role will be eligible for a discretionary bonus.
 
 
   What You Will Do
 
 
   Create a framework that enables the development team to understand the full impact of their features for stakeholders, including testing before it is enabled in production Providing insights into performance and reliability on actual production, guaranteeing no customer impact
   Design a visualization framework that provides the ability to visualize all queries in all environments, including production, while also designing improvements for better insights into potential issues and query plan manipulation
   Develop a service that automatically finds and resolves data corruption in the system, at all stages of development, including in production
   Responsible for creating a testing platform, meant to find correctness and reliability issues in pre-production environments
   Creation of automated system to safely orchestrate the enablement of features in production that will automatically detect and mitigate production issues, for rapid end-to-end feature rollout process at scale
   Identify infrastructure gaps, for which you can design and implement automated solutions
   Contribute to the design, development, and maintenance of some of our existing projects
   Help ensure data governance and security are effectively managed and maintained in their organizations.
 
 
   What You Will Need
 
 
   5+ years hands-on software engineering experience
   Advanced CS fundamentals including data structures, algorithms, and distributed systems
   Good understanding of database fundamentals
   Background in database tooling, database internals, schema design, or building components for large scale data processing systems
   Systems programming skills with fluency in Java, JavaScript or Python
   Track record of identifying and implementing creative solutions with data from multiple sources
 
 
 
   What You Will Get
 
 
   The unique and exciting opportunity to work at one of the leading global entertainment companies
   Access to the tools, leadership, and resources you will need to create and drive a center of excellence
   The opportunity to do the best work of your career
   Work in an inclusive and diverse company culture
   Competitive benefits and programs to support your well-being
   Experience working in a collaborative environment with room to grow
 
 
 
   About UTA
 
 
 
   UTA unites ideas, opportunities and talent. The company represents some of the world's most iconic, barrier-breaking artists, creators and changemakers—from actors, athletes and musicians to writers, gamers and digital influencers. One of the most influential companies in global entertainment, UTA's business spans talent representation, content production, as well as strategic advisory and marketing work with some of the world's biggest brands. Affiliated companies include Digital Brand Architects, KLUTCH Sports Group, Curtis Brown Group, and MediaLink. UTA is headquartered in Los Angeles with offices in Atlanta, Chicago, Nashville, New York and London.
 
 
 
   For more information: 
  
   https://www.unitedtalent.com/about/
  
   
   UTA and its Affiliated Companies are Equal Employment Opportunity employers and welcome all job seekers including individuals with disabilities and veterans with disabilities.
 
 
 
   #LI-CB1","<div>
 <div>
  UTA is committed to building high performance data and application platforms by utilizing the most effective cutting-edge technology we can. UTA&apos;s Engineering Team&apos;s mission is to use Continuous Integration and Continuous Delivery methods to create a sustainable and secure pipeline in delivering solutions to its business stakeholders. We do this by continuously analyzing areas of improvement and identifying areas of opportunity to automate, secure and codify our environment.
 </div>
 <div></div>
 <div>
   As a Data Infrastructure Engineer at UTA, you will play a crucial role in optimizing workload, query performance, and distributed query execution. You&apos;ll work in a fast-paced development and operational environment, contributing to world-class tooling, automation, and infrastructure for our SaaS platform.
 </div>
 <div></div>
 <div>
   The salary range for this role is &#x24;170,000 to &#x24;200,000 commensurate with experience and skills. This role will be eligible for a discretionary bonus.
 </div>
 <div>
  <br> What You Will Do
 </div>
 <ul>
  <li> Create a framework that enables the development team to understand the full impact of their features for stakeholders, including testing before it is enabled in production Providing insights into performance and reliability on actual production, guaranteeing no customer impact</li>
  <li> Design a visualization framework that provides the ability to visualize all queries in all environments, including production, while also designing improvements for better insights into potential issues and query plan manipulation</li>
  <li> Develop a service that automatically finds and resolves data corruption in the system, at all stages of development, including in production</li>
  <li> Responsible for creating a testing platform, meant to find correctness and reliability issues in pre-production environments</li>
  <li> Creation of automated system to safely orchestrate the enablement of features in production that will automatically detect and mitigate production issues, for rapid end-to-end feature rollout process at scale</li>
  <li> Identify infrastructure gaps, for which you can design and implement automated solutions</li>
  <li> Contribute to the design, development, and maintenance of some of our existing projects</li>
  <li> Help ensure data governance and security are effectively managed and maintained in their organizations.</li>
 </ul>
 <div>
  <br> What You Will Need
 </div>
 <ul>
  <li> 5+ years hands-on software engineering experience</li>
  <li> Advanced CS fundamentals including data structures, algorithms, and distributed systems</li>
  <li> Good understanding of database fundamentals</li>
  <li> Background in database tooling, database internals, schema design, or building components for large scale data processing systems</li>
  <li> Systems programming skills with fluency in Java, JavaScript or Python</li>
  <li> Track record of identifying and implementing creative solutions with data from multiple sources</li>
 </ul>
 <div></div>
 <div>
   What You Will Get
 </div>
 <ul>
  <li> The unique and exciting opportunity to work at one of the leading global entertainment companies</li>
  <li> Access to the tools, leadership, and resources you will need to create and drive a center of excellence</li>
  <li> The opportunity to do the best work of your career</li>
  <li> Work in an inclusive and diverse company culture</li>
  <li> Competitive benefits and programs to support your well-being</li>
  <li> Experience working in a collaborative environment with room to grow</li>
 </ul>
 <div></div>
 <div>
   About UTA
 </div>
 <div></div>
 <div>
   UTA unites ideas, opportunities and talent. The company represents some of the world&apos;s most iconic, barrier-breaking artists, creators and changemakers&#x2014;from actors, athletes and musicians to writers, gamers and digital influencers. One of the most influential companies in global entertainment, UTA&apos;s business spans talent representation, content production, as well as strategic advisory and marketing work with some of the world&apos;s biggest brands. Affiliated companies include Digital Brand Architects, KLUTCH Sports Group, Curtis Brown Group, and MediaLink. UTA is headquartered in Los Angeles with offices in Atlanta, Chicago, Nashville, New York and London.
 </div>
 <div></div>
 <div>
   For more information: 
  <div>
   https://www.unitedtalent.com/about/
  </div>
  <br> 
  <br> UTA and its Affiliated Companies are Equal Employment Opportunity employers and welcome all job seekers including individuals with disabilities and veterans with disabilities.
 </div>
 <div></div>
 <div>
   #LI-CB1
 </div>
</div>
<div></div>",https://unitedtalent.wd5.myworkdayjobs.com/en-US/UTA/job/Remote/Data-Infrastructure-Engineer_R2861,bc15c50a09fd93ff,,Full-time,,,Remote,Data Infrastructure Engineer,13 days ago,2023-10-05T13:34:34.771Z,,,"$170,000 - $200,000 a year",2023-10-18T13:34:34.772Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=bc15c50a09fd93ff&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
54,For People,"For People is a team of skilled technologists improving government digital services for disadvantaged and vulnerable populations. We embed directly in government agencies to modernize software, systems, and platforms so that they better serve people.
  Your Impact We are a dedicated team focused on creating and managing an extensive Medicare data warehouse at the Centers for Medicare & Medicaid Services (CMS) to serve Medicare beneficiaries' demographic, enrollment, and claims data in a FHIR (Fast Healthcare Interoperability Resources) format. We are responsible for providing data to several Medicare APIs so that those systems can seamlessly exchange data between various healthcare providers, insurers, and patients. You will directly impact the quality of healthcare that over 65 million Medicare enrollees nationwide receive.
  Our Culture For People is a team of humans. We place a significant amount of emphasis on positive work-life balance, setting healthy expectations, and making sure our loved ones are taken care of first. That means picking a child up from school during the day or going for a mid-day walk is okay!
  This position is 100% remote. Our entire team is remote across the United States, from the West Coast to the East Coast. There will never be a return-to-office, as we have none!
  This position's published base salary range is between $125,000 and $160,000 annually, plus generous benefits (e.g., For People pays 100% of Gold-tier employee health insurance premiums) and annual company profit sharing.
  Your Opportunities
 
   Lead the implementation of FHIR (Fast Healthcare Interoperability Resources) standards within the data warehouse, ensuring accuracy and efficient data exchange across the ecosystem of partner APIs.
   Ingest healthcare data from source systems into FHIR resources and profiles through developing ETL (Extract, Transform, Load) processes, checking for data quality and integrity.
   Create and maintain data mapping specifications to transform non-FHIR data formats into FHIR-compliant data.
   Design and maintain the data warehouse's FHIR-based data model to meet the needs of downstream API systems.
   Implement security measures and access controls to protect sensitive healthcare data and comply with healthcare data privacy regulations, such as HIPAA.
   Maintain comprehensive documentation of FHIR implementations, data transformation processes, and data flows.
   Stay informed about industry best practices and evolving FHIR standards.
 
  You Bring
 
   A humble and caring attitude
   In-depth knowledge and experience with FHIR standards and resource types.
   Expert-level Java programming abilities, alongside some familiarity with Python and Bash scripts.
   Proficiency in designing and implementing data ingestion and transformation processes.
   Strong database design and data modeling skills, with experience creating and maintaining data models in a healthcare context.
   A systematic approach to identifying and resolving issues related to FHIR data integration, data quality, and performance.
   Demonstrated commitment to staying updated on industry best practices, evolving FHIR standards, and opportunities for process improvement.
 
  If you're passionate about healthcare technology and ready to positively impact the quality of healthcare for millions of Medicare enrollees nationwide, we encourage you to apply. Join us in revolutionizing healthcare data accessibility.
  Some fine print. You will be working on a United States government platform, and they have a few basic requirements for contractors like ourselves. You must perform all work physically within the United States at all times. In addition, you must be a United States citizen and be able to pass a government-performed public trust background check.
  For People is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, and/or veteran status. 
  
 1D2WJ7jBEl","<div>
 <p>For People is a team of skilled technologists improving government digital services for disadvantaged and vulnerable populations. We embed directly in government agencies to modernize software, systems, and platforms so that they better serve people.</p>
 <p><b> Your Impact</b><br> We are a dedicated team focused on creating and managing an extensive Medicare data warehouse at the Centers for Medicare &amp; Medicaid Services (CMS) to serve Medicare beneficiaries&apos; demographic, enrollment, and claims data in a FHIR (Fast Healthcare Interoperability Resources) format. We are responsible for providing data to several Medicare APIs so that those systems can seamlessly exchange data between various healthcare providers, insurers, and patients. You will <i>directly impact the quality of healthcare that over 65 million Medicare enrollees nationwide receive</i>.</p>
 <p><b> Our Culture</b><br> For People is a team of humans. We place a significant amount of emphasis on positive work-life balance, setting healthy expectations, and making sure our loved ones are taken care of first. That means picking a child up from school during the day or going for a mid-day walk is okay!</p>
 <p> This position is <b><i>100% remote</i></b>. Our entire team is remote across the United States, from the West Coast to the East Coast. There will never be a return-to-office, as we have none!</p>
 <p> This position&apos;s published base salary range is between <i>&#x24;125,000 and &#x24;160,000</i> annually, plus generous benefits (e.g., For People pays 100% of Gold-tier employee health insurance premiums) and annual company profit sharing.</p>
 <p><b> Your Opportunities</b></p>
 <ul>
  <li> Lead the implementation of FHIR (Fast Healthcare Interoperability Resources) standards within the data warehouse, ensuring accuracy and efficient data exchange across the ecosystem of partner APIs.</li>
  <li> Ingest healthcare data from source systems into FHIR resources and profiles through developing ETL (Extract, Transform, Load) processes, checking for data quality and integrity.</li>
  <li> Create and maintain data mapping specifications to transform non-FHIR data formats into FHIR-compliant data.</li>
  <li> Design and maintain the data warehouse&apos;s FHIR-based data model to meet the needs of downstream API systems.</li>
  <li> Implement security measures and access controls to protect sensitive healthcare data and comply with healthcare data privacy regulations, such as HIPAA.</li>
  <li> Maintain comprehensive documentation of FHIR implementations, data transformation processes, and data flows.</li>
  <li> Stay informed about industry best practices and evolving FHIR standards.</li>
 </ul>
 <p><b> You Bring</b></p>
 <ul>
  <li> A humble and caring attitude</li>
  <li> In-depth knowledge and experience with <b>FHIR</b> <b>standards</b> and resource types.</li>
  <li> Expert-level <b>Java</b> programming abilities, alongside some familiarity with Python and Bash scripts.</li>
  <li> Proficiency in designing and implementing data ingestion and transformation processes.</li>
  <li> Strong database design and data modeling skills, with experience creating and maintaining data models in a healthcare context.</li>
  <li> A systematic approach to identifying and resolving issues related to FHIR data integration, data quality, and performance.</li>
  <li> Demonstrated commitment to staying updated on industry best practices, evolving FHIR standards, and opportunities for process improvement.</li>
 </ul>
 <p> If you&apos;re passionate about healthcare technology and ready to positively impact the quality of healthcare for millions of Medicare enrollees nationwide, we encourage you to apply. Join us in revolutionizing healthcare data accessibility.</p>
 <p><b><i> Some fine print.</i></b> You will be working on a United States government platform, and they have a few basic requirements for contractors like ourselves. You must perform all work physically within the United States at all times. In addition, you must be a United States citizen and be able to pass a government-performed public trust background check.</p>
 <p> For People is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, and/or veteran status.<br> </p>
 <p> </p>
 <p>1D2WJ7jBEl</p>
</div>",https://forpeople.applytojob.com/apply/1D2WJ7jBEl/Senior-Backend-Engineer-FHIR-Data?source=INDE,b151e97f7474d1c8,,Full-time,,,Remote,"Senior Backend Engineer, FHIR Data",6 days ago,2023-10-12T13:34:40.341Z,,,"$125,000 - $160,000 a year",2023-10-18T13:34:40.343Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=b151e97f7474d1c8&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
55,"Gevo, Inc.","About the role: 
  The team at Gevo works with experts in software engineering and data science. sustainability, agronomy, carbon accounting, and data science to help farmers create new value for sustainable practices. We are looking for a team member with excellent geospatial programming, analytics, and data engineering skills for the position of Geospatial Agronomist, Data Engineer. To be successful in this role you will: 
 
  Lead the overall geospatial data management program 
  Design scalable processes and programs to help acquire, organize, analyze, and display layered and temporal geospatial data sets. 
  Be the interface with both growers and external partners to lead collection and transfer of field and crop management boundaries files and other shapefiles from the farm into Verity Tracking. 
  Conduct manual (and developing automated processes for the) clean-up, corrections, and adjustments of geometries as well as attributional data, to fit into the platforms overall data flow. 
  Review, compare, validate, and normalize naming and acreage discrepancies between in-platform maps, USDA shapefiles, and third-party shapefiles. 
  Ensure maps are of acceptable quality and format with the appropriate tract, field, and farm boundaries demarcated. 
  Manage geospatial data associated with projects related to remote sensing, including data acquisition, processing, analysis, and storytelling with data. 
  Integrating remote sensing data with GIS databases and other geospatial information to provide an integrated and comprehensive understanding of the area(s) of interest. 
  Work with the data science and sustainability teams to analyze and interpret remote sensing data. This includes identifying features, objects, characteristics, and patterns within the imagery, such as land cover classification, vegetation indices, change detection, and anomaly detection. 
  Resolve problems by assisting, coaching, and training dealers/distributors on technical problem investigation and solution implementation. 
  Present and promote data management best practices to retailers, agronomy partners, and farmers. 
  Collaborate with the data engineering and data science teams to build automated geospatial workflows. 
  Identification of geospatial trends and insights to solve key tactical and strategic business problems 
  Write clean, testable, and modularized code 
  Work with key stakeholders to identify opportunities to enhance the flow, analysis, and presentation of multiple different geospatial data sets into and across the company 
  Transparently communicate priorities, obstacles, and progress on a regular basis to the Product, Sustainability, and Marketing teams. 
  Comfortably lead technical geospatial operations while following agile principles 
  
 Who you are:
  
  
  Degree or equivalent experience in a quantitative field such as data science, data analytics, geospatial engineering, or computer science; with applied experience in technical geospatial data implementation (programming and visualization) 
  Expert in common geospatial platforms and programming languages such as Python and proficient with data management systems (e.g. ArcGis, GEOJSON etc.) 
  Experience with databases / SQL dialects is preferred 
  Experience in agriculture sales and/or business development and/or crop production preferred. 
  Comfortable in data validation, verification, and quality assurance methods for geospatial data - strong analytical skills and an eye for detail are crucial for fulfilling your duties. 
  Logic-driven critical thinker committed to accuracy, precision, and increasing understanding though geospatial data visualization. 
  Strong ability to manage projects, ask the right questions, and propose product solutions. 
  Passionate about environmental sustainability with domain knowledge in agricultural practices, industrial processes, and data. 
  Experienced in building trusted business relationships. 
  Customer-oriented, with a demonstrated ability to respect and earn the respect of farmers and retail distribution partners 
  Strong written and verbal communicator 
  Enjoy attending farm shows to promote the use of products and services 
  Proficient at managing time, priorities, and expenses. 
  Self-starter, reliable, and able to work independently. 
  Computer Skills: To perform this job successfully, an individual should be proficient in Google Suite applications (and similar office suites). 
 
 Who We Are 
  
  We are People First 
  We are Mission-Focused 
  We are Agile 
  We are Innovators 
  
 What Gevo Offers You 
 
  Free health, dental, vision, life and disability insurance for employee and family 
  21 days of paid time off plus 10 paid holidays 
  401k contribution plan 
  Annual incentive plan, based on Company performance 
  Paid community service time 
  Dog friendly office 
  Be part of a smart, high performing, passionate team 
  Work-from-home stipend, if remote 
  
 Commitment to Diversity and Inclusion
  
  Gevo, Inc. is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.","<p></p>
<div>
 <p><b>About the role:</b></p> 
 <p> The team at Gevo works with experts in software engineering and data science. sustainability, agronomy, carbon accounting, and data science to help farmers create new value for sustainable practices. We are looking for a team member with excellent geospatial programming, analytics, and data engineering skills for the position of Geospatial Agronomist, Data Engineer. To be successful in this role you will: </p>
 <ul>
  <li>Lead the overall geospatial data management program </li>
  <li>Design scalable processes and programs to help acquire, organize, analyze, and display layered and temporal geospatial data sets. </li>
  <li>Be the interface with both growers and external partners to lead collection and transfer of field and crop management boundaries files and other shapefiles from the farm into Verity Tracking. </li>
  <li>Conduct manual (and developing automated processes for the) clean-up, corrections, and adjustments of geometries as well as attributional data, to fit into the platforms overall data flow. </li>
  <li>Review, compare, validate, and normalize naming and acreage discrepancies between in-platform maps, USDA shapefiles, and third-party shapefiles. </li>
  <li>Ensure maps are of acceptable quality and format with the appropriate tract, field, and farm boundaries demarcated. </li>
  <li>Manage geospatial data associated with projects related to remote sensing, including data acquisition, processing, analysis, and storytelling with data. </li>
  <li>Integrating remote sensing data with GIS databases and other geospatial information to provide an integrated and comprehensive understanding of the area(s) of interest. </li>
  <li>Work with the data science and sustainability teams to analyze and interpret remote sensing data. This includes identifying features, objects, characteristics, and patterns within the imagery, such as land cover classification, vegetation indices, change detection, and anomaly detection. </li>
  <li>Resolve problems by assisting, coaching, and training dealers/distributors on technical problem investigation and solution implementation. </li>
  <li>Present and promote data management best practices to retailers, agronomy partners, and farmers. </li>
  <li>Collaborate with the data engineering and data science teams to build automated geospatial workflows. </li>
  <li>Identification of geospatial trends and insights to solve key tactical and strategic business problems </li>
  <li>Write clean, testable, and modularized code </li>
  <li>Work with key stakeholders to identify opportunities to enhance the flow, analysis, and presentation of multiple different geospatial data sets into and across the company </li>
  <li>Transparently communicate priorities, obstacles, and progress on a regular basis to the Product, Sustainability, and Marketing teams. </li>
  <li>Comfortably lead technical geospatial operations while following agile principles</li> 
 </ul> 
 <p><b>Who you are:</b></p>
 <br> 
 <ul> 
  <li>Degree or equivalent experience in a quantitative field such as data science, data analytics, geospatial engineering, or computer science; with applied experience in technical geospatial data implementation (programming and visualization) </li>
  <li>Expert in common geospatial platforms and programming languages such as Python and proficient with data management systems (e.g. ArcGis, GEOJSON etc.) </li>
  <li>Experience with databases / SQL dialects is preferred </li>
  <li>Experience in agriculture sales and/or business development and/or crop production preferred. </li>
  <li>Comfortable in data validation, verification, and quality assurance methods for geospatial data - strong analytical skills and an eye for detail are crucial for fulfilling your duties. </li>
  <li>Logic-driven critical thinker committed to accuracy, precision, and increasing understanding though geospatial data visualization. </li>
  <li>Strong ability to manage projects, ask the right questions, and propose product solutions. </li>
  <li>Passionate about environmental sustainability with domain knowledge in agricultural practices, industrial processes, and data. </li>
  <li>Experienced in building trusted business relationships. </li>
  <li>Customer-oriented, with a demonstrated ability to respect and earn the respect of farmers and retail distribution partners </li>
  <li>Strong written and verbal communicator </li>
  <li>Enjoy attending farm shows to promote the use of products and services </li>
  <li>Proficient at managing time, priorities, and expenses. </li>
  <li>Self-starter, reliable, and able to work independently. </li>
  <li>Computer Skills: To perform this job successfully, an individual should be proficient in Google Suite applications (and similar office suites). </li>
 </ul>
 <p><b>Who We Are</b></p> 
 <ul> 
  <li>We are People First </li>
  <li>We are Mission-Focused </li>
  <li>We are Agile </li>
  <li>We are Innovators</li> 
 </ul> 
 <p><b>What Gevo Offers You</b> </p>
 <ul>
  <li>Free health, dental, vision, life and disability insurance for employee and family </li>
  <li>21 days of paid time off plus 10 paid holidays </li>
  <li>401k contribution plan </li>
  <li>Annual incentive plan, based on Company performance </li>
  <li>Paid community service time </li>
  <li>Dog friendly office </li>
  <li>Be part of a smart, high performing, passionate team </li>
  <li>Work-from-home stipend, if remote</li> 
 </ul> 
 <p><b>Commitment to Diversity and Inclusion</b></p>
 <br> 
 <p> Gevo, Inc. is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.</p>
</div>",https://gevo-inc-careers.rippling-ats.com/job/679776/geospatial-data-engineer?s=in,96205e6a1d6acd47,,,,,Remote,Geospatial Data Engineer,6 days ago,2023-10-12T13:34:45.202Z,,,"$75,000 - $114,000 a year",2023-10-18T13:34:45.204Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=96205e6a1d6acd47&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
56,The Dedham Group,"About Us:
 Our mission at Pulse Analytics is to help decision-makers in oncology and other specialty therapeutic areas, identify and reduce access barriers across the healthcare industry, ensuring patients have access to the treatments they need. Our web-based decision support application connects healthcare industry organizations and key influencers to deliver targeted quality insights to support our client’s customer engagement strategies. We are currently growing and are looking to bring on talented and driven individuals to help build the future of B2B healthcare data analytics products 

 This role is for someone who is excited about architecting data infrastructure and building out data platforms from the ground up. As a Data Engineer, you will have the opportunity to work on key data delivery initiatives – automating data extraction processes and enhancing data sources for our clients. If you are passionate about leveraging data to drive business decisions and thrive in a dynamic environment, we want to hear from you. 

 
In this role you will:
 Design, build, and maintain efficient data pipelines from various sources to support key business initiatives. 
Perform data cleansing and validation processes to ensure the integrity and quality of data used for analysis. 
Act as a data evangelist within the company, promoting the value and impact of data science and engineering initiatives. 
Collaborate closely with leadership, software engineers, and product managers to understand data requirements and align data solutions with business objectives. 
Work alongside Business Analysts to identify opportunities for automated data acquisition and deliver high-quality data that drives actionable insights. 
Develop and implement data governance policies and procedures to ensure the security, privacy, and quality of data. 
Requirements 

 
Minimum Qualifications:
 3+ years of experience as a Data Engineer or in a similar role 
Proficiency in Python and SQL, with the ability to write complex scripts and queries 
Strong knowledge of data modeling, data warehousing, and ETL pipeline development 
Understanding of data management fundamentals and data storage principles 
Exceptional problem-solving and communication skills 
Experience with cloud platforms such as AWS, Azure, or Google Cloud 
Proficiency in Cloud Orchestration tools such as Airflow, Dagster, or Prefect 

 
Preferred Qualifications:
 Bachelors degree in Computer Science or related field 
Experience with Big Data Technologies (e.g. Hadoop, Hive, Spark) 
Familiarity with the AWS Ecosystem (e.g. AWS S3, AWS Athena, AWS Glue) 
Knowledge of Distributed Systems 
Benefits 

 
Company Culture and Values:
 At Pulse Analytics, we foster a collaborative and innovative environment where your ideas are heard and valued. Our core values: 

 
Service: We adopt a client-first approach to solutions 

Agile: Being flexible in our approach allows us to adapt and iterate quickly to client demands or needs 

Innovation: We strive for excellence, embrace risk-taking, and take bold actions to innovate and improve 

Transparency: When everyone is on the same page, we produce our best work 

Ownership: Everyone is a product owner and operates with integrity and self-accountability 

 
Perks and Benefits:
 401k match 
Medical, Vision, and Dental Insurance Coverage 
Casual dress code 
Remote work flexibility 
Generous PTO/Vacation 
Stipend for conferences 

 The expected base salary for this position ranges from $115,000 - $150,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus. 

 The Dedham Group is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people’s differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual’s abilities, skills, performance and behavior and our business requirements. The Dedham Group operates a zero tolerance policy to any form of discrimination, abuse or harassment. 

 #LI-REMOTE 

 #LI-YK1","<b>About Us:</b>
<br> Our mission at Pulse Analytics is to help decision-makers in oncology and other specialty therapeutic areas, identify and reduce access barriers across the healthcare industry, ensuring patients have access to the treatments they need. Our web-based decision support application connects healthcare industry organizations and key influencers to deliver targeted quality insights to support our client&#x2019;s customer engagement strategies. We are currently growing and are looking to bring on talented and driven individuals to help build the future of B2B healthcare data analytics products 
<br>
<br> This role is for someone who is excited about architecting data infrastructure and building out data platforms from the ground up. As a Data Engineer, you will have the opportunity to work on key data delivery initiatives &#x2013; automating data extraction processes and enhancing data sources for our clients. If you are passionate about leveraging data to drive business decisions and thrive in a dynamic environment, we want to hear from you. 
<br>
<br> 
<b>In this role you will:</b>
<br> Design, build, and maintain efficient data pipelines from various sources to support key business initiatives. 
<br>Perform data cleansing and validation processes to ensure the integrity and quality of data used for analysis. 
<br>Act as a data evangelist within the company, promoting the value and impact of data science and engineering initiatives. 
<br>Collaborate closely with leadership, software engineers, and product managers to understand data requirements and align data solutions with business objectives. 
<br>Work alongside Business Analysts to identify opportunities for automated data acquisition and deliver high-quality data that drives actionable insights. 
<br>Develop and implement data governance policies and procedures to ensure the security, privacy, and quality of data. 
<br>Requirements 
<br>
<br> 
<b>Minimum Qualifications:</b>
<br> 3+ years of experience as a Data Engineer or in a similar role 
<br>Proficiency in Python and SQL, with the ability to write complex scripts and queries 
<br>Strong knowledge of data modeling, data warehousing, and ETL pipeline development 
<br>Understanding of data management fundamentals and data storage principles 
<br>Exceptional problem-solving and communication skills 
<br>Experience with cloud platforms such as AWS, Azure, or Google Cloud 
<br>Proficiency in Cloud Orchestration tools such as Airflow, Dagster, or Prefect 
<br>
<br> 
<b>Preferred Qualifications:</b>
<br> Bachelors degree in Computer Science or related field 
<br>Experience with Big Data Technologies (e.g. Hadoop, Hive, Spark) 
<br>Familiarity with the AWS Ecosystem (e.g. AWS S3, AWS Athena, AWS Glue) 
<br>Knowledge of Distributed Systems 
<br>Benefits 
<br>
<br> 
<b>Company Culture and Values:</b>
<br> At Pulse Analytics, we foster a collaborative and innovative environment where your ideas are heard and valued. Our core values: 
<br>
<br> 
<b>Service:</b> We adopt a client-first approach to solutions 
<br>
<b>Agile:</b> Being flexible in our approach allows us to adapt and iterate quickly to client demands or needs 
<br>
<b>Innovation:</b> We strive for excellence, embrace risk-taking, and take bold actions to innovate and improve 
<br>
<b>Transparency:</b> When everyone is on the same page, we produce our best work 
<br>
<b>Ownership:</b> Everyone is a product owner and operates with integrity and self-accountability 
<br>
<br> 
<b>Perks and Benefits:</b>
<br> 401k match 
<br>Medical, Vision, and Dental Insurance Coverage 
<br>Casual dress code 
<br>Remote work flexibility 
<br>Generous PTO/Vacation 
<br>Stipend for conferences 
<br>
<br> The expected base salary for this position ranges from &#x24;115,000 - &#x24;150,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus. 
<br>
<br> The Dedham Group is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people&#x2019;s differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual&#x2019;s abilities, skills, performance and behavior and our business requirements. The Dedham Group operates a zero tolerance policy to any form of discrimination, abuse or harassment. 
<br>
<br> #LI-REMOTE 
<br>
<br> #LI-YK1",https://apply.workable.com/the-dedham-group/j/D3C968FDA4/,38d10858e6384637,,Full-time,,,"New York, NY",Data Engineer,6 days ago,2023-10-12T13:34:54.381Z,,,"$115,000 - $150,000 a year",2023-10-18T13:34:54.384Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=38d10858e6384637&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
57,WorkCog,"Job Description:

 Minimum 9+ years exp Mandatory
 Mandatory Tech: We consider the following technologies as mandatory and should be familiar to the candidate:
 GCP
 Data Warehousing
 GIT
 Core Tech: Our ideal candidate should possess strong proficiency in the following technologies:
 Airflow 2years
 Python - 3 Years
 SQL
 Spark - 3 Years
 ETL

Job Type: Contract
Pay: $55.00 - $85.00 per hour
Expected hours: 40 per week
Benefits:

 Referral program

Compensation package:

 Hourly pay

Experience level:

 10 years
 11+ years
 9 years

Schedule:

 8 hour shift

Experience:

 Informatica: 5 years (Preferred)
 SQL: 6 years (Preferred)
 Data warehouse: 7 years (Preferred)

Work Location: Remote","<p><b>Job Description:</b></p>
<ul>
 <li>Minimum 9+ years exp Mandatory</li>
 <li>Mandatory Tech: We consider the following technologies as mandatory and should be familiar to the candidate:</li>
 <li>GCP</li>
 <li>Data Warehousing</li>
 <li>GIT</li>
 <li>Core Tech: Our ideal candidate should possess strong proficiency in the following technologies:</li>
 <li>Airflow 2years</li>
 <li>Python - 3 Years</li>
 <li>SQL</li>
 <li>Spark - 3 Years</li>
 <li>ETL</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;55.00 - &#x24;85.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Benefits:</p>
<ul>
 <li>Referral program</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Hourly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 5 years (Preferred)</li>
 <li>SQL: 6 years (Preferred)</li>
 <li>Data warehouse: 7 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,879c5ddd69953e54,,Contract,,,Remote,"Senior Data Engineer (Contract ""W2"". With 10+ Year's)",5 days ago,2023-10-13T13:35:01.469Z,,,$55 - $85 an hour,2023-10-18T13:35:01.472Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=879c5ddd69953e54&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
58,IBM,"Introduction
   At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.
   
   
  Your Role and Responsibilities
   Octo, an IBM company, is an industry-leading, award-winning provider of technical solutions for the federal government. At Octo, we specialize in providing agile software engineering, user experience design, cloud services, and digital strategy services that address government's most pressing missions. Octo delivers intelligent solutions and rapid results, yielding lower costs and measurable outcomes.
   Our team is what makes Octo great. At Octo you'll work beside some of the smartest and most accomplished staff you'll find in your career. Octo offers fantastic benefits and an amazing workplace culture where you will feel valued while you perform mission critical work for our government. Voted one of the region’s best places to work multiple times, Octo is an employer of choice!
   
  
  As a Data Engineer, you will work closely with architects, engineers, and integrators to assess customer requirements and to design and support our team to unlock insights from the massive amounts of data within the Veterans Affairs ecosystem. You will be tasked with overall onboarding, operationalizing, administration, and maintenance of key big data/data science/machine learning platforms like Databricks and other cutting-edge technologies.
   Previous experience with Veterans Affairs and/or health/clinical data is a major plus.
   Us...
   We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.
   Program Mission...
   This program supports Veterans Affairs' strategic mission of furthering efforts to modernize its data analytics platform and enhance accessibility to enterprise data and reporting tools.
   
  
  Responsibilities...
  
    Serves as a technical consultant to implement Analytics solutions and produce Data Domain ETL Scripts.
    Uses PowerBI/dashboards to support problem identification and resolution.
    Develops and maintains documentation on various operational and design aspects of the Platform. Assist in troubleshooting issues and resolving them.
    Builds awareness, increases knowledge and drives adoption of modern technologies, sharing user and engineering benefits to gain buy-in.
    Effectively communicates with and influences key stakeholders across the enterprise, at all levels of the organization.
    Operates as a trusted advisor for technology, platform, or capability domain, helping to shape use cases and implementation in a unified manner.
  
   Years of Experience: Must have at least 5 years of experience with Microsoft database and BI technologies, including at least 2-3 years of experience with Azure Data Lake Storage, Azure Data Factory, Azure SQL DW, Azure Synapse, Databricks, Spark, and/or Python
   Education: Bachelor's degree in computer science or related area OR 8 years of additional experience will be considered in lieu of degree. 
  Location: Remote within the United States.
   Clearance: Ability to obtain a Public Trust security clearance.
   Required Technical and Professional Expertise
   
  
    See below for experience and educational requirements.
    Experience defining and implementing strategies for extracting, transforming, and loading data from multiple data sources into analytic data stores.
    Knowledge of Cloud Data Analytics platforms 
   Experience programming in PowerShell, Python, SQL.
    Experience with cloud data storage formats such as Parquet, Avro. 
   Experience with data transformation techniques.
    Ability to test data integrity and develop tests and quality checks. 
   Experience preparing data for various types of data analysis: descriptive, diagnostic, predictive, prescriptive. 
   Performance analysis and tuning experience
    Experience with Data Warehouse or Big Data solutions
    Experience with ML models
    Experience with data modeling and database design
    Strong communication, interpersonal, and collaboration skills working in a team-oriented environment.
  
   
   Preferred Technical and Professional Expertise
   
  
    Experience supporting Department of Veterans Affairs (VA) and/or other federal organizations.
    Advanced SQL, NoSQL query, and scripting. Experience with Python, Java.
    Experience with Azure Data Lake Storage, Azure Data Factory, Azure SQL DW, Azure Synapse, Databricks, Spark, and/or Python
    Experience with relational database systems (i.e., DB2, SQL Server) and non-relational databases such as (Azure SQL, Amazon RDBS, MongoDB, Hadoop tools).
    Understanding of data design concepts (i.e., data modeling, data mapping, OLTP, and OLAP).
    Experience modeling data, message, and service interoperability.
    Azure PowerShell knowledge
  
 
 
  
    
    About Business Unit
   IBM Consulting is IBM’s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients’ businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.
 
 
  
    
    Your Life @ IBM
   In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.
   Being an IBMer means you’ll be able to learn and develop yourself and your career, you’ll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.
   Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.
   Are you ready to be an IBMer?
 
 
  
    About IBM
   IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.
   
   Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. 
   
   At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.
 
 
  
    
    Location Statement
   IBM offers a competitive and comprehensive benefits program. Eligible employees may have access to:
   
   
  
   Healthcare benefits including medical & prescription drug coverage, dental, vision, and mental health & well being
   - Financial programs such as 401(k), the IBM Employee Stock Purchase Plan, financial counseling, life insurance, short & long- term disability coverage, and opportunities for performance based salary incentive programs
   
  
   Generous paid time off including 12 holidays, minimum 56 hours sick time, 120 hours vacation, 12 weeks parental bonding leave in accordance with IBM Policy, and other Paid Care Leave programs. IBM also offers paid family leave benefits to eligible employees where required by applicable law
   Training and educational resources on our personalized, AI-driven learning platform where IBMers can grow skills and obtain industry-recognized certifications to achieve their career goals
   Diverse and inclusive employee resource groups, giving & volunteer opportunities, and discounts on retail products, services & experiences
  
   The compensation range and benefits for this position are based on a full-time schedule for a full calendar year. The salary will vary depending on your job-related skills, experience and location. Pay increment and frequency of pay will be in accordance with employment classification and applicable laws. For part time roles, your compensation and benefits will be adjusted to reflect your hours. Benefits may be pro-rated for those who start working during the calendar year. 
   
   We consider qualified applicants with criminal histories, consistent with applicable law.
 
 
  
    
    Being You @ IBM
   IBM is committed to creating a diverse environment and is proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, caste, genetics, pregnancy, disability, neurodivergence, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.","<div>
 <div>
  <b>Introduction</b>
  <br> At IBM, work is more than a job - it&apos;s a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you&apos;ve never thought possible. Are you ready to lead in this new era of technology and solve some of the world&apos;s most challenging problems? If so, lets talk.
  <br> 
  <br> 
  <b>Your Role and Responsibilities</b>
  <p><br> Octo, an IBM company, is an industry-leading, award-winning provider of technical solutions for the federal government. At Octo, we specialize in providing agile software engineering, user experience design, cloud services, and digital strategy services that address government&apos;s most pressing missions. Octo delivers intelligent solutions and rapid results, yielding lower costs and measurable outcomes.</p>
  <p> Our team is what makes Octo great. At Octo you&apos;ll work beside some of the smartest and most accomplished staff you&apos;ll find in your career. Octo offers fantastic benefits and an amazing workplace culture where you will feel valued while you perform mission critical work for our government. Voted one of the region&#x2019;s best places to work multiple times, Octo is an employer of choice!</p>
  <p> </p>
  <p></p>
  <p>As a <b>Data Engineer</b>, you will work closely with architects, engineers, and integrators to assess customer requirements and to design and support our team to unlock insights from the massive amounts of data within the Veterans Affairs ecosystem. You will be tasked with overall onboarding, operationalizing, administration, and maintenance of key big data/data science/machine learning platforms like Databricks and other cutting-edge technologies.</p>
  <p> Previous experience with Veterans Affairs and/or health/clinical data is a major plus.</p>
  <p><b> Us...</b></p>
  <p> We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client&#x2019;s missions.</p>
  <p><b> Program Mission...</b></p>
  <p> This program supports Veterans Affairs&apos; strategic mission of furthering efforts to modernize its data analytics platform and enhance accessibility to enterprise data and reporting tools.</p>
  <p> </p>
  <p></p>
  <p><b>Responsibilities...</b></p>
  <ul>
   <li> Serves as a technical consultant to implement Analytics solutions and produce Data Domain ETL Scripts.</li>
   <li> Uses PowerBI/dashboards to support problem identification and resolution.</li>
   <li> Develops and maintains documentation on various operational and design aspects of the Platform. Assist in troubleshooting issues and resolving them.</li>
   <li> Builds awareness, increases knowledge and drives adoption of modern technologies, sharing user and engineering benefits to gain buy-in.</li>
   <li> Effectively communicates with and influences key stakeholders across the enterprise, at all levels of the organization.</li>
   <li> Operates as a trusted advisor for technology, platform, or capability domain, helping to shape use cases and implementation in a unified manner.</li>
  </ul>
  <p><b> Years of Experience: </b>Must have at<b> </b>least 5 years of experience with Microsoft database and BI technologies, including at least 2-3 years of experience with Azure Data Lake Storage, Azure Data Factory, Azure SQL DW, Azure Synapse, Databricks, Spark, and/or Python</p>
  <p><b> Education: </b>Bachelor&apos;s degree in computer science or related area OR 8 years of additional experience will be considered in lieu of degree. </p>
  <p><b>Location: </b>Remote within the United States.</p>
  <p><b> Clearance:</b> Ability to obtain a Public Trust security clearance.</p>
  <b><br> Required Technical and Professional Expertise</b>
  <br> 
  <ul>
   <li> See below for experience and educational requirements.</li>
   <li> Experience defining and implementing strategies for extracting, transforming, and loading data from multiple data sources into analytic data stores.</li>
   <li> Knowledge of Cloud Data Analytics platforms </li>
   <li>Experience programming in PowerShell, Python, SQL.</li>
   <li> Experience with cloud data storage formats such as Parquet, Avro. </li>
   <li>Experience with data transformation techniques.</li>
   <li> Ability to test data integrity and develop tests and quality checks. </li>
   <li>Experience preparing data for various types of data analysis: descriptive, diagnostic, predictive, prescriptive. </li>
   <li>Performance analysis and tuning experience</li>
   <li> Experience with Data Warehouse or Big Data solutions</li>
   <li> Experience with ML models</li>
   <li> Experience with data modeling and database design</li>
   <li> Strong communication, interpersonal, and collaboration skills working in a team-oriented environment.</li>
  </ul>
  <br> 
  <b> Preferred Technical and Professional Expertise</b>
  <br> 
  <ul>
   <li> Experience supporting Department of Veterans Affairs (VA) and/or other federal organizations.</li>
   <li> Advanced SQL, NoSQL query, and scripting. Experience with Python, Java.</li>
   <li> Experience with Azure Data Lake Storage, Azure Data Factory, Azure SQL DW, Azure Synapse, Databricks, Spark, and/or Python</li>
   <li> Experience with relational database systems (i.e., DB2, SQL Server) and non-relational databases such as (Azure SQL, Amazon RDBS, MongoDB, Hadoop tools).</li>
   <li> Understanding of data design concepts (i.e., data modeling, data mapping, OLTP, and OLAP).</li>
   <li> Experience modeling data, message, and service interoperability.</li>
   <li> Azure PowerShell knowledge</li>
  </ul>
 </div>
 <div>
  <div>
   <br> 
   <b> About Business Unit</b>
  </div> IBM Consulting is IBM&#x2019;s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients&#x2019; businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.
 </div>
 <div>
  <div>
   <br> 
   <b> Your Life @ IBM</b>
  </div> In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.
  <p> Being an IBMer means you&#x2019;ll be able to learn and develop yourself and your career, you&#x2019;ll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.</p>
  <p> Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.</p>
  <p> Are you ready to be an IBMer?</p>
 </div>
 <div>
  <div>
   <b><br> About IBM</b>
  </div> IBM&#x2019;s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.
  <br> 
  <br> Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we&#x2019;re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. 
  <br> 
  <br> At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it&#x2019;s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.
 </div>
 <div>
  <div>
   <br> 
   <b> Location Statement</b>
  </div> IBM offers a competitive and comprehensive benefits program. Eligible employees may have access to:
  <br> 
  <br> 
  <ul>
   <li>Healthcare benefits including medical &amp; prescription drug coverage, dental, vision, and mental health &amp; well being</li>
  </ul> - Financial programs such as 401(k), the IBM Employee Stock Purchase Plan, financial counseling, life insurance, short &amp; long- term disability coverage, and opportunities for performance based salary incentive programs
  <br> 
  <ul>
   <li>Generous paid time off including 12 holidays, minimum 56 hours sick time, 120 hours vacation, 12 weeks parental bonding leave in accordance with IBM Policy, and other Paid Care Leave programs. IBM also offers paid family leave benefits to eligible employees where required by applicable law</li>
   <li>Training and educational resources on our personalized, AI-driven learning platform where IBMers can grow skills and obtain industry-recognized certifications to achieve their career goals</li>
   <li>Diverse and inclusive employee resource groups, giving &amp; volunteer opportunities, and discounts on retail products, services &amp; experiences</li>
  </ul>
  <br> The compensation range and benefits for this position are based on a full-time schedule for a full calendar year. The salary will vary depending on your job-related skills, experience and location. Pay increment and frequency of pay will be in accordance with employment classification and applicable laws. For part time roles, your compensation and benefits will be adjusted to reflect your hours. Benefits may be pro-rated for those who start working during the calendar year. 
  <br> 
  <br> We consider qualified applicants with criminal histories, consistent with applicable law.
 </div>
 <div>
  <div>
   <br> 
   <b> Being You @ IBM</b>
  </div> IBM is committed to creating a diverse environment and is proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, caste, genetics, pregnancy, disability, neurodivergence, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
 </div>
</div>",https://careers.ibm.com/job/19269307/data-engineer-remote/?codes=JB_Indeed&codes=1-INDEED,4e41a47b763034ef,,Full-time,,,"Washington, DC 20001",Data Engineer,6 days ago,2023-10-12T13:34:49.642Z,3.9,32635.0,"$59,000 - $112,000 a year",2023-10-18T13:34:49.644Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=4e41a47b763034ef&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
59,DGR Systems LLC,"DGR Systems is looking for a Sr. Engineer, Infrastructure Solutions, with experience in data solutions backup. You will be responsible for designing and implementing backup solutions with our clients using Cohesity DataProtect. You will collaborate with internal and external stakeholders to develop and maintain robust data protection strategies and solutions.
  The ideal candidate must be an excellent communicator with an ability to simplify complex topics into clear messages and must be consultative in focus with a growth mindset and passion for continuous learning to bring the best solutions to our clients. The candidate will bring technical experience, solutions leadership, and business acumen to DGR Systems.
  
  
 Why DGR? 
 DGR Systems helps solve the most complex business and operational challenges for their clients. Our team of top-level industry experts takes an innovative and straightforward approach to consulting, design, deployment, and ongoing Assurance Services to meet client needs. 
 At a glance, DGR Systems was founded in 2009 in Tampa, Florida and provides full-service solutions in the areas of Modern Workplace (Endpoint Solutions, Collaboration), Security (Identity and Access Management, Zero Trust, Information Protection) Modern Infrastructure and Cloud, and Applications (Collaboration Apps, SQL Reporting, Power Platform). With an impressive depth of experience across the Microsoft technology solution stack combined with our focus on integrating solutions from multiple leading vendors, we help organizations design and execute against their most challenging digital transformations. At DGR Systems, our culture is built around one simple standard: Excellence is our Baseline - and we deliver on that standard with every client, every day.
  Core Values
  DGR Systems core values are an essential and enduring tenant of our organization. They are a small set of timeless guiding principles describing who we are, how we treat people and how we run our business. 
 
  Passion - Love what you do and make it evident through your approach to your work and the attitude you display. 
  Ownership - Be accountable for outcomes. Take initiative to start and move things forward to make something better. 
  Integrity - Do the right thing. Always. Every time. Without exception. 
  Navigation - Find solutions to problems. Evolve, adapt, and embrace change around you for tomorrow will be different than today. 
  Teamwork - Be approachable and engage with the team around you constantly. We win or lose together. 
 
  
 Responsibilities 
 
  Provide service ownership through the understanding and evangelizing of how technology solutions can help solve business challenges and guide the development of services to meet those needs 
  Ensure the execution of service delivery for our clients to a standard of excellence expected by our clients 
  Engage with existing and prospective clients, partnered with our sales organization, to help design, implement, and maintain backup and recovery solutions, including data backup, replication, and restoration processes to solve client challenges 
  Develop backup and data retention policies that align with client’s business objectives and compliance requirements. 
  Implement security best practices for data protection and ensure compliance with industry regulations and data privacy laws 
  Identify and resolve backup and recovery issues promptly. Perform root cause analysis for failures and implement corrective actions 
  Assess storage capacity requirements and plan for scalability to accommodate future data growth 
  Create comprehensive documentation of backup and recovery processes, configurations, and procedures 
  Work closely with internal and external teams, including system administrators, network engineers, and database administrators, to ensure data protection is integrated seamlessly into the organization's infrastructure 
  All other duties as assigned. 
 
 Requirements
  
  5+ years of experience in backup solutions design and implementation 
  Strong knowledge of backup concepts, methodologies, and technologies 
  Understanding of disaster recovery principles and methodologies 
  Desired experience in Cohesity Backup products 
  Proficient in backup solutions for various platforms, applications, and databases, such as Windows, Linux, VMware, Hyper-V, SQL Server, Oracle, Exchange, SharePoint, etc. 
  Experience in backup solutions for cloud environments, such as AWS, Azure, GCP, etc. 
  Experience in backup solutions for hybrid and multi-cloud scenarios 
  Experience in backup solutions for large-scale and complex environments 
  Experience in backup solutions for disaster recovery and business continuity 
  Demonstrated client-focus solutions provider excelling at understanding customer needs and translating those needs to solutions 
  Excellent problem-solving, collaboration, organizational, presentation, product demonstration, writing, and verbal communication skills 
  Ability to work independently and collaboratively in a fast-paced and dynamic environment 
  Certification in Cohesity products is a plus 
  Certifications in relevant areas (e.g., Certified Information Systems Security Professional (CISSP), Certified Backup and Recovery Engineer (CBRE)) are a plus 
 
 Benefits
  
  Health Care Plans (Medical, Dental & Vision) 
  Health Savings Account 
  Retirement Plan (401k, IRA) 
  Life Insurance (Basic, Voluntary & AD&D) 
  Paid Time Off (Vacation, Sick & Public Holidays) 
  Family Leave (Maternity, Paternity) 
 
 
  Short-Term & Long-Term Disability 
  Training & Development 
  Work from Home Program 
 
 
 We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.","<div>
 <p>DGR Systems is looking for a Sr. Engineer, Infrastructure Solutions, with experience in data solutions backup. You will be responsible for designing and implementing backup solutions with our clients using Cohesity DataProtect. You will collaborate with internal and external stakeholders to develop and maintain robust data protection strategies and solutions.</p>
 <p> The ideal candidate must be an excellent communicator with an ability to simplify complex topics into clear messages and must be consultative in focus with a growth mindset and passion for continuous learning to bring the best solutions to our clients. The candidate will bring technical experience, solutions leadership, and business acumen to DGR Systems.</p>
 <br> 
 <p></p> 
 <h3 class=""jobSectionHeader""><b>Why DGR?</b></h3> 
 <p>DGR Systems helps solve the most complex business and operational challenges for their clients. Our team of top-level industry experts takes an innovative and straightforward approach to consulting, design, deployment, and ongoing Assurance Services to meet client needs.</p> 
 <p>At a glance, DGR Systems was founded in 2009 in Tampa, Florida and provides full-service solutions in the areas of Modern Workplace (Endpoint Solutions, Collaboration), Security (Identity and Access Management, Zero Trust, Information Protection) Modern Infrastructure and Cloud, and Applications (Collaboration Apps, SQL Reporting, Power Platform). With an impressive depth of experience across the Microsoft technology solution stack combined with our focus on integrating solutions from multiple leading vendors, we help organizations design and execute against their most challenging digital transformations. At DGR Systems, our culture is built around one simple standard: <b>Excellence is our Baseline</b> - and we deliver on that standard with every client, every day.</p>
 <p><b> Core Values</b></p>
 <p><br> DGR Systems core values are an essential and enduring tenant of our organization. They are a small set of timeless guiding principles describing who we are, how we treat people and how we run our business.</p> 
 <ul>
  <li><b>Passion -</b> Love what you do and make it evident through your approach to your work and the attitude you display.</li> 
  <li><b>Ownership -</b> Be accountable for outcomes. Take initiative to start and move things forward to make something better.</li> 
  <li><b>Integrity -</b> Do the right thing. Always. Every time. Without exception.</li> 
  <li><b>Navigation -</b> Find solutions to problems. Evolve, adapt, and embrace change around you for tomorrow will be different than today.</li> 
  <li><b>Teamwork -</b> Be approachable and engage with the team around you constantly. We win or lose together.</li> 
 </ul>
 <br> 
 <h3 class=""jobSectionHeader""><b>Responsibilities</b></h3> 
 <ul>
  <li>Provide service ownership through the understanding and evangelizing of how technology solutions can help solve business challenges and guide the development of services to meet those needs</li> 
  <li>Ensure the execution of service delivery for our clients to a standard of excellence expected by our clients</li> 
  <li>Engage with existing and prospective clients, partnered with our sales organization, to help design, implement, and maintain backup and recovery solutions, including data backup, replication, and restoration processes to solve client challenges </li>
  <li>Develop backup and data retention policies that align with client&#x2019;s business objectives and compliance requirements. </li>
  <li>Implement security best practices for data protection and ensure compliance with industry regulations and data privacy laws</li> 
  <li>Identify and resolve backup and recovery issues promptly. Perform root cause analysis for failures and implement corrective actions</li> 
  <li>Assess storage capacity requirements and plan for scalability to accommodate future data growth</li> 
  <li>Create comprehensive documentation of backup and recovery processes, configurations, and procedures</li> 
  <li>Work closely with internal and external teams, including system administrators, network engineers, and database administrators, to ensure data protection is integrated seamlessly into the organization&apos;s infrastructure</li> 
  <li>All other duties as assigned.</li> 
 </ul>
 <p><b>Requirements</b></p>
 <ul> 
  <li>5+ years of experience in backup solutions design and implementation</li> 
  <li>Strong knowledge of backup concepts, methodologies, and technologies </li>
  <li>Understanding of disaster recovery principles and methodologies</li> 
  <li>Desired experience in Cohesity Backup products</li> 
  <li>Proficient in backup solutions for various platforms, applications, and databases, such as Windows, Linux, VMware, Hyper-V, SQL Server, Oracle, Exchange, SharePoint, etc.</li> 
  <li>Experience in backup solutions for cloud environments, such as AWS, Azure, GCP, etc.</li> 
  <li>Experience in backup solutions for hybrid and multi-cloud scenarios</li> 
  <li>Experience in backup solutions for large-scale and complex environments</li> 
  <li>Experience in backup solutions for disaster recovery and business continuity</li> 
  <li>Demonstrated client-focus solutions provider excelling at understanding customer needs and translating those needs to solutions</li> 
  <li>Excellent problem-solving, collaboration, organizational, presentation, product demonstration, writing, and verbal communication skills</li> 
  <li>Ability to work independently and collaboratively in a fast-paced and dynamic environment</li> 
  <li>Certification in Cohesity products is a plus</li> 
  <li>Certifications in relevant areas (e.g., Certified Information Systems Security Professional (CISSP), Certified Backup and Recovery Engineer (CBRE)) are a plus</li> 
 </ul>
 <p><b>Benefits</b></p>
 <ul> 
  <li>Health Care Plans (Medical, Dental &amp; Vision) </li>
  <li>Health Savings Account</li> 
  <li>Retirement Plan (401k, IRA) </li>
  <li>Life Insurance (Basic, Voluntary &amp; AD&amp;D) </li>
  <li>Paid Time Off (Vacation, Sick &amp; Public Holidays) </li>
  <li>Family Leave (Maternity, Paternity) </li>
 </ul>
 <ul>
  <li>Short-Term &amp; Long-Term Disability </li>
  <li>Training &amp; Development </li>
  <li>Work from Home Program</li> 
 </ul>
 <p></p>
 <p><i>We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.</i></p>
</div>
<p></p>",https://apply.workable.com/dgrsystems/j/27F3E20EF9,8df3ef9ef08ef072,,Full-time,,,"Tampa, FL",Sr. Engineer- Infrastructure Solutions - Data Solutions Backup,5 days ago,2023-10-13T13:34:59.565Z,3.5,2.0,"$80,000 - $120,000 a year",2023-10-18T13:34:59.649Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=8df3ef9ef08ef072&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
60,"Community Reinvestment Fund, USA","Posted on October 12, 2023 
       
      
     
    
   
  
 
 
  
   
    
     
      
       
        
         
          
           About the Position 
          
         
         
          
           The Data Engineer is responsible for supporting the implementation of projects focused on collecting, aggregating, storing, reconciling, and making data accessible from disparate sources to enable analysis and decision making. The Data Engineer will play a critical role in the data supply chain by ensuring stakeholders can access and manipulate data for routine and ad hoc analysis. The Data engineer will additionally support the full lifecycle of data from sources through analytics to action. 
            The Data Engineer must be an experienced user of Power BI. The Data Engineer will develop scalable, reliable, and high-impact solutions leveraging the Azure cloud platform to create a modern enterprise data platform. The Data Engineer will work closely with business stakeholders and data analysts to understand the data needs and design, implement, and maintain Power BI reports, dashboards, and visualizations. 
          
         
        
       
      
     
     
      
       
        
         
          
           Key Responsibilities & Accountabilities 
          
         
         
          
           
            Translate business requirements to technical solutions leveraging strong business acumen 
            Analyze current business practices, processes and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics PaaS Services 
            Support the planning and implementation of data design services, providing sizing and configuration assistance and performing needs assessments. 
            Deliver of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies. 
            Design and Build Azure Data Pipelines using Databricks. 
            Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics. 
            Expose data to end users using Power BI or any other modern visualization platform or experience 
            Implement effective metrics and monitoring processes 
           
          
         
        
       
      
     
     
      
       
        
         
          
           About You 
          
         
         
          
           
            Bachelor’s degree in Computer Science (or related field), or equivalent work experience 
            Minimum of 4 years data engineering experience 
            Demonstrated experience of turning business use cases and requirements to technical solutions 
            Experience in business processing mapping of data and analytics solutions. 
            Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows. 
            Strong team collaboration and experience working with remote teams 
            The ability to apply such methods to solve business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration 
            Experienced in Data Transformation using ETL/ELT tools such as AWS Glue, Azure Data Factory, Talend, EAI 
            Knowledge in business intelligence tools such as Power BI, Tableau, Qlik, Cognos TM1 
            Knowledge of Azure Data Factory, Azure Data Lake, Relational Databases SQL DW, and SQL, Azure App Service is required. Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics is a plus 
            Experienced in Cloud Data-related tool such as Microsoft Azure, Amazon S3 
            Ability to leverage on variety of programming languages & data management/processing tools to ensure data reliability, quality & efficiency 
            Knowledge of Python is a plus 
            Designing and building Data Pipelines using streams of IOT data 
            Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals 
           
          
         
         
         
          
           Organizational Policies 
          
         
         
          
           Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law. 
          
         
         
          
           Candidates must reside in and be authorized to work in the United States without sponsorship. 
          
         
         
          
           CRF’s Theory of Change CRF’s Theory of Change is the strategic framework used to guide its work. While CRF’s mission remains constant, the Theory of Change outlines how it contributes to the changes it seeks. 
          
         
         
          
           The challenge CRF works to address The current economic system, perpetuated by institutional racism, individual biases and disparities, is unjust. It fosters the inequities that are causing widening disparities in incomes, wealth, and opportunity gaps. 
          
         
         
          
           A just economy that works for all CRF’s Theory of Change is underpinned by the conviction that small businesses are the backbone of our economy, employing nearly half of the U.S. workforce and generating two-thirds of new jobs. CRF believes that providing small businesses equitable access to capital and support services is essential to creating a just economy that works for all. 
          
         
         
          
           CRF activates its mission by:
           
             Co-creating and deploying innovative financial products and services that address the barriers and inequities small businesses operated by historically excluded people face.
             Designing and managing financial programs that attract incremental impact capital to communities with a history of underinvestment.
             Orchestrating a network of trusted small business support organizations enabled by technology.
             Growing the capacity of community development finance organizations.
             Helping small businesses navigate the complexities of the small business support ecosystem.
            
          
         
         
          
           CRF STANDS IN SOLIDARITY Community Reinvestment Fund, USA (CRF) stands in solidarity with all fighting for social justice, equity, and transformational change. They know that the social changes taking place today will yield a lasting positive impact on the lives of millions. 
          
         
         
          
           DIVERSITY, EQUITY & INCLUSION: CRF is dedicated to building and sustaining a truly diverse, equitable, and inclusive culture. These are not just words on a page – Diversity, Equity & Inclusion are top priorities for the organization, and tie deeply to each of their core values and overall vision for the future. CRF is an equal opportunity employer that evaluate applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity, or other characteristics protected by law. 
          
         
         
          
           CORE VALUES Create Equitable Economic Opportunities, Lead Through Collaboration, Transform Through Innovation, Excel In All They Do, Act with Integrity. 
          
         
         
          
           How We Help Together with its partners – including community leaders, nonprofit lenders, financial institutions, foundations and more – CRF is creating new strategies and technologies that build stronger local economies, create jobs and support economic mobility. 
          
         
         
          
           CRF is headquartered in Minneapolis, Minnesota. For a more detailed description of the incredible work we do, how we do it, and who we are, please visit www.crfusa.com. 
          
         
        
       
      
     
    
   
   
    
     
      
       
        
         
          
           
            
             
              
               
                Department: 
               
              
             
             
              
               
                Data and Analytics
                
              
             
            
           
          
         
         
          
           
            
             
              
               
                Location:
                
              
             
             
              
               
                Minneapolis, MN – remote work is available
                
              
             
            
           
          
         
         
          
           
            
             
              
               
                Salary:
                
              
             
             
              
               
                $85,000 to $105,000 Annually (exempt)
                
              
             
            
           
          
         
        
       
      
     
     
     
      
       
        
         
          
           What We Offer 
          
         
         
          
           A collaborative working environment comprised of driven and highly engaged individuals committed to diversity, equity, and inclusion and the mission, vision and values of CRF. CRF is proud to extend its employees a wide array of benefits including, but not limited to: 
          
         
         
          
           
            Health and dental insurance 
            403B and Roth IRA 
            Paid Time Off (PTO) 
            Wellness Program 
            Educational Assistance 
            Long-Term and Short-Term Disability 
            Life Insurance 
            Flexible schedules and telecommuting options 
            10 Federal Holidays plus 2 Floating Holidays 
           
          
         
        
       
      
     
     
      
       
        
         
          
           How to Apply 
          
         
         
          
           To apply for this or other positions, please send your resume to: recruiting@crfusa.com 
          
         
        
       
      
     
    
   
  
 
 
  
   
    
     
      
       Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law. 
      
     
     
      
       CRF USA, Inc. requires all employees to be vaccinated for COVID-19. As a condition of hire with CRF USA, Inc. candidates must be able to show proof of vaccination for COVID-19 prior to an extension of an offer of employment. Accommodations will be considered for disabilities or sincerely held religious beliefs. 
      
     
     
      
       Candidates must reside in and be authorized to work in the United States without sponsorship.","<div></div>
<div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <ul>
        <li>Posted on October 12, 2023 </li>
       </ul>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h3 class=""jobSectionHeader""><b>About the Position</b></h3> 
          </div>
         </div>
         <div>
          <div>
           <p>The Data Engineer is responsible for supporting the implementation of projects focused on collecting, aggregating, storing, reconciling, and making data accessible from disparate sources to enable analysis and decision making. The Data Engineer will play a critical role in the data supply chain by ensuring stakeholders can access and manipulate data for routine and ad hoc analysis. The Data engineer will additionally support the full lifecycle of data from sources through analytics to action.</p> 
           <p> The Data Engineer must be an experienced user of Power BI. The Data Engineer will develop scalable, reliable, and high-impact solutions leveraging the Azure cloud platform to create a modern enterprise data platform. The Data Engineer will work closely with business stakeholders and data analysts to understand the data needs and design, implement, and maintain Power BI reports, dashboards, and visualizations. </p>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h2 class=""jobSectionHeader""><b>Key Responsibilities &amp; Accountabilities</b></h2> 
          </div>
         </div>
         <div>
          <div>
           <ul>
            <li>Translate business requirements to technical solutions leveraging strong business acumen </li>
            <li>Analyze current business practices, processes and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data &amp; Analytics PaaS Services</li> 
            <li>Support the planning and implementation of data design services, providing sizing and configuration assistance and performing needs assessments.</li> 
            <li>Deliver of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.</li> 
            <li>Design and Build Azure Data Pipelines using Databricks.</li> 
            <li>Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.</li> 
            <li>Expose data to end users using Power BI or any other modern visualization platform or experience</li> 
            <li>Implement effective metrics and monitoring processes</li> 
           </ul>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h2 class=""jobSectionHeader""><b>About You</b></h2> 
          </div>
         </div>
         <div>
          <div>
           <ul>
            <li>Bachelor&#x2019;s degree in Computer Science (or related field), or equivalent work experience</li> 
            <li>Minimum of 4 years data engineering experience </li>
            <li>Demonstrated experience of turning business use cases and requirements to technical solutions </li>
            <li>Experience in business processing mapping of data and analytics solutions. </li>
            <li>Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.</li> 
            <li>Strong team collaboration and experience working with remote teams</li> 
            <li>The ability to apply such methods to solve business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration</li> 
            <li>Experienced in Data Transformation using ETL/ELT tools such as AWS Glue, Azure Data Factory, Talend, EAI</li> 
            <li>Knowledge in business intelligence tools such as Power BI, Tableau, Qlik, Cognos TM1</li> 
            <li>Knowledge of Azure Data Factory, Azure Data Lake, Relational Databases SQL DW, and SQL, Azure App Service is required. Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics is a plus</li> 
            <li>Experienced in Cloud Data-related tool such as Microsoft Azure, Amazon S3</li> 
            <li>Ability to leverage on variety of programming languages &amp; data management/processing tools to ensure data reliability, quality &amp; efficiency</li> 
            <li>Knowledge of Python is a plus</li> 
            <li>Designing and building Data Pipelines using streams of IOT data</li> 
            <li>Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals</li> 
           </ul>
          </div>
         </div>
         <div></div>
         <div>
          <div>
           <h3 class=""jobSectionHeader""><b>Organizational Policies</b></h3> 
          </div>
         </div>
         <div>
          <div>
           <p>Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law.</p> 
          </div>
         </div>
         <div>
          <div>
           <p>Candidates must reside in and be authorized to work in the United States without sponsorship.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>CRF&#x2019;s Theory of Change</b><br> CRF&#x2019;s Theory of Change is the strategic framework used to guide its work. While CRF&#x2019;s mission remains constant, the Theory of Change outlines how it contributes to the changes it seeks.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>The challenge CRF works to address</b><br> The current economic system, perpetuated by institutional racism, individual biases and disparities, is unjust. It fosters the inequities that are causing widening disparities in incomes, wealth, and opportunity gaps.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>A just economy that works for all</b><br> CRF&#x2019;s Theory of Change is underpinned by the conviction that small businesses are the backbone of our economy, employing nearly half of the U.S. workforce and generating two-thirds of new jobs. CRF believes that providing small businesses equitable access to capital and support services is essential to creating a just economy that works for all.</p> 
          </div>
         </div>
         <div>
          <div>
           <p>CRF activates its mission by:</p>
           <ul>
            <li> Co-creating and deploying innovative financial products and services that address the barriers and inequities small businesses operated by historically excluded people face.</li>
            <li> Designing and managing financial programs that attract incremental impact capital to communities with a history of underinvestment.</li>
            <li> Orchestrating a network of trusted small business support organizations enabled by technology.</li>
            <li> Growing the capacity of community development finance organizations.</li>
            <li> Helping small businesses navigate the complexities of the small business support ecosystem.</li>
           </ul> 
          </div>
         </div>
         <div>
          <div>
           <p><b>CRF STANDS IN SOLIDARITY</b><br> Community Reinvestment Fund, USA (CRF) stands in solidarity with all fighting for social justice, equity, and transformational change. They know that the social changes taking place today will yield a lasting positive impact on the lives of millions.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>DIVERSITY, EQUITY &amp; INCLUSION:</b><br> CRF is dedicated to building and sustaining a truly diverse, equitable, and inclusive culture. These are not just words on a page &#x2013; Diversity, Equity &amp; Inclusion are top priorities for the organization, and tie deeply to each of their core values and overall vision for the future. CRF is an equal opportunity employer that evaluate applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity, or other characteristics protected by law.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>CORE VALUES</b><br> Create Equitable Economic Opportunities, Lead Through Collaboration, Transform Through Innovation, Excel In All They Do, Act with Integrity.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>How We Help</b><br> Together with its partners &#x2013; including community leaders, nonprofit lenders, financial institutions, foundations and more &#x2013; CRF is creating new strategies and technologies that build stronger local economies, create jobs and support economic mobility.</p> 
          </div>
         </div>
         <div>
          <div>
           <p>CRF is headquartered in Minneapolis, Minnesota. For a more detailed description of the incredible work we do, how we do it, and who we are, please visit www.crfusa.com.</p> 
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                Department: 
               </div>
              </div>
             </div>
             <div>
              <div>
               <div>
                Data and Analytics
               </div> 
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                Location:
               </div> 
              </div>
             </div>
             <div>
              <div>
               <div>
                Minneapolis, MN &#x2013; remote work is available
               </div> 
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                Salary:
               </div> 
              </div>
             </div>
             <div>
              <div>
               <div>
                &#x24;85,000 to &#x24;105,000 Annually (exempt)
               </div> 
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div></div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h3 class=""jobSectionHeader""><b>What We Offer</b></h3> 
          </div>
         </div>
         <div>
          <div>
           A collaborative working environment comprised of driven and highly engaged individuals committed to diversity, equity, and inclusion and the mission, vision and values of CRF. CRF is proud to extend its employees a wide array of benefits including, but not limited to: 
          </div>
         </div>
         <div>
          <div>
           <ul>
            <li>Health and dental insurance</li> 
            <li>403B and Roth IRA</li> 
            <li>Paid Time Off (PTO)</li> 
            <li>Wellness Program</li> 
            <li>Educational Assistance</li> 
            <li>Long-Term and Short-Term Disability</li> 
            <li>Life Insurance</li> 
            <li>Flexible schedules and telecommuting options</li> 
            <li>10 Federal Holidays plus 2 Floating Holidays</li> 
           </ul>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h2 class=""jobSectionHeader""><b>How to Apply</b></h2> 
          </div>
         </div>
         <div>
          <div>
           To apply for this or other positions, please send your resume to: recruiting@crfusa.com 
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law. 
      </div>
     </div>
     <div>
      <div>
       CRF USA, Inc. requires all employees to be vaccinated for COVID-19. As a condition of hire with CRF USA, Inc. candidates must be able to show proof of vaccination for COVID-19 prior to an extension of an offer of employment. Accommodations will be considered for disabilities or sincerely held religious beliefs. 
      </div>
     </div>
     <div>
      <div>
       Candidates must reside in and be authorized to work in the United States without sponsorship.
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<div></div>",https://crfusa.com/data-engineer/,16c48fc80404f2f3,,,,,"801 Nicollet Mall Ste 1700, Minneapolis, MN 55402",Data Engineer,5 days ago,2023-10-13T13:35:05.331Z,4.0,6.0,"$85,000 - $105,000 a year",2023-10-18T13:35:05.334Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=16c48fc80404f2f3&from=jasx&tk=1hd1ft1i8joou800&vjs=3
61,Gap Inc.,"About Gap Inc. 
 Our brands bridge the gaps we see in the world. Old Navy democratizes style to ensure everyone has access to quality fashion at every price point. Athleta unleashes the potential of every woman, regardless of body size, age or ethnicity. Banana Republic believes in sustainable luxury for all. And Gap inspires the world to bring individuality to modern, responsibly made essentials. 
 This simple idea—that we all deserve to belong, and on our own terms—is core to who we are as a company and how we make decisions. Our team is made up of thousands of people across the globe who take risks, think big, and do good for our customers, communities, and the planet. Ready to learn fast, create with audacity and lead boldly? Join our team.
  About the Role
  In this role, you will design highly scalable and high performing technology solutions in an Agile work environment and produce and deliver code and/or test cases using your knowledge of software development and Agile practice. You will collaborate closely with business support teams, product managers, security and architecture to assist in resolving critical production issues to help simplify and improve business processes through the latest in technology and automation. You are a technical expert that will lead through the requirements gathering, design, development, deployment, and support phases of a product. You are proficient in at least one core programming languages or packages.  As a Senior Engineer - ML/Data you will be contributing to the build of applications and services off of our Gap Data Platform. The tools will be KPI based, focusing on trend analysis of the different brands we are servicing: GAP, Banana Republic, Old Navy and Athleta. Strong understanding of CICD pipelines, data operations and Dev Ops are beneficial to this role.   This is a remote role is based out Dallas, TX. However, the Company may require you in the future to work in an on-site location designated by Gap Inc. on a full-time or part-time basis.
  What You'll Do
 
   Define technical specifications and development requirements that result in high performing technologies that are also domain specific.
   Develop and enhance product and/or applications with limited direction to solve business problems of medium complexity by keeping customer experience at the forefront.
   Adopt and model a DevOps mindset by applying automation, continuous integration and continuous delivery in everything we do.
   Foster innovation by applying best practices and learning from emerging technologies and through collaboration with cross functional stakeholders.
   Serve as application expert in support of domain areas.
   Communicate difficult concepts, providing technical and professional interpretations and recommendations.
   Advise and mentor junior team members and enable collaboration to help teams achieve their best.
 
  Who You Are
 
   Strong working experience utilizing Python and Databricks
   Software Development experience and understanding of security, secure coding/testing and data structures and aware of industry and competitor practices.
   Comprehensive knowledge of software development, practice, concepts and technology.
   Proficiency with various software languages and platforms such as Java, Oracle, Azure etc.
   Experience with related technology stack and platforms.
   Experience with building and sustaining effective relationships with immediate team and stakeholders.
 
  Benefits at Gap Inc. 
 
  Merchandise discount for our brands: 50% off regular-priced merchandise at Old Navy, Gap, Banana Republic and Athleta, and 30% off at Outlet for all employees. 
  One of the most competitive Paid Time Off plans in the industry.* 
  Employees can take up to five “on the clock” hours each month to volunteer at a charity of their choice.* 
  Extensive 401(k) plan with company matching for contributions up to four percent of an employee’s base pay.* 
  Employee stock purchase plan.* 
  Medical, dental, vision and life insurance.* 
  See more of the benefits we offer. 
 
 
  For eligible employees
 
  Gap Inc. is an equal-opportunity employer and is committed to providing a workplace free from harassment and discrimination. We are committed to recruiting, hiring, training and promoting qualified people of all backgrounds, and make all employment decisions without regard to any protected status. We have received numerous awards for our long-held commitment to equality and will continue to foster a diverse and inclusive environment of belonging. In 2022, we were recognized by Forbes as one of the World's Best Employers and one of the Best Employers for Diversity.  Salary Range: $95,300 - $138,200 USD Employee pay will vary based on factors such as qualifications, experience, skill level, competencies and work location. We will meet minimum wage or minimum of the pay range (whichever is higher) based on city, county and state requirements.  US Candidates Please note that effective, June 30, 2022, Gap Inc. will no longer require any of its employees to wear face masks or require proof of COVID vaccination, unless required by local or state/provincial mandates or as part of Gap Inc’s quarantine guidelines after being exposed to or testing positive for COVID. Therefore, please disregard any language in any job posting that refers to Gap Inc.’s face mask and proof of vaccination policy as said policy is no longer effective.","<div>
 <h2 class=""jobSectionHeader""><b>About Gap Inc.</b></h2> 
 <p>Our brands bridge the gaps we see in the world. Old Navy democratizes style to ensure everyone has access to quality fashion at every price point. Athleta unleashes the potential of every woman, regardless of body size, age or ethnicity. Banana Republic believes in sustainable luxury for all. And Gap inspires the world to bring individuality to modern, responsibly made essentials. </p>
 <p>This simple idea&#x2014;that we all deserve to belong, and on our own terms&#x2014;is core to who we are as a company and how we make decisions. Our team<b> </b>is made up of thousands of people across the globe who take risks, think big, and do good for our customers, communities, and the planet. Ready to learn fast, create with audacity and lead boldly? Join our team.</p>
 <h2 class=""jobSectionHeader""><b> About the Role</b></h2>
 <p> In this role, you will design highly scalable and high performing technology solutions in an Agile work environment and produce and deliver code and/or test cases using your knowledge of software development and Agile practice. You will collaborate closely with business support teams, product managers, security and architecture to assist in resolving critical production issues to help simplify and improve business processes through the latest in technology and automation. You are a technical expert that will lead through the requirements gathering, design, development, deployment, and support phases of a product. You are proficient in at least one core programming languages or packages.<br> <br> As a Senior Engineer - ML/Data you will be contributing to the build of applications and services off of our Gap Data Platform. The tools will be KPI based, focusing on trend analysis of the different brands we are servicing: GAP, Banana Republic, Old Navy and Athleta. Strong understanding of CICD pipelines, data operations and Dev Ops are beneficial to this role. <br> <br> This is a remote role is based out Dallas, TX. However, the Company may require you in the future to work in an on-site location designated by Gap Inc. on a full-time or part-time basis.</p>
 <h2 class=""jobSectionHeader""><b> What You&apos;ll Do</b></h2>
 <ul>
  <li> Define technical specifications and development requirements that result in high performing technologies that are also domain specific.</li>
  <li> Develop and enhance product and/or applications with limited direction to solve business problems of medium complexity by keeping customer experience at the forefront.</li>
  <li> Adopt and model a DevOps mindset by applying automation, continuous integration and continuous delivery in everything we do.</li>
  <li> Foster innovation by applying best practices and learning from emerging technologies and through collaboration with cross functional stakeholders.</li>
  <li> Serve as application expert in support of domain areas.</li>
  <li> Communicate difficult concepts, providing technical and professional interpretations and recommendations.</li>
  <li> Advise and mentor junior team members and enable collaboration to help teams achieve their best.</li>
 </ul>
 <h2 class=""jobSectionHeader""><b> Who You Are</b></h2>
 <ul>
  <li> Strong working experience utilizing Python and Databricks</li>
  <li> Software Development experience and understanding of security, secure coding/testing and data structures and aware of industry and competitor practices.</li>
  <li> Comprehensive knowledge of software development, practice, concepts and technology.</li>
  <li> Proficiency with various software languages and platforms such as Java, Oracle, Azure etc.</li>
  <li> Experience with related technology stack and platforms.</li>
  <li> Experience with building and sustaining effective relationships with immediate team and stakeholders.</li>
 </ul>
 <h2 class=""jobSectionHeader""><b> Benefits at Gap Inc.</b></h2> 
 <ul>
  <li>Merchandise discount for our brands: 50% off regular-priced merchandise at Old Navy, Gap, Banana Republic and Athleta, and 30% off at Outlet for all employees.</li> 
  <li>One of the most competitive Paid Time Off plans in the industry.*</li> 
  <li>Employees can take up to five &#x201c;on the clock&#x201d; hours each month to volunteer at a charity of their choice.*</li> 
  <li>Extensive 401(k) plan with company matching for contributions up to four percent of an employee&#x2019;s base pay.*</li> 
  <li>Employee stock purchase plan.*</li> 
  <li>Medical, dental, vision and life insurance.*</li> 
  <li>See more of the benefits we offer.</li> 
 </ul>
 <ul>
  <li><i>For eligible employees</i></li>
 </ul>
 <p> Gap Inc. is an equal-opportunity employer and is committed to providing a workplace free from harassment and discrimination. We are committed to recruiting, hiring, training and promoting qualified people of all backgrounds, and make all employment decisions without regard to any protected status. We have received numerous awards for our long-held commitment to equality and will continue to foster a diverse and inclusive environment of belonging. In 2022, we were recognized by Forbes as one of the World&apos;s Best Employers and one of the Best Employers for Diversity.<br> <br> Salary Range: &#x24;95,300 - &#x24;138,200 USD<br> Employee pay will vary based on factors such as qualifications, experience, skill level, competencies and work location. We will meet minimum wage or minimum of the pay range (whichever is higher) based on city, county and state requirements.<br> <br> <b>US Candidates</b><br> Please note that effective, June 30, 2022, Gap Inc. will no longer require any of its employees to wear face masks or require proof of COVID vaccination, unless required by local or state/provincial mandates or as part of Gap Inc&#x2019;s quarantine guidelines after being exposed to or testing positive for COVID. Therefore, please disregard any language in any job posting that refers to Gap Inc.&#x2019;s face mask and proof of vaccination policy as said policy is no longer effective.</p>
</div>","https://www.gapinc.com/en-us/jobs/w73/16/senior-engineer-ml-data-remote-dallas,-tx?rx_campaign=indeed0&rx_ch=jobp4p&rx_group=116953&rx_job=R137316&rx_r=none&rx_source=Indeed&rx_ts=20231018T080143Z&rx_vp=cpc&src=JB-12080&src=JB-10324&rx_p=CTNK5L83XE&rx_viewer=28bbc4b76dbb11ee91c7ddc6db95c7d1c5c6cf3fd1ba4e4a95fa1cde1a72997f",18da6255e1db9629,,Full-time,,,"San Francisco, CA 94105",Senior Engineer - ML/Data (Remote,5 days ago,2023-10-13T13:35:06.949Z,3.7,3496.0,"$95,300 - $138,200 a year",2023-10-18T13:35:06.952Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=18da6255e1db9629&from=jasx&tk=1hd1ft1i8joou800&vjs=3
62,IBR (Imagine Believe Realize),"The Senior Data Engineer must be able to meet the key criteria below:

 Location: 100% telework
 Years' Experience: 10+ years
 Education: Bachelor’s in IT related field
 Security Clearance: IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required.
 Work Authorization: Must show that applicant is legally permitted to work in the United States.
 Employment Type: Full-Time, W-2
 Key Skills:
 10+ years of IT experience focusing on enterprise data architecture and management
 Experience with Databricks required
 8+ years experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
 Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services
 Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
 Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus

Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities

 Plan, create, and maintain data architectures, ensuring alignment with business requirements
 Obtain data, formulate dataset processes, and store optimized data
 Identify problems and inefficiencies and apply solutions
 Determine tasks where manual participation can be eliminated with automation.
 Identify and optimize data bottlenecks, leveraging automation where possible
 Create and manage data lifecycle policies (retention, backups/restore, etc)
 Create, maintain, and manage ETL/ELT pipelines
 Create, maintain, and manage data transformations
 Maintain/update documentation
 Create, maintain, and manage data pipeline schedules
 Monitor data pipelines
 Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality
 Support AI/ML teams with optimizing feature engineering code
 Spark updates
 Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT
 Research existing data in the data lake to determine best sources for data
 Create, manage, and maintain ksqlDB and Kafka Streams queries/code
 Maintain and update Python-based data processing scripts executed on AWS Lambdas
 Unit tests for all the Spark, Python data processing and Lambda codes
 Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)

Qualifications

 10+ years of IT experience focusing on enterprise data architecture and management
 Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
 Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required
 Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark
 Data Lake concepts such as time travel and schema evolution and optimization
 Structured Streaming and Delta Live Tables with Databricks a bonus
 Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
 Advanced level understanding of streaming data pipelines and how they differ from batch systems
 Formalize concepts of how to handle late data, defining windows, and data freshness
 Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc
 Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.
 Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus
 Understanding of streaming data pipelines and batch systems
 Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness
 Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
 Indexing and partitioning strategy experience
 Debug, troubleshoot, design and implement solutions to complex technical issues
 Experience with large-scale, high-performance enterprise big data application deployment and solution
 Understanding how to create DAGs to define workflows
 Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required
 Architecture experience in AWS environment a bonus
 Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale (UNICORN) a bonus
 Experience with Docker, Jenkins, and CloudWatch
 Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines
 Experience working with AWS Lambdas for configuration and optimization
 Experience working with DynamoDB to query and write data
 Experience with S3
 Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus
 Familiarity with Pytest and Unittest a bonus
 Experience working with JSON and defining JSON Schemas a bonus
 Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus
 Familiarity with Schema Registry, message formats such as Avro, ORC, etc.
 Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams
 Ability to thrive in a team-based environment
 Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management

Physical Demands
Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.
Work Environment
Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.
About IBRImagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:

 Nationwide medical, dental, and vision insurance
 3 weeks of Paid Time Off and 11 Paid Federal Holidays
 401k matching
 Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees
 Flexible spending accounts and Dependent Care spending accounts
 Wellness incentives
 Reimbursement for professional development and certifications
 Training assistance opportunities

Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.
Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.”Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional contact information has been provided below:
info@teamibr.com​​​​​​​407.459.1830
Job Type: Full-time
Pay: $131,243.38 - $158,056.53 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Referral program
 Vision insurance

Experience level:

 10 years

Schedule:

 Monday to Friday

Work Location: Remote","<p><b>The Senior Data Engineer must be able to meet the key criteria below:</b></p>
<ul>
 <li><b>Location: </b>100% telework</li>
 <li><b>Years&apos; Experience: </b>10+ years</li>
 <li><b>Education: </b>Bachelor&#x2019;s in IT related field</li>
 <li><b>Security Clearance:</b> IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required.</li>
 <li><b>Work Authorization:</b> Must show that applicant is legally permitted to work in the United States.</li>
 <li><b>Employment Type:</b> Full-Time, W-2</li>
 <li><b>Key Skills:</b></li>
 <li>10+ years of IT experience focusing on enterprise data architecture and management</li>
 <li>Experience with Databricks required</li>
 <li>8+ years experience in Conceptual/Logical/Physical Data Modeling &amp; expertise in Relational and Dimensional Data Modeling</li>
 <li>Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services</li>
 <li>Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)</li>
 <li>Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus</li>
</ul>
<p><b>Overview</b></p>
<p>Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations &amp; Maintenance activities.</p>
<p><b>Responsibilities</b></p>
<ul>
 <li>Plan, create, and maintain data architectures, ensuring alignment with business requirements</li>
 <li>Obtain data, formulate dataset processes, and store optimized data</li>
 <li>Identify problems and inefficiencies and apply solutions</li>
 <li>Determine tasks where manual participation can be eliminated with automation.</li>
 <li>Identify and optimize data bottlenecks, leveraging automation where possible</li>
 <li>Create and manage data lifecycle policies (retention, backups/restore, etc)</li>
 <li>Create, maintain, and manage ETL/ELT pipelines</li>
 <li>Create, maintain, and manage data transformations</li>
 <li>Maintain/update documentation</li>
 <li>Create, maintain, and manage data pipeline schedules</li>
 <li>Monitor data pipelines</li>
 <li>Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality</li>
 <li>Support AI/ML teams with optimizing feature engineering code</li>
 <li>Spark updates</li>
 <li>Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT</li>
 <li>Research existing data in the data lake to determine best sources for data</li>
 <li>Create, manage, and maintain ksqlDB and Kafka Streams queries/code</li>
 <li>Maintain and update Python-based data processing scripts executed on AWS Lambdas</li>
 <li>Unit tests for all the Spark, Python data processing and Lambda codes</li>
 <li>Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)</li>
</ul>
<p><b>Qualifications</b></p>
<ul>
 <li>10+ years of IT experience focusing on enterprise data architecture and management</li>
 <li>Experience in Conceptual/Logical/Physical Data Modeling &amp; expertise in Relational and Dimensional Data Modeling</li>
 <li>Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required</li>
 <li>Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark</li>
 <li>Data Lake concepts such as time travel and schema evolution and optimization</li>
 <li>Structured Streaming and Delta Live Tables with Databricks a bonus</li>
 <li>Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support</li>
 <li>Advanced level understanding of streaming data pipelines and how they differ from batch systems</li>
 <li>Formalize concepts of how to handle late data, defining windows, and data freshness</li>
 <li>Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc</li>
 <li>Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.</li>
 <li>Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus</li>
 <li>Understanding of streaming data pipelines and batch systems</li>
 <li>Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness</li>
 <li>Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)</li>
 <li>Indexing and partitioning strategy experience</li>
 <li>Debug, troubleshoot, design and implement solutions to complex technical issues</li>
 <li>Experience with large-scale, high-performance enterprise big data application deployment and solution</li>
 <li>Understanding how to create DAGs to define workflows</li>
 <li>Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required</li>
 <li>Architecture experience in AWS environment a bonus</li>
 <li>Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale (UNICORN) a bonus</li>
 <li>Experience with Docker, Jenkins, and CloudWatch</li>
 <li>Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines</li>
 <li>Experience working with AWS Lambdas for configuration and optimization</li>
 <li>Experience working with DynamoDB to query and write data</li>
 <li>Experience with S3</li>
 <li>Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus</li>
 <li>Familiarity with Pytest and Unittest a bonus</li>
 <li>Experience working with JSON and defining JSON Schemas a bonus</li>
 <li>Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus</li>
 <li>Familiarity with Schema Registry, message formats such as Avro, ORC, etc.</li>
 <li>Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams</li>
 <li>Ability to thrive in a team-based environment</li>
 <li>Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management</li>
</ul>
<p><b>Physical Demands</b></p>
<p>Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.</p>
<p><b>Work Environment</b></p>
<p>Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.</p>
<p><b>About IBR</b><br>Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:</p>
<ul>
 <li>Nationwide medical, dental, and vision insurance</li>
 <li>3 weeks of Paid Time Off and 11 Paid Federal Holidays</li>
 <li>401k matching</li>
 <li>Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees</li>
 <li>Flexible spending accounts and Dependent Care spending accounts</li>
 <li>Wellness incentives</li>
 <li>Reimbursement for professional development and certifications</li>
 <li>Training assistance opportunities</li>
</ul>
<p>Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.</p>
<p>Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.&#x201d;<br>Learn more at <i>http://www.teamibr.com</i></p>
<p>If alternative methods of assistance are needed with the application process, additional contact information has been provided below:</p>
<p><i>info@teamibr.com</i><br>&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;407.459.1830</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;131,243.38 - &#x24;158,056.53 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Referral program</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,c1dc76acaaac3b9b,,Full-time,,,Remote,Senior Data Engineer,5 days ago,2023-10-13T13:35:17.517Z,,,"$131,243 - $158,057 a year",2023-10-18T13:35:17.521Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=c1dc76acaaac3b9b&from=jasx&tk=1hd1ft1i8joou800&vjs=3
63,PrismHR,"Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.
  We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:
 
   Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
   Defining streaming event data feeds required for real-time analytics and reporting
   Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance
 
  As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product. 
 Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!
  Responsibilities:
 
   Build our next generation data warehouse
   Build our event stream platform
   Translate user requirements for reporting and analysis into actionable deliverables
   Enhance automation, operation, and expansion of real-time and batch data environment
   Manage numerous projects in an ever-changing work environment
   Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
   Build processes for topnotch security, performance, reliability, and accuracy
   Provide mentorship and collaborate with fellow team members
 
 
  Requirements:
 
   Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
   3+ years of experience building data pipelines
   3+ years of experience building data frameworks for unit testing, data lineage tracking, and automation 
  Fluency in Scala is required
   Working knowledge of Apache Spark
   Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)
 
 
  Nice to Have:
 
   Experience with Machine Learning 
  Familiarity with Looker a plus
   Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)
 
 
 
   Please note: This position can be remote/telecommute. Notice for candidates located in the following states: CA, CO, NJ, NY, WA: The base salary range for this position is between $100,000 - $145,000 (salary is dependent on location, experience, knowledge, and skills based on the responsibilities outlined in the job description).
 
  #LI-REMOTE
  PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners. 
 Diversity, Equity and Inclusion Program/Affirmative Action Plan: We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.  Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.  As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.  The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers. 
 Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy. 
 PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response. 
 #LI-ML1
  
 vlsNrOjLD3","<div>
 <p>Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We&apos;re hiring Data Engineers to help build and optimize the foundational architecture of our product&apos;s data.</p>
 <p> We&#x2019;ve built a strong data engineering team to date, but have a lot of work ahead of us, including:</p>
 <ul>
  <li> Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds</li>
  <li> Defining streaming event data feeds required for real-time analytics and reporting</li>
  <li> Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance</li>
 </ul>
 <p> As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product. </p>
 <p>Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You&apos;ll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You&#x2019;ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!</p>
 <p><b> Responsibilities:</b></p>
 <ul>
  <li> Build our next generation data warehouse</li>
  <li> Build our event stream platform</li>
  <li> Translate user requirements for reporting and analysis into actionable deliverables</li>
  <li> Enhance automation, operation, and expansion of real-time and batch data environment</li>
  <li> Manage numerous projects in an ever-changing work environment</li>
  <li> Extract, transform, and load complex data into the data warehouse using cutting-edge technologies</li>
  <li> Build processes for topnotch security, performance, reliability, and accuracy</li>
  <li> Provide mentorship and collaborate with fellow team members</li>
 </ul>
 <p></p>
 <p><b><br> Requirements:</b></p>
 <ul>
  <li> Bachelor&#x2019;s or Master&#x2019;s degree in Computer Science, Information Systems, Operations Research, or related field required</li>
  <li> 3+ years of experience building data pipelines</li>
  <li> 3+ years of experience building data frameworks for unit testing, data lineage tracking, and automation </li>
  <li>Fluency in Scala is required</li>
  <li> Working knowledge of Apache Spark</li>
  <li> Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)</li>
 </ul>
 <p></p>
 <p><b><br> Nice to Have:</b></p>
 <ul>
  <li> Experience with Machine Learning </li>
  <li>Familiarity with Looker a plus</li>
  <li> Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)</li>
 </ul>
 <p></p>
 <ul>
  <li><br> Please note: This position can be remote/telecommute. Notice for candidates located in the following states: CA, CO, NJ, NY, WA: The base salary range for this position is between &#x24;100,000 - &#x24;145,000 (salary is dependent on location, experience, knowledge, and skills based on the responsibilities outlined in the job description).</li>
 </ul>
 <p> #LI-REMOTE</p>
 <p> PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company&#x2019;s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR&#x2019;s Marketplace Partners.</p> 
 <p><b>Diversity, Equity and Inclusion Program/Affirmative Action Plan:</b><br> We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.<br> <br> Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren&apos;t just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you&#x2019;re excited about our roles but your past experience doesn&#x2019;t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.<br> <br> As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual&#x2019;s qualifications as they relate to the job under consideration. The Company&#x2019;s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.<br> <br> The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers. </p>
 <p>Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy.</p> 
 <p><i>PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.</i></p> 
 <p>#LI-ML1</p>
 <p> </p>
 <p>vlsNrOjLD3</p>
</div>",https://prismhr.applytojob.com/apply/vlsNrOjLD3/Senior-Data-Engineer?source=INDE,3cf3060bcd436793,,Full-time,,,Remote,Senior Data Engineer,5 days ago,2023-10-13T13:35:12.830Z,3.2,38.0,"$100,000 - $145,000 a year",2023-10-18T13:35:12.833Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=3cf3060bcd436793&from=jasx&tk=1hd1ft1i8joou800&vjs=3
64,SourceMantra,"Hello,

 Job Title: Senior AWS Data Engineer with Strong Python Development(10+)
 Location: Reston, VA – Once in a month
 Duration: 12+ Months Contract position
 Experience : 10+ years

Need to be strong in AWS SNS, SQS, Lambda functions.
Job Description :

 • Strong knowledge on AWS services such as (S3, RDS, EC2, Lambda, SQS, SNS, Redshift)
 • Having prior working experience in Fannie Mae will be added advantage.
 • Good Knowledge on Java and Database (Oracle Postgres)

Thanks & RegardsSwarna - swarna@sourcemantra.com|Source Mantra IncTechnical Recruiter Desk No: (908-381-0321)LinkedIn: linkedin.com/in/swarna-yenumula-6512a1167295 Durham Ave, Suite # 201, South Plainfield, NJ 07080Source Mantra Inc | Certified Minority Business Enterprise (MBE)
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:

 Health insurance

Compensation package:

 Hourly pay

Experience level:

 10 years

Schedule:

 8 hour shift

Experience:

 Java: 4 years (Preferred)
 Python: 3 years (Preferred)
 Redshift: 3 years (Preferred)

Work Location: Remote","<p>Hello,</p>
<ul>
 <li><b>Job Title:</b> Senior AWS Data Engineer with Strong Python Development(10+)</li>
 <li><b>Location:</b> Reston, VA &#x2013; Once in a month</li>
 <li><b>Duration:</b> 12+ Months Contract position</li>
 <li><b>Experience :</b> 10+ years</li>
</ul>
<p>Need to be strong in AWS SNS, SQS, Lambda functions.</p>
<p><b>Job Description :</b></p>
<ul>
 <li>&#x2022; Strong knowledge on AWS services such as (S3, RDS, EC2, Lambda, SQS, SNS, Redshift)</li>
 <li>&#x2022; Having prior working experience in Fannie Mae will be added advantage.</li>
 <li>&#x2022; Good Knowledge on Java and Database (Oracle Postgres)</li>
</ul>
<p><b>Thanks &amp; Regards</b><br><b>Swarna - </b>swarna@sourcemantra.com|<b>Source Mantra Inc</b><br><b>Technical Recruiter </b><br><b>Desk No</b>: (908-381-0321)<br><b>LinkedIn</b>: linkedin.com/in/swarna-yenumula-6512a1167<br><b>295 Durham Ave, Suite # 201, South Plainfield, NJ 07080</b><br><b>Source Mantra Inc | Certified Minority Business Enterprise (MBE)</b></p>
<p>Job Type: Contract</p>
<p>Salary: &#x24;60.00 - &#x24;70.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Hourly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Java: 4 years (Preferred)</li>
 <li>Python: 3 years (Preferred)</li>
 <li>Redshift: 3 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,6154aa92888e85ea,,Contract,,,Remote,AWS Data Engineer with Python 10+Years,5 days ago,2023-10-13T13:35:21.881Z,,,$60 - $70 an hour,2023-10-18T13:35:21.883Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=6154aa92888e85ea&from=jasx&tk=1hd1ft1i8joou800&vjs=3
65,Hire IT People Inc,"**Position:** Azure Date Engineer
**Location:** [Remote] / Customer is located in Washington DC.
**Responsibilities:**
1. Provide support for Azure cloud-based applications and infrastructure.
2. Diagnose and troubleshoot technical issues related to Azure cloud services.
3. Collaborate with cross-functional teams to implement best practices and ensure service optimization.
4. Continuously monitor and manage cloud environment to prevent and mitigate risks.
5. Assist client in designing and implementing Azure cloud solutions based on their unique needs by moving forward siting applications into Cloud environment.
6. Keep up to date with the latest Azure updates and features, ensuring optimal utilization and implementation.
7. Offer guidance and recommendations on cost-management strategies within Azure.
8. Contribute to the development of internal tools and processes to enhance Azure cloud management and support.
**Requirements:**
1. Proven experience as a Cloud Support Engineer, preferably with a focus on Azure.
2. Strong knowledge of Azure services, including but not limited to Azure Active Directory, Azure DevOps, Azure Kubernetes Service, and more.
3. Excellent problem-solving skills and the ability to diagnose complex cloud-based issues.
4. Strong verbal and written communication skills.
5. Must be a US citizen.
6. Must successfully pass Level 4 Public Trust verification.
7. Relevant certifications in Azure, such as AZ-104 or AZ-303/304, are a plus.
Job Type: Full-time
Salary: From $140,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Vision insurance

Work Location: Remote","<p>**Position:** Azure Date Engineer</p>
<p>**Location:** [Remote] / Customer is located in Washington DC.</p>
<p>**Responsibilities:**</p>
<p>1. Provide support for Azure cloud-based applications and infrastructure.</p>
<p>2. Diagnose and troubleshoot technical issues related to Azure cloud services.</p>
<p>3. Collaborate with cross-functional teams to implement best practices and ensure service optimization.</p>
<p>4. Continuously monitor and manage cloud environment to prevent and mitigate risks.</p>
<p>5. Assist client in designing and implementing Azure cloud solutions based on their unique needs by moving forward siting applications into Cloud environment.</p>
<p>6. Keep up to date with the latest Azure updates and features, ensuring optimal utilization and implementation.</p>
<p>7. Offer guidance and recommendations on cost-management strategies within Azure.</p>
<p>8. Contribute to the development of internal tools and processes to enhance Azure cloud management and support.</p>
<p>**Requirements:**</p>
<p>1. Proven experience as a Cloud Support Engineer, preferably with a focus on Azure.</p>
<p>2. Strong knowledge of Azure services, including but not limited to Azure Active Directory, Azure DevOps, Azure Kubernetes Service, and more.</p>
<p>3. Excellent problem-solving skills and the ability to diagnose complex cloud-based issues.</p>
<p>4. Strong verbal and written communication skills.</p>
<p>5. Must be a US citizen.</p>
<p>6. Must successfully pass Level 4 Public Trust verification.</p>
<p>7. Relevant certifications in Azure, such as AZ-104 or AZ-303/304, are a plus.</p>
<p>Job Type: Full-time</p>
<p>Salary: From &#x24;140,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Work Location: Remote</p>",,649f0d12515402f8,,Full-time,,,Remote,Azure Data Engineer,5 days ago,2023-10-13T13:35:29.972Z,,,"From $140,000 a year",2023-10-18T13:35:30.158Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=649f0d12515402f8&from=jasx&tk=1hd1ft1i8joou800&vjs=3
66,Blueprint Technologies,"Senior Data Engineer, Azure 
  Senior Data Engineer 
  Remote-US Only 
  Who is Blueprint? 
  We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun. 
  What does Blueprint do? 
  Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies. 
  Why Blueprint? 
  At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint. 
  What will I be doing? 
  Blueprint is looking for an Azure, Senior Data Engineer to join us as we build cutting-edge technology solutions! This is a fast-paced role that needs a dedicated and passionate individual focused on team and client satisfaction! The ideal candidate will have a solid background in consulting, with demonstrating experience in leading clients through the process of building modern data estates. You will also be responsible for mentoring any junior developers on the engagement. 
  Responsibilities: 
  
  Develop and implement effective data architecture solutions using Databricks and Lakehouse 
  Optimize and tune data pipelines for performance and scalability 
  Monitor and troubleshoot data pipelines to ensure data availability and reliability 
  Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from data 
  Implement best practices for data governance, data security, and data quality to ensure data integrity across all data sources 
  Create and maintain documentation related to data architecture, data pipelines, and data models 
  Stay up to date with emerging technologies and best practices in data engineering and big data processing 
  Mentor and train other data engineers on best practices for data engineering and Databricks usage 
  Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally 
  
 Qualifications: 
  
  Bachelor's degree in computer science or equivalent experience 
  At least 5+ -years of experience as a data engineer 
  At least 5+ years of experience with SQL Development (ETL transformations, stored procedure) 
  Data Ingestion experience from inception to Gold Medallion 
 
 
  Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices 
 
 
  At least 3-5 -years of experience programming with PySpark performing various transformations 
  Design, build, implement and maintain our data infrastructure to power analytics and ML 
  Partner with engineers, producers and designers to deliver data insights that impact our players 
  Contribute to our investments into various open-source and 3rd party tools to build a system that scales with the company 
  Collaborate with our Data Scientists and Analytics Engineers to make pipeline implementation faster, more straightforward, and more trustworthy driven decisions that will shape our business 
  2-5+ years building large scale data infrastructure on Spark/Databricks or similar 
  3+ years experience working with real-time data ingestion/processing 
  Working knowledge of Databricks DLT(Delta Live Table) and Unity Catalog a plus 
  Experience with relational and non-relational database technologies (i.e., NoSQL, blob storage, etc.) 
  Experience with data wrangling skills with csv, tsv, parquet, and json 
  Experience designing, building and optimizing Big Data platforms that are robust, scalable and reliable 
  Excellent problem-solving and troubleshooting skills 
  
 Salary Range 
  Pay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $146,400 to $175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range. 
  Equal Opportunity Employer 
  Blueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law. 
  If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.com 
  Blueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes: 
  
  Medical, dental, and vision coverage 
  Flexible Spending Account 
  401k program 
  Competitive PTO offerings 
  Parental Leave 
  Opportunities for professional growth and development","<div>
 <p><b>Senior Data Engineer, Azure</b></p> 
 <p><b> Senior Data Engineer</b></p> 
 <p><b> Remote-US Only</b></p> 
 <p><b> Who is Blueprint?</b></p> 
 <p> We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We&apos;re bold, smart, agile, and fun.</p> 
 <p><b> What does Blueprint do?</b></p> 
 <p> Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.</p> 
 <p><b> Why Blueprint?</b></p> 
 <p> At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You&apos;ll focus on solving unique business problems while gaining hands-on experience with the world&apos;s best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you&apos;ll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won&apos;t find a better place to work and thrive than at Blueprint.</p> 
 <p><b> What will I be doing?</b></p> 
 <p> Blueprint is looking for an <b>Azure, Senior Data Engineer </b>to join us as we build cutting-edge technology solutions! This is a fast-paced role that needs a dedicated and passionate individual focused on team and client satisfaction! The ideal candidate will have a solid background in consulting, with demonstrating experience in leading clients through the process of building modern data estates. You will also be responsible for mentoring any junior developers on the engagement.</p> 
 <p><b> Responsibilities:</b></p> 
 <ul> 
  <li>Develop and implement effective data architecture solutions using Databricks and Lakehouse</li> 
  <li>Optimize and tune data pipelines for performance and scalability</li> 
  <li>Monitor and troubleshoot data pipelines to ensure data availability and reliability</li> 
  <li>Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from data</li> 
  <li>Implement best practices for data governance, data security, and data quality to ensure data integrity across all data sources</li> 
  <li>Create and maintain documentation related to data architecture, data pipelines, and data models</li> 
  <li>Stay up to date with emerging technologies and best practices in data engineering and big data processing</li> 
  <li>Mentor and train other data engineers on best practices for data engineering and Databricks usage</li> 
  <li>Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally</li> 
 </ul> 
 <p><b>Qualifications:</b></p> 
 <ul> 
  <li>Bachelor&apos;s degree in computer science or equivalent experience</li> 
  <li>At least 5+ -years of experience as a data engineer</li> 
  <li>At least 5+ years of experience with SQL Development (ETL transformations, stored procedure)</li> 
  <li>Data Ingestion experience from inception to Gold Medallion</li> 
 </ul>
 <ul>
  <li>Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices</li> 
 </ul>
 <ul>
  <li>At least 3-5 -years of experience programming with PySpark performing various transformations</li> 
  <li>Design, build, implement and maintain our data infrastructure to power analytics and ML</li> 
  <li>Partner with engineers, producers and designers to deliver data insights that impact our players</li> 
  <li>Contribute to our investments into various open-source and 3rd party tools to build a system that scales with the company</li> 
  <li>Collaborate with our Data Scientists and Analytics Engineers to make pipeline implementation faster, more straightforward, and more trustworthy driven decisions that will shape our business</li> 
  <li>2-5+ years building large scale data infrastructure on Spark/Databricks or similar</li> 
  <li>3+ years experience working with real-time data ingestion/processing</li> 
  <li>Working knowledge of Databricks DLT(Delta Live Table) and Unity Catalog a plus</li> 
  <li>Experience with relational and non-relational database technologies (i.e., NoSQL, blob storage, etc.)</li> 
  <li>Experience with data wrangling skills with csv, tsv, parquet, and json</li> 
  <li>Experience designing, building and optimizing Big Data platforms that are robust, scalable and reliable</li> 
  <li>Excellent problem-solving and troubleshooting skills</li> 
 </ul> 
 <p><b>Salary Range</b></p> 
 <p> Pay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: &#x24;146,400 to &#x24;175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate&apos;s qualifications and experience and may be outside this range.</p> 
 <p><b> Equal Opportunity Employer</b></p> 
 <p> Blueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.</p> 
 <p> If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.com</p> 
 <p> Blueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes:</p> 
 <ul> 
  <li>Medical, dental, and vision coverage</li> 
  <li>Flexible Spending Account</li> 
  <li>401k program</li> 
  <li>Competitive PTO offerings</li> 
  <li>Parental Leave</li> 
  <li>Opportunities for professional growth and development</li>
 </ul>
</div>
<p></p>",https://bpcs.com/careers/jobs/avail?gh_jid=5433144&gh_src=57f83ce11us,75fadf29bd2cd624,,,,,Remote,Sr. Data Engineer,5 days ago,2023-10-13T13:35:25.126Z,3.2,57.0,"$146,400 - $175,100 a year",2023-10-18T13:35:25.130Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=75fadf29bd2cd624&from=jasx&tk=1hd1ft1i8joou800&vjs=3
68,eTeam Inc.,"Job Title :- Data engineer (10+ years candidate required) Job Location :- Remote   Job Description :- 
  LinkedIn is Mandatory 
  Description:- 
  
  
   Bachelors degree in the areas of Computer Science, Engineering, Information Systems, Business, or equivalent field of study required 
   7+ years of experience in working with data solutions. 
   3+ years of experience coding in Python, or Scala or similar scripting language. 
   3+ years of experience in developing data pipelines in AWS Cloud Platform (preferred), Azure, or Snowflake at scale. 
   2+ years Experience in designing and implementing data ingestion with real-time data streaming tools like Kafka, Kinesis or any similar tools.SAP/Client or other cloud integrations are preferred. 
   3+ years experience working with MPP databases such as Snowflake (Preferred) , Redshift or similar MPP databases. 
   2+ years experience working with Serverless ETL processes (Lambda, AWS Glue, Matillion or similar) 
   1+ years experience with big data technologies like EMR, Hadoop, Spark, Cassandra, MongoDB or other open source big data tools. 
   Knowledge of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations. 
   Experience designing, documenting, and defending designs for key components in large distributed computing systems 
   Demonstrated ability to learn new technologies quickly and independently 
   Demonstrated ability to achieve stretch goals in a very innovative and fast paced environment 
   Ability to handle multiple competing priorities in a fast-paced environment 
   Excellent verbal and written communication skills, especially in technical communications 
   Strong interpersonal skills and a desire to work collaboratively 
  
  Experience participating in an Agile software development team, e.g. SCRUM 
  
  Job Responsibilities : 
  
   Responsible for the building, deployment, and maintenance of critical scalable Data Pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements 
   Work closely with SMEs, Data Modeler, Architects, Analysts and other team members on requirements to build scalable real time/near real time/batch data solutions. 
   Contributes design, code, configurations, and documentation for components that manage data ingestion, real time streaming, batch processing, data extraction, transformation, and loading into Data Lake/Cloud Data Warehouse/MPP (Snowflake/Redshift/similar Technologies ) . 
   Owns one or more key components of the infrastructure and works to continually improve it, identifying gaps and improving the platforms quality, robustness, maintainability, and speed. 
   Cross-trains other team members on technologies being developed, while also continuously learning new technologies from other team members. 
   Interacts with technical teams across and ensures that solutions meet customer requirements in terms of functionality, performance, availability, scalability, and reliability. 
   Performs development, QA, and dev-ops roles as needed to ensure total end to end responsibility of solutions. 
   Keep up with current trends in big data and Analytics , evaluate tools and pace yourself for innovation. 
  
  Mentor Junior engineers ,create necessary documentation and Run-books while still being able to deliver on goals","<div>
 <div>
  <p><b>Job Title :- Data engineer (10+ years candidate required)</b><br> <b>Job Location :-</b><b> </b><b>Remote</b><b> </b><br> <br> <b>Job Description :-</b><b> </b></p>
  <p><b>LinkedIn is Mandatory</b><b> </b></p>
  <p><b>Description:-</b> </p>
  <p></p>
  <ul>
   <li>Bachelors degree in the areas of Computer Science, Engineering, Information Systems, Business, or equivalent field of study required</li> 
   <li>7+ years of experience in working with data solutions.</li> 
   <li>3+ years of experience coding in Python, or Scala or similar scripting language. </li>
   <li>3+ years of experience in developing data pipelines in AWS Cloud Platform (preferred), Azure, or Snowflake at scale.</li> 
   <li>2+ years Experience in designing and implementing data ingestion with real-time data streaming tools like Kafka, Kinesis or any similar tools.SAP/Client or other cloud integrations are preferred.</li> 
   <li>3+ years experience working with MPP databases such as Snowflake (Preferred) , Redshift or similar MPP databases.</li> 
   <li>2+ years experience working with Serverless ETL processes (Lambda, AWS Glue, Matillion or similar)</li> 
   <li>1+ years experience with big data technologies like EMR, Hadoop, Spark, Cassandra, MongoDB or other open source big data tools. </li>
   <li>Knowledge of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations.</li> 
   <li>Experience designing, documenting, and defending designs for key components in large distributed computing systems</li> 
   <li>Demonstrated ability to learn new technologies quickly and independently</li> 
   <li>Demonstrated ability to achieve stretch goals in a very innovative and fast paced environment</li> 
   <li>Ability to handle multiple competing priorities in a fast-paced environment</li> 
   <li>Excellent verbal and written communication skills, especially in technical communications</li> 
   <li>Strong interpersonal skills and a desire to work collaboratively</li> 
  </ul>
  <p>Experience participating in an Agile software development team, e.g. SCRUM</p> 
  <p></p>
  <p><b>Job Responsibilities</b><b> </b>: </p>
  <ul>
   <li>Responsible for the building, deployment, and maintenance of critical scalable Data Pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements</li> 
   <li>Work closely with SMEs, Data Modeler, Architects, Analysts and other team members on requirements to build scalable real time/near real time/batch data solutions.</li> 
   <li>Contributes design, code, configurations, and documentation for components that manage data ingestion, real time streaming, batch processing, data extraction, transformation, and loading into Data Lake/Cloud Data Warehouse/MPP (Snowflake/Redshift/similar Technologies ) .</li> 
   <li>Owns one or more key components of the infrastructure and works to continually improve it, identifying gaps and improving the platforms quality, robustness, maintainability, and speed.</li> 
   <li>Cross-trains other team members on technologies being developed, while also continuously learning new technologies from other team members.</li> 
   <li>Interacts with technical teams across and ensures that solutions meet customer requirements in terms of functionality, performance, availability, scalability, and reliability.</li> 
   <li>Performs development, QA, and dev-ops roles as needed to ensure total end to end responsibility of solutions.</li> 
   <li>Keep up with current trends in big data and Analytics , evaluate tools and pace yourself for innovation.</li> 
  </ul>
  <p>Mentor Junior engineers ,create necessary documentation and Run-books while still being able to deliver on goals</p>
 </div>
</div>",https://www2.jobdiva.com/portal/?a=svjdnwzkulao5hqo7t0ifgvj8s71sf01d7dtgdstyhdixakxt6ty85zljsdyhgz2&jobid=20650894#/jobs/20650894?compid=0&SearchString=&StatesString=&id=20650894&source=indeed.com,a03e6162d11ae269,,Contract,,,"Cary, NC",Data engineer,6 days ago,2023-10-12T13:35:44.281Z,,,$45 - $55 an hour,2023-10-18T13:35:44.282Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=a03e6162d11ae269&from=jasx&tk=1hd1fvcrnllot800&vjs=3
69,Sanametrix,"***US Citzenship is Required******
Sanametrix, Inc. is a fast-growing small business headquartered in Arlington, VA. We are dedicated to providing federal agencies with legendary customer service and focused solutions for their business and technology needs.
This role is responsible for building data pipelines for transferring data from source systems (virtual machines, Microsoft SQL Server) into AWS Cloud using AWS Native Tools. This resource has strong data modeling and scripting experience and has a strong knowledge of AWS Data Services.
Responsibilities:

 Perform data processing, algorithm / structures, pipeline orchestration, data quality, governance, discovery


 Work with structured and unstructured data, blob data


 Develop and work with APIs


 Collect and organize data using data warehousing technique and file storage technologies • Perform ELT and ETL processes


 Gather data requirements


 Develop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.


 Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization.


 Implement processes and systems to monitor data quality, to ensure production data accuracy, and ensure key stakeholder and business process access.


 Write unit/integration tests, contribute to engineering wiki, and documents.


 Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.


 Work closely with a team of front-end and back-end engineers, product managers, and analysts.


 Design data integrations and data quality framework based on established requirements.

Qualifications & Skills:
Scripting • SQL • Python • Spark • Linux / shell scripting
Services / Tools (six or more)

 S3


 Lambda


 Redshift


 Lake Formation


 Glue ETL


 Kinesis


 DMS


 Glue catalog/Crawlers


 Git


 Jira


 Airflow /Orchestration

Education, Experience, and Licensing Requirements:

 BS or MS degree in Computer Science or a related technical field


 4+ years of Python or Java development experience


 4+ years of SQL or NoSQL experience


 4+ years of experience with schema design and dimensional data modeling


 Ability in managing and communicating data warehouse plans to internal clients


 Experience designing, building, and maintaining data processing systems


 AWS Certified is preferred

Job Type: Full-time
Pay: $110,656.51 - $120,080.95 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible schedule
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Tuition reimbursement

Compensation package:

 Yearly pay

Experience level:

 4 years

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Experience:

 Informatica: 4 years (Preferred)
 SQL: 4 years (Required)
 Data warehouse: 4 years (Preferred)

Work Location: Remote","<p><b>***US Citzenship is Required******</b></p>
<p>Sanametrix, Inc. is a fast-growing small business headquartered in Arlington, VA. We are dedicated to providing federal agencies with legendary customer service and focused solutions for their business and technology needs.</p>
<p>This role is responsible for building data pipelines for transferring data from source systems (virtual machines, Microsoft SQL Server) into AWS Cloud using AWS Native Tools. This resource has strong data modeling and scripting experience and has a strong knowledge of AWS Data Services.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Perform data processing, algorithm / structures, pipeline orchestration, data quality, governance, discovery</li>
</ul>
<ul>
 <li>Work with structured and unstructured data, blob data</li>
</ul>
<ul>
 <li>Develop and work with APIs</li>
</ul>
<ul>
 <li>Collect and organize data using data warehousing technique and file storage technologies &#x2022; Perform ELT and ETL processes</li>
</ul>
<ul>
 <li>Gather data requirements</li>
</ul>
<ul>
 <li>Develop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.</li>
</ul>
<ul>
 <li>Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization.</li>
</ul>
<ul>
 <li>Implement processes and systems to monitor data quality, to ensure production data accuracy, and ensure key stakeholder and business process access.</li>
</ul>
<ul>
 <li>Write unit/integration tests, contribute to engineering wiki, and documents.</li>
</ul>
<ul>
 <li>Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.</li>
</ul>
<ul>
 <li>Work closely with a team of front-end and back-end engineers, product managers, and analysts.</li>
</ul>
<ul>
 <li>Design data integrations and data quality framework based on established requirements.</li>
</ul>
<p>Qualifications &amp; Skills:</p>
<p>Scripting &#x2022; SQL &#x2022; Python &#x2022; Spark &#x2022; Linux / shell scripting</p>
<p>Services / Tools (six or more)</p>
<ul>
 <li>S3</li>
</ul>
<ul>
 <li>Lambda</li>
</ul>
<ul>
 <li>Redshift</li>
</ul>
<ul>
 <li>Lake Formation</li>
</ul>
<ul>
 <li>Glue ETL</li>
</ul>
<ul>
 <li>Kinesis</li>
</ul>
<ul>
 <li>DMS</li>
</ul>
<ul>
 <li>Glue catalog/Crawlers</li>
</ul>
<ul>
 <li>Git</li>
</ul>
<ul>
 <li>Jira</li>
</ul>
<ul>
 <li>Airflow /Orchestration</li>
</ul>
<p>Education, Experience, and Licensing Requirements:</p>
<ul>
 <li>BS or MS degree in Computer Science or a related technical field</li>
</ul>
<ul>
 <li>4+ years of Python or Java development experience</li>
</ul>
<ul>
 <li>4+ years of SQL or NoSQL experience</li>
</ul>
<ul>
 <li>4+ years of experience with schema design and dimensional data modeling</li>
</ul>
<ul>
 <li>Ability in managing and communicating data warehouse plans to internal clients</li>
</ul>
<ul>
 <li>Experience designing, building, and maintaining data processing systems</li>
</ul>
<ul>
 <li>AWS Certified is preferred</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;110,656.51 - &#x24;120,080.95 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Flexible schedule</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Tuition reimbursement</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>4 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 4 years (Preferred)</li>
 <li>SQL: 4 years (Required)</li>
 <li>Data warehouse: 4 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,6116c05f42022336,,Full-time,,,Remote,Data Engineer,30+ days ago,2023-09-18T13:35:55.040Z,4.4,7.0,"$110,657 - $120,081 a year",2023-10-18T13:35:55.054Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CyQKdz8_lqdlgY-c-amsQST66Z8QjChsyYA8vzcGklWI54h1yaGRml5nZ8zCgFfjK0ZW-ufu-_JHCgbraA_8Fg5ATid4dqS3sKjF52i-1M3_0-M7jyZvHimBaYOdqijbr2o_GnQcYG7gNZLx4-WxgznWbghRNnlHq-GXY6nBkPNqceCYdURQ3TT3immrKviUaPlqf-c_Ib7N-Q0_i_sOXrXBN1vqLBBUi3-4BfTVVixgQFa21zTokCNHLrwz3Kny0Ad23IYAIYfeC_d-rEdT2v5haWsybFd3UNkJ0t79P5xuzLgUur1_DcxTIVo_zkOPIMUDEaZsnbN0I9AGB_So0CcTeVs6N061zI8As2fYAtew8Z5xKsM8ICPSUFtjxKk9wSnYeyfITOW7XC1XRbItQQkvB3GfRObW2uQ8CwkAbHPmVi3ksTfsplb2w2gX85ZpG77ZYLjDKNQ3DrsOeKYW9gA7fjHjPza3iu-T03BvSXuWj8cYVxjdjVKR5AaUXJY0c4qaJPllg0vvLJfrgSosp0otfqB5CqsLSuY3V30mAz5vrQpTRbjswWX0cleaIuMDtEmZGYX13rTMm-A_sJfjuvGutDwZCFKhLhvKVAeszbeA%3D%3D&xkcb=SoDC-_M3JhXw-q2as50FbzkdCdPP&p=14&fvj=1&vjs=3&jsa=8121&tk=1hd1fvosi2gtk000&from=jasx&wvign=1
70,kraken,"Building the Future of Crypto
  
  
  
    Our Krakenites are a world-class team with crypto conviction, united by our desire to discover and unlock the potential of crypto and blockchain technology.
  
  
  
    What makes us different? Kraken is a mission-focused company rooted in crypto values. As a Krakenite, you’ll join us on our mission to accelerate the global adoption of crypto, so that everyone can achieve financial freedom and inclusion. For over a decade, Kraken’s focus on our mission and crypto ethos has attracted many of the most talented crypto experts in the world.
  
  
  
    Before you apply, please read the Kraken Culture page to learn more about our internal culture, values, and mission.
  
  
  
    As a fully remote company, we have Krakenites in 60+ countries who speak over 50 languages. Krakenites are industry pioneers who develop premium crypto products for experienced traders, institutions, and newcomers to the space. Kraken is committed to industry-leading security, crypto education, and world-class client support through our products like Kraken Pro, Kraken NFT, and Kraken Futures.
  
  
  
    Become a Krakenite and build the future of crypto!
  
  
  
    Proof of work
  
  
  
    The team
  
  
  
    Join our Data Infrastructure team and play a pivotal role in upholding the reliability, scalability, and efficiency of our robust Data platform. As a Senior Site Reliability Engineer (SRE) specialized in Data Infrastructure, you will collaborate closely with diverse cross-functional teams to conceive, execute, and oversee the foundational data infrastructure that empowers our array of applications and services. As a key member of our Data Infrastructure team, you will be at the forefront of ensuring the unfaltering availability and performance of our platform. Your profound proficiency in cloud technologies, infrastructure as code, automation, monitoring/alerting, logging, user and machine AuthNZ, and certificate management will be instrumental in upholding the exceptional operational standards we set for our services.
  
  
  
    This role is destined to candidates based in the Americas.
  
  
 
  
   The Opportunity
   
    
      Architect and implement data infrastructure solutions (self service) that support the needs of 10+ business units and over 100 engineering and data analysts
      Utilize Infrastructure as Code (IaC) principles to design, provision, and manage both on-premises and cloud (AWS) infrastructure components using tools such as Terraform
      Collaborate with teams to ensure seamless integration of data-related services with existing systems.
      Develop and maintain automation scripts using bash/shell scripting and to automate operational tasks and deployments.
      Enhance and manage CI/CD pipelines to facilitate consistent software deployments across the data infrastructure.
      Enable engineering self-service under tight security requirements using ChatOps and GitOps methodologies
      Implement robust data monitoring and alerting solutions to proactively detect anomalies and performance issues.
      Manage user and machine authentication and authorization mechanisms to ensure secure access to data and resources.
      Evangelize and implement role-based access control (RBAC) and permissions for a multitude of user groups and machine workflows across different environments
      Design and deploy MLOps platforms, using AWS Sagemaker and GitOps methodologies.
      Manage and maintain real-time streaming data architecture using technologies like Kafka and Debezium Change Data Capture (CDC).
      Ensure the timely and accurate processing of streaming data, enabling data analysts and engineers to gain insights from up-to-date information.
      Utilize Kubernetes to manage containerized applications within the data infrastructure, ensuring efficient deployment, scaling, and orchestration.
      Implement effective incident response procedures and participate in on-call rotations.
      Troubleshoot and resolve incidents promptly to minimize downtime and impact.
      Collaborate with data analysts, engineers, and cross-functional teams to understand requirements and implement appropriate solutions.
      Document architecture, processes, and best practices to enable knowledge sharing and support continuous improvement.
      Enable environments for ML experimentation
      Create and manage MLOps flows for training, validation and deployment of models
      Implement efficient, reproducible production deployment of ML models for inference
    
   
  
  
 
  
   Skills you should HODL
   
    
      Bachelor’s degree in Computer Science, Engineering, or a related field (or equivalent experience).
      Proven experience (5+ years) working as a Site Reliability Engineer, Infrastructure Engineer, or similar roles, with a focus on data infrastructure and security.
      Experience with real-time data processing technologies, such as Kafka and Debezium
      Strong expertise in cloud technologies, particularly AWS and (HashiCorp nice to have).
      Proficiency in Infrastructure as Code tools such as Terraform and Atlantis.
      Experience with containerization and orchestration tools, particularly Kubernetes.
      Solid understanding of bash/shell scripting and proficiency in at least one programming language.
      Familiarity with CI/CD deployment pipelines and related tools.
      Knowledge of HashiCorp products like Vault, Nomad, and Consul is a plus.
      Strong problem-solving skills and the ability to troubleshoot complex systems.
      Expertise in zero-trust architecture and service meshes is a plus
      Experience with data-related technologies (databases, airflow, data warehousing, data lakes) is a plus.
    
   
  
  
 
  
   $117,000 - $176,000 a year
  
  
    This is the target annual salary range for this role. This range is not inclusive of other additional compensation elements, such as our Bonus program, Equity program, Wellness allowance, and other benefits [US Only] (including medical, dental, vision and 401(k)).
  
  
  
    The compensation range provided is influenced by various factors and represents the initial target range. Our salary offerings are dynamic and we strive to ensure that our base salary and total compensation package aligns and recognizes the top talent we aim to attract and retain. The compensation package of the successful candidate is based on various factors such as their skillset, experience, and job scope.
  
  
 
  
   Location Tagging: #US #LI-Remote
  
  
  
    Kraken is powered by people from around the world and we celebrate all Krakenites for their diverse talents, backgrounds, contributions and unique perspectives. We hire strictly based on merit, meaning we seek out the candidates with the right abilities, knowledge, and skills considered the most suitable for the job. We encourage you to apply for roles where you don't fully meet the listed requirements, especially if you're passionate or knowledgable about crypto!
  
  
  
    As an equal opportunity employer, we don’t tolerate discrimination or harassment of any kind. Whether that’s based on race, ethnicity, age, gender identity, citizenship, religion, sexual orientation, disability, pregnancy, veteran status or any other protected characteristic as outlined by federal, state or local laws.
  
  
  
    Stay in the know
  
  
  
    Follow us on Twitter
  
  
    Learn on the Kraken Blog
  
  
    Connect on LinkedIn","<div>
 <div>
  <div>
   <b>Building the Future of Crypto</b>
  </div>
  <div></div>
  <div>
   <br> Our Krakenites are a world-class team with crypto conviction, united by our desire to discover and unlock the potential of crypto and blockchain technology.
  </div>
  <div></div>
  <div>
   <br> What makes us different? Kraken is a mission-focused company rooted in crypto values. As a Krakenite, you&#x2019;ll join us on our mission to accelerate the global adoption of crypto, so that everyone can achieve financial freedom and inclusion. For over a decade, Kraken&#x2019;s focus on our mission and crypto ethos has attracted many of the most talented crypto experts in the world.
  </div>
  <div></div>
  <div>
   <br> Before you apply, please read the Kraken Culture page to learn more about our internal culture, values, and mission.
  </div>
  <div></div>
  <div>
   <br> As a fully remote company, we have Krakenites in 60+ countries who speak over 50 languages. Krakenites are industry pioneers who develop premium crypto products for experienced traders, institutions, and newcomers to the space. Kraken is committed to industry-leading security, crypto education, and world-class client support through our products like Kraken Pro, Kraken NFT, and Kraken Futures.
  </div>
  <div></div>
  <div>
   <br> Become a Krakenite and build the future of crypto!
  </div>
  <div></div>
  <div>
   <b><br> Proof of work</b>
  </div>
  <div></div>
  <div>
   <b><br> The team</b>
  </div>
  <div></div>
  <div>
   <br> Join our Data Infrastructure team and play a pivotal role in upholding the reliability, scalability, and efficiency of our robust Data platform. As a Senior Site Reliability Engineer (SRE) specialized in Data Infrastructure, you will collaborate closely with diverse cross-functional teams to conceive, execute, and oversee the foundational data infrastructure that empowers our array of applications and services. As a key member of our Data Infrastructure team, you will be at the forefront of ensuring the unfaltering availability and performance of our platform. Your profound proficiency in cloud technologies, infrastructure as code, automation, monitoring/alerting, logging, user and machine AuthNZ, and certificate management will be instrumental in upholding the exceptional operational standards we set for our services.
  </div>
  <div></div>
  <div>
   <br> This role is destined to candidates based in the Americas.
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>The Opportunity</b></h3>
   <ul>
    <ul>
     <li> Architect and implement data infrastructure solutions (self service) that support the needs of 10+ business units and over 100 engineering and data analysts</li>
     <li> Utilize Infrastructure as Code (IaC) principles to design, provision, and manage both on-premises and cloud (AWS) infrastructure components using tools such as Terraform</li>
     <li> Collaborate with teams to ensure seamless integration of data-related services with existing systems.</li>
     <li> Develop and maintain automation scripts using bash/shell scripting and to automate operational tasks and deployments.</li>
     <li> Enhance and manage CI/CD pipelines to facilitate consistent software deployments across the data infrastructure.</li>
     <li> Enable engineering self-service under tight security requirements using ChatOps and GitOps methodologies</li>
     <li> Implement robust data monitoring and alerting solutions to proactively detect anomalies and performance issues.</li>
     <li> Manage user and machine authentication and authorization mechanisms to ensure secure access to data and resources.</li>
     <li> Evangelize and implement role-based access control (RBAC) and permissions for a multitude of user groups and machine workflows across different environments</li>
     <li> Design and deploy MLOps platforms, using AWS Sagemaker and GitOps methodologies.</li>
     <li> Manage and maintain real-time streaming data architecture using technologies like Kafka and Debezium Change Data Capture (CDC).</li>
     <li> Ensure the timely and accurate processing of streaming data, enabling data analysts and engineers to gain insights from up-to-date information.</li>
     <li> Utilize Kubernetes to manage containerized applications within the data infrastructure, ensuring efficient deployment, scaling, and orchestration.</li>
     <li> Implement effective incident response procedures and participate in on-call rotations.</li>
     <li> Troubleshoot and resolve incidents promptly to minimize downtime and impact.</li>
     <li> Collaborate with data analysts, engineers, and cross-functional teams to understand requirements and implement appropriate solutions.</li>
     <li> Document architecture, processes, and best practices to enable knowledge sharing and support continuous improvement.</li>
     <li> Enable environments for ML experimentation</li>
     <li> Create and manage MLOps flows for training, validation and deployment of models</li>
     <li> Implement efficient, reproducible production deployment of ML models for inference</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>Skills you should HODL</b></h3>
   <ul>
    <ul>
     <li> Bachelor&#x2019;s degree in Computer Science, Engineering, or a related field (or equivalent experience).</li>
     <li> Proven experience (5+ years) working as a Site Reliability Engineer, Infrastructure Engineer, or similar roles, with a focus on data infrastructure and security.</li>
     <li> Experience with real-time data processing technologies, such as Kafka and Debezium</li>
     <li> Strong expertise in cloud technologies, particularly AWS and (HashiCorp nice to have).</li>
     <li> Proficiency in Infrastructure as Code tools such as Terraform and Atlantis.</li>
     <li> Experience with containerization and orchestration tools, particularly Kubernetes.</li>
     <li> Solid understanding of bash/shell scripting and proficiency in at least one programming language.</li>
     <li> Familiarity with CI/CD deployment pipelines and related tools.</li>
     <li> Knowledge of HashiCorp products like Vault, Nomad, and Consul is a plus.</li>
     <li> Strong problem-solving skills and the ability to troubleshoot complex systems.</li>
     <li> Expertise in zero-trust architecture and service meshes is a plus</li>
     <li> Experience with data-related technologies (databases, airflow, data warehousing, data lakes) is a plus.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   &#x24;117,000 - &#x24;176,000 a year
  </div>
  <div>
    This is the target annual salary range for this role. This range is not inclusive of other additional compensation elements, such as our Bonus program, Equity program, Wellness allowance, and other benefits [US Only] (including medical, dental, vision and 401(k)).
  </div>
  <div></div>
  <div>
   <br> The compensation range provided is influenced by various factors and represents the initial target range. Our salary offerings are dynamic and we strive to ensure that our base salary and total compensation package aligns and recognizes the top talent we aim to attract and retain. The compensation package of the successful candidate is based on various factors such as their skillset, experience, and job scope.
  </div>
 </div> 
 <div>
  <div>
   Location Tagging: #US #LI-Remote
  </div>
  <div></div>
  <div>
   <br> Kraken is powered by people from around the world and we celebrate all Krakenites for their diverse talents, backgrounds, contributions and unique perspectives. We hire strictly based on merit, meaning we seek out the candidates with the right abilities, knowledge, and skills considered the most suitable for the job. We encourage you to apply for roles where you don&apos;t fully meet the listed requirements, especially if you&apos;re passionate or knowledgable about crypto!
  </div>
  <div></div>
  <div>
   <br> As an equal opportunity employer, we don&#x2019;t tolerate discrimination or harassment of any kind. Whether that&#x2019;s based on race, ethnicity, age, gender identity, citizenship, religion, sexual orientation, disability, pregnancy, veteran status or any other protected characteristic as outlined by federal, state or local laws.
  </div>
  <div></div>
  <div>
   <b><br> Stay in the know</b>
  </div>
  <div></div>
  <div>
   <br> Follow us on Twitter
  </div>
  <div>
    Learn on the Kraken Blog
  </div>
  <div>
    Connect on LinkedIn
  </div>
 </div>
</div>",https://jobs.lever.co/kraken/4bc1b010-f1b4-4a64-ae6a-956e2be4ddf3?lever-source=Indeed,d2f7d960ce9f68e6,,Full-time,,,Remote,Site Reliability Engineer - Data Platform,30+ days ago,2023-09-18T13:36:03.509Z,3.6,35.0,"$117,000 - $176,000 a year",2023-10-18T13:36:03.513Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d2f7d960ce9f68e6&from=jasx&tk=1hd1fvosi2gtk000&vjs=3
71,Dollar Tree,"We are seeking a highly skilled and experienced HR Data Engineer to join our team and lead the data migration process to Workday, a leading cloud-based human capital management system. Additionally, you will be responsible for establishing an HR Data Mart, to enhance our People Analytics teams data analytics capabilities. This is an exciting opportunity to be at the forefront of our digital transformation journey and contribute to the success of our HR operations.
  
  
 Responsibilities:
  
  1. Data Migration Strategy: Collaborate with cross-functional teams, including HR, IT, and business stakeholders, to understand data requirements, source systems, and mapping rules to ultimately help develop a comprehensive data migration strategy and plan for the transition from our existing HR systems to Workday
  
  2. HR Data Mart Implementation: Establish an HR Data Mart to centralize HR data from multiple sources and enable enhanced data analytics capabilities. Design and implement the data architecture, including data models, dimensional structures, and reporting frameworks, to support the HR Data Mart. Currently we are using Power BI with Microsoft Fabric and Google Bigquery.
  
  3. ETL Development: Design, develop, and maintain ETL (Extract, Transform, Load) processes and workflows to extract data from source systems, transform it based on mapping rules, and load it into Workday. Optimize ETL processes for efficiency and scalability, considering factors like data volume, complexity, and system performance.
  
  4. Data Validation and Testing: Implement rigorous data validation and testing methodologies to ensure the accuracy, completeness, and consistency of migrated data. Identify and resolve data discrepancies or issues, collaborating with relevant stakeholders as necessary.
  
  5. Data Mapping and Transformation: Analyze the structure and quality of HR data in the current systems and define mapping rules to transform and load the data into the Workday platform. Ensure data accuracy, integrity, and adherence to data governance standards during the migration process.
  
  6. Data Governance and Security: Ensure compliance with data governance policies, security protocols, and regulatory requirements throughout the migration process and HR Data Mart implementation. Implement appropriate data access controls, encryption, and anonymization techniques to protect sensitive HR data.
  
  7. Documentation and Training: Document data migration processes, workflows, mapping rules, and HR Data Mart architecture for future reference. Conduct training sessions and provide support to HR and IT teams on the usage and maintenance of migrated data and the HR Data Mart.
  
  8. Project Management: Lead the data migration project and HR Data Mart implementation, ensuring timely delivery of milestones, monitoring progress, and managing risks and issues. Collaborate with project managers and stakeholders to align data migration and HR Data Mart activities with overall project timelines and objectives.
  
  9. Continuous Improvement: Stay up to date with the latest industry trends, best practices, and technologies related to HR data management, data warehousing, and analytics. Proactively identify opportunities for process improvement and optimization within the HR data ecosystem, including the HR Data Mart.
  
  
 Qualifications:
  
  
 
  Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
  5+ years’ experience as a HR Data Engineer or similar role, with a focus on HR systems, implementing data warehousing solutions and data migration.
  Extensive knowledge of HR data management principles, data integration techniques, ETL processes, and data warehousing concepts.
  Strong expertise in working with HR systems, preferably with hands-on experience in implementing or migrating to Workday.
  Proficient in SQL and scripting languages (Python, R, etc.) for data manipulation, transformation, and validation.
  Familiarity with data governance practices, data privacy regulations, and security protocols.
  Experience in designing and implementing data warehousing solutions, including data modeling and dimensional structures.
  Excellent analytical and problem-solving skills, with the ability to handle complex data challenges.
  Effective communication and interpersonal skills, with the ability to collaborate with diverse stakeholders.
  Project management experience, including planning, coordinating, and monitoring project activities.
  Strong attention to detail and commitment to delivering high-quality results.
 
  If you are a driven and talented HR Data Engineer with a passion for data migration, HR systems, and data warehousing, we would love to hear from you. Join our team and play a key role in transforming our HR data ecosystem by leading the migration to Workday and establishing an HR Data Mart for advanced analytics capabilities.
  
  Compensation range: $120,000 - $133,000 based on experience + bonus + RSUs
  Dollar Tree offers Health, Dental, & Vision, flexible spending account, life and disability insurance benefits, 401k plan, 12 days of PTO & 7 paid holidays annually, and an employee stock purchase plan.","<div>
 We are seeking a highly skilled and experienced HR Data Engineer to join our team and lead the data migration process to Workday, a leading cloud-based human capital management system. Additionally, you will be responsible for establishing an HR Data Mart, to enhance our People Analytics teams data analytics capabilities. This is an exciting opportunity to be at the forefront of our digital transformation journey and contribute to the success of our HR operations.
 <br> 
 <br> 
 <b>Responsibilities:</b>
 <br> 
 <br> 1. Data Migration Strategy: Collaborate with cross-functional teams, including HR, IT, and business stakeholders, to understand data requirements, source systems, and mapping rules to ultimately help develop a comprehensive data migration strategy and plan for the transition from our existing HR systems to Workday
 <br> 
 <br> 2. HR Data Mart Implementation: Establish an HR Data Mart to centralize HR data from multiple sources and enable enhanced data analytics capabilities. Design and implement the data architecture, including data models, dimensional structures, and reporting frameworks, to support the HR Data Mart. Currently we are using Power BI with Microsoft Fabric and Google Bigquery.
 <br> 
 <br> 3. ETL Development: Design, develop, and maintain ETL (Extract, Transform, Load) processes and workflows to extract data from source systems, transform it based on mapping rules, and load it into Workday. Optimize ETL processes for efficiency and scalability, considering factors like data volume, complexity, and system performance.
 <br> 
 <br> 4. Data Validation and Testing: Implement rigorous data validation and testing methodologies to ensure the accuracy, completeness, and consistency of migrated data. Identify and resolve data discrepancies or issues, collaborating with relevant stakeholders as necessary.
 <br> 
 <br> 5. Data Mapping and Transformation: Analyze the structure and quality of HR data in the current systems and define mapping rules to transform and load the data into the Workday platform. Ensure data accuracy, integrity, and adherence to data governance standards during the migration process.
 <br> 
 <br> 6. Data Governance and Security: Ensure compliance with data governance policies, security protocols, and regulatory requirements throughout the migration process and HR Data Mart implementation. Implement appropriate data access controls, encryption, and anonymization techniques to protect sensitive HR data.
 <br> 
 <br> 7. Documentation and Training: Document data migration processes, workflows, mapping rules, and HR Data Mart architecture for future reference. Conduct training sessions and provide support to HR and IT teams on the usage and maintenance of migrated data and the HR Data Mart.
 <br> 
 <br> 8. Project Management: Lead the data migration project and HR Data Mart implementation, ensuring timely delivery of milestones, monitoring progress, and managing risks and issues. Collaborate with project managers and stakeholders to align data migration and HR Data Mart activities with overall project timelines and objectives.
 <br> 
 <br> 9. Continuous Improvement: Stay up to date with the latest industry trends, best practices, and technologies related to HR data management, data warehousing, and analytics. Proactively identify opportunities for process improvement and optimization within the HR data ecosystem, including the HR Data Mart.
 <br> 
 <br> 
 <b>Qualifications:</b>
 <br> 
 <br> 
 <ul>
  <li>Bachelor&apos;s or Master&apos;s degree in Computer Science, Information Systems, or a related field.</li>
  <li>5+ years&#x2019; experience as a HR Data Engineer or similar role, with a focus on HR systems, implementing data warehousing solutions and data migration.</li>
  <li>Extensive knowledge of HR data management principles, data integration techniques, ETL processes, and data warehousing concepts.</li>
  <li>Strong expertise in working with HR systems, preferably with hands-on experience in implementing or migrating to Workday.</li>
  <li>Proficient in SQL and scripting languages (Python, R, etc.) for data manipulation, transformation, and validation.</li>
  <li>Familiarity with data governance practices, data privacy regulations, and security protocols.</li>
  <li>Experience in designing and implementing data warehousing solutions, including data modeling and dimensional structures.</li>
  <li>Excellent analytical and problem-solving skills, with the ability to handle complex data challenges.</li>
  <li>Effective communication and interpersonal skills, with the ability to collaborate with diverse stakeholders.</li>
  <li>Project management experience, including planning, coordinating, and monitoring project activities.</li>
  <li>Strong attention to detail and commitment to delivering high-quality results.</li>
 </ul>
 <br> If you are a driven and talented HR Data Engineer with a passion for data migration, HR systems, and data warehousing, we would love to hear from you. Join our team and play a key role in transforming our HR data ecosystem by leading the migration to Workday and establishing an HR Data Mart for advanced analytics capabilities.
 <br> 
 <br> Compensation range: &#x24;120,000 - &#x24;133,000 based on experience + bonus + RSUs
 <br> Dollar Tree offers Health, Dental, &amp; Vision, flexible spending account, life and disability insurance benefits, 401k plan, 12 days of PTO &amp; 7 paid holidays annually, and an employee stock purchase plan.
</div>",https://careers.dollartree.com/us/en/job/DTYDTJUS544004BREXTERNALENUS/Senior-HR-Data-Engineer-Remote-Eligible?utm_source=indeed&utm_medium=phenom-feeds&Codes=Indeed,2dbc3a3fcff92774,,,,,"500 Volvo Pkwy, Chesapeake, VA 23320",Senior HR Data Engineer - Remote Eligible,30+ days ago,2023-09-18T13:36:14.926Z,3.3,24770.0,"$120,000 - $133,000 a year",2023-10-18T13:36:14.928Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2dbc3a3fcff92774&from=jasx&tk=1hd1fvosi2gtk000&vjs=3
72,ShipHero,"Hello. We are ShipHero (https://shiphero.com). We have built a software platform entrusted by hundreds of eCommerce companies, large and small to run their operations and we continue to grow. About US$5 billion of eCommerce orders are shipped a year via ShipHero. Our customers sell on Shopify, Amazon, Etsy, eBay, WooCommerce, BigCommerce, and many other platforms. We're driven to help our customers grow their businesses by providing a platform that solves complex problems, and is engineered to be reliable and fast. We are obsessed with building great technology that is beautiful, easy to use, and is loved by our customers. Our culture also reflects our ethos and belief that by bringing passionate, talented, and great people together – you can do great things. 
 Our team is fully remote, the company has always been remote. We communicate regularly using video chat and Slack and put a strong emphasis on asynchronous work so people have large chunks of uninterrupted time to focus and do deep work. 
 About You 
 
  You understand that great things are accomplished when teams work together.
   You've made a lot of mistakes, and most importantly, have learned from them.
   You are experienced in operating and improving the reliability of data storage and processing systems (relational databases, data warehouses, data lakes and distributed processing systems), including operational optimization (e.g. indexes, query tuning, monitoring).
   You have a solid understanding of stream processing and operating streaming solutions (using Kafka/Kinesis or some other solution) and/or CDC workloads.
   You have experience in planning, provisioning, scaling and maintaining reliable data processing systems in AWS or GCP (using Terraform/Ansible).
   You are comfortable with Python and are familiar with maintaining ETL jobs using Airflow or some other solution.
   You are always eager to learn more and love to try out new solutions on your own.
   Experience with the AWS data ecosystem is highly appreciated (Amazon Aurora MySQL, DocumentDB/MongoDB, OpenSearch/ElasticSearch, Redshift, Glue/Spark, MSK/Kafka, Kinesis, Debezium, S3/Apache Hudi, MWAA/Airflow)
  
 The Role
 
   Operate, improve and extend ShipHero's existing data infrastructure.
   Support the teams working on improving the performance of reports, listings, searches and other data and analytics services.
   Collaborate with our DevOps team to identify and remediate current and future data infrastructure performance or reliability issues.
   Work with ETL and streaming solutions.
   Review features and requirements, design and implement solutions together with our data engineers, data scientists, developers and designers.
 
  
 The Perks 
 
  $2.500 so you can buy any equipment you need to be happy at your job
   20 days paid vacation + new year & Christmas
   Conference days don't count against your vacation days, we want you to stay up-to-date
   We will pay for courses & conferences, if you learn we all learn","<div>
 <p>Hello. We are ShipHero (https://shiphero.com). We have built a software platform entrusted by hundreds of eCommerce companies, large and small to run their operations and we continue to grow. About US&#x24;5 billion of eCommerce orders are shipped a year via ShipHero. Our customers sell on Shopify, Amazon, Etsy, eBay, WooCommerce, BigCommerce, and many other platforms. We&apos;re driven to help our customers grow their businesses by providing a platform that solves complex problems, and is engineered to be reliable and fast. We are obsessed with building great technology that is beautiful, easy to use, and is loved by our customers. Our culture also reflects our ethos and belief that by bringing passionate, talented, and great people together &#x2013; you can do great things.</p> 
 <p>Our team is fully remote, the company has always been remote. We communicate regularly using video chat and Slack and put a strong emphasis on asynchronous work so people have large chunks of uninterrupted time to focus and do deep work.</p> 
 <h2 class=""jobSectionHeader""><b>About You</b></h2> 
 <ul>
  <li>You understand that great things are accomplished when teams work together.</li>
  <li> You&apos;ve made a lot of mistakes, and most importantly, have learned from them.</li>
  <li> You are experienced in operating and improving the reliability of data storage and processing systems (relational databases, data warehouses, data lakes and distributed processing systems), including operational optimization (e.g. indexes, query tuning, monitoring).</li>
  <li> You have a solid understanding of stream processing and operating streaming solutions (using Kafka/Kinesis or some other solution) and/or CDC workloads.</li>
  <li> You have experience in planning, provisioning, scaling and maintaining reliable data processing systems in AWS or GCP (using Terraform/Ansible).</li>
  <li> You are comfortable with Python and are familiar with maintaining ETL jobs using Airflow or some other solution.</li>
  <li> You are always eager to learn more and love to try out new solutions on your own.</li>
  <li> Experience with the AWS data ecosystem is highly appreciated (Amazon Aurora MySQL, DocumentDB/MongoDB, OpenSearch/ElasticSearch, Redshift, Glue/Spark, MSK/Kafka, Kinesis, Debezium, S3/Apache Hudi, MWAA/Airflow)</li>
 </ul> 
 <h2 class=""jobSectionHeader""><b>The Role</b></h2>
 <ul>
  <li> Operate, improve and extend ShipHero&apos;s existing data infrastructure.</li>
  <li> Support the teams working on improving the performance of reports, listings, searches and other data and analytics services.</li>
  <li> Collaborate with our DevOps team to identify and remediate current and future data infrastructure performance or reliability issues.</li>
  <li> Work with ETL and streaming solutions.</li>
  <li> Review features and requirements, design and implement solutions together with our data engineers, data scientists, developers and designers.</li>
 </ul>
 <p></p> 
 <h2 class=""jobSectionHeader""><b>The Perks</b></h2> 
 <ul>
  <li>&#x24;2.500 so you can buy any equipment you need to be happy at your job</li>
  <li> 20 days paid vacation + new year &amp; Christmas</li>
  <li> Conference days don&apos;t count against your vacation days, we want you to stay up-to-date</li>
  <li> We will pay for courses &amp; conferences, if you learn we all learn</li>
 </ul>
</div>",https://shiphero.breezy.hr/p/41dafea81338-senior-data-platform-reliability-engineer?source=indeed&ittk=AITEBWA4YD,c7bdf04e7b7bd148,,Full-time,,,Remote,Senior Data Platform Reliability Engineer,30+ days ago,2023-09-18T13:36:00.732Z,3.4,25.0,"$90,000 - $120,000 a year",2023-10-18T13:36:00.735Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=c7bdf04e7b7bd148&from=jasx&tk=1hd1fvosi2gtk000&vjs=3
73,ManTech International Corporation,"Secure our Nation, Ignite your Future 
 
 Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement. 
 
 
  Currently, ManTech is seeking a motivated, career and customer-oriented Data Services Lead Engineer to join our team! 
  
  This is a remote, technical position with an Agile Scrum team that will require a broad scope of skills and knowledge of DoD systems. 
  
  Responsibilities include but are not limited to : 
  
   Designs, develops, tests, debugs, and implements operating systems components, software tools and utilities. 
   Ensures that system improvements are successfully implemented and monitored to increase efficiency. 
   Generates systems software engineering policies, standards, and procedures. 
   Demonstrates proficiency in Java, Python, JavaScript, and HTML5 for software development and integration within the MIP. 
   Operating Systems: Experience with both Windows and RedHat Linux operating systems for deploying and managing data services and integrations. 
   JavaScript and Web Development: Proficiency in JavaScript, JSON, HTML5, CSS, and AJAX, along with experience in jQuery for web interface development and integration. 
   Development Tools: Familiarity with Integrated Development Environments (IDEs) such as Eclipse and IntelliJ for efficient software development. 
   Uses debugging tools and methodologies to troubleshoot and debug UI and software components. 
   Demonstrates ability to think critically and creatively, contributing to the development of innovative solutions to software problems within the MIP. 
   Writes automation test cases to validate system requirements, ensuring the reliability and performance of data services and integrations. 
  
  
  Basic Qualifications: 
  
   A minimum of 4 years of professional experience in software development. 
   Expertise in Java: Strong expertise in Java, including Java Web Containers (Tomcat/JBoss) and utilization of technologies like JDBC for database connectivity. 
   System Integration: Extensive experience in project, product, tools, and system integration, with a track record of successfully integrating components within the MIP. 
   Team Collaboration: Proven ability to work effectively in a team environment, fostering collaboration and achieving project goals collectively. 
   Experience with Operating Systems like Windows and RedHat Linux 
   Strong knowledge of interface development and experience with HTML 5, Java, JavaScript, CSS, and AJAX, JQuery, JSON, Python. 
   Experience utilizing debugging tools and methodologies for debugging UI and components. 
   Excellent oral and written communication skills, and ability to facilitate effective team discussions. 
   Demonstrates the ability to think critically and creatively to develop innovative solutions to software problems. 
   Familiarity with Agile methodologies and frameworks. 
   Solid understanding of software development life cycle (SDLC), iterative development, and release management processes. 
   Excellent oral and written communication skills, ability to collaborate with diverse stakeholders, and facilitate effective team discussions. 
  
  
  Preferred Qualifications: 
  
   Bachelor’s degree in Information Technology, or a related field. 
   Working knowledge of military health systems, along with the positive track record in the DHA (Defense Health Agency) customer space. 
  
  
  Security Clearance Requirement: 
  
   US Citizenship. 
   Must have an active Public Trust clearance. 
   Hired candidate will be processed for a Secret clearance. 
  
  
  Physical Requirements: 
  
   Sedentary work that primarily involves sitting/standing/walking/Talking and must be able to remain in a stationary position 50%. 
   Moving about to accomplish tasks or moving from one work site to another. 
   The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc. 
   Requires frequently communicates with co-workers, management, and customers. 
   Communicating with others to exchange information. 
   Working with computers. 
   Must be able to lift and move hardware weighing up to 50 pounds. 
  
 
 The projected compensation range for this position is $82,400-$137,000. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it’s employees beyond just compensation. ManTech’s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections. 
 
 For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone. 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                       ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law. 
                       
                       If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services. 
                       
                       If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access","<div>
 <p><b>Secure our Nation, Ignite your Future </b></p>
 <p></p>
 <p>Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At <b>ManTech, </b>you&#x2019;ll help protect our national security while working on innovative projects that offer opportunities for advancement. </p>
 <p></p>
 <div>
  <p>Currently, <b>ManTech </b>is seeking a motivated, career and customer-oriented <b>Data Services </b><b>Lead Engineer </b>to join our team! </p>
  <p></p>
  <p><b>This is a remote, technical position with an Agile Scrum team that will require a broad scope of skills and knowledge of DoD systems. </b></p>
  <p></p>
  <p><b>Responsibilities include but are not limited to </b>: </p>
  <ul>
   <li><p>Designs, develops, tests, debugs, and implements operating systems components, software tools and utilities. </p></li>
   <li><p>Ensures that system improvements are successfully implemented and monitored to increase efficiency. </p></li>
   <li><p>Generates systems software engineering policies, standards, and procedures. </p></li>
   <li><p>Demonstrates proficiency in Java, Python, JavaScript, and HTML5 for software development and integration within the MIP. </p></li>
   <li><p>Operating Systems: Experience with both Windows and RedHat Linux operating systems for deploying and managing data services and integrations. </p></li>
   <li><p>JavaScript and Web Development: Proficiency in JavaScript, JSON, HTML5, CSS, and AJAX, along with experience in jQuery for web interface development and integration. </p></li>
   <li><p>Development Tools: Familiarity with Integrated Development Environments (IDEs) such as Eclipse and IntelliJ for efficient software development. </p></li>
   <li><p>Uses debugging tools and methodologies to troubleshoot and debug UI and software components. </p></li>
   <li><p>Demonstrates ability to think critically and creatively, contributing to the development of innovative solutions to software problems within the MIP. </p></li>
   <li><p>Writes automation test cases to validate system requirements, ensuring the reliability and performance of data services and integrations. </p></li>
  </ul>
  <p></p>
  <p><b>Basic Qualifications: </b></p>
  <ul>
   <li><p>A minimum of 4 years of professional experience in software development. </p></li>
   <li><p>Expertise in Java: Strong expertise in Java, including Java Web Containers (Tomcat/JBoss) and utilization of technologies like JDBC for database connectivity. </p></li>
   <li><p>System Integration: Extensive experience in project, product, tools, and system integration, with a track record of successfully integrating components within the MIP. </p></li>
   <li><p>Team Collaboration: Proven ability to work effectively in a team environment, fostering collaboration and achieving project goals collectively. </p></li>
   <li><p>Experience with Operating Systems like Windows and RedHat Linux </p></li>
   <li><p>Strong knowledge of interface development and experience with HTML 5, Java, JavaScript, CSS, and AJAX, JQuery, JSON, Python. </p></li>
   <li><p>Experience utilizing debugging tools and methodologies for debugging UI and components. </p></li>
   <li><p>Excellent oral and written communication skills, and ability to facilitate effective team discussions. </p></li>
   <li><p>Demonstrates the ability to think critically and creatively to develop innovative solutions to software problems. </p></li>
   <li><p>Familiarity with Agile methodologies and frameworks. </p></li>
   <li><p>Solid understanding of software development life cycle (SDLC), iterative development, and release management processes. </p></li>
   <li><p>Excellent oral and written communication skills, ability to collaborate with diverse stakeholders, and facilitate effective team discussions. </p></li>
  </ul>
  <p></p>
  <p><b>Preferred Qualifications: </b></p>
  <ul>
   <li><p>Bachelor&#x2019;s degree in Information Technology, or a related field. </p></li>
   <li><p>Working knowledge of military health systems, along with the positive track record in the DHA (Defense Health Agency) customer space.<br> </p></li>
  </ul>
  <p></p>
  <p><b>Security Clearance Requirement: </b></p>
  <ul>
   <li><p>US Citizenship. </p></li>
   <li><p>Must have an active Public Trust clearance. </p></li>
   <li><p>Hired candidate will be processed for a Secret clearance. </p></li>
  </ul>
  <p></p>
  <p><b>Physical Requirements: </b></p>
  <ul>
   <li><p>Sedentary work that primarily involves sitting/standing/walking/Talking and must be able to remain in a stationary position 50%. </p></li>
   <li><p>Moving about to accomplish tasks or moving from one work site to another. </p></li>
   <li><p>The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc. </p></li>
   <li><p>Requires frequently communicates with co-workers, management, and customers. </p></li>
   <li><p>Communicating with others to exchange information. </p></li>
   <li><p>Working with computers. </p></li>
   <li><p>Must be able to lift and move hardware weighing up to 50 pounds. </p></li>
  </ul>
 </div>
 <p></p>The projected compensation range for this position is &#x24;82,400-&#x24;137,000. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it&#x2019;s employees beyond just compensation. ManTech&#x2019;s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections. 
 <p></p>
 <p>For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone. </p>
 <p></p>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                <div>
                 <div>
                  <div>
                   <div>
                    <div>
                     <div>
                      <div>
                       <p>ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law. </p>
                       <p></p>
                       <p>If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech&apos;s Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech&apos;s utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services. </p>
                       <p></p>
                       <p>If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access</p>
                      </div>
                     </div>
                    </div>
                   </div>
                  </div>
                 </div>
                </div>
               </div>
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>",https://mantech.wd1.myworkdayjobs.com/en-US/External/job/USA-Remote-Work/Data-Services-Lead-Engineer--Remote-_R42973?source=Indeed,8b7d96de56a29c5d,,Full-time,,,Remote,Data Services Lead Engineer (Remote),30+ days ago,2023-09-18T13:36:28.725Z,3.9,1701.0,"$82,400 - $137,000 a year",2023-10-18T13:36:28.729Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=8b7d96de56a29c5d&from=jasx&tk=1hd1fvosi2gtk000&vjs=3
74,GEICO,"Position Summary
 
 
   GEICO is seeking an experienced Principal Engineer with a passion for building high performance, low maintenance, zero-downtime platforms, and applications. You will help drive our insurance business transformation as we transition from a traditional IT model to a tech organization with engineering excellence as its mission, while co-creating the culture of psychological safety and continuous improvement.
 
 
 
   Position Description
 
 
   Our Principal Engineer works with our Distinguished Engineer and Sr. Engineers to innovate and build new systems, improve, and enhance existing systems as well as identify new opportunities to apply your knowledge to solve critical problems. You will lead the strategy and execution of a technical roadmap that will increase the velocity of delivering products and unlock new engineering capabilities. The ideal candidate has deep technical expertise in software engineering, cryptography, and open-source IaaS platform domain
 
 
 
   Position Responsibilities
 
 
   As a Principal Engineer, you will:
 
 
   Focus on multiple areas and provide technical and thought leadership to the enterprise
   Collaborate with product managers, team members, customers, and other engineering teams to solve our toughest problems
   Develop and execute technical software development strategy for the IaaS Engineering domain, while optimizing for performance and efficiency
   Own accountability for the quality, usability, and performance of the solutions
   Be a role model and mentor, helping to coach and strengthen the technical expertise and know-how of our engineering and product community. Influence and educate executives
   Consistently share best practices and improve processes within and across teams
   Analyze cost and forecast, incorporating them into business plans
   Determine and support resource requirements, evaluate operational processes, measure outcomes to ensure desired results, demonstrate adaptability and sponsor continuous learning
   Take on-call and operation support
 
 
 
   Qualifications
 
 
   Strong foundations in software engineering
   Deep hands-on experience in complex system design and data pipeline and architectures, scale and performance, tuning, with good knowledge on Docker and Kubernetes
   Professional experience in software development at least one modern programming language, including Go, Python, Java, or Rust
   Hands-on experience with public and/or private cloud environments (OpenShift, Kubernetes, Azure, AWS, GCP, etc.)
   Experience and technical knowledge of security engineering, system and network security, authentication and security protocols and cryptography
   Experience in CI/CD pipeline and related open-source tools like GIT/Jenkin/CircleCI/SonarQube and knowledge in Terraform will be big plus
   Demonstrated ability to design and implement resilient, scalable, and efficient solutions
   Strong problem-solving abilities and a proactive approach to identifying and mitigating risks
   Excellent communication skills, able to communicate complex technical concepts to technical and non-technical stakeholders
   Knowledge on Open-source monitoring software like Grafana and Prometheus
   One of more of the following certifications are highly desired:
   Certified Information Systems Security Professional (CISSP)
   Certified Information Security Manager (CISM)
   o Certified Information Systems Auditor (CISA)
 
 
 
   Experience
 
 
   6+ years of professional IaaS experience
   4+ years of experience in open-source frameworks
   3+ years of experience with architecture and design
   3+ years of experience with AWS, GCP, Azure, or another cloud service
   1+ years of people management experience
 
 
 
   Education
 
 
   Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience
 
 
   Benefits:
   As an Associate, you’ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including:
   
  
   Premier Medical, Dental and Vision Insurance with no waiting period**
   Paid Vacation, Sick and Parental Leave
   401(k) Plan
   Tuition Reimbursement
   Paid Training and Licensures
   Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
   
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
   
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
 
 
 
   #LI-RP2
 
 
   #DICE
 
 
 
   Annual Salary
  $100,000.00 - $204,500.00
 
   The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
 
  
  At this time, GEICO will not sponsor a new applicant for employment authorization for this position.
 
 
   Benefits:
 
 
   As an Associate, you’ll enjoy our 
  
   Total Rewards Program
  
  
   to help secure your financial future and preserve your health and well-being, including:
  
 
 
   Premier Medical, Dental and Vision Insurance with no waiting period**
   Paid Vacation, Sick and Parental Leave
   401(k) Plan
   Tuition Reimbursement
   Paid Training and Licensures
 
 
 
  Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
 
 
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
 
 
 
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
 
 
 
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.","<div>
 <div>
  Position Summary
 </div>
 <div>
   GEICO is seeking an experienced Principal Engineer with a passion for building high performance, low maintenance, zero-downtime platforms, and applications. You will help drive our insurance business transformation as we transition from a traditional IT model to a tech organization with engineering excellence as its mission, while co-creating the culture of psychological safety and continuous improvement.
 </div>
 <div></div>
 <div>
   Position Description
 </div>
 <div>
   Our Principal Engineer works with our Distinguished Engineer and Sr. Engineers to innovate and build new systems, improve, and enhance existing systems as well as identify new opportunities to apply your knowledge to solve critical problems. You will lead the strategy and execution of a technical roadmap that will increase the velocity of delivering products and unlock new engineering capabilities. The ideal candidate has deep technical expertise in software engineering, cryptography, and open-source IaaS platform domain
 </div>
 <div></div>
 <div>
   Position Responsibilities
 </div>
 <div>
   As a Principal Engineer, you will:
 </div>
 <ul>
  <li> Focus on multiple areas and provide technical and thought leadership to the enterprise</li>
  <li> Collaborate with product managers, team members, customers, and other engineering teams to solve our toughest problems</li>
  <li> Develop and execute technical software development strategy for the IaaS Engineering domain, while optimizing for performance and efficiency</li>
  <li> Own accountability for the quality, usability, and performance of the solutions</li>
  <li> Be a role model and mentor, helping to coach and strengthen the technical expertise and know-how of our engineering and product community. Influence and educate executives</li>
  <li> Consistently share best practices and improve processes within and across teams</li>
  <li> Analyze cost and forecast, incorporating them into business plans</li>
  <li> Determine and support resource requirements, evaluate operational processes, measure outcomes to ensure desired results, demonstrate adaptability and sponsor continuous learning</li>
  <li> Take on-call and operation support</li>
 </ul>
 <div></div>
 <div>
   Qualifications
 </div>
 <ul>
  <li> Strong foundations in software engineering</li>
  <li> Deep hands-on experience in complex system design and data pipeline and architectures, scale and performance, tuning, with good knowledge on Docker and Kubernetes</li>
  <li> Professional experience in software development at least one modern programming language, including Go, Python, Java, or Rust</li>
  <li> Hands-on experience with public and/or private cloud environments (OpenShift, Kubernetes, Azure, AWS, GCP, etc.)</li>
  <li> Experience and technical knowledge of security engineering, system and network security, authentication and security protocols and cryptography</li>
  <li> Experience in CI/CD pipeline and related open-source tools like GIT/Jenkin/CircleCI/SonarQube and knowledge in Terraform will be big plus</li>
  <li> Demonstrated ability to design and implement resilient, scalable, and efficient solutions</li>
  <li> Strong problem-solving abilities and a proactive approach to identifying and mitigating risks</li>
  <li> Excellent communication skills, able to communicate complex technical concepts to technical and non-technical stakeholders</li>
  <li> Knowledge on Open-source monitoring software like Grafana and Prometheus</li>
  <li> One of more of the following certifications are highly desired:</li>
  <li> Certified Information Systems Security Professional (CISSP)</li>
  <li> Certified Information Security Manager (CISM)</li>
  <li> o Certified Information Systems Auditor (CISA)</li>
 </ul>
 <div></div>
 <div>
   Experience
 </div>
 <ul>
  <li> 6+ years of professional IaaS experience</li>
  <li> 4+ years of experience in open-source frameworks</li>
  <li> 3+ years of experience with architecture and design</li>
  <li> 3+ years of experience with AWS, GCP, Azure, or another cloud service</li>
  <li> 1+ years of people management experience</li>
 </ul>
 <div></div>
 <div>
   Education
 </div>
 <ul>
  <li> Bachelor&#x2019;s degree in Computer Science, Information Systems, or equivalent education or work experience</li>
 </ul>
 <div>
  <br> Benefits:
  <br> As an Associate, you&#x2019;ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including:
  <br> 
  <ul>
   <li>Premier Medical, Dental and Vision Insurance with no waiting period**</li>
   <li>Paid Vacation, Sick and Parental Leave</li>
   <li>401(k) Plan</li>
   <li>Tuition Reimbursement</li>
   <li>Paid Training and Licensures</li>
   <li>Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.</li>
  </ul> **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
  <br> 
  <br> The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
  <br> 
  <br> GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
 </div>
 <div></div>
 <div>
   #LI-RP2
 </div>
 <div>
   #DICE
 </div>
 <div></div>
 <div>
  <br> Annual Salary
 </div> &#x24;100,000.00 - &#x24;204,500.00
 <div>
   The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate&#x2019;s work experience, education and training, the work location as well as market and business considerations.
 </div>
 <br> 
 <div></div> At this time, GEICO will not sponsor a new applicant for employment authorization for this position.
 <div></div>
 <div>
  <br> Benefits:
 </div>
 <div>
   As an Associate, you&#x2019;ll enjoy our 
  <div>
   Total Rewards Program
  </div>
  <ul>
   <li>to help secure your financial future and preserve your health and well-being, including:</li>
  </ul>
 </div>
 <ul>
  <li> Premier Medical, Dental and Vision Insurance with no waiting period**</li>
  <li> Paid Vacation, Sick and Parental Leave</li>
  <li> 401(k) Plan</li>
  <li> Tuition Reimbursement</li>
  <li> Paid Training and Licensures</li>
 </ul>
 <div></div>
 <ul>
  <li>Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.</li>
 </ul>
 <div>
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
 </div>
 <div></div>
 <div>
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
 </div>
 <div></div>
 <div>
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
 </div>
</div>",https://geico.wd1.myworkdayjobs.com/en-US/External/job/Chevy-Chase-MD/Principal-Engineer---IaaS--Cryptography-and-Data-Protection---REMOTE-_R0046548,9db0754fcf79928c,,Full-time,,,"Chevy Chase, MD",Principal Software Engineer – IaaS (Cryptography and Data Protection) (REMOTE),30+ days ago,2023-09-18T13:36:33.001Z,3.2,7497.0,"$100,000 - $204,500 a year",2023-10-18T13:36:33.022Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=9db0754fcf79928c&from=jasx&tk=1hd1fvosi2gtk000&vjs=3
75,Balsam Brands,"Job Description
  As Data Engineer, you will be responsible for designing and developing robust and scalable data warehousing solutions. The Data Engineer will be responsible for building data solutions based on the business requirements. Data solutions may involve retrieval, transformation, storage, and delivery of the data. The Data Engineer must follow standards and implement best practices while writing code and provide production support for the enterprise data warehouse. Our ideal candidate is a skillful data wrangler who enjoys building data solutions from the ground up and optimizing their performance.
  This full-time position reports to the Manager of Data Engineering and can work remote from any U.S. state where Balsam Brands is currently setup as an employer, which includes: CA, CO, FL, GA, ID, IL, IN, KS, KY, MD, MA, MO, NJ, NC, OH, OR, PA, TN, TX, VA, and WA. This role can also work locally in our Redwood City, CA or Boise, ID office location. Our local teams work in a hybrid model, which currently includes Tuesday and Wednesday in-office.
  To ensure sufficient overlap with functional and cross-functional team members globally, some flexibility with this role's regular work schedule will be required. Most of our teams have overlap with early morning and/or early evening PST. Specific scheduling needs for this role will be discussed in the initial interview. 
 What you’ll do: 
 
  Be accountable for building and maintaining the data infrastructure for the organization
   Collaborate with systems analysts and cross functional partners to understand data requirements
   Champion data warehouse, create denormalized data foundation layer and normalized data marts
   Define strategies to capture all data sources and impact of business process changes on data coming from those sources
   Work on all aspects of the data warehouse/BI environment including architecture, design, development, automation, caching and performance tuning
   Continually explore new technologies like Big Data, Artificial Intelligence, Generative AI, Machine Learning, and Predictive Data Modeling
 
  What you bring to the table: 
 
  5+ years of professional experience in the data engineering field
   Demonstrated history of designing and building schemas, tables, views, and data pipelines
   Experience in cloud technologies like Azure, AWS
   Experience in Azure Data Factory (ADF) or equivalent ETL tool
   Knowledge and experience of working with SQL and relational databases like SQL Server, Oracle, Postgres and MySQL
   Ability to understand and tell the story embedded in the data at the core of our business
   Ability to communicate with non-technical audience from a variety of business functions
   Strong knowledge of coding standards, best practices and data governance
 
  Travel for remote team members: At Balsam Brands, we believe that time spent together, in-person, collaborating and building relationships is important to who we are. For our newest remote Brandits, we will arrange travel to one of our local offices within your first three months of employment so you can meet and train with your new team in-person. You may also get to travel an additional 1 – 2 times a year for events such as team retreats, offsites, or learning and development opportunities.
  Notes: This is a full-time, permanent position with benefits. Please submit a cover letter and resume, and only apply if you are able to live and work full-time in one of the states listed in this posting. State locations and specifics are subject to change as our hiring requirements shift.
  About Us: Balsam Brands is a global, eCommerce retailer with roots in holiday and home décor. We strive for excellence in everything we do and present a unique opportunity for those seeking to have a meaningful impact in a people-first company that values relationship building, authenticity, and doing the right thing. We have steadily growing teams in Boise, the Bay Area, Dublin, the Philippines - and most recently, Windsor, Canada!
  The company's mission is to create joy together. We empower our team and partners to love what they do, provide products and experiences that inspire meaningful moments with family and friends, and give back to our families and communities in impactful ways. When you join Balsam Brands, you'll find a culture of caring people doing challenging work and building a welcoming workplace.
 
   Check out our flagship brand, Balsam Hill: www.balsamhill.com
   Balsam Brands in Forbes: https://bit.ly/balsambrandsforbes
   Balsam Brands on LinkedIn: http://www.linkedin.com/company/balsam-brands/
   Glassdoor: https://bit.ly/balsambrands-glassdoor
 
  At Balsam Brands, we strive to offer a competitive compensation and benefits package. For permanent, full-time team members, our current package includes:
 
   Competitive compensation, including a cash-based incentive plan; salary is reviewed yearly and may be adjusted as part of the normal compensation review process
   Comprehensive Medical, Dental, and Vision coverage, with 100% of monthly premiums covered for team members, and 85%+ employer-paid premiums for other coverage tiers that include dependents
   Up to $2,000 annual funding toward HSA accounts
   Medical, transit, dependent care FSA
   Infertility coverage offered on all medical plans
   Generous parental leave program and flexible return options
   Company-paid life and AD&D insurance
   Company-paid short and long-term disability insurance
   401(k) with dollar-for-dollar company match up to $4,000 per calendar year
   Employee Assistance Program (EAP) and other mental health and wellness perks
   Paid holidays, annual shutdown week, PTO, and volunteer time-off (VTO) packages
   Paid 5-week sabbatical leave after 10 years of employment
   Annual continuous learning benefit up to $1,000 per person, per fiscal year
   Up to $300 flexible reimbursement to support setup of new team member's work-from-home environment
   Generous team member merchandise discount
   Valuable extras: identity theft protection, subsidized parking, monthly wellness, pet insurance, accident & critical illness insurance
 
  The base pay range for this position is: $111,000 to $162,000. Where an individual falls within that range will vary based on several factors including geographic location and may vary depending on candidate qualifications and experience, applicable skills, and other job-related factors. We benchmark our pay ranges against current external data sources and regularly review compensation for our team members. Balsam Brands is committed to providing our team members with an internally fair, externally competitive, and fiscally prudent total compensation package administered in a simple and consistent manner.
  At Balsam Brands, we strive to build a diverse, equitable, and inclusive team to fulfill our purpose to create joy together. Balsam Brands is proud to be an equal opportunity employer. We encourage people from all backgrounds, ages, abilities, and experiences to apply. We do not discriminate on the basis of race, ethnicity, religion, national origin, citizenship, marital or family status, disability, sexual orientation, gender identity or expression, pregnancy or caregiver status, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application and interview process, to perform essential job functions, and to receive other benefits and privileges of employment.
  #DICE
 
 

 Additional Information
  All your information will be kept confidential according to EEO guidelines.","<div>
 Job Description
 <p><br> As Data Engineer, you will be responsible for designing and developing robust and scalable data warehousing solutions. The Data Engineer will be responsible for building data solutions based on the business requirements. Data solutions may involve retrieval, transformation, storage, and delivery of the data. The Data Engineer must follow standards and implement best practices while writing code and provide production support for the enterprise data warehouse. Our ideal candidate is a skillful data wrangler who enjoys building data solutions from the ground up and optimizing their performance.</p>
 <p> This full-time position reports to the Manager of Data Engineering and can work remote from any U.S. state where Balsam Brands is currently setup as an employer, which includes: CA, CO, FL, GA, ID, IL, IN, KS, KY, MD, MA, MO, NJ, NC, OH, OR, PA, TN, TX, VA, and WA. This role can also work locally in our Redwood City, CA or Boise, ID office location. Our local teams work in a hybrid model, which currently includes Tuesday and Wednesday in-office.</p>
 <p> To ensure sufficient overlap with functional and cross-functional team members globally, some flexibility with this role&apos;s regular work schedule will be required. Most of our teams have overlap with early morning and/or early evening PST. Specific scheduling needs for this role will be discussed in the initial interview.<i> </i></p>
 <p><b>What you&#x2019;ll do: </b></p>
 <ul>
  <li>Be accountable for building and maintaining the data infrastructure for the organization</li>
  <li> Collaborate with systems analysts and cross functional partners to understand data requirements</li>
  <li> Champion data warehouse, create denormalized data foundation layer and normalized data marts</li>
  <li> Define strategies to capture all data sources and impact of business process changes on data coming from those sources</li>
  <li> Work on all aspects of the data warehouse/BI environment including architecture, design, development, automation, caching and performance tuning</li>
  <li> Continually explore new technologies like Big Data, Artificial Intelligence, Generative AI, Machine Learning, and Predictive Data Modeling</li>
 </ul>
 <p><b> What you bring to the table: </b></p>
 <ul>
  <li>5+ years of professional experience in the data engineering field</li>
  <li> Demonstrated history of designing and building schemas, tables, views, and data pipelines</li>
  <li> Experience in cloud technologies like Azure, AWS</li>
  <li> Experience in Azure Data Factory (ADF) or equivalent ETL tool</li>
  <li> Knowledge and experience of working with SQL and relational databases like SQL Server, Oracle, Postgres and MySQL</li>
  <li> Ability to understand and tell the story embedded in the data at the core of our business</li>
  <li> Ability to communicate with non-technical audience from a variety of business functions</li>
  <li> Strong knowledge of coding standards, best practices and data governance</li>
 </ul>
 <p><b> Travel for remote team members: </b>At Balsam Brands, we believe that time spent together, in-person, collaborating and building relationships is important to who we are. For our newest remote Brandits, we will arrange travel to one of our local offices within your first three months of employment so you can meet and train with your new team in-person. You may also get to travel an additional 1 &#x2013; 2 times a year for events such as team retreats, offsites, or learning and development opportunities.</p>
 <p><b> Notes: </b>This is a full-time, permanent position with benefits. Please submit a cover letter and resume, and only apply if you are able to live and work full-time in one of the states listed in this posting. State locations and specifics are subject to change as our hiring requirements shift.</p>
 <p><b> About Us: </b>Balsam Brands is a global, eCommerce retailer with roots in holiday and home d&#xe9;cor. We strive for excellence in everything we do and present a unique opportunity for those seeking to have a meaningful impact in a people-first company that values relationship building, authenticity, and doing the right thing. We have steadily growing teams in Boise, the Bay Area, Dublin, the Philippines - and most recently, Windsor, Canada!</p>
 <p> The company&apos;s mission is to create joy together. We empower our team and partners to love what they do, provide products and experiences that inspire meaningful moments with family and friends, and give back to our families and communities in impactful ways. When you join Balsam Brands, you&apos;ll find a culture of caring people doing challenging work and building a welcoming workplace.</p>
 <ul>
  <li> Check out our flagship brand, Balsam Hill: www.balsamhill.com</li>
  <li> Balsam Brands in Forbes: https://bit.ly/balsambrandsforbes</li>
  <li> Balsam Brands on LinkedIn: http://www.linkedin.com/company/balsam-brands/</li>
  <li> Glassdoor: https://bit.ly/balsambrands-glassdoor</li>
 </ul>
 <p> At Balsam Brands, we strive to offer a competitive compensation and benefits package. For permanent, full-time team members, our current package includes:</p>
 <ul>
  <li> Competitive compensation, including a cash-based incentive plan; salary is reviewed yearly and may be adjusted as part of the normal compensation review process</li>
  <li> Comprehensive Medical, Dental, and Vision coverage, with 100% of monthly premiums covered for team members, and 85%+ employer-paid premiums for other coverage tiers that include dependents</li>
  <li> Up to &#x24;2,000 annual funding toward HSA accounts</li>
  <li> Medical, transit, dependent care FSA</li>
  <li> Infertility coverage offered on all medical plans</li>
  <li> Generous parental leave program and flexible return options</li>
  <li> Company-paid life and AD&amp;D insurance</li>
  <li> Company-paid short and long-term disability insurance</li>
  <li> 401(k) with dollar-for-dollar company match up to &#x24;4,000 per calendar year</li>
  <li> Employee Assistance Program (EAP) and other mental health and wellness perks</li>
  <li> Paid holidays, annual shutdown week, PTO, and volunteer time-off (VTO) packages</li>
  <li> Paid 5-week sabbatical leave after 10 years of employment</li>
  <li> Annual continuous learning benefit up to &#x24;1,000 per person, per fiscal year</li>
  <li> Up to &#x24;300 flexible reimbursement to support setup of new team member&apos;s work-from-home environment</li>
  <li> Generous team member merchandise discount</li>
  <li> Valuable extras: identity theft protection, subsidized parking, monthly wellness, pet insurance, accident &amp; critical illness insurance</li>
 </ul>
 <p> The base pay range for this position is: &#x24;111,000 to &#x24;162,000. Where an individual falls within that range will vary based on several factors including geographic location and may vary depending on candidate qualifications and experience, applicable skills, and other job-related factors. We benchmark our pay ranges against current external data sources and regularly review compensation for our team members. Balsam Brands is committed to providing our team members with an internally fair, externally competitive, and fiscally prudent total compensation package administered in a simple and consistent manner.</p>
 <p><i> At Balsam Brands, we strive to build a diverse, equitable, and inclusive team to fulfill our purpose to create joy together. Balsam Brands is proud to be an equal opportunity employer. We encourage people from all backgrounds, ages, abilities, and experiences to apply. We do not discriminate on the basis of race, ethnicity, religion, national origin, citizenship, marital or family status, disability, sexual orientation, gender identity or expression, pregnancy or caregiver status, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application and interview process, to perform essential job functions, and to receive other benefits and privileges of employment.</i></p>
 <p> #DICE</p>
</div> 
<br> 
<div>
 Additional Information
 <p><br> All your information will be kept confidential according to EEO guidelines.</p>
</div>",https://jobs.smartrecruiters.com/BalsamBrands/743999935137570-data-engineer-remote-option-,07b20c6371c1a9da,,Full-time,,,"Boise, ID",Data Engineer (Remote Option),12 days ago,2023-10-06T13:36:49.456Z,3.9,14.0,"$111,000 - $162,000 a year",2023-10-18T13:36:49.458Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=07b20c6371c1a9da&from=jasx&tk=1hd1g1f082gvk000&vjs=3
76,Fusemachines,"About Fusemachines
  Fusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer Maskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI. With a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time employees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.
  About the role:
  This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and maintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and Advanced Analytics).
  Salary Range: US$7000/month
  Qualification / Skill Set Requirement:
 
   3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)
   Proven experience as a Snowflake Developer, with a strong understanding of Snowflake architecture and concepts.
   Proficient in snowflake services such as snowpipe, stages, stored procedures, views, materialized views, tasks and streams.
   Strong programming skills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and manipulation.
   Robust understanding of data partitioning and other optimization techniques in Snowflake.
   Knowledge of data security measures in Snowflake, including role-based access control (RBAC) and data encryption.
   Highly skilled in one or more languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration, storage, processing and manipulation.
   Strong knowledge of SDLC tools and technologies, including project management software (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or similar) and binary repository manager (AWS CodeArtifact or similar).
   Skilled in Data Integration from different sources such as APIs, databases, flat files, event streaming.
   Good understanding of Data Modeling and Database Design Principles. Being able to design and implement efficient database schemas that meet the requirements of the data architecture to support data solutions.
   Strong experience in working with ELT and ETL tools and being able to develop custom integration solutions as needed.
   Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and Kafka, to be able to handle large volumes of data.
   Strong experience in designing and implementing Data Warehousing solutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that extract data from source systems, transform it (DBT), and load it into the data warehouse.
   Strong experience in Orchestration using Apache Airflow.
   Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS services like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etc
   Good understanding of Data Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that data is accurate, complete, and consistent.
   Good Problem-Solving skills: being able to troubleshoot data processing pipelines and identify performance bottlenecks and other issues.
 
  Responsibilities:
 
   Follow established design, constructed data architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination. Handle ELT processes, including data extraction, loading, transformation and load data from various sources into Snowflake.
   Ensure the reliability, scalability, and efficiency of data systems are maintained at all times
   Assist in the configuration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior team members.
   Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and Analysts to thoroughly understand data requirements and provide data engineering support.
   Contribute to data quality assurance efforts, such as implementing data validation checks and tests.
   Evaluate and implement cutting-edge technologies and continue learning and expanding skills in data engineering and cloud platforms.
   Develop, design, and execute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance frameworks that align with current analytics demands and industry best practices
   Document data engineering processes and data flows.
   Care about architecture, observability, testing, and building reliable infrastructure and data pipelines.
   Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and performance tuning.
   Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL queries and database operations.
   Assess best practices and design schemas that matches business needs for delivering a modern analytics solution (descriptive, diagnostic, predictive, prescriptive)
   Be an active member of our Agile team, participating in all ceremonies and continuous improvement activities.
 
  Equal Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.
  
 1ukLvPmZsT","<div>
 <p><b>About Fusemachines</b></p>
 <p> Fusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer Maskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI. With a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time employees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.</p>
 <p><b> About the role:</b></p>
 <p> This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and maintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and Advanced Analytics).</p>
 <p> Salary Range: US&#x24;7000/month</p>
 <p><b> Qualification / Skill Set Requirement:</b></p>
 <ul>
  <li> 3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)</li>
  <li> Proven experience as a Snowflake Developer, with a strong understanding of Snowflake architecture and concepts.</li>
  <li> Proficient in snowflake services such as snowpipe, stages, stored procedures, views, materialized views, tasks and streams.</li>
  <li> Strong programming skills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and manipulation.</li>
  <li> Robust understanding of data partitioning and other optimization techniques in Snowflake.</li>
  <li> Knowledge of data security measures in Snowflake, including role-based access control (RBAC) and data encryption.</li>
  <li> Highly skilled in one or more languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration, storage, processing and manipulation.</li>
  <li> Strong knowledge of SDLC tools and technologies, including project management software (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or similar) and binary repository manager (AWS CodeArtifact or similar).</li>
  <li> Skilled in Data Integration from different sources such as APIs, databases, flat files, event streaming.</li>
  <li> Good understanding of Data Modeling and Database Design Principles. Being able to design and implement efficient database schemas that meet the requirements of the data architecture to support data solutions.</li>
  <li> Strong experience in working with ELT and ETL tools and being able to develop custom integration solutions as needed.</li>
  <li> Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and Kafka, to be able to handle large volumes of data.</li>
  <li> Strong experience in designing and implementing Data Warehousing solutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that extract data from source systems, transform it (DBT), and load it into the data warehouse.</li>
  <li> Strong experience in Orchestration using Apache Airflow.</li>
  <li> Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS services like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etc</li>
  <li> Good understanding of Data Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that data is accurate, complete, and consistent.</li>
  <li> Good Problem-Solving skills: being able to troubleshoot data processing pipelines and identify performance bottlenecks and other issues.</li>
 </ul>
 <p><b> Responsibilities:</b></p>
 <ul>
  <li> Follow established design, constructed data architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination. Handle ELT processes, including data extraction, loading, transformation and load data from various sources into Snowflake.</li>
  <li> Ensure the reliability, scalability, and efficiency of data systems are maintained at all times</li>
  <li> Assist in the configuration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior team members.</li>
  <li> Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and Analysts to thoroughly understand data requirements and provide data engineering support.</li>
  <li> Contribute to data quality assurance efforts, such as implementing data validation checks and tests.</li>
  <li> Evaluate and implement cutting-edge technologies and continue learning and expanding skills in data engineering and cloud platforms.</li>
  <li> Develop, design, and execute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance frameworks that align with current analytics demands and industry best practices</li>
  <li> Document data engineering processes and data flows.</li>
  <li> Care about architecture, observability, testing, and building reliable infrastructure and data pipelines.</li>
  <li> Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and performance tuning.</li>
  <li> Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL queries and database operations.</li>
  <li> Assess best practices and design schemas that matches business needs for delivering a modern analytics solution (descriptive, diagnostic, predictive, prescriptive)</li>
  <li> Be an active member of our Agile team, participating in all ceremonies and continuous improvement activities.</li>
 </ul>
 <p><i> Equal Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.</i></p>
 <p> </p>
 <p>1ukLvPmZsT</p>
</div>",https://fusemachines.applytojob.com/apply/1ukLvPmZsT/Data-Engineer?source=INDE,b7211874c261a672,,Full-time,Contract,,Remote,Data Engineer,13 days ago,2023-10-05T13:36:54.738Z,,,"$7,000 a month",2023-10-18T13:36:54.741Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=b7211874c261a672&from=jasx&tk=1hd1g1f082gvk000&vjs=3
77,Healthe systems,"Healthesystems offers workplace flexibility with our Work-From-Home model, and a competitive compensation and benefits package including healthcare coverage, PTO, paid holidays, 401(k), company-provided life insurance/disability coverage, wellness options, and more.
   
   
  Note: we are unable to hire in every state 
   
   
    
     
      Summary: Responsible for the analysis, design, documentation, development, unit testing, and support of Data Integration and database objects development for software applications. Provides support and guidance regarding Data Integration and T-SQL best practices and development standards. Promotes approved agile methodologies, leading the design and development efforts for the agile team. Actively coaches, guides, and mentors team members in providing valuable solutions to our customer. 
       Key Responsibilities: “To simplify complexities for each customer.” 
       
       
        Collaborates with stakeholders and development team members to achieve business results.
        
       
        Work closely with other engineers to integrate databases with other applications.
        
       
        Leads the design, development, and implementation of database applications and solutions for managing and integrating data between operational systems, data repositories, and reporting and analytical applications. This includes but is not limited to ETL, stored procedures, views, and functions.
        
       
        Recommends and provide guidance regarding Data Integration and database development, T-SQL best practices, and standards to the development team members as needed.
        
       
        Create and propose technical design documentation which includes current and future functionality, database objects affected, specifications, and flows/diagrams to detail the proposed database and/or Data Integration implementation.
        
       
        Has a deep understanding of the business processes and the technology platform that enables it.
        
       
        Translates stakeholder’s requirements into common language that can be adopted for the use with Behavior Driven Development (BDD) or Test Driven Development (TDD).
        
       
        Participates in industry and other professional networks to ensure awareness of industry standards, trends and best practices in order to strengthen organizational and technical knowledge.
        
       
        Provides support for investigating and troubleshooting production issues.
        
       
        Promotes the establishment of group standards and processes. Participates in the Communities of Practice.
        
       
        Works continually on improving performance of source code using industry standard methodologies.
        
       
        Helps drive technology direction and choices of technologies by making recommendations based on experience and research.
       
      
       
       
       Qualifications/Education/Certifications: 
       Bachelor's degree from four-year college or university (in Information Technology or Computer Science preferred), plus five to eight years related experience and/or training; or equivalent combination of education and experience. 
       Knowledge, Skills and Abilities: 
       Prefer experience in Healthcare, PBM and/or ABM, workers’ compensation and/or insurance industry. 
       
       Required experience: 
        
         5+ years SQL Server 2008/2014 
        
       
        5+ years Data Integration technologies and principles 
        Advanced knowledge of T-SQL including complex SQL queries (ex: using various joins and sub-queries) and best practices 
        Advanced knowledge of index design and T-SQL performance tuning techniques 
        Advanced experience integrating data from structured and unstructured formats: flat files, XML, EDI, JSON, EXCEL 
        Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases and schemas 
        Advanced knowledge of Data Warehousing methodologies and concepts 
        Experience with TDD / BDD 
       
       The following knowledge is not required, but is preferred: 
       
        Experience with BI Tools is a plus 
        Basic understanding of object oriented programming 
        Experience in distributed architectures such as Microservices, SOA, and RESTful APIs 
        Continuous Integration 
        Cucumber, Gherkin 
        Jira 
       
       Agile Competency Requirements: 
       
        Requires an understanding of the application of Agile development methodology. 
        Must be comfortable with change, close collaboration, and have conflict resolution skills. 
        Knowledge of or willingness to learn Agile / DevOps values. 
        Takes initiative and are passionate about what they do. 
        Adaption, Ability & Desire to Learn, Team Oriented - tolerance & helpful, and Quality Focus 
        
      
      Physical Demands/Working Conditions: 
       Duties are performed primarily in a home office setting utilizing computer equipment. Travel to attend meetings and visit locations throughout the country may be required. While performing the duties of this job, the employee is regularly required to sit and talk or hear. The employee is frequently required to use hands. The employee is occasionally required to stand and walk.*** Job descriptions will be reviewed and are subject to changes of business necessity. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. 
      
    
   
  
  
   
    
     Pay is based on several factors including but not limited to education, work experience, certifications, geographical cost of labor, etc. In addition to base pay, Healthesystems offers a comprehensive benefits package including, health, dental, vision, disability and life insurance, wellness resources, recognition programs, 401k contribution, and PTO & Holiday pay (all subject to eligibility requirements). Applicable statutory benefits also provided. https://healthesystems.com/careers/
    
    
      Anticipated Starting Pay Range
    
    
      $98,100—$135,000 USD
    
   
   
  
   To facilitate working from home, and as a requirement for this role, candidates must provide their own reliable, high speed internet access with sufficient bandwidth to execute all job functions. Company laptop will be provided.","<p></p>
<div>
 <div>
  <div>
   <div>
    <b>Healthesystems offers workplace flexibility with our Work-From-Home model, and a competitive compensation and benefits package including healthcare coverage, PTO, paid holidays, 401(k), company-provided life insurance/disability coverage, wellness options, and more.</b>
   </div>
  </div> 
  <p><i>Note: we are unable to hire in every state</i></p> 
  <div> 
   <div>
    <div>
     <div>
      <p><b>Summary:</b> Responsible for the analysis, design, documentation, development, unit testing, and support of Data Integration and database objects development for software applications. Provides support and guidance regarding Data Integration and T-SQL best practices and development standards. Promotes approved agile methodologies, leading the design and development efforts for the agile team. Actively coaches, guides, and mentors team members in providing valuable solutions to our customer.</p> 
      <p><b> </b><b>Key Responsibilities: &#x201c;To simplify complexities for each customer.&#x201d;</b></p> 
      <div> 
       <ul>
        <li>Collaborates with stakeholders and development team members to achieve business results.</li>
       </ul> 
       <ul>
        <li>Work closely with other engineers to integrate databases with other applications.</li>
       </ul> 
       <ul>
        <li>Leads the design, development, and implementation of database applications and solutions for managing and integrating data between operational systems, data repositories, and reporting and analytical applications. This includes but is not limited to ETL, stored procedures, views, and functions.</li>
       </ul> 
       <ul>
        <li>Recommends and provide guidance regarding Data Integration and database development, T-SQL best practices, and standards to the development team members as needed.</li>
       </ul> 
       <ul>
        <li>Create and propose technical design documentation which includes current and future functionality, database objects affected, specifications, and flows/diagrams to detail the proposed database and/or Data Integration implementation.</li>
       </ul> 
       <ul>
        <li>Has a deep understanding of the business processes and the technology platform that enables it.</li>
       </ul> 
       <ul>
        <li>Translates stakeholder&#x2019;s requirements into common language that can be adopted for the use with Behavior Driven Development (BDD) or Test Driven Development (TDD).</li>
       </ul> 
       <ul>
        <li>Participates in industry and other professional networks to ensure awareness of industry standards, trends and best practices in order to strengthen organizational and technical knowledge.</li>
       </ul> 
       <ul>
        <li>Provides support for investigating and troubleshooting production issues.</li>
       </ul> 
       <ul>
        <li>Promotes the establishment of group standards and processes. Participates in the Communities of Practice.</li>
       </ul> 
       <ul>
        <li>Works continually on improving performance of source code using industry standard methodologies.</li>
       </ul> 
       <ul>
        <li>Helps drive technology direction and choices of technologies by making recommendations based on experience and research.</li>
       </ul>
      </div>
      <br> 
      <p></p> 
      <p><b> Qualifications/Education/Certifications:</b></p> 
      <p> Bachelor&apos;s degree from four-year college or university (in Information Technology or Computer Science preferred), plus five to eight years related experience and/or training; or equivalent combination of education and experience.</p> 
      <p><b> Knowledge, Skills and Abilities:</b></p> 
      <p> Prefer experience in Healthcare, PBM and/or ABM, workers&#x2019; compensation and/or insurance industry.</p> 
      <ul> 
       <li>Required experience: 
        <ul>
         <li>5+ years SQL Server 2008/2014</li> 
        </ul></li>
       <ul>
        <li>5+ years Data Integration technologies and principles</li> 
        <li>Advanced knowledge of T-SQL including complex SQL queries (ex: using various joins and sub-queries) and best practices</li> 
        <li>Advanced knowledge of index design and T-SQL performance tuning techniques</li> 
        <li>Advanced experience integrating data from structured and unstructured formats: flat files, XML, EDI, JSON, EXCEL</li> 
        <li>Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases and schemas</li> 
        <li>Advanced knowledge of Data Warehousing methodologies and concepts</li> 
        <li>Experience with TDD / BDD</li> 
       </ul>
       <li>The following knowledge is not required, but is preferred:</li> 
       <ul>
        <li>Experience with BI Tools is a plus</li> 
        <li>Basic understanding of object oriented programming</li> 
        <li>Experience in distributed architectures such as Microservices, SOA, and RESTful APIs</li> 
        <li>Continuous Integration</li> 
        <li>Cucumber, Gherkin</li> 
        <li>Jira</li> 
       </ul>
       <li>Agile Competency Requirements:</li> 
       <ul>
        <li>Requires an understanding of the application of Agile development methodology.</li> 
        <li>Must be comfortable with change, close collaboration, and have conflict resolution skills.</li> 
        <li>Knowledge of or willingness to learn Agile / DevOps values.</li> 
        <li>Takes initiative and are passionate about what they do.</li> 
        <li>Adaption, Ability &amp; Desire to Learn, Team Oriented - tolerance &amp; helpful, and Quality Focus</li> 
       </ul> 
      </ul>
      <p><b>Physical Demands/Working Conditions:</b></p> 
      <p> Duties are performed primarily in a home office setting utilizing computer equipment. Travel to attend meetings and visit locations throughout the country may be required. While performing the duties of this job, the employee is regularly required to sit and talk or hear. The employee is frequently required to use hands. The employee is occasionally required to stand and walk.<i>*** Job descriptions will be reviewed and are subject to changes of business necessity. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.</i></p> 
     </div> 
    </div>
   </div>
  </div>
  <div>
   <div>
    <div>
     <p>Pay is based on several factors including but not limited to education, work experience, certifications, geographical cost of labor, etc. In addition to base pay, Healthesystems offers a comprehensive benefits package including, health, dental, vision, disability and life insurance, wellness resources, recognition programs, 401k contribution, and PTO &amp; Holiday pay (all subject to eligibility requirements). Applicable statutory benefits also provided. https://healthesystems.com/careers/</p>
    </div>
    <div>
      Anticipated Starting Pay Range
    </div>
    <div>
      &#x24;98,100&#x2014;&#x24;135,000 USD
    </div>
   </div>
  </div> 
  <div>
   <p>To facilitate working from home, and as a requirement for this role, candidates must provide their own reliable, high speed internet access with sufficient bandwidth to execute all job functions. Company laptop will be provided.</p>
  </div>
 </div>
</div>",https://healthesystems.com/careers-list/?gh_jid=5763661003,d4ed26d290fe9912,,,,,"Tampa, FL",Senior Data Integration Engineer - Remote,7 days ago,2023-10-11T13:36:50.893Z,3.7,57.0,"$98,100 - $135,000 a year",2023-10-18T13:36:50.896Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d4ed26d290fe9912&from=jasx&tk=1hd1g1f082gvk000&vjs=3
78,GTECH LLC,"Role- Data EngineerLocation- RemoteJob Description:
Undergraduate degree or equivalent work experience
8+ years’ experience in Development, design, test and implementation of complex database programs using Oracle and third-party tools. within a distributed, service-based enterprise environment
4+ years Hands-on development using Oracle PL/SQL.
Demonstrates expertise in a variety of data warehousing and business intelligence concepts, practices, and procedures.
Strong experience with oracle functions, procedures, triggers, packages & performance tuning,
Significant experience and comfortable with production support (and willing to take on slots within our 24/7 support rotation).Providing technical assistance, problem resolution and troubleshooting support issues.
Analytical approach to problem solving
At least 2+ year practical experience of developing solutions hosting within key major cloud providers such as OpenShift, Kubernetes, AWS, Google Cloud and Azure
Experience in using modern software engineering and product development tools including Agile / SAFE, Continuous Integration, Continuous Delivery, DevOps etc.
Demonstrate being an avid supporter of the Open-Source software community
Excellent time management, communication, decision making, and presentation skills
Display a strong desire to achieve and attain high levels of both internal and external customer satisfaction
Strong experience of operating in a quickly changing environment and driving technological innovation to meet business requirement
Proven track record of building relationships across cross-functional teams
Positive attitude and easy to work with
Initiative-taker. Has grit and can solve problems without management oversight.
Takes ownership for work. When something goes wrong, stance is introspective rather than blaming.
Engineering mindset(automate manual process) when it comes to ETL processes.
Accustomed to developing code in Git and using CI/CD practices.
Preferred: development experience using Java or open-source technologies, developing Restful APIs
Job Type: Full-time
Salary: $83,521.75 - $130,759.23 per year
Benefits:

 Health insurance

Experience level:

 8 years
 9 years

Schedule:

 Monday to Friday

Work Location: Remote","<p>Role- Data Engineer<br>Location- Remote<br><b>Job Description:</b></p>
<p>Undergraduate degree or equivalent work experience</p>
<p>8+ years&#x2019; experience in Development, design, test and implementation of complex database programs using Oracle and third-party tools. within a distributed, service-based enterprise environment</p>
<p>4+ years Hands-on development using Oracle PL/SQL.</p>
<p>Demonstrates expertise in a variety of data warehousing and business intelligence concepts, practices, and procedures.</p>
<p>Strong experience with oracle functions, procedures, triggers, packages &amp; performance tuning,</p>
<p>Significant experience and comfortable with production support (and willing to take on slots within our 24/7 support rotation).Providing technical assistance, problem resolution and troubleshooting support issues.</p>
<p>Analytical approach to problem solving</p>
<p>At least 2+ year practical experience of developing solutions hosting within key major cloud providers such as OpenShift, Kubernetes, AWS, Google Cloud and Azure</p>
<p>Experience in using modern software engineering and product development tools including Agile / SAFE, Continuous Integration, Continuous Delivery, DevOps etc.</p>
<p>Demonstrate being an avid supporter of the Open-Source software community</p>
<p>Excellent time management, communication, decision making, and presentation skills</p>
<p>Display a strong desire to achieve and attain high levels of both internal and external customer satisfaction</p>
<p>Strong experience of operating in a quickly changing environment and driving technological innovation to meet business requirement</p>
<p>Proven track record of building relationships across cross-functional teams</p>
<p>Positive attitude and easy to work with</p>
<p>Initiative-taker. Has grit and can solve problems without management oversight.</p>
<p>Takes ownership for work. When something goes wrong, stance is introspective rather than blaming.</p>
<p>Engineering mindset(automate manual process) when it comes to ETL processes.</p>
<p>Accustomed to developing code in Git and using CI/CD practices.</p>
<p>Preferred: development experience using Java or open-source technologies, developing Restful APIs</p>
<p>Job Type: Full-time</p>
<p>Salary: &#x24;83,521.75 - &#x24;130,759.23 per year</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,6ca598d4286ea230,,Full-time,,,Remote,Sr. Data Engineer,7 days ago,2023-10-11T13:37:14.443Z,,,"$83,522 - $130,759 a year",2023-10-18T13:37:14.444Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=6ca598d4286ea230&from=jasx&tk=1hd1g273r2gvn004&vjs=3
79,Talent Group,"Must Have: Azure Data Engineering, RDBMS, Airflow, ADF, API, ETL Knowledge, Python, Communication, L1 Support
 Should have 8+ Years of Experience
 Provide L1 support - job monitoring, re-run failed jobs, analyze reasons for failures, bug fixes.
 Strong Expertise in Azure Cloud.

Job Types: Permanent, Full-time
Salary: Up to $110,000.00 per year
Benefits:

 401(k)

Experience level:

 8 years

Schedule:

 8 hour shift

Work Location: Remote","<ul>
 <li><b>Must Have:</b> Azure Data Engineering, RDBMS, Airflow, ADF, API, ETL Knowledge, Python, Communication, L1 Support</li>
 <li>Should have 8+ Years of Experience</li>
 <li>Provide L1 support - job monitoring, re-run failed jobs, analyze reasons for failures, bug fixes.</li>
 <li>Strong Expertise in Azure Cloud.</li>
</ul>
<p>Job Types: Permanent, Full-time</p>
<p>Salary: Up to &#x24;110,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,e7aca82a03dbb9b5,,Full-time,Permanent,,Remote,Azure Data Engineer - Fulltime,8 days ago,2023-10-10T13:37:13.561Z,4.2,20.0,"Up to $110,000 a year",2023-10-18T13:37:13.563Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e7aca82a03dbb9b5&from=jasx&tk=1hd1g273r2gvn004&vjs=3
80,Novul Solutions,"Location: Ashburn, VA
Work Schedule: Fully remote with occasional onsite meetings
Clearance: DoD Top Secret or CBP Background Investigation
As a Data Engineer, you will play a crucial role in managing and optimizing data. Your responsibilities will involve handling database systems, data warehousing, ETL processing, machine learning support, and programming to ensure efficient data operations.
Responsibilities:

 Utilize SQL and NoSQL to build and manage relational database systems, organizing data in tables with rows and columns.
 Maintain data warehouses that store extensive historical and current data, sourced from various systems like CRM, accounting, and ERP.
 Extract, transform, and load data (ETL) from different sources into data warehouses, ensuring data is in a suitable format for analysis.
 Collaborate with data scientists to implement machine learning algorithms/models for making predictions based on historical data.
 Work with data APIs, enabling software applications to access and retrieve data efficiently.
 Use programming languages such as Python, Java, and Scala to develop data-related solutions.
 While the primary focus is data processing and optimization, having a basic understanding of algorithms and data structures is beneficial for aligning data tasks with overall business goals.

Qualifications:

 Bachelor's degree in a relevant field or equivalent work experience.
 Proven experience in managing database systems (SQL and NoSQL).
 Familiarity with data warehousing solutions and ETL tools.
 Proficiency in Python and at least one other programming language (Java or Scala).
 Experience with data APIs and integrating data into software applications.
 Basic knowledge of machine learning concepts is a plus.
 Strong analytical and problem-solving skills.
 Excellent communication and teamwork abilities.

Benefits
​
Core Benefits:

 Paid Time Off (PTO): TEN (10) Paid days off & FIVE (5) Floating days off.
 Holidays: 11 Paid Holidays. Flex time can be utilized in lieu of holiday time usage.
 Payroll: Paid Bi-Monthly.
 401(k): Partnered with the SECOND LARGEST Retirement plan provider in the U.S. Guaranteed 3% match. Eligibility – 21 years of age or older, after 3 months of employment
 Individual or company-wide performance and recognition awards (Quarterly)

Health Benefits:

 UNITED HEALTHCARE PPO, extensive national coverage.
 INCLUDES: Medical/Dental/Vision/HSA.
 Eligible on the first of the month, immediately after the start date.
 Submit the enrollment form within 30 days
 of your start date otherwise, you will have to wait until October for the new year enrollment.

Quality of Life Benefits:

 Training & Career Development Reimbursement of Tuition and training needed to support career development.
 $150 monthly reimbursement contribution paid monthly towards parking expenses.
 Receipts must be submitted by the close of business on the 25th of each month.
 Reimbursements will be paid on the first payroll AFTER reimbursements are submitted each month.

Special Benefits:

 Performance bonus – Project-based
 Yearly bonus – Company based

​
Job Type: Full-time
Pay: $150,000.00 - $160,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Vision insurance

Experience level:

 3 years

Schedule:

 8 hour shift

Experience:

 Python, Java, and Scala programming language: 3 years (Required)
 ETL: 3 years (Required)
 SQL and noSQL: 3 years (Required)

Security clearance:

 Top Secret (Required)

Work Location: Remote","<p>Location: Ashburn, VA</p>
<p>Work Schedule: Fully remote with occasional onsite meetings</p>
<p>Clearance: DoD Top Secret or CBP Background Investigation</p>
<p>As a Data Engineer, you will play a crucial role in managing and optimizing data. Your responsibilities will involve handling database systems, data warehousing, ETL processing, machine learning support, and programming to ensure efficient data operations.</p>
<p>Responsibilities:</p>
<ul>
 <li>Utilize SQL and NoSQL to build and manage relational database systems, organizing data in tables with rows and columns.</li>
 <li>Maintain data warehouses that store extensive historical and current data, sourced from various systems like CRM, accounting, and ERP.</li>
 <li>Extract, transform, and load data (ETL) from different sources into data warehouses, ensuring data is in a suitable format for analysis.</li>
 <li>Collaborate with data scientists to implement machine learning algorithms/models for making predictions based on historical data.</li>
 <li>Work with data APIs, enabling software applications to access and retrieve data efficiently.</li>
 <li>Use programming languages such as Python, Java, and Scala to develop data-related solutions.</li>
 <li>While the primary focus is data processing and optimization, having a basic understanding of algorithms and data structures is beneficial for aligning data tasks with overall business goals.</li>
</ul>
<p>Qualifications:</p>
<ul>
 <li>Bachelor&apos;s degree in a relevant field or equivalent work experience.</li>
 <li>Proven experience in managing database systems (SQL and NoSQL).</li>
 <li>Familiarity with data warehousing solutions and ETL tools.</li>
 <li>Proficiency in Python and at least one other programming language (Java or Scala).</li>
 <li>Experience with data APIs and integrating data into software applications.</li>
 <li>Basic knowledge of machine learning concepts is a plus.</li>
 <li>Strong analytical and problem-solving skills.</li>
 <li>Excellent communication and teamwork abilities.</li>
</ul>
<p><b>Benefits</b></p>
<p>&#x200b;</p>
<p><b>Core Benefits:</b></p>
<ul>
 <li>Paid Time Off (PTO): TEN (10) Paid days off &amp; FIVE (5) Floating days off.</li>
 <li>Holidays: 11 Paid Holidays. Flex time can be utilized in lieu of holiday time usage.</li>
 <li>Payroll: Paid Bi-Monthly.</li>
 <li>401(k): Partnered with the SECOND LARGEST Retirement plan provider in the U.S. Guaranteed 3% match. Eligibility &#x2013; 21 years of age or older, after 3 months of employment</li>
 <li>Individual or company-wide performance and recognition awards (Quarterly)</li>
</ul>
<p><b>Health Benefits:</b></p>
<ul>
 <li>UNITED HEALTHCARE PPO, extensive national coverage.</li>
 <li>INCLUDES: Medical/Dental/Vision/HSA.</li>
 <li>Eligible on the first of the month, immediately after the start date.</li>
 <li>Submit the enrollment form within 30 days</li>
 <li>of your start date otherwise, you will have to wait until October for the new year enrollment.</li>
</ul>
<p><b>Quality of Life Benefits:</b></p>
<ul>
 <li>Training &amp; Career Development Reimbursement of Tuition and training needed to support career development.</li>
 <li>&#x24;150 monthly reimbursement contribution paid monthly towards parking expenses.</li>
 <li>Receipts must be submitted by the close of business on the 25th of each month.</li>
 <li>Reimbursements will be paid on the first payroll AFTER reimbursements are submitted each month.</li>
</ul>
<p><b>Special Benefits:</b></p>
<ul>
 <li>Performance bonus &#x2013; Project-based</li>
 <li>Yearly bonus &#x2013; Company based</li>
</ul>
<p>&#x200b;</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;150,000.00 - &#x24;160,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Python, Java, and Scala programming language: 3 years (Required)</li>
 <li>ETL: 3 years (Required)</li>
 <li>SQL and noSQL: 3 years (Required)</li>
</ul>
<p>Security clearance:</p>
<ul>
 <li>Top Secret (Required)</li>
</ul>
<p>Work Location: Remote</p>",,73eb2333d67e7087,,Full-time,,,"Ashburn, VA",Data Engineer,11 days ago,2023-10-07T13:37:17.986Z,,,"$150,000 - $160,000 a year",2023-10-18T13:37:18.004Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=73eb2333d67e7087&from=jasx&tk=1hd1g27112f34000&vjs=3
81,Hope.Tech,"About the Company:
Wellplaece is a technology company building a B2B, SaaS-enabled marketplace for dental practices to centralize and consolidate dental supply purchasing with best pricing on quality dental products. We are an early-stage company that recently closed a $3 million seed round and are currently working with beta-phase customers.
Wellplaece is an equal opportunity company.
About the Role:
As the first dedicated Staff Data Engineer at Wellplaece, you will work closely with the CTO and play a critical role in building, maintaining, and optimizing our data infrastructure. This is initially an IC role, with the opportunity to to build and lead a team as the company grows.
Responsibilities:

 Design, develop, and maintain scalable, reliable, and performant web scrapers and data pipelines to ingest, process, and store large datasets.
 Maximize data quality and freshness across multiple sources, to ensure our product catalogs are always accurate and up to date.
 Optimize data architectures and database queries for performance and scalability.
 Stay updated with the latest technologies and trends in the data engineering field.

Requirements:

 Bachelor’s or Master’s degree in Computer Science or related field.
 Strong CS fundamentals, ability to reverse engineer websites and APIs.
 Strong experience with relational databases (Postgres preferred) and SQL.
 Some professional programming experience in NodeJS with TypeScript.
 Experience with deployments on cloud platforms (AWS preferred).
 Functionally Fluent in English.

Nice to Have:

 Experience working at early stage startups and building from scratch. Bonus points if able to show meaningful side projects (e.g. open source contributions, bootstrapped a product/company).
 Experience working in a remote setting and following best practices such as CI/CD, automated testing (both frontend and backend), version control (GitHub), code reviews, etc.

Job Type: Full-time
Pay: $135,000.00 - $150,000.00 per year
Benefits:

 Flexible schedule
 Paid time off

Experience level:

 5 years

Schedule:

 Monday to Friday

Work Location: Remote","<p><b>About the Company:</b></p>
<p>Wellplaece is a technology company building a B2B, SaaS-enabled marketplace for dental practices to centralize and consolidate dental supply purchasing with best pricing on quality dental products. We are an early-stage company that recently closed a &#x24;3 million seed round and are currently working with beta-phase customers.</p>
<p>Wellplaece is an equal opportunity company.</p>
<p><b>About the Role:</b></p>
<p>As the first dedicated Staff Data Engineer at Wellplaece, you will work closely with the CTO and play a critical role in building, maintaining, and optimizing our data infrastructure. This is initially an IC role, with the opportunity to to build and lead a team as the company grows.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Design, develop, and maintain scalable, reliable, and performant web scrapers and data pipelines to ingest, process, and store large datasets.</li>
 <li>Maximize data quality and freshness across multiple sources, to ensure our product catalogs are always accurate and up to date.</li>
 <li>Optimize data architectures and database queries for performance and scalability.</li>
 <li>Stay updated with the latest technologies and trends in the data engineering field.</li>
</ul>
<p><b>Requirements:</b></p>
<ul>
 <li>Bachelor&#x2019;s or Master&#x2019;s degree in Computer Science or related field.</li>
 <li>Strong CS fundamentals, ability to reverse engineer websites and APIs.</li>
 <li>Strong experience with relational databases (Postgres preferred) and SQL.</li>
 <li>Some professional programming experience in NodeJS with TypeScript.</li>
 <li>Experience with deployments on cloud platforms (AWS preferred).</li>
 <li>Functionally Fluent in English.</li>
</ul>
<p><b>Nice to Have:</b></p>
<ul>
 <li>Experience working at early stage startups and building from scratch. Bonus points if able to show meaningful side projects (e.g. open source contributions, bootstrapped a product/company).</li>
 <li>Experience working in a remote setting and following best practices such as CI/CD, automated testing (both frontend and backend), version control (GitHub), code reviews, etc.</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;135,000.00 - &#x24;150,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>Flexible schedule</li>
 <li>Paid time off</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,cebc0b8cc9a9f1a0,,Full-time,,,Remote,Senior Data Engineer,12 days ago,2023-10-06T13:37:19.529Z,,,"$135,000 - $150,000 a year",2023-10-18T13:37:19.555Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=cebc0b8cc9a9f1a0&from=jasx&tk=1hd1g26uejoov800&vjs=3
82,Clairvoyant Inc.,"Location: EST Preferred
Type: FTE
Min requirements 

 4+ years of experience in data extraction and creating data pipeline workflows on
 Bigdata (Hive, HQL/PySpark) with knowledge of Data Engineering concepts.
 Experience in analyzing large data sets from multiple data sources, perform validation of data.
 Knowledge of Hadoop eco-system components like HDFS, Spark, Hive, Sqoop.
 Experience writing codes in Python.
 Knowledge of SQL/HQL to write optimized queries.
 Hands on with GCP Cloud Services such as Big Query, Airflow DAG, Dataflow, Beam etc.
 Ability to build a migration plan in collaboration with various stakeholders.
 Analytical, problem-solving and excellent comm skills.
 Must have US Citizenship/Green Card

Job Type: Full-time
Salary: $130,000.00 - $140,000.00 per year
Benefits:

 Health insurance

Experience level:

 10 years

Schedule:

 Monday to Friday

Experience:

 Hive, HQL/PySpark: 4 years (Required)
 Python: 1 year (Required)
 Hadoop: 1 year (Required)
 GCP: 1 year (Required)
 Big Query: 1 year (Required)

Work Location: Remote","<p><b>Location: EST Preferred</b></p>
<p><b>Type: FTE</b></p>
<p><b>Min requirements </b></p>
<ul>
 <li>4+ years of experience in data extraction and creating data pipeline workflows on</li>
 <li>Bigdata (Hive, HQL/PySpark) with knowledge of Data Engineering concepts.</li>
 <li>Experience in analyzing large data sets from multiple data sources, perform validation of data.</li>
 <li>Knowledge of Hadoop eco-system components like HDFS, Spark, Hive, Sqoop.</li>
 <li>Experience writing codes in Python.</li>
 <li>Knowledge of SQL/HQL to write optimized queries.</li>
 <li>Hands on with GCP Cloud Services such as Big Query, Airflow DAG, Dataflow, Beam etc.</li>
 <li>Ability to build a migration plan in collaboration with various stakeholders.</li>
 <li>Analytical, problem-solving and excellent comm skills.</li>
 <li>Must have US Citizenship/Green Card</li>
</ul>
<p>Job Type: Full-time</p>
<p>Salary: &#x24;130,000.00 - &#x24;140,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Hive, HQL/PySpark: 4 years (Required)</li>
 <li>Python: 1 year (Required)</li>
 <li>Hadoop: 1 year (Required)</li>
 <li>GCP: 1 year (Required)</li>
 <li>Big Query: 1 year (Required)</li>
</ul>
<p>Work Location: Remote</p>",,989549b8e8d9773e,,Full-time,,,Remote,Data Engineer,8 days ago,2023-10-10T13:37:20.022Z,,,"$130,000 - $140,000 a year",2023-10-18T13:37:20.024Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=989549b8e8d9773e&from=jasx&tk=1hd1g27112f34000&vjs=3
83,"Source Select Group, LLC","PLEASE NO VENDORS OR CANDIDATES THAT REQUIRE SPONSORSHIP NOW OR IN THE FUTURE.
Sr. Azure Data Engineer
Type: Contract to Hire
Location: Florida, Georgia, Hawaii, Virginia, Texas, Colorado, North Carolina, TN
The ideal candidate will have 10+ years of hands-on experience in designing, implementing, and delivering into production, large-scale, multi-tenant, and near real-time data warehouses.
Required Skills 

 10 years of hands-on experience designing complex data models for both OLTP and OLAP systems
 10 years of hands-on experience designing and implementing large-scale data warehouses
 5 years of experience in designing, developing, and tuning Azure Data Lake including in-depth knowledge of performance tuning techniques
 5 years of experience working with multi-tenant data warehouses
 5 years of experience working with data governance
 Well-versed in Change Data Capture (CDC) solutions for structured, semi-structured, and unstructured data
 Well-versed with Azure Databricks, Databricks SQL, Unity Catalog, and Delta Lake and desire to become SME with Databricks technologies
 Well-versed in data lake storage formats
 Well-versed in techniques for handling slowly changing dimensions (SCDs)
 Familiar with data orchestration tools such as Airflow
 Broad experience in Microsoft SQL technologies including Power BI
 Experience with data integration through APIs, Web Services, and/or REST services


 Bachelor's degree in CS or related field, master’s degree preferred
 10+ years of experience with designing and developing complex data architecture solutions
 5+ years of design and development experience with Microsoft Azure data architecture and related solutions
 Working knowledge of Azure Databricks, Databricks SQL, Unity Catalog, and Delta Lake

Job Types: Contract, Full-time
Pay: $80.00 - $90.00 per hour
Expected hours: 40 per week
Benefits:

 401(k)
 Dental insurance
 Health insurance

Schedule:

 Monday to Friday

Application Question(s):

 We are unable to work with candidates that work thru a Vendor or 3rd Party. 

All ELIGIBLE candidates must work on our W2 and must be authorized to work in the USA without sponsorship now or in the future.
Do you meet this requirement?
Experience:

 data architecture: 10 years (Required)
 Azure databricks: 3 years (Required)
 SQL: 10 years (Required)
 ETL: 5 years (Required)
 Azure Data architecture: 4 years (Required)

Work Location: Remote","<p><b>PLEASE NO VENDORS OR CANDIDATES THAT REQUIRE SPONSORSHIP NOW OR IN THE FUTURE.</b></p>
<p><b>Sr. Azure Data Engineer</b></p>
<p><b>Type: Contract to Hire</b></p>
<p><b>Location: </b>Florida, Georgia, Hawaii, Virginia, Texas, Colorado, North Carolina, TN</p>
<p>The ideal candidate will have 10+ years of hands-on experience in designing, implementing, and delivering into production, large-scale, multi-tenant, and near real-time data warehouses.</p>
<p><b>Required Skills </b></p>
<ul>
 <li>10 years of hands-on experience designing complex data models for both OLTP and OLAP systems</li>
 <li>10 years of hands-on experience designing and implementing large-scale data warehouses</li>
 <li>5 years of experience in designing, developing, and tuning Azure Data Lake including in-depth knowledge of performance tuning techniques</li>
 <li>5 years of experience working with multi-tenant data warehouses</li>
 <li>5 years of experience working with data governance</li>
 <li><b>Well-versed in Change Data Capture (CDC) solutions for structured, semi-structured, and unstructured data</b></li>
 <li><b>Well-versed with Azure Databricks, Databricks SQL, Unity Catalog, and Delta Lake and desire to become SME with Databricks technologies</b></li>
 <li>Well-versed in data lake storage formats</li>
 <li>Well-versed in techniques for handling slowly changing dimensions (SCDs)</li>
 <li>Familiar with data orchestration tools such as Airflow</li>
 <li>Broad experience in Microsoft SQL technologies including Power BI</li>
 <li>Experience with data integration through APIs, Web Services, and/or REST services</li>
</ul>
<ul>
 <li>Bachelor&apos;s degree in CS or related field, master&#x2019;s degree preferred</li>
 <li>10+ years of experience with designing and developing complex data architecture solutions</li>
 <li>5+ years of design and development experience with Microsoft Azure data architecture and related solutions</li>
 <li>Working knowledge of Azure Databricks, Databricks SQL, Unity Catalog, and Delta Lake</li>
</ul>
<p>Job Types: Contract, Full-time</p>
<p>Pay: &#x24;80.00 - &#x24;90.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>We are unable to work with candidates that work thru a Vendor or 3rd Party. </li>
</ul>
<p>All ELIGIBLE candidates must work on our W2 and must be authorized to work in the USA without sponsorship now or in the future.</p>
<p>Do you meet this requirement?</p>
<p>Experience:</p>
<ul>
 <li>data architecture: 10 years (Required)</li>
 <li>Azure databricks: 3 years (Required)</li>
 <li>SQL: 10 years (Required)</li>
 <li>ETL: 5 years (Required)</li>
 <li>Azure Data architecture: 4 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,33b4ed0d98ba78e0,,Full-time,Contract,,Texas,Sr. Azure Data Engineer,7 days ago,2023-10-11T13:37:34.862Z,,,$80 - $90 an hour,2023-10-18T13:37:34.965Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dknu-XJx1lvG7TapgMlWnDguf9J9bebwcn7i5H53jr-drpQ5Li0Kh0ocmNMFc5deE_9wtv9DXwuMqE2mt0p40WQPf2PCXMnynHuk9iib6LchjbRvPjkkC4egmG7QqF7fSptQJF1ivoDgybiTo63AqUuZ1PIr0vteL-AJdzEYaN8RgV2NXCRr0c0KDg3H-vbXWLgH-TMvKCw9ljydPqnptMvZGW_ZmGI6FZvOWoQm4T6_uEpQu0Es2bHuALkGKzPTv_RQNPb0iF8Uo35n-NkCtlP-1nRHp7VuMhDK62sntBPLt_mbiJRDPed-i2RlMHhjZVqSc5lUbhkCer2QUhAnooErSx1SU31qIKR1jPoprU_SKH2xzWpBBiWqwYNEMwnhb2IvG5K_UNr4nvPAxctvBqr-wmNLZj8GNE25iim4ycJ__6_9LIUEibVaXp8lHpV1bsR4kPIqwmAkM7WLtC8GEH2_HgCheJrMNtFOD-kLj5blINUWPyKS361fRlDw4Hf5FwQNegF_3tRAN-4hl_rrGj7RLFgA5a5-qMQyuZJYzMVItfRW7HhKSJzsLIadWE-lPkVs4lw3kWne3ZjUVZLKaJvANSw3XwsetZYnm6N4D3AQPR_qDzHb8g&xkcb=SoDr-_M3JhoFdXxiNZ0cbzkdCdPP&p=8&fvj=1&vjs=3&jsa=2704&tk=1hd1g2l89joov801&from=jasx&wvign=1
84,Recruiting From Scratch,"This is for a client of Recruiting from Scratch. 
  
  
  
   Who is Recruiting from Scratch: 
  
  
   Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more. 
  
  
  
   https://www.recruitingfromscratch.com/ 
  
  
   Our Client: 
  
  
   
    
     
      
       Our client is looking for a senior data engineer to help revolutionize healthcare supply chain data. This role will be responsible for architecting, designing, and implementing both improvements to our existing data processes and new processes going forward. 
       This role is responsible for: 
       
        Designing, implementing, deploying, and operating data pipelines and platforms 
        Writing the code for our pipelines, and reviewing the code of others 
        Prototyping and iterating tools for data review and validation by our domain experts 
        Determining our data roadmap and prioritizing tasks to create the best value/cost ratio possible 
        Analyzing our current and future data sets to aid the above prioritization 
        Being a leader on our data team as the company grows 
       
       Skills Required 
       
        5+ years experience as a software engineer, of which at least 2+ years experience in data-based software engineering role 
        Ability to architect solutions on AWS and design for appropriate scales 
        Strong experience with Python and SQL 
        Ability to add tests/validations for your data pipelining code 
        DevOps experience and the ability to operationalize the code you write (creating the developer experience, CI/CD, managing infrastructure to run the code, etc.) 
        The drive to take over project timelines and deliverables 
       
       Skills that would be nice to have 
       
        Experience with natural language processing 
        Experience with web-scraping 
        Experience working with any healthcare IT Infrastructure 
        Experience with supply chain IT or management 
       
       Additional requirements 
       
        Bachelor’s degree in a technical field (sciences, engineering, computer science, etc.) 
        Authorized to work in the US 
       
       We offer 
       
        Competitive base compensation 
        Profit-sharing bonuses 
        401k Match 
        Company-provided health insurance plans 
        US based Remote role with scheduled team in-person meet-ups 
        Close collaboration with our healthcare data management team 
        A collaborative team focused on making healthcare better 
        Immediate impact on the roadmap and priorities so you can help define your own agenda 
        Opportunity to grow and take ownership of various business initiatives 
       
      
     
     
     
      
       Salary Range: $125,000-$150,000 base. Equity. Medical, Dental, Vision. 
      
     
    
   
   
   
    
     https://www.recruitingfromscratch.com/","<div>
 <div>
  <div>
   <b>This is for a client of Recruiting from Scratch. </b>
  </div>
  <div></div>
  <div>
   <b>Who is Recruiting from Scratch: </b>
  </div>
  <div>
   Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more. 
  </div>
  <div></div>
  <div>
   https://www.recruitingfromscratch.com/ 
  </div>
  <div>
   <b>Our Client: </b>
  </div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <p>Our client is looking for a senior data engineer to help revolutionize healthcare supply chain data. This role will be responsible for architecting, designing, and implementing both improvements to our existing data processes and new processes going forward. </p>
       <h3 class=""jobSectionHeader""><b>This role is responsible for: </b></h3>
       <ul>
        <li>Designing, implementing, deploying, and operating data pipelines and platforms </li>
        <li>Writing the code for our pipelines, and reviewing the code of others </li>
        <li>Prototyping and iterating tools for data review and validation by our domain experts </li>
        <li>Determining our data roadmap and prioritizing tasks to create the best value/cost ratio possible </li>
        <li>Analyzing our current and future data sets to aid the above prioritization </li>
        <li>Being a leader on our data team as the company grows </li>
       </ul>
       <h3 class=""jobSectionHeader""><b>Skills Required </b></h3>
       <ul>
        <li>5+ years experience as a software engineer, of which at least 2+ years experience in data-based software engineering role </li>
        <li>Ability to architect solutions on AWS and design for appropriate scales </li>
        <li>Strong experience with Python and SQL </li>
        <li>Ability to add tests/validations for your data pipelining code </li>
        <li>DevOps experience and the ability to operationalize the code you write (creating the developer experience, CI/CD, managing infrastructure to run the code, etc.) </li>
        <li>The drive to take over project timelines and deliverables </li>
       </ul>
       <h3 class=""jobSectionHeader""><b>Skills that would be nice to have </b></h3>
       <ul>
        <li>Experience with natural language processing </li>
        <li>Experience with web-scraping </li>
        <li>Experience working with any healthcare IT Infrastructure </li>
        <li>Experience with supply chain IT or management </li>
       </ul>
       <h3 class=""jobSectionHeader""><b>Additional requirements </b></h3>
       <ul>
        <li>Bachelor&#x2019;s degree in a technical field (sciences, engineering, computer science, etc.) </li>
        <li>Authorized to work in the US </li>
       </ul>
       <h3 class=""jobSectionHeader""><b>We offer </b></h3>
       <ul>
        <li>Competitive base compensation </li>
        <li>Profit-sharing bonuses </li>
        <li>401k Match </li>
        <li>Company-provided health insurance plans </li>
        <li>US based Remote role with scheduled team in-person meet-ups </li>
        <li>Close collaboration with our healthcare data management team </li>
        <li>A collaborative team focused on making healthcare better </li>
        <li>Immediate impact on the roadmap and priorities so you can help define your own agenda </li>
        <li>Opportunity to grow and take ownership of various business initiatives </li>
       </ul>
      </div>
     </div>
     <div></div>
     <div>
      <div>
       <b>Salary Range: &#x24;125,000-&#x24;150,000 base. </b>Equity. Medical, Dental, Vision. 
      </div>
     </div>
    </div>
   </div>
   <div></div>
   <div>
    <div>
     https://www.recruitingfromscratch.com/
    </div>
   </div>
  </div>
 </div>
</div>",https://recruiterflow.com/recruitingfromscratch/jobs/1996?source=indeed&utm_channel=recruiterflow-posting&location=1,f8bf5f4c80df4dc1,,Full-time,,,Remote,Senior Data Engineer,12 days ago,2023-10-06T13:37:31.818Z,,,"$130,000 - $170,000 a year",2023-10-18T13:37:31.848Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=f8bf5f4c80df4dc1&from=jasx&tk=1hd1g2n5fjm7b800&vjs=3
85,YCHARTS INC,"Description: 
  YCharts enables its customers to make smarter investment decisions. Our cloud-based financial data and investment research platform provides investment advisors, wealth managers, and institutional and retail investors with comprehensive data, powerful visualization tools, and advanced analytics. YCharts has transitioned from a fintech startup to a thriving growth company (7x Inc. 5000 Fastest Growing Company). YCharts proudly services industry-leading names such as WealthManagement.com, Seeking Alpha, TD Ameritrade, Fidelity, Charles Schwab, The Wall Street Journal, Morgan Stanley, and Wells Fargo.
  The Position:
 
  As a software engineer on the Data team, you can expect to work with technologies that ensure our data pipeline efficiently maintains and calculates data for the 200,000+ securities in our universe. Engineers on the Data team typically work on features that ensure financial data is consistent, reliable, timely and accurate by importing data from third party data providers, performing custom calculations on this imported data, and efficiently storing this data.
  Being a Senior Engineer you will have the autonomy to design and prioritize projects independently, demonstrating your expertise. Writing efficient, well-designed, and thoroughly tested code will be a key responsibility, while also actively participating in code reviews to ensure code quality across the team.
  As an engineering organization, we put a lot of focus on collaboration, building new products, and refactoring our codebase which has allowed us to scale and grow with as little pain as possible. We look for engineers who have an attention to detail and have a strong desire to do things the right way even if it requires taking a step back rather than the first thing that comes to mind.
  You will be working alongside other high-performing engineers and there is never a shortage of challenging and interesting projects that will keep you busy and keep you constantly learning new things both from a technical and domain perspective.
  Job Responsibilities:
  
 
 
   Write code in Python and the Django framework to implement complex backend features and ensure that your code is well-tested
   Use tools like Airflow to build / enhance financial data pipelines for over 200,000 securities
   Make use of pandas and asynchronous task tools like celery to efficiently calculate 4,000+ unique data points for each security
   Design robust data models that account for the structure and type of data to store
   Design, plan, estimate, and ticket features that are scoped by our product team
   Work directly with our product team to clarify feature requests and negotiate solutions
   Take ownership of projects and be responsible for the entire lifecycle of your code: Development, test, production, and subsequent fixes and improvements
   Perform code reviews for other engineers on the team
   Document the work you have done both in repo as well as outside of the repo for future engineers
   Collaborate with other engineers both verbally and in writing to plan, design, and build a world-class financial research platform
  Requirements: 
  About You:
  
 
 
   5+ years of relevant industry or academic experience
   Experience developing in Python (knowledge of the Django framework a plus)
   Experience designing, building and maintaining application features on the backend
   Experience participating in multi-month projects from conception to maintenance with multiple team members
   Desire to take ownership and responsibility when a problem or opportunity arises
   Experience working with non-technical teams (product, business, etc) where explaining technical concepts is needed
   Self-organized and able to work independently or within a team
 
  Compensation:
 
   Base salary range: $150,000 - $180,000 annually
   This position is also eligible for an annual bonus based on individual and team goals and company performance.
 
  Why YCharts?:
 
   Opportunity to work in a fast-growing fintech company that is shaping the future of investment research and data analytics.
   Collaborative and inclusive work environment that encourages creativity and innovation.
 
  Awards and Accolades:
 
   7x Inc. 5000 “Fastest Growing Companies”
   American Banker's ""Best Fintechs to Work For""
   Built in Chicago’s “Best Places to Work” and “Best Small Company to Work For”
   Inc.’s “Best Places to Work”
   Inc.’s “Top Regionals: Midwest”
   Crain's ""Best Places to Work in Chicago""
   InvestmentNews' ""Biggest Fintech Innovations""
   Technology Tools for Today & Inside Information’s “Top Tool Advisors Are Thinking About Adding”
   Kitces Report’s “Investment Data” market leader, #1 most-adopted in the last year, #2 in market share
   Business Intelligent Group’s “Best Places to Work”
   Hired’s “Top Employers Winning Tech Talent”
 
  Perks & Rec:
 
   Chicago (River North) & NY (Chelsea) offices with flexible remote options
   100% Employer-covered medical, dental & vision insurance
   Flexible Spending Accounts (Healthcare and Dependent Care)
   401(k) match
   Paid parental leave
   Discounted Pet Insurance
   Great Work/Life Balance: Generous PTO including Vacation Days, Paid Holidays, Sick Days, Professional Education Days, and “Celebration Days”
   DEI commitment
   Continued education via “Starbucks and Study”
   Opportunity to join committees (Educational committee, DEI committee, Social Committee, Women at YCharts, and more!)
   Summer hours— we head out early during the warm months!
 
  In-Office Perks:
 
   Weekly Grubhub credits for in-office lunches
   Rotating selection of high-quality coffees
   Craft beer, kombucha, and cold brew on tap
   Snacks and drinks to get you through the day
   Opportunity to join team leagues like kickball
   Fun company outings including an annual celebration in Chicago, Whirlyball, community service, baseball games and happy hours!
 
 
  YCharts provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
  At YCharts, we feel strongly that respect and inclusion are essential ingredients for creativity, innovation, and success. While we know there’s more work to be done to advance diversity and inclusion, we’re proud of our success thus far and excited for the journey to come.","<div>
 Description: 
 <p> YCharts enables its customers to make smarter investment decisions. Our cloud-based financial data and investment research platform provides investment advisors, wealth managers, and institutional and retail investors with comprehensive data, powerful visualization tools, and advanced analytics. YCharts has transitioned from a fintech startup to a thriving growth company (7x Inc. 5000 Fastest Growing Company). YCharts proudly services industry-leading names such as WealthManagement.com, Seeking Alpha, TD Ameritrade, Fidelity, Charles Schwab, The Wall Street Journal, Morgan Stanley, and Wells Fargo.</p>
 <p> The Position:</p>
 <p></p>
 <p><br> As a software engineer on the Data team, you can expect to work with technologies that ensure our data pipeline efficiently maintains and calculates data for the 200,000+ securities in our universe. Engineers on the Data team typically work on features that ensure financial data is consistent, reliable, timely and accurate by importing data from third party data providers, performing custom calculations on this imported data, and efficiently storing this data.</p>
 <p> Being a Senior Engineer you will have the autonomy to design and prioritize projects independently, demonstrating your expertise. Writing efficient, well-designed, and thoroughly tested code will be a key responsibility, while also actively participating in code reviews to ensure code quality across the team.</p>
 <p> As an engineering organization, we put a lot of focus on collaboration, building new products, and refactoring our codebase which has allowed us to scale and grow with as little pain as possible. We look for engineers who have an attention to detail and have a strong desire to do things the right way even if it requires taking a step back rather than the first thing that comes to mind.</p>
 <p> You will be working alongside other high-performing engineers and there is never a shortage of challenging and interesting projects that will keep you busy and keep you constantly learning new things both from a technical and domain perspective.</p>
 <p> Job Responsibilities:</p>
 <br> 
 <p></p>
 <ul>
  <li> Write code in Python and the Django framework to implement complex backend features and ensure that your code is well-tested</li>
  <li> Use tools like Airflow to build / enhance financial data pipelines for over 200,000 securities</li>
  <li> Make use of pandas and asynchronous task tools like celery to efficiently calculate 4,000+ unique data points for each security</li>
  <li> Design robust data models that account for the structure and type of data to store</li>
  <li> Design, plan, estimate, and ticket features that are scoped by our product team</li>
  <li> Work directly with our product team to clarify feature requests and negotiate solutions</li>
  <li> Take ownership of projects and be responsible for the entire lifecycle of your code: Development, test, production, and subsequent fixes and improvements</li>
  <li> Perform code reviews for other engineers on the team</li>
  <li> Document the work you have done both in repo as well as outside of the repo for future engineers</li>
  <li> Collaborate with other engineers both verbally and in writing to plan, design, and build a world-class financial research platform</li>
 </ul> Requirements: 
 <p> About You:</p>
 <br> 
 <p></p>
 <ul>
  <li> 5+ years of relevant industry or academic experience</li>
  <li> Experience developing in Python (knowledge of the Django framework a plus)</li>
  <li> Experience designing, building and maintaining application features on the backend</li>
  <li> Experience participating in multi-month projects from conception to maintenance with multiple team members</li>
  <li> Desire to take ownership and responsibility when a problem or opportunity arises</li>
  <li> Experience working with non-technical teams (product, business, etc) where explaining technical concepts is needed</li>
  <li> Self-organized and able to work independently or within a team</li>
 </ul>
 <p> Compensation:</p>
 <ul>
  <li> Base salary range: &#x24;150,000 - &#x24;180,000 annually</li>
  <li> This position is also eligible for an annual bonus based on individual and team goals and company performance.</li>
 </ul>
 <p> Why YCharts?:</p>
 <ul>
  <li> Opportunity to work in a fast-growing fintech company that is shaping the future of investment research and data analytics.</li>
  <li> Collaborative and inclusive work environment that encourages creativity and innovation.</li>
 </ul>
 <p> Awards and Accolades:</p>
 <ul>
  <li> 7x Inc. 5000 &#x201c;Fastest Growing Companies&#x201d;</li>
  <li> American Banker&apos;s &quot;Best Fintechs to Work For&quot;</li>
  <li> Built in Chicago&#x2019;s &#x201c;Best Places to Work&#x201d; and &#x201c;Best Small Company to Work For&#x201d;</li>
  <li> Inc.&#x2019;s &#x201c;Best Places to Work&#x201d;</li>
  <li> Inc.&#x2019;s &#x201c;Top Regionals: Midwest&#x201d;</li>
  <li> Crain&apos;s &quot;Best Places to Work in Chicago&quot;</li>
  <li> InvestmentNews&apos; &quot;Biggest Fintech Innovations&quot;</li>
  <li> Technology Tools for Today &amp; Inside Information&#x2019;s &#x201c;Top Tool Advisors Are Thinking About Adding&#x201d;</li>
  <li> Kitces Report&#x2019;s &#x201c;Investment Data&#x201d; market leader, #1 most-adopted in the last year, #2 in market share</li>
  <li> Business Intelligent Group&#x2019;s &#x201c;Best Places to Work&#x201d;</li>
  <li> Hired&#x2019;s &#x201c;Top Employers Winning Tech Talent&#x201d;</li>
 </ul>
 <p> Perks &amp; Rec:</p>
 <ul>
  <li> Chicago (River North) &amp; NY (Chelsea) offices with flexible remote options</li>
  <li> 100% Employer-covered medical, dental &amp; vision insurance</li>
  <li> Flexible Spending Accounts (Healthcare and Dependent Care)</li>
  <li> 401(k) match</li>
  <li> Paid parental leave</li>
  <li> Discounted Pet Insurance</li>
  <li> Great Work/Life Balance: Generous PTO including Vacation Days, Paid Holidays, Sick Days, Professional Education Days, and &#x201c;Celebration Days&#x201d;</li>
  <li> DEI commitment</li>
  <li> Continued education via &#x201c;Starbucks and Study&#x201d;</li>
  <li> Opportunity to join committees (Educational committee, DEI committee, Social Committee, Women at YCharts, and more!)</li>
  <li> Summer hours&#x2014; we head out early during the warm months!</li>
 </ul>
 <p> In-Office Perks:</p>
 <ul>
  <li> Weekly Grubhub credits for in-office lunches</li>
  <li> Rotating selection of high-quality coffees</li>
  <li> Craft beer, kombucha, and cold brew on tap</li>
  <li> Snacks and drinks to get you through the day</li>
  <li> Opportunity to join team leagues like kickball</li>
  <li> Fun company outings including an annual celebration in Chicago, Whirlyball, community service, baseball games and happy hours!</li>
 </ul>
 <p></p>
 <p><br> YCharts provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.</p>
 <p> At YCharts, we feel strongly that respect and inclusion are essential ingredients for creativity, innovation, and success. While we know there&#x2019;s more work to be done to advance diversity and inclusion, we&#x2019;re proud of our success thus far and excited for the journey to come.</p>
</div>",https://recruiting.paylocity.com/recruiting/jobs/Details/2001320/YCHARTS-INC/Senior-Software-Engineer--Data-Team?source=Indeed_Feed,fd53daa023410125,,Full-time,,,Remote,Senior Software Engineer- Data Team,8 days ago,2023-10-10T13:37:36.986Z,4.0,3.0,"$150,000 - $180,000 a year",2023-10-18T13:37:36.989Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=fd53daa023410125&from=jasx&tk=1hd1g2n5fjm7b800&vjs=3
86,Healthesystems,"Healthesystems offers workplace flexibility with our Work-From-Home model, and a competitive compensation and benefits package including healthcare coverage, PTO, paid holidays, 401(k), company-provided life insurance/disability coverage, wellness options, and more.
  
 
  Note: we are unable to hire in every state 
  
  
   
    
     Summary: Responsible for the analysis, design, documentation, development, unit testing, and support of Data Integration and database objects development for software applications. Provides support and guidance regarding Data Integration and T-SQL best practices and development standards. Promotes approved agile methodologies, leading the design and development efforts for the agile team. Actively coaches, guides, and mentors team members in providing valuable solutions to our customer. 
      Key Responsibilities: ""To simplify complexities for each customer."" 
      
      
       Collaborates with stakeholders and development team members to achieve business results.
       
      
       Work closely with other engineers to integrate databases with other applications.
       
      
       Leads the design, development, and implementation of database applications and solutions for managing and integrating data between operational systems, data repositories, and reporting and analytical applications. This includes but is not limited to ETL, stored procedures, views, and functions.
       
      
       Recommends and provide guidance regarding Data Integration and database development, T-SQL best practices, and standards to the development team members as needed.
       
      
       Create and propose technical design documentation which includes current and future functionality, database objects affected, specifications, and flows/diagrams to detail the proposed database and/or Data Integration implementation.
       
      
       Has a deep understanding of the business processes and the technology platform that enables it.
       
      
       Translates stakeholder's requirements into common language that can be adopted for the use with Behavior Driven Development (BDD) or Test Driven Development (TDD).
       
      
       Participates in industry and other professional networks to ensure awareness of industry standards, trends and best practices in order to strengthen organizational and technical knowledge.
       
      
       Provides support for investigating and troubleshooting production issues.
       
      
       Promotes the establishment of group standards and processes. Participates in the Communities of Practice.
       
      
       Works continually on improving performance of source code using industry standard methodologies.
       
      
       Helps drive technology direction and choices of technologies by making recommendations based on experience and research.
      
     
      
      
      Qualifications/Education/Certifications: 
      Bachelor's degree from four-year college or university (in Information Technology or Computer Science preferred), plus five to eight years related experience and/or training; or equivalent combination of education and experience. 
      Knowledge, Skills and Abilities: 
      Prefer experience in Healthcare, PBM and/or ABM, workers' compensation and/or insurance industry. 
      
      Required experience: 
       
        5+ years SQL Server 2008/2014 
       
      
       5+ years Data Integration technologies and principles 
       Advanced knowledge of T-SQL including complex SQL queries (ex: using various joins and sub-queries) and best practices 
       Advanced knowledge of index design and T-SQL performance tuning techniques 
       Advanced experience integrating data from structured and unstructured formats: flat files, XML, EDI, JSON, EXCEL 
       Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases and schemas 
       Advanced knowledge of Data Warehousing methodologies and concepts 
       Experience with TDD / BDD 
      
      The following knowledge is not required, but is preferred: 
      
       Experience with BI Tools is a plus 
       Basic understanding of object oriented programming 
       Experience in distributed architectures such as Microservices, SOA, and RESTful APIs 
       Continuous Integration 
       Cucumber, Gherkin 
       Jira 
      
      Agile Competency Requirements: 
      
       Requires an understanding of the application of Agile development methodology. 
       Must be comfortable with change, close collaboration, and have conflict resolution skills. 
       Knowledge of or willingness to learn Agile / DevOps values. 
       Takes initiative and are passionate about what they do. 
       Adaption, Ability & Desire to Learn, Team Oriented - tolerance & helpful, and Quality Focus 
       
     
     Physical Demands/Working Conditions: 
      Duties are performed primarily in a home office setting utilizing computer equipment. Travel to attend meetings and visit locations throughout the country may be required. While performing the duties of this job, the employee is regularly required to sit and talk or hear. The employee is frequently required to use hands. The employee is occasionally required to stand and walk.*** Job descriptions will be reviewed and are subject to changes of business necessity. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. 
     
   
  
 
 
  
   Pay is based on several factors including but not limited to education, work experience, certifications, geographical cost of labor, etc. In addition to base pay, Healthesystems offers a comprehensive benefits package including, health, dental, vision, disability and life insurance, wellness resources, recognition programs, 401k contribution, and PTO & Holiday pay (all subject to eligibility requirements). Applicable statutory benefits also provided. https://healthesystems.com/careers/
  
   Anticipated Starting Pay Range
  
    $98,100—$135,000 USD
  
 
 
   To facilitate working from home, and as a requirement for this role, candidates must provide their own reliable, high speed internet access with sufficient bandwidth to execute all job functions. Company laptop will be provided.","<p></p>
<div>
 <div>
  <div>
   <b>Healthesystems offers workplace flexibility with our Work-From-Home model, and a competitive compensation and benefits package including healthcare coverage, PTO, paid holidays, 401(k), company-provided life insurance/disability coverage, wellness options, and more.</b>
  </div>
 </div>
 <p><i> Note: we are unable to hire in every state</i></p> 
 <div> 
  <div>
   <div>
    <div>
     <p><b>Summary:</b> Responsible for the analysis, design, documentation, development, unit testing, and support of Data Integration and database objects development for software applications. Provides support and guidance regarding Data Integration and T-SQL best practices and development standards. Promotes approved agile methodologies, leading the design and development efforts for the agile team. Actively coaches, guides, and mentors team members in providing valuable solutions to our customer.</p> 
     <p><b> </b><b>Key Responsibilities: &quot;To simplify complexities for each customer.&quot;</b></p> 
     <div> 
      <ul>
       <li>Collaborates with stakeholders and development team members to achieve business results.</li>
      </ul> 
      <ul>
       <li>Work closely with other engineers to integrate databases with other applications.</li>
      </ul> 
      <ul>
       <li>Leads the design, development, and implementation of database applications and solutions for managing and integrating data between operational systems, data repositories, and reporting and analytical applications. This includes but is not limited to ETL, stored procedures, views, and functions.</li>
      </ul> 
      <ul>
       <li>Recommends and provide guidance regarding Data Integration and database development, T-SQL best practices, and standards to the development team members as needed.</li>
      </ul> 
      <ul>
       <li>Create and propose technical design documentation which includes current and future functionality, database objects affected, specifications, and flows/diagrams to detail the proposed database and/or Data Integration implementation.</li>
      </ul> 
      <ul>
       <li>Has a deep understanding of the business processes and the technology platform that enables it.</li>
      </ul> 
      <ul>
       <li>Translates stakeholder&apos;s requirements into common language that can be adopted for the use with Behavior Driven Development (BDD) or Test Driven Development (TDD).</li>
      </ul> 
      <ul>
       <li>Participates in industry and other professional networks to ensure awareness of industry standards, trends and best practices in order to strengthen organizational and technical knowledge.</li>
      </ul> 
      <ul>
       <li>Provides support for investigating and troubleshooting production issues.</li>
      </ul> 
      <ul>
       <li>Promotes the establishment of group standards and processes. Participates in the Communities of Practice.</li>
      </ul> 
      <ul>
       <li>Works continually on improving performance of source code using industry standard methodologies.</li>
      </ul> 
      <ul>
       <li>Helps drive technology direction and choices of technologies by making recommendations based on experience and research.</li>
      </ul>
     </div>
     <br> 
     <p></p> 
     <p><b> Qualifications/Education/Certifications:</b></p> 
     <p> Bachelor&apos;s degree from four-year college or university (in Information Technology or Computer Science preferred), plus five to eight years related experience and/or training; or equivalent combination of education and experience.</p> 
     <p><b> Knowledge, Skills and Abilities:</b></p> 
     <p> Prefer experience in Healthcare, PBM and/or ABM, workers&apos; compensation and/or insurance industry.</p> 
     <ul> 
      <li>Required experience: 
       <ul>
        <li>5+ years SQL Server 2008/2014</li> 
       </ul></li>
      <ul>
       <li>5+ years Data Integration technologies and principles</li> 
       <li>Advanced knowledge of T-SQL including complex SQL queries (ex: using various joins and sub-queries) and best practices</li> 
       <li>Advanced knowledge of index design and T-SQL performance tuning techniques</li> 
       <li>Advanced experience integrating data from structured and unstructured formats: flat files, XML, EDI, JSON, EXCEL</li> 
       <li>Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases and schemas</li> 
       <li>Advanced knowledge of Data Warehousing methodologies and concepts</li> 
       <li>Experience with TDD / BDD</li> 
      </ul>
      <li>The following knowledge is not required, but is preferred:</li> 
      <ul>
       <li>Experience with BI Tools is a plus</li> 
       <li>Basic understanding of object oriented programming</li> 
       <li>Experience in distributed architectures such as Microservices, SOA, and RESTful APIs</li> 
       <li>Continuous Integration</li> 
       <li>Cucumber, Gherkin</li> 
       <li>Jira</li> 
      </ul>
      <li>Agile Competency Requirements:</li> 
      <ul>
       <li>Requires an understanding of the application of Agile development methodology.</li> 
       <li>Must be comfortable with change, close collaboration, and have conflict resolution skills.</li> 
       <li>Knowledge of or willingness to learn Agile / DevOps values.</li> 
       <li>Takes initiative and are passionate about what they do.</li> 
       <li>Adaption, Ability &amp; Desire to Learn, Team Oriented - tolerance &amp; helpful, and Quality Focus</li> 
      </ul> 
     </ul>
     <p><b>Physical Demands/Working Conditions:</b></p> 
     <p> Duties are performed primarily in a home office setting utilizing computer equipment. Travel to attend meetings and visit locations throughout the country may be required. While performing the duties of this job, the employee is regularly required to sit and talk or hear. The employee is frequently required to use hands. The employee is occasionally required to stand and walk.<i>*** Job descriptions will be reviewed and are subject to changes of business necessity. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.</i></p> 
    </div> 
   </div>
  </div>
 </div>
 <div>
  <div>
   <p>Pay is based on several factors including but not limited to education, work experience, certifications, geographical cost of labor, etc. In addition to base pay, Healthesystems offers a comprehensive benefits package including, health, dental, vision, disability and life insurance, wellness resources, recognition programs, 401k contribution, and PTO &amp; Holiday pay (all subject to eligibility requirements). Applicable statutory benefits also provided. https://healthesystems.com/careers/</p>
  </div>
  <p><b> Anticipated Starting Pay Range</b></p>
  <div>
    &#x24;98,100&#x2014;&#x24;135,000 USD
  </div>
 </div>
 <div>
  <p> To facilitate working from home, and as a requirement for this role, candidates must provide their own reliable, high speed internet access with sufficient bandwidth to execute all job functions. Company laptop will be provided.</p>
 </div>
</div>",https://healthesystems.com/careers-list/?gh_jid=5763661003&gh_src=8466226f3us,2ca88fbc827420a3,,,,,Remote,Senior Data Integration Engineer - Remote,7 days ago,2023-10-11T13:37:39.602Z,3.7,57.0,"$98,100 - $135,000 a year",2023-10-18T13:37:39.606Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2ca88fbc827420a3&from=jasx&tk=1hd1g2npm2f32000&vjs=3
87,DIGITAL ENVOY,"Description: 
  Digital Envoy (DE) is the leader in geolocation and VPN data for over 23 years for customers in various verticals - Streaming Media, Ad Tech, Cybersecurity, E-commerce, and Data Analytics. Our core product IP address geolocation powers customers like Netflix, Hulu, Trade Desk, Paypal, Adobe, ebay, and many others. Our VPN and Proxy data coupled with accurate IP address geolocation data helps companies determine malicious activity for threat intelligence, authentication, and digital piracy.
  Digital Envoy is looking for a full-time Senior Data Engineer who will support the development and maintenance of sustainable data integration pipelines that enable our team to perform timely analyses. The Data Engineer will also build scalable tools and work with data warehousing systems needed to support customers.
  This person can work remotely from any of the following states: California (CA), Colorado (CO), Connecticut (CT), Florida (FL), Georgia (GA), Hawaii (HI), Louisiana (LA), Massachusetts (MA), Michigan (MI), New York (NY), New Jersey (NJ), Ohio (OH), Pennsylvania (PA), Tennessee (TN), Texas (TX), Virginia (VA), Washington (WA)
  Georgia residents are encouraged and welcomed to join us a couple days/week in our Peachtree Corners, Georgia office.
  Please note Digital Envoy is not sponsoring visas for this position.
  WHAT YOU'LL DO:
 
   Write Spark, Python, and SQL to perform ETL on billions of location records per day
   Implement ETL pipelines in AWS EMR + Airflow to support feature stores for customer exports, internal analysis and machine learning use cases.
   Write complex SQL, including geospatial, to fulfill customer requests for analysis
   Build dashboards in Tableau to surface data-driven insights
   Develop scripts in Python, Spark and Postgis to acquire and curate spatial data
 
  WHAT WE OFFER:
 
   Competitive Salary & Bonus program
   Medical, Dental and Vision
   Paid Holidays & Unlimited PTO policy
   401(k) with employer contribution match
   We value your input: make a real impact in a growing company!
 
  At Digital Envoy, we’re excited about building a diverse team and creating an inclusive environment where everyone can thrive, and we encourage all applicants of any educational background, gender identity and expression, sexual orientation, religion, ethnicity, age, citizenship, socioeconomic status, disability, and veteran status to apply. Requirements: 
  WHO YOU ARE:
 
   4+ years of data engineering or relevant industry experience
   2+ years experience and working proficiency with Spark
   Bachelor’s Degree in Computer Science or related technical areas like Math, Statistics, and/or other Engineering degrees
   Strong proficiency with Python
   Advanced proficiency with SQL, comfortable with complex joins
   Experience with AWS data engineering products (S3, RDS, EMR, Glue, Athena...) or similar tools from other cloud providers
   Familiarity with orchestration systems, preferably Airflow
   Self-initiative and an entrepreneurial mindset
   Strong communication skills
   Passion for data
 
  Nice-To-Haves:
 
   Experience with spatial data, joins and operations
   Working knowledge of Scala
   Proficiency with building Tableau dashboards
   Familiarity with Snowflake
   Experience with Machine Learning","<div>
 Description: 
 <p> Digital Envoy (DE) is the leader in geolocation and VPN data for over 23 years for customers in various verticals - Streaming Media, Ad Tech, Cybersecurity, E-commerce, and Data Analytics. Our core product IP address geolocation powers customers like Netflix, Hulu, Trade Desk, Paypal, Adobe, ebay, and many others. Our VPN and Proxy data coupled with accurate IP address geolocation data helps companies determine malicious activity for threat intelligence, authentication, and digital piracy.</p>
 <p> Digital Envoy is looking for a full-time Senior Data Engineer who will support the development and maintenance of sustainable data integration pipelines that enable our team to perform timely analyses. The Data Engineer will also build scalable tools and work with data warehousing systems needed to support customers.</p>
 <p><b> This person can work remotely from any of the following states: California (CA), Colorado (CO), Connecticut (CT), Florida (FL), Georgia (GA), Hawaii (HI), Louisiana (LA), Massachusetts (MA), Michigan (MI), New York (NY), New Jersey (NJ), Ohio (OH), Pennsylvania (PA), Tennessee (TN), Texas (TX), Virginia (VA), Washington (WA)</b></p>
 <p><b> Georgia residents are encouraged and welcomed to join us a couple days/week in our Peachtree Corners, Georgia office.</b></p>
 <p><b> Please note Digital Envoy is not sponsoring visas for this position.</b></p>
 <p><b> WHAT YOU&apos;LL DO:</b></p>
 <ul>
  <li> Write Spark, Python, and SQL to perform ETL on billions of location records per day</li>
  <li> Implement ETL pipelines in AWS EMR + Airflow to support feature stores for customer exports, internal analysis and machine learning use cases.</li>
  <li> Write complex SQL, including geospatial, to fulfill customer requests for analysis</li>
  <li> Build dashboards in Tableau to surface data-driven insights</li>
  <li> Develop scripts in Python, Spark and Postgis to acquire and curate spatial data</li>
 </ul>
 <p><b> WHAT WE OFFER:</b></p>
 <ul>
  <li> Competitive Salary &amp; Bonus program</li>
  <li> Medical, Dental and Vision</li>
  <li> Paid Holidays &amp; Unlimited PTO policy</li>
  <li> 401(k) with employer contribution match</li>
  <li> We value your input: make a real impact in a growing company!</li>
 </ul>
 <p> At Digital Envoy, we&#x2019;re excited about building a diverse team and creating an inclusive environment where everyone can thrive, and we encourage all applicants of any educational background, gender identity and expression, sexual orientation, religion, ethnicity, age, citizenship, socioeconomic status, disability, and veteran status to apply.</p> Requirements: 
 <p><b> WHO YOU ARE:</b></p>
 <ul>
  <li> 4+ years of data engineering or relevant industry experience</li>
  <li> 2+ years experience and working proficiency with Spark</li>
  <li> Bachelor&#x2019;s Degree in Computer Science or related technical areas like Math, Statistics, and/or other Engineering degrees</li>
  <li> Strong proficiency with Python</li>
  <li> Advanced proficiency with SQL, comfortable with complex joins</li>
  <li> Experience with AWS data engineering products (S3, RDS, EMR, Glue, Athena...) or similar tools from other cloud providers</li>
  <li> Familiarity with orchestration systems, preferably Airflow</li>
  <li> Self-initiative and an entrepreneurial mindset</li>
  <li> Strong communication skills</li>
  <li> Passion for data</li>
 </ul>
 <h4 class=""jobSectionHeader""><b><i> Nice-To-Haves:</i></b></h4>
 <ul>
  <li> Experience with spatial data, joins and operations</li>
  <li> Working knowledge of Scala</li>
  <li> Proficiency with building Tableau dashboards</li>
  <li> Familiarity with Snowflake</li>
  <li> Experience with Machine Learning</li>
 </ul>
</div>",https://recruiting.paylocity.com/recruiting/jobs/Details/1996356/DIGITAL-ENVOY/Senior-Data-Engineer?source=Indeed_Feed,00fe614f1245546c,,Full-time,,,Remote,Senior Data Engineer,12 days ago,2023-10-06T13:37:47.270Z,,,"$140,000 - $160,000 a year",2023-10-18T13:37:47.273Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=00fe614f1245546c&from=jasx&tk=1hd1g2npm2f32000&vjs=3
88,RVO Health,"AT A GLANCE 
   RVO Health is looking to grow our Talent Analytics team by adding a Senior Data Engineer. In this role, you'll be challenged to help shape our Talent Analytics strategy while working on high-priority data efforts – all in line with our broader mission of attracting diverse talent and giving them an experience that will bring out their very best. You will be responsible for scoping, executing, and delivering technical projects to stakeholders across the Human Capital organization, and producing data engineering & analytical solutions that connect them to the data they need. 
  
 
  What You'll Do 
  
   Develop/maintain data pipelines from various data sources (ADP WFN, Greenhouse Recruiting/Onboarding, CultureAmp, Docebo, etc) to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies. 
   Conduct hands-on, advanced data engineering & analytics using multiple data sources originating from different applications and systems. 
   Collaborate with the data science team to identify new opportunities for deep analytics within the Human Capital organization. 
   Provide input into strategies as they drive the team forward with delivery of business value and technical acumen. 
   Execute on proof of concepts, where appropriate, to help improve our technical processes. 
   Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations. 
  
  What We're Looking For 
  
   5+ years of Data Engineering experience 
   3+ years of writing SQL experience against complex databases for data extraction using AWS Athena (Presto), Databricks Delta Lake along with Data Modeling & Data warehousing experience. 
   3+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines and experience data processing using Parquet and Avro 
   3+ years of Python coding experience, familiar with utilizing packages such as pandas, boto3, requests, json, csv, os 
   3+ years of experience working on AWS services including Glue, Athena, Lambda, S3, SNS, SQS, Cloud formation, Step Functions, Serverless architecture. 
   Experience with GitHub, Code check-in, versioning, Git commands 
   Introduce and drive adoption of CI/CD framework within the team and build/deploy CI/CD Pipelines using Terraform or AWS Cloud Formation 
   Experience with visualization tools such as Tableau, Looker or PowerBI to build dynamic/scalable dashboards and reports. 
   Strong analytical and interpersonal skills 
   Knowledge or experience within Talent/People analytics is a plus 
   Enthusiastic, highly motivated and ability to learn quickly. 
   Able to work through ambiguity in a fast-paced, dynamically changing business environment. 
   Ability to manage multiple tasks at the same time with minimal supervision. 
   
  Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements. 
   
   Starting Salary: $100,000 - $170,000 
    
     Note actual salary is based on geographic location, qualifications and experience
     
   Access to a Free Udemy for Business subscription—thousands of hours of learning content on hundreds of different subjects at your fingertips 
   Health Insurance Coverage (medical, dental, and vision) 
   Life Insurance 
   Short and Long-Term Disability Insurance 
   Flexible Spending Accounts 
   Paid Time Off 
   Holiday Pay 
   401(k) with match 
   Employee Assistance Program 
   Paid Parental Bonding Benefit Program 
   
  This position may occasionally require travel for training and other work-related duties. 
   Who We Are: 
   Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health & wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group's Optum Health. Together we're focused on delivering on our vision of a stronger and healthier world. 
   RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife. 
   We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program. 
   RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person's merit and qualifications. 
   We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com. 
   #LI-REMOTE 
 
 
   RVO Health Privacy Policy: https://rvohealth.com/legal/privacy","<div>
 <div>
  <h2 class=""jobSectionHeader""><b>AT A GLANCE</b></h2> 
  <p> RVO Health is looking to grow our Talent Analytics team by adding a Senior Data Engineer. In this role, you&apos;ll be challenged to help shape our Talent Analytics strategy while working on high-priority data efforts &#x2013; all in line with our broader mission of attracting diverse talent and giving them an experience that will bring out their very best. You will be responsible for scoping, executing, and delivering technical projects to stakeholders across the Human Capital organization, and producing data engineering &amp; analytical solutions that connect them to the data they need.</p> 
 </div> 
 <div>
  <h2 class=""jobSectionHeader""><b>What You&apos;ll Do</b></h2> 
  <ul>
   <li>Develop/maintain data pipelines from various data sources (ADP WFN, Greenhouse Recruiting/Onboarding, CultureAmp, Docebo, etc) to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies.</li> 
   <li>Conduct hands-on, advanced data engineering &amp; analytics using multiple data sources originating from different applications and systems.</li> 
   <li>Collaborate with the data science team to identify new opportunities for deep analytics within the Human Capital organization.</li> 
   <li>Provide input into strategies as they drive the team forward with delivery of business value and technical acumen.</li> 
   <li>Execute on proof of concepts, where appropriate, to help improve our technical processes.</li> 
   <li>Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations.</li> 
  </ul>
  <h2 class=""jobSectionHeader""><b>What We&apos;re Looking For</b></h2> 
  <ul>
   <li>5+ years of Data Engineering experience</li> 
   <li>3+ years of writing SQL experience against complex databases for data extraction using AWS Athena (Presto), Databricks Delta Lake along with Data Modeling &amp; Data warehousing experience.</li> 
   <li>3+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines and experience data processing using Parquet and Avro</li> 
   <li>3+ years of Python coding experience, familiar with utilizing packages such as pandas, boto3, requests, json, csv, os</li> 
   <li>3+ years of experience working on AWS services including Glue, Athena, Lambda, S3, SNS, SQS, Cloud formation, Step Functions, Serverless architecture.</li> 
   <li>Experience with GitHub, Code check-in, versioning, Git commands</li> 
   <li>Introduce and drive adoption of CI/CD framework within the team and build/deploy CI/CD Pipelines using Terraform or AWS Cloud Formation</li> 
   <li>Experience with visualization tools such as Tableau, Looker or PowerBI to build dynamic/scalable dashboards and reports.</li> 
   <li>Strong analytical and interpersonal skills</li> 
   <li>Knowledge or experience within Talent/People analytics is a plus</li> 
   <li>Enthusiastic, highly motivated and ability to learn quickly.</li> 
   <li>Able to work through ambiguity in a fast-paced, dynamically changing business environment.</li> 
   <li>Ability to manage multiple tasks at the same time with minimal supervision.</li> 
  </ul> 
  <p><b>Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements.</b></p> 
  <ul> 
   <li>Starting Salary: &#x24;100,000 - &#x24;170,000<br> 
    <ul>
     <li><i>Note actual salary is based on geographic location, qualifications and experience</i></li>
    </ul></li> 
   <li>Access to a Free Udemy for Business subscription&#x2014;thousands of hours of learning content on hundreds of different subjects at your fingertips</li> 
   <li>Health Insurance Coverage (medical, dental, and vision)</li> 
   <li>Life Insurance</li> 
   <li>Short and Long-Term Disability Insurance</li> 
   <li>Flexible Spending Accounts</li> 
   <li>Paid Time Off</li> 
   <li>Holiday Pay</li> 
   <li>401(k) with match</li> 
   <li>Employee Assistance Program</li> 
   <li>Paid Parental Bonding Benefit Program</li> 
  </ul> 
  <p><b>This position may occasionally require travel for training and other work-related duties.</b></p> 
  <p><b> Who We Are:</b></p> 
  <p> Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health &amp; wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group&apos;s Optum Health. Together we&apos;re focused on delivering on our vision of a stronger and healthier world.</p> 
  <p> RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife.</p> 
  <p> We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.</p> 
  <p> RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person&apos;s merit and qualifications.</p> 
  <p> We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com.</p> 
  <p> #LI-REMOTE</p> 
 </div>
 <div>
  <p> RVO Health Privacy Policy: https://rvohealth.com/legal/privacy</p>
 </div>
</div>",https://boards.greenhouse.io/rvohealth/jobs/4326840005?gh_src=4ad89c055us,623c55b56f304bb5,,,,,"Charlotte, NC","Senior Data Engineer, Talent Analytics",8 days ago,2023-10-10T13:37:44.564Z,,,"$100,000 - $170,000 a year",2023-10-18T13:37:44.566Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=623c55b56f304bb5&from=jasx&tk=1hd1g2npm2f32000&vjs=3
89,Novanta,"Build a career powered by innovations that matter! At Novanta, our innovations power technology products that are transforming healthcare and advanced manufacturing—improving productivity, enhancing people’s lives and redefining what’s possible. We create for our global customers engineered components and sub-systems that deliver extreme precision and performance for a range of mission-critical applications—from minimally invasive surgery to robotics to 3D metal printing.
 
 
 
   Novanta is one global team with over 26 offices located in The Americas, Europe and Asia-Pacific. Looking for a great place to work? You have found it with a culture that embraces teamwork, collaboration and empowerment. Come explore Novanta.
 
 
 
   This position is part of Novanta’s Corporate and Shared Services global teams. Novanta’s Corporate and Shared Services teams play an important role in executing the company’s strategic mission and operations. Included in Corporate and Shared Services are the business functions including Finance, Accounting, Human Resources, Information Technology, Legal, Compliance, Corporate Development and Corporate Marketing. The Corporate and Shared Services teams work closely with all Novanta business units to support operating initiatives contributing to the organization’s financial success.
 
 
 
   Job Summary
 
 
 
   As a Data Analytics Engineer, you will be responsible for building and maintaining the data layer for our analytics stack, top to bottom. Your role will span multiple disciplines from data engineering to data analytics and visualization across all stages of data maturity for the purpose of delivering robust Business Intelligence solutions. You will consider software engineering best practices including version control, automated testing, documentation, code review and continuous integration, as essential to any data stack.
 
 
 
   Primary Responsibilities
 
 
   Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.
   Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)
   Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.
   Automate standard report creation and sharing using tools or scripts
   Convert raw data into consumable information applying business logic and utilizing clean engineering workflows
   Ensure that data, systems, architecture, business logic, and metrics are well-documented
   Support the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics database
   Serve as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence
   Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challenges
 
 
   Draw insights from data and clearly communicate findings to stakeholders and external customers
   Provide exceptional customer service to stakeholders through project execution and timely delivery of solutions
 
 
 
   Required Experience, Education, Skills and Competencies
 
 
   Bachelor’s degree in Information Technology preferred or relevant experience.
   3+ years - Experience with MS SQL Server and Snowflake
   3+ years - Experience with ETL/ELT Tools (ex. Mulesoft, API, Informatica)
   3+ years - Experience using Power BI, Tableau, or similar data visualization tool
   Expert SQL Fluency (Well versed in CTEs and window functions)
   Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDW
   Experienced building data warehouse infrastructure and BI tables
   Motivated individual with strong analytic, problem solving, and troubleshooting skills
 
 
 
   Travel Requirements
 
 
   Less than 20%
 
 
 
   Compensation and Benefits
 
 
   The base pay for this position ranges from $90,000 to $120,000 depending on the geographic market
   Dependent on the position offered, annual bonuses and other forms of compensation may be provided as part of the compensation package.
   Novanta supports all aspects of your life. This position provides a full range of benefits including paid parental and family leave.
 
 
 
   Novanta is proud to be an equal employment opportunity and affirmative action workplace. We consider all qualified applicants without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender identity or expression, national origin, military and veteran status, disability, genetics, or any other category protected by federal law or Novanta policy.
 
 
 
   Please call +1 781-266-5700 if you need a disability accommodation for any part of the employment process.","<div>
 <div>
  Build a career powered by innovations that matter! At Novanta, our innovations power technology products that are transforming healthcare and advanced manufacturing&#x2014;improving productivity, enhancing people&#x2019;s lives and redefining what&#x2019;s possible. We create for our global customers engineered components and sub-systems that deliver extreme precision and performance for a range of mission-critical applications&#x2014;from minimally invasive surgery to robotics to 3D metal printing.
 </div>
 <div></div>
 <div>
   Novanta is one global team with over 26 offices located in The Americas, Europe and Asia-Pacific. Looking for a great place to work? You have found it with a culture that embraces teamwork, collaboration and empowerment. Come explore Novanta.
 </div>
 <div></div>
 <div>
   This position is part of Novanta&#x2019;s Corporate and Shared Services global teams. Novanta&#x2019;s Corporate and Shared Services teams play an important role in executing the company&#x2019;s strategic mission and operations. Included in Corporate and Shared Services are the business functions including Finance, Accounting, Human Resources, Information Technology, Legal, Compliance, Corporate Development and Corporate Marketing. The Corporate and Shared Services teams work closely with all Novanta business units to support operating initiatives contributing to the organization&#x2019;s financial success.
 </div>
 <div></div>
 <div>
   Job Summary
 </div>
 <div></div>
 <div>
   As a Data Analytics Engineer, you will be responsible for building and maintaining the data layer for our analytics stack, top to bottom. Your role will span multiple disciplines from data engineering to data analytics and visualization across all stages of data maturity for the purpose of delivering robust Business Intelligence solutions. You will consider software engineering best practices including version control, automated testing, documentation, code review and continuous integration, as essential to any data stack.
 </div>
 <div></div>
 <div>
   Primary Responsibilities
 </div>
 <ul>
  <li> Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.</li>
  <li> Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)</li>
  <li> Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.</li>
  <li> Automate standard report creation and sharing using tools or scripts</li>
  <li> Convert raw data into consumable information applying business logic and utilizing clean engineering workflows</li>
  <li> Ensure that data, systems, architecture, business logic, and metrics are well-documented</li>
  <li> Support the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics database</li>
  <li> Serve as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence</li>
  <li> Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challenges</li>
 </ul>
 <ul>
  <li> Draw insights from data and clearly communicate findings to stakeholders and external customers</li>
  <li> Provide exceptional customer service to stakeholders through project execution and timely delivery of solutions</li>
 </ul>
 <div></div>
 <div>
   Required Experience, Education, Skills and Competencies
 </div>
 <ul>
  <li> Bachelor&#x2019;s degree in Information Technology preferred or relevant experience.</li>
  <li> 3+ years - Experience with MS SQL Server and Snowflake</li>
  <li> 3+ years - Experience with ETL/ELT Tools (ex. Mulesoft, API, Informatica)</li>
  <li> 3+ years - Experience using Power BI, Tableau, or similar data visualization tool</li>
  <li> Expert SQL Fluency (Well versed in CTEs and window functions)</li>
  <li> Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDW</li>
  <li> Experienced building data warehouse infrastructure and BI tables</li>
  <li> Motivated individual with strong analytic, problem solving, and troubleshooting skills</li>
 </ul>
 <div></div>
 <div>
   Travel Requirements
 </div>
 <ul>
  <li> Less than 20%</li>
 </ul>
 <div></div>
 <div>
   Compensation and Benefits
 </div>
 <ul>
  <li> The base pay for this position ranges from &#x24;90,000 to &#x24;120,000 depending on the geographic market</li>
  <li> Dependent on the position offered, annual bonuses and other forms of compensation may be provided as part of the compensation package.</li>
  <li> Novanta supports all aspects of your life. This position provides a full range of benefits including paid parental and family leave.</li>
 </ul>
 <div></div>
 <div>
   Novanta is proud to be an equal employment opportunity and affirmative action workplace. We consider all qualified applicants without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender identity or expression, national origin, military and veteran status, disability, genetics, or any other category protected by federal law or Novanta policy.
 </div>
 <div></div>
 <div>
   Please call +1 781-266-5700 if you need a disability accommodation for any part of the employment process.
 </div>
</div>",https://novanta.wd5.myworkdayjobs.com/en-US/Novanta-Careers/job/Remote---US/Data-Analytics-Engineer_R006258,7a36e7ca655affdd,,Full-time,,,Remote,Data Analytics Engineer,11 days ago,2023-10-07T13:37:48.908Z,2.9,31.0,"$90,000 - $120,000 a year",2023-10-18T13:37:48.911Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=7a36e7ca655affdd&from=jasx&tk=1hd1g2npm2f32000&vjs=3
90,Proactive Logic Consulting Inc,"Company: Proactive Logic Consulting INC
Location: Remote
Contract Type: Corp to Corp
Duration: 3 Months
About Us:
Proactive Logic Consulting INC is a boutique technology consulting firm specializing in assessments and roadmaps, Azure cloud migrations, and app modernization. We are committed to delivering excellence and empowering businesses to achieve their full potential.
Job Description:
We are seeking a highly skilled Data Factory Engineer with experience in Microsoft Dataverse and the healthcare sector. This is a remote, 3-month Corp to Corp contract position. The ideal candidate will have a strong background in data integration, ETL processes, and cloud-based solutions.
Responsibilities:

 Design, develop, and deploy data pipelines using Azure Data Factory
 Integrate Microsoft Dataverse to streamline data flow and improve operational efficiency
 Work closely with healthcare clients to understand their data requirements and provide solutions
 Ensure data quality and compliance with healthcare regulations
 Collaborate with cross-functional teams to deliver on project milestones

Requirements:

 Minimum 3 years of experience with Azure Data Factory
 Hands-on experience with Microsoft Dataverse
 Prior experience in the healthcare sector is a strong plus
 Strong understanding of ETL processes and data warehousing concepts
 Excellent communication skills

Job Type: Contract
Pay: $75.00 - $100.00 per hour
Expected hours: 40 per week
Experience:

 Azure Data Factory: 5 years (Required)
 Microsoft Dynamics 365: 3 years (Preferred)
 Power BI: 5 years (Required)
 ETL: 10 years (Required)

Work Location: Remote","<p><b>Company: Proactive Logic Consulting INC</b></p>
<p><b>Location: Remote</b></p>
<p><b>Contract Type: Corp to Corp</b></p>
<p><b>Duration: 3 Months</b></p>
<p><b>About Us:</b></p>
<p>Proactive Logic Consulting INC is a boutique technology consulting firm specializing in assessments and roadmaps, Azure cloud migrations, and app modernization. We are committed to delivering excellence and empowering businesses to achieve their full potential.</p>
<p><b>Job Description:</b></p>
<p>We are seeking a highly skilled Data Factory Engineer with experience in Microsoft Dataverse and the healthcare sector. This is a remote, 3-month Corp to Corp contract position. The ideal candidate will have a strong background in data integration, ETL processes, and cloud-based solutions.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Design, develop, and deploy data pipelines using Azure Data Factory</li>
 <li>Integrate Microsoft Dataverse to streamline data flow and improve operational efficiency</li>
 <li>Work closely with healthcare clients to understand their data requirements and provide solutions</li>
 <li>Ensure data quality and compliance with healthcare regulations</li>
 <li>Collaborate with cross-functional teams to deliver on project milestones</li>
</ul>
<p><b>Requirements:</b></p>
<ul>
 <li>Minimum 3 years of experience with Azure Data Factory</li>
 <li>Hands-on experience with Microsoft Dataverse</li>
 <li>Prior experience in the healthcare sector is a strong plus</li>
 <li>Strong understanding of ETL processes and data warehousing concepts</li>
 <li>Excellent communication skills</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;75.00 - &#x24;100.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Experience:</p>
<ul>
 <li>Azure Data Factory: 5 years (Required)</li>
 <li>Microsoft Dynamics 365: 3 years (Preferred)</li>
 <li>Power BI: 5 years (Required)</li>
 <li>ETL: 10 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,2d5efc02e5091f2d,,Contract,,,Remote,"Azure Data Factory Engineer (Remote, Contract)",12 days ago,2023-10-06T13:37:58.579Z,,,$75 - $100 an hour,2023-10-18T13:37:58.585Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2d5efc02e5091f2d&from=jasx&tk=1hd1g2n5fjm7b800&vjs=3
91,Zscaler,"About Zscaler 
   Zscaler (NASDAQ: ZS) accelerates digital transformation so that customers can be more agile, efficient, resilient, and secure. The Zscaler Zero Trust Exchange is the company's cloud-native platform that protects thousands of customers from cyberattacks and data loss by securely connecting users, devices, and applications in any location. 
   With more than 10 years of experience developing, operating, and scaling the cloud, Zscaler serves thousands of enterprise customers around the world, including 450 of the Forbes Global 2000 organizations. In addition to protecting customers from damaging threats, such as ransomware and data exfiltration, it helps them slash costs, reduce complexity, and improve the user experience by eliminating stacks of latency-creating gateway appliances. 
   Zscaler was founded in 2007 with a mission to make the cloud a safe place to do business and a more enjoyable experience for enterprise users. Zscaler's purpose-built security platform puts a company's defenses and controls where the connections occur—the internet—so that every connection is fast and secure, no matter how or where users connect or where their applications and workloads reside.
 
  Position: Data Engineer 
  Location: Remote within United States 
  Responsibilities/What You'll Do 
  
  Collaborate with Data & Technical architects, integration and engineering teams to capture inbound/outbound data pipeline requirements, conceptualize and develop solutions. 
  Support the evaluation and implementation of the current and future data applications/technologies to support the evolving Zscaler business needs. 
  Collaborate with IT business engagement & applications engineer teams, enterprise data engineering and business data partner teams to identify data source requirements. 
  Profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating into Zscaler's data warehouse in Snowflake. 
  Continuously optimize existing data integrations, data models and views while developing new features and capabilities to meet our business partners needs. 
  Work with Data Platform Lead to design and implement data management standards and best practices. 
  Continue to learn and develop next generation technology/ data capabilities that enhance our data engineering solutions. 
  Develop large scale and mission-critical data pipelines using modern cloud and big data architectures. 
 
 Qualifications/Your Background: 
 
  
   3 - 5 years of experience in data warehouse design & development. 
   Proficiency in building data pipelines to integrate business applications (salesforce, Netsuite, Google Analytics etc) with Snowflake 
   Must have proficiency in data modeling techniques (Dimensional) – able to write structured and efficient queries on large data sets 
   Must have hands-on experience in Python to extract data from APIs, build data pipelines. 
   Completely proficient in advanced SQL, Python/Snowpark(PySpark)/Scala (any Object Oriented language Concepts), ML libraries. 
   Strong hands-on experience in ELT Tools like Matillion, Fivetran, Talend, IDMC (Matillion preferred) , data transformational tool – DBT and in using AWS services like EC2, s3, lambda, glue. 
   Solid understanding of CI/CD process, git versioning, & advanced snowflake concepts like warehouse optimizations, SQL tuning/pruning 
   Experience in using data orchestration workflows using open-source tools like Apache Airflow, Prefect 
   Knowledge of data visualization tools such as Tableau, and/or Power BI 
   Must demonstrate good analytical skills, should be detail-oriented, team-player and must have ability to manage multiple projects simultaneously. 
   
  #LI-YC2 
   #LI-Remote 
 
 
  
    Zscaler’s salary ranges are benchmarked and are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training. 
    The base salary range listed for this full-time position excludes commission/ bonus/ equity (if applicable) + benefits.
  
   Base Pay Range
  
    $110,000—$135,000 USD
  
 
 
   Zscaler is proud to be an equal opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy or related medical conditions), age, national origin, sexual orientation, gender identity or expression, genetic information, disability status, protected veteran status or any other characteristics protected by federal, state, or local laws. 
   See more information by clicking on the Know Your Rights: Workplace Discrimination is Illegal link. 
   Pay Transparency 
   Zscaler complies with all applicable federal, state, and local pay transparency rules. For additional information about the federal requirements, click here. 
   Zscaler is committed to providing reasonable support (called accommodations or adjustments) in our recruiting processes for candidates who are differently abled, have long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support. If you need support, please contact us by sending an email to accommodations@zscaler.com. This email address is used specifically for accommodation requests only, and resumes, CV's, or questions other than accommodations will not be replied to or accepted.","<div>
 <div>
  <p><b>About Zscaler</b></p> 
  <p> Zscaler (NASDAQ: ZS) accelerates digital transformation so that customers can be more agile, efficient, resilient, and secure. The Zscaler Zero Trust Exchange is the company&apos;s cloud-native platform that protects thousands of customers from cyberattacks and data loss by securely connecting users, devices, and applications in any location.</p> 
  <p> With more than 10 years of experience developing, operating, and scaling the cloud, Zscaler serves thousands of enterprise customers around the world, including 450 of the Forbes Global 2000 organizations. In addition to protecting customers from damaging threats, such as ransomware and data exfiltration, it helps them slash costs, reduce complexity, and improve the user experience by eliminating stacks of latency-creating gateway appliances.</p> 
  <p> Zscaler was founded in 2007 with a mission to make the cloud a safe place to do business and a more enjoyable experience for enterprise users. Zscaler&apos;s purpose-built security platform puts a company&apos;s defenses and controls where the connections occur&#x2014;the internet&#x2014;so that every connection is fast and secure, no matter how or where users connect or where their applications and workloads reside.</p>
 </div>
 <p><b> Position: Data Engineer</b></p> 
 <p><b> Location: Remote within United States</b></p> 
 <p><b> Responsibilities/What You&apos;ll Do</b></p> 
 <ul> 
  <li>Collaborate with Data &amp; Technical architects, integration and engineering teams to capture inbound/outbound data pipeline requirements, conceptualize and develop solutions.</li> 
  <li>Support the evaluation and implementation of the current and future data applications/technologies to support the evolving Zscaler business needs.</li> 
  <li>Collaborate with IT business engagement &amp; applications engineer teams, enterprise data engineering and business data partner teams to identify data source requirements.</li> 
  <li>Profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating into Zscaler&apos;s data warehouse in Snowflake.</li> 
  <li>Continuously optimize existing data integrations, data models and views while developing new features and capabilities to meet our business partners needs.</li> 
  <li>Work with Data Platform Lead to design and implement data management standards and best practices.</li> 
  <li>Continue to learn and develop next generation technology/ data capabilities that enhance our data engineering solutions.</li> 
  <li>Develop large scale and mission-critical data pipelines using modern cloud and big data architectures.</li> 
 </ul>
 <h3 class=""jobSectionHeader""><b>Qualifications/Your Background:</b></h3> 
 <div>
  <ul>
   <li>3 - 5 years of experience in data warehouse design &amp; development.</li> 
   <li>Proficiency in building data pipelines to integrate business applications (salesforce, Netsuite, Google Analytics etc) with Snowflake</li> 
   <li>Must have proficiency in data modeling techniques (Dimensional) &#x2013; able to write structured and efficient queries on large data sets</li> 
   <li>Must have hands-on experience in Python to extract data from APIs, build data pipelines.</li> 
   <li>Completely proficient in advanced SQL, Python/Snowpark(PySpark)/Scala (any Object Oriented language Concepts), ML libraries.</li> 
   <li>Strong hands-on experience in ELT Tools like Matillion, Fivetran, Talend, IDMC (Matillion preferred) , data transformational tool &#x2013; DBT and in using AWS services like EC2, s3, lambda, glue.</li> 
   <li>Solid understanding of CI/CD process, git versioning, &amp; advanced snowflake concepts like warehouse optimizations, SQL tuning/pruning</li> 
   <li>Experience in using data orchestration workflows using open-source tools like Apache Airflow, Prefect</li> 
   <li>Knowledge of data visualization tools such as Tableau, and/or Power BI</li> 
   <li>Must demonstrate good analytical skills, should be detail-oriented, team-player and must have ability to manage multiple projects simultaneously.</li> 
  </ul> 
  <p>#LI-YC2</p> 
  <p> #LI-Remote</p> 
 </div>
 <div>
  <div>
   <p> Zscaler&#x2019;s salary ranges are benchmarked and are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.</p> 
   <p> The base salary range listed for this full-time position excludes commission/ bonus/ equity (if applicable) + benefits.</p>
  </div>
  <p><b> Base Pay Range</b></p>
  <div>
    &#x24;110,000&#x2014;&#x24;135,000 USD
  </div>
 </div>
 <div>
  <p> Zscaler is proud to be an equal opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy or related medical conditions), age, national origin, sexual orientation, gender identity or expression, genetic information, disability status, protected veteran status or any other characteristics protected by federal, state, or local laws.</p> 
  <p><i> See more information by clicking on the </i><i>Know Your Rights: Workplace Discrimination is Illegal </i><i>link.</i></p> 
  <p><i> Pay Transparency</i></p> 
  <p><i> Zscaler complies with all applicable federal, state, and local pay transparency rules. For additional information about the federal requirements, </i><i>click here</i><i>.</i></p> 
  <p> Zscaler is committed to providing reasonable support (called accommodations or adjustments) in our recruiting processes for candidates who are differently abled, have long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support. If you need support, please contact us by sending an email to accommodations@zscaler.com. This email address is used specifically for accommodation requests only, and resumes, CV&apos;s, or questions other than accommodations will not be replied to or accepted.</p>
 </div>
</div>",https://boards.greenhouse.io/zscaler/jobs/4101969007?gh_src=29836c077us,211c5cd9297af263,,Full-time,,,"San Jose, CA",Data Engineer,7 days ago,2023-10-11T13:37:53.645Z,3.6,42.0,"$110,000 - $135,000 a year",2023-10-18T13:37:53.659Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=211c5cd9297af263&from=jasx&tk=1hd1g2npm2f32000&vjs=3
92,TIAG,"TIAG is now hiring a
   Senior Data Engineer to join our team supporting the Federal Emergency Management Agency (FEMA). To address FEMA’s data and analytics gaps, the Office of Policy and Program Analysis (OPPA) established the Enterprise Data and Analytics Modernization Initiative (EDAMI) Program to create an enterprise analytics capability by making improvements in people, process, and technology. While this role is primarily remote, to meet the requirements of our FEMA customer, this role must have the ability to travel to FEMA HQ in Washington DC as needed.
  
 
 
  The Senior Data Engineer will be a key technical leader in supporting TIAG's growing team in the Washington DC Metro area, Charleston, SC, and other approved areas. The Senior Data Engineer is responsible for coordinating, communicating, and executing to provide comprehensive data migration, engineering solutions, and source system integration support to deliver mission needs through a focused effort on creating a strong cadre of subject matter experts, streamlined and re-engineered business processes, and development and delivery of a new IT system supporting a greater enterprise data analytics business capability.
  
 
 
  The ideal candidate for this position will possess a wide range of data engineering skills including technical, analytical, and communication skills that can support FEMA in its effort to develop, design, and implement innovative data strategies and solutions. The candidate must possess a strong understanding of data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. In this position, you will work with data (raw, structured, unstructured) to identify opportunities to make improvements, identify correlations, patterns or trends that support data driven decision making.
  
 
 
  The Senior Data Engineer will oversee the department's
  
 
 
  Additional responsibilities include:
  
 
  Creating effective technological solutions for working with and improving processes and systems with big data, using automation where possible. 
  Data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. Conduct complex data analysis and report on results. 
  Analyze and organize raw data, build data systems and pipelines, build algorithms and prototypes. Prepare data for prescriptive and predictive modeling. 
  Evaluate business needs and objectives, interpret trends and patterns. 
  Supports the development of tools, workflows, or other analytical products that support data management initiatives and/or increase program efficiencies. 
  Work with business lines to identify patterns and relationships across a variety of data sets and communicate the technical data assessment to non-technical individuals, communicate and work with individuals and groups in a constructive and collaborative manner. 
  Providing relevant data-related reports to leadership and key stakeholders for decision-making, action planning, and continuous improvement. 
  Providing technical assistance and building understanding among partners about the effective use of data engineering. 
  Work with the team to identify and resolve technical debt to improve the team’s throughput. 
  Identifies and resolves problems and/or issues. 
  Assures quality of task products, services, and deliverables, including participating in reviews, audits, and site visits. 
  Prepare, analyze, and brief recommendations for new technical approaches and technologies to sustain mission success. 
  Perform job responsibilities under Agile project methodologies. 
  Documents technical deliverables, provide training and complete/review other technical documents as required. 
  Support new business proposals and contract performance details by preparing technical past performance references, approach-based solutions and supporting color team reviews. 
 
 
  Required Experience:
  
 
  Bachelor’s degree in computer science or similar technical area desired. 
  Eight (8) years of technical experience in support of data engineering and/or data warehouse programs. 
  Three (3) years executing Agile IT execution methodologies and implementation to include sprint planning, retrospectives and agile tools such as Azure Dev Ops, Jira, etc. 
  Direct experience supporting FEMA is a strong preference. 
  Experience with cloud technologies (Azure), data science, machine learning, decision support tools and programming technologies such as Python, Spark and SQL Scala. 
  Databricks Certified Data Engineer Associate, equivalent certification, or equivalent experience to qualify as a contractor capable of migrating source system data into a Databricks Serverless SQL environment highly desired. 
  Growth minded individual with strong desire for supporting a diverse variety of efforts. 
  Experience supporting proposal development activities desired. 
  Must have the ability to manage and ensure the successful completion of multiple technical tasks in assigned project(s). 
  Maturity, high judgment, negotiation skills, ability to influence, analytical talent and leadership are essential to success in this role. 
  Position requires an active Public Trust at minimum to be considered. 
  
 
   TIAG is an equal opportunity and affirmative action employer that does not discriminate on the basis of race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations. TIAG's policy applies to all terms and conditions of employment. To achieve our goal of equal opportunity, TIAG maintains an affirmative action plan through which it makes good faith efforts to recruit, hire, and advance in employment qualified minorities, women, individuals with disabilities, and protected veterans.","<div>
 <div>
  TIAG is now hiring a
  <b> Senior Data Engineer</b> to join our team supporting the Federal Emergency Management Agency (FEMA). To address FEMA&#x2019;s data and analytics gaps, the Office of Policy and Program Analysis (OPPA) established the Enterprise Data and Analytics Modernization Initiative (EDAMI) Program to create an enterprise analytics capability by making improvements in people, process, and technology. While this role is primarily remote, to meet the requirements of our FEMA customer, this role must have the ability to travel to FEMA HQ in Washington DC as needed.
 </div> 
 <div></div>
 <div>
  The Senior Data Engineer will be a key technical leader in supporting TIAG&apos;s growing team in the Washington DC Metro area, Charleston, SC, and other approved areas. The Senior Data Engineer is responsible for coordinating, communicating, and executing to provide comprehensive data migration, engineering solutions, and source system integration support to deliver mission needs through a focused effort on creating a strong cadre of subject matter experts, streamlined and re-engineered business processes, and development and delivery of a new IT system supporting a greater enterprise data analytics business capability.
 </div> 
 <div></div>
 <div>
  The ideal candidate for this position will possess a wide range of data engineering skills including technical, analytical, and communication skills that can support FEMA in its effort to develop, design, and implement innovative data strategies and solutions. The candidate must possess a strong understanding of data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. In this position, you will work with data (raw, structured, unstructured) to identify opportunities to make improvements, identify correlations, patterns or trends that support data driven decision making.
 </div> 
 <div></div>
 <div>
  The Senior Data Engineer will oversee the department&apos;s
 </div> 
 <div></div>
 <div>
  Additional responsibilities include:
 </div> 
 <ul>
  <li>Creating effective technological solutions for working with and improving processes and systems with big data, using automation where possible.</li> 
  <li>Data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. Conduct complex data analysis and report on results.</li> 
  <li>Analyze and organize raw data, build data systems and pipelines, build algorithms and prototypes. Prepare data for prescriptive and predictive modeling.</li> 
  <li>Evaluate business needs and objectives, interpret trends and patterns.</li> 
  <li>Supports the development of tools, workflows, or other analytical products that support data management initiatives and/or increase program efficiencies.</li> 
  <li>Work with business lines to identify patterns and relationships across a variety of data sets and communicate the technical data assessment to non-technical individuals, communicate and work with individuals and groups in a constructive and collaborative manner.</li> 
  <li>Providing relevant data-related reports to leadership and key stakeholders for decision-making, action planning, and continuous improvement.</li> 
  <li>Providing technical assistance and building understanding among partners about the effective use of data engineering.</li> 
  <li>Work with the team to identify and resolve technical debt to improve the team&#x2019;s throughput.</li> 
  <li>Identifies and resolves problems and/or issues.</li> 
  <li>Assures quality of task products, services, and deliverables, including participating in reviews, audits, and site visits.</li> 
  <li>Prepare, analyze, and brief recommendations for new technical approaches and technologies to sustain mission success.</li> 
  <li>Perform job responsibilities under Agile project methodologies.</li> 
  <li>Documents technical deliverables, provide training and complete/review other technical documents as required.</li> 
  <li>Support new business proposals and contract performance details by preparing technical past performance references, approach-based solutions and supporting color team reviews.</li> 
 </ul>
 <div>
  Required Experience:
 </div> 
 <ul>
  <li>Bachelor&#x2019;s degree in computer science or similar technical area desired.</li> 
  <li>Eight (8) years of technical experience in support of data engineering and/or data warehouse programs.</li> 
  <li>Three (3) years executing Agile IT execution methodologies and implementation to include sprint planning, retrospectives and agile tools such as Azure Dev Ops, Jira, etc.</li> 
  <li>Direct experience supporting FEMA is a strong preference.</li> 
  <li>Experience with cloud technologies (Azure), data science, machine learning, decision support tools and programming technologies such as Python, Spark and SQL Scala.</li> 
  <li>Databricks Certified Data Engineer Associate, equivalent certification, or equivalent experience to qualify as a contractor capable of migrating source system data into a Databricks Serverless SQL environment highly desired.</li> 
  <li>Growth minded individual with strong desire for supporting a diverse variety of efforts.</li> 
  <li>Experience supporting proposal development activities desired.</li> 
  <li>Must have the ability to manage and ensure the successful completion of multiple technical tasks in assigned project(s).</li> 
  <li>Maturity, high judgment, negotiation skills, ability to influence, analytical talent and leadership are essential to success in this role.</li> 
  <li>Position requires an active Public Trust at minimum to be considered.</li> 
 </ul> 
 <div>
  <br> TIAG is an equal opportunity and affirmative action employer that does not discriminate on the basis of race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations. TIAG&apos;s policy applies to all terms and conditions of employment. To achieve our goal of equal opportunity, TIAG maintains an affirmative action plan through which it makes good faith efforts to recruit, hire, and advance in employment qualified minorities, women, individuals with disabilities, and protected veterans.
 </div>
</div>",https://tiag.net/careers/career-opportunities/?gnk=job&gni=8a7883ac8afcf316018b0136b3fb463e&gns=Indeed+Free,90d55aeca2904404,,,,,Remote,Senior Data Engineer,12 days ago,2023-10-06T13:38:09.552Z,4.2,16.0,"$100,000 - $140,000 a year",2023-10-18T13:38:09.562Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=90d55aeca2904404&from=jasx&tk=1hd1g3ucrjm7b801&vjs=3
93,NTT DATA,"NTT DATA Services strives to hire exceptional, innovative, and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now!
  
  We are currently seeking a 
 Data Engineer with Java Background to join our team HYBRID in Jersey City, New Jersey.
  
  
 Job Title: Data Engineer - Java 
 Location: Jersey City, NJ 
 Job Type: Hybrid 
 Duration: 12+ months
  
  
 Responsibilities:
  
 
  Develop, maintain, and optimize data pipelines to extract, transform, and load large datasets from diverse sources into our data ecosystem.
  Design and implement efficient and scalable data models that align with business requirements, ensuring data integrity and performance.
  Collaborate with cross-functional teams to understand data needs and deliver solutions that meet those requirements.
  Work closely with data scientists, analysts, and software engineers to ensure seamless integration of data solutions into larger systems.
  Identify and resolve data quality issues, ensuring accuracy, reliability, and consistency of the data infrastructure.
  Continuously monitor and improve data pipelines and processes, identifying opportunities for automation and optimization.
  Stay updated with emerging trends, technologies, and best practices in data engineering, data modeling, and backend Java engineering.
  Provide technical guidance and mentorship to junior team members, fostering their growth and development.
 
  
  Requirements:
  
 
  Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
  5+years of hands-on experience as a Data Engineer, working on complex data projects and implementing data modeling solutions.
 
  
  Data Engineering
  
  
 Must have
  
 
  Solid understanding of SQL and expertise in working with relational databases (e.g., PostgreSQL, MySQL).
  In-depth knowledge of data modeling techniques and experience with data modeling tools.
  Proficiency in designing and optimizing data pipelines using ETL/ELT frameworks and tools (e.g., Informatica, Apache Spark, Airflow, AWS Glue).
  Working knowledge on Data warehousing
  Familiarity with cloud-based data platforms and services (e.g., Snowflake, AWS, Google Cloud, Azure).
  Experience with version control systems (e.g., Git) and agile software development methodologies.
  Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders.
  Excellent problem-solving skills and the ability to work independently and collaboratively in a fast-paced environment.
 
  
  Good to Have
  
 
  JAVA 8, REST APIs, and microservices, Spring Boot framework
  Alteryx
  UNIX scripting
 
  
  About NTT DATA Services: NTT DATA Services is a recognized leader in IT and business services, including cloud, data, and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients' long-term success. Visit nttdata.com or LinkedIn to learn more.
  
  NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
  
  This position is eligible for company benefits including participation in medical, dental, and vision insurance, flexible spending or health savings account, and AD&D insurance, employee assistance, participation in a 401k program, and additional voluntary or legally required benefits.
  
  Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting pay range for this remote role is [$50.00 - $62.00/hour W2 only]. This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on several factors, including the candidate's actual work location, relevant experience, technical skills, and other qualifications. This position may also be eligible for incentive compensation based on individual and/or company performance.
  
  #LI-IST","<div>
 NTT DATA Services strives to hire exceptional, innovative, and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now!
 <br> 
 <br> We are currently seeking a 
 <b>Data Engineer with Java Background to join our team HYBRID in Jersey City, New Jersey.</b>
 <br> 
 <br> 
 <b>Job Title: Data Engineer - Java </b>
 <b>Location: Jersey City, NJ</b> 
 <b>Job Type: Hybrid</b> 
 <b>Duration: 12+ months</b>
 <br> 
 <br> 
 <b>Responsibilities:</b>
 <br> 
 <ul>
  <li>Develop, maintain, and optimize data pipelines to extract, transform, and load large datasets from diverse sources into our data ecosystem.</li>
  <li>Design and implement efficient and scalable data models that align with business requirements, ensuring data integrity and performance.</li>
  <li>Collaborate with cross-functional teams to understand data needs and deliver solutions that meet those requirements.</li>
  <li>Work closely with data scientists, analysts, and software engineers to ensure seamless integration of data solutions into larger systems.</li>
  <li>Identify and resolve data quality issues, ensuring accuracy, reliability, and consistency of the data infrastructure.</li>
  <li>Continuously monitor and improve data pipelines and processes, identifying opportunities for automation and optimization.</li>
  <li>Stay updated with emerging trends, technologies, and best practices in data engineering, data modeling, and backend Java engineering.</li>
  <li>Provide technical guidance and mentorship to junior team members, fostering their growth and development.</li>
 </ul>
 <br> 
 <b> Requirements:</b>
 <br> 
 <ul>
  <li>Bachelor&apos;s or Master&apos;s degree in Computer Science, Engineering, or a related field.</li>
  <li>5+years of hands-on experience as a Data Engineer, working on complex data projects and implementing data modeling solutions.</li>
 </ul>
 <br> 
 <b> Data Engineering</b>
 <br> 
 <br> 
 <b>Must have</b>
 <br> 
 <ul>
  <li>Solid understanding of <b>SQL</b> and expertise in working with <b>relational databases</b> (e.g., PostgreSQL, MySQL).</li>
  <li>In-depth knowledge of data modeling techniques and experience with <b>data modeling tools</b>.</li>
  <li>Proficiency in designing and optimizing data pipelines using <b>ETL/ELT</b> frameworks and tools (e.g., Informatica, Apache Spark, Airflow, AWS Glue).</li>
  <li>Working knowledge on <b>Data warehousing</b></li>
  <li>Familiarity with <b>cloud-based data platforms</b> and services (e.g., <b>Snowflake</b>, AWS, Google Cloud, Azure).</li>
  <li>Experience with version control systems (e.g., <b>Git</b>) and agile software development methodologies.</li>
  <li>Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders.</li>
  <li>Excellent problem-solving skills and the ability to work independently and collaboratively in a fast-paced environment.</li>
 </ul>
 <br> 
 <b> Good to Have</b>
 <br> 
 <ul>
  <li>JAVA 8, <b>REST</b> <b>APIs</b>, and <b>microservices</b>, <b>Spring Boot</b> framework</li>
  <li>Alteryx</li>
  <li>UNIX scripting</li>
 </ul>
 <br> 
 <b> About NTT DATA Services:</b> NTT DATA Services is a recognized leader in IT and business services, including cloud, data, and applications, headquartered in Texas. As part of NTT DATA, a &#x24;30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients&apos; long-term success. Visit nttdata.com or LinkedIn to learn more.
 <br> 
 <br> NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
 <br> 
 <br> This position is eligible for company benefits including participation in medical, dental, and vision insurance, flexible spending or health savings account, and AD&amp;D insurance, employee assistance, participation in a 401k program, and additional voluntary or legally required benefits.
 <br> 
 <br> Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting pay range for this remote role is [&#x24;50.00 - &#x24;62.00/hour W2 only]. This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on several factors, including the candidate&apos;s actual work location, relevant experience, technical skills, and other qualifications. This position may also be eligible for incentive compensation based on individual and/or company performance.
 <br> 
 <br> #LI-IST
</div>",https://click.appcast.io/track/hnnmduu-org?cs=fzm&jg=2guz&bid=lUf2CslKyPxm6i440ZgUYA==&jobPipeline=Indeed&ittk=8S9CDO3UTW,46368a3a77ba0041,,,,,"Jersey City, NJ 07302",Data Engineer - Java,13 days ago,2023-10-05T13:38:17.802Z,3.5,3971.0,$50 - $62 an hour,2023-10-18T13:38:17.805Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=46368a3a77ba0041&from=jasx&tk=1hd1g40sjjm7l801&vjs=3
94,Abercrombie and Fitch Co.,"Company Description
  Job Description
  The primary responsibility of the Senior Engineer, Global Data & Insights - Data Management is to build data pipelines, model and prepare data, perform complex data analysis to answer Business questions, build and automate data pipeline and quality framework to enable and promote self service data pipelines, assist in operationalizing the AI / ML Engineering solutions. This role is expected to lead and guide other team members and evangelize the design patterns as well as coding standards.
  This role plays an active part in our Data Modernization project to migrate the from on-prem platforms such as IBM Netezza to cloud project
  What Will You Be Doing?
 
   Team up with the engineering teams and enterprise architecture (EA) to define standards, design patterns, accelerators, development practices, DevOps and CI/CD automation
   Create and maintain the data ingestion, quality testing and audit framework
   Conduct complex data analysis to answer the queries from Business Users or Technology team partners either directly from Analysts or stemmed from one of the Reporting tools suchs PowerBI, Tableau, OBIEE.
   Build and automate the data ingestion, transformation and aggregation pipelines using Azure Data Factory, Databricks / Spark, Snowflake, Kafka as well as Enterprise Scheduler tools such as CA Workload automation or Control M
   Setup and evangelize the metadata driven approach to data pipelines to promote self service
   Setup and continuously improve the data quality and audit monitoring as well as alerting
   Constantly evaluate the process automation options and collaborate with engineering as well as architecture to review the proposed design.
   Demonstrate mastery of build and release engineering principles and methodologies including source control, branch management, build and smoke testing, archiving and retention practices
   Adhere to and enhance and document the design principles, best practices by collaborating with Solution and in some cases Enterprise Architects
   Participate in and support the Data Academy and Data Literacy program to train the Business Users and Technology teams on Data
   Respond SLA driven production data quality or pipeline issues
   Work in a fast-paced Agile/Scrum environment
   Identify and assist with implementation of DevOps practices in support of fully automated deployments
   Document the Data Flow Diagrams, Data Models, Technical Data Mapping and Production Support Information for Data Pipelines
   Follow the Industry standard data security practices and evangelize the same across the team.
 
  What Do You Need To Bring?
 
   Bachelor’s degree in Computer Science or Engineering or Mathematics or related field and 5+ years of experience in various cloud technologies within a large-scale organization
   Personal Attributes: Self-starter, Collaborative, Curious, Strong work ethic, highly motivated, Team oriented
   Experience designing and building complex data pipelines in an agile environment
   Expertise on data analysis and wrangling using sql, python, databricks
   Experience with modern cloud development and design concepts; software development lifecycle; multi-developer code versioning and conflict resolution; planning, design, and problem resolution enterprise data applications / solutions
   Demonstrated ability in developing a culture that embraces innovation, and challenges existing paradigms
   5+ years of experience in an Enterprise Data Management or Data Engineering role
   3+ of hands on experience in building metadata driven data pipelines using Azure Data Factory, Databricks / Spark for Cloud Datalake
   5+ years hands on experience with using one or more of the following for data analysis and wrangling Databricks, Python / PySpark, Jupyter Notebooks
   Expert level SQL knowledge on databases such as but not limited to Snowflake, Netezza, Oracle, Sql Server, MySQL, Teradata
   3+ years of hands on experience on one or more of big data technologies such as Cloudera Hadoop, Pivotal, Vertica, MapR is a plus
   Experience working in a multi developer environment and hands on experience in using either azure devops or gitlab
   Preferably experienced in SLA driven Production Data Pipeline or Quality support
   Experience or strong understanding of the traditional enterprise ETL platforms such as IBM Datastage, Informatica, Pentaho, Ab Initio etc.
   Functional knowledge of some of the following technologies - Terraform, Azure CLI, PowerShell, Containerization (Kubernetes, Docker)
   Functional knowledge of one or more Reporting tools such as PowerBI, Tableau, OBIEE
   Team player with excellent communication skills, ability to communicate with the customer directly and able to explain the status of the deliverables in scrum calls
   Ability to implement Agile methodologies and work in an Agile DevOps environment
 
  Our Company
  Abercrombie & Fitch Co. (A&F Co.) is a global retailer of five iconic, omnichannel lifestyle brands catering to the kid through millennial customer: Abercrombie & Fitch, abercrombie kids, Hollister, Gilly Hicks and Social Tourist. At A&F Co., we’re here for our associates, customers and communities on the journey to being and becoming who they are – and because no journey is the same, we strive to create an inclusive culture, where everyone is free to share ideas.
  Our Values
  We lead with purpose and always put our people first, which is evidenced by our Great Place to Work™ Certification, as well as being a 2021 recipient of Fortune’s Best Workplaces in Retail, and named a Best Place to Work for LGBTQ+ Equality by the Human Rights Campaign for 16 consecutive years. We’re proud to offer equitable compensation and benefits, including flexibility and competitive Paid Time Off, as well as education and engagement events, including various Associate Resource Groups, volunteer opportunities and additional time off to give back to our global communities.
  What You'll Get
  As an Abercrombie & Fitch Co. (A&F Co.) associate, you’ll be eligible to participate in a variety of benefit programs designed to fit you and your lifestyle. A&F is committed to providing simple, competitive, and comprehensive benefits that align with our Company’s culture and values, but most importantly – with you! We also provide competitive incentives to reward the commitment our associates have for moving our global business forward:
 
   Incentive Bonus Program
   Paid Time Off and Work From Anywhere Flexibility
   Paid Volunteer Day per Year, allowing you to give back to your community
   Merchandise Discount
   Medical, Dental and Vision Insurance Available
   Life and Disability Insurance
   Associate Assistance Program
   Paid Parental and Adoption Leave
   Access to Carrot to support your unique parenthood journey
   Access to Headspace dedicated to creating healthier, happier lives from the inside out
   401(K) Savings Plan with Company Match
   Opportunities for Career Advancement, we believe in promoting from within
   A Global Team of People Who'll Celebrate you for Being YOU
 
 
 

 Additional Information
  ABERCROMBIE & FITCH CO. IS AN EQUAL OPPORTUNITY EMPLOYER
  Notice (For Colorado, New York, California and Washington): The recruiting pay range for this position is $110,000 - $136,000. Factors that may be used to determine your actual salary may include your specific skills, your years of experience, your work location, comparison to other employees in similar or related roles, or market demands. The range may be modified in the future.","<div>
 Company Description
 <p><b><br> Job Description</b></p>
 <p> The primary responsibility of the <i>Senior Engineer, Global Data &amp; Insights - Data Management</i> is to build data pipelines, model and prepare data, perform complex data analysis to answer Business questions, build and automate data pipeline and quality framework to enable and promote self service data pipelines, assist in operationalizing the AI / ML Engineering solutions. This role is expected to lead and guide other team members and evangelize the design patterns as well as coding standards.</p>
 <p> This role plays an active part in our Data Modernization project to migrate the from on-prem platforms such as IBM Netezza to cloud project</p>
 <p><b> What Will You Be Doing?</b></p>
 <ul>
  <li><p> Team up with the engineering teams and enterprise architecture (EA) to define standards, design patterns, accelerators, development practices, DevOps and CI/CD automation</p></li>
  <li><p> Create and maintain the data ingestion, quality testing and audit framework</p></li>
  <li><p> Conduct complex data analysis to answer the queries from Business Users or Technology team partners either directly from Analysts or stemmed from one of the Reporting tools suchs PowerBI, Tableau, OBIEE.</p></li>
  <li><p> Build and automate the data ingestion, transformation and aggregation pipelines using Azure Data Factory, Databricks / Spark, Snowflake, Kafka as well as Enterprise Scheduler tools such as CA Workload automation or Control M</p></li>
  <li><p> Setup and evangelize the metadata driven approach to data pipelines to promote self service</p></li>
  <li><p> Setup and continuously improve the data quality and audit monitoring as well as alerting</p></li>
  <li><p> Constantly evaluate the process automation options and collaborate with engineering as well as architecture to review the proposed design.</p></li>
  <li><p> Demonstrate mastery of build and release engineering principles and methodologies including source control, branch management, build and smoke testing, archiving and retention practices</p></li>
  <li><p> Adhere to and enhance and document the design principles, best practices by collaborating with Solution and in some cases Enterprise Architects</p></li>
  <li><p> Participate in and support the Data Academy and Data Literacy program to train the Business Users and Technology teams on Data</p></li>
  <li><p> Respond SLA driven production data quality or pipeline issues</p></li>
  <li><p> Work in a fast-paced Agile/Scrum environment</p></li>
  <li><p> Identify and assist with implementation of DevOps practices in support of fully automated deployments</p></li>
  <li><p> Document the Data Flow Diagrams, Data Models, Technical Data Mapping and Production Support Information for Data Pipelines</p></li>
  <li><p> Follow the Industry standard data security practices and evangelize the same across the team.</p></li>
 </ul>
 <p><b> What Do You Need To Bring?</b></p>
 <ul>
  <li><p> Bachelor&#x2019;s degree in Computer Science or Engineering or Mathematics or related field and 5+ years of experience in various cloud technologies within a large-scale organization</p></li>
  <li><p> Personal Attributes: Self-starter, Collaborative, Curious, Strong work ethic, highly motivated, Team oriented</p></li>
  <li><p> Experience designing and building complex data pipelines in an agile environment</p></li>
  <li><p> Expertise on data analysis and wrangling using sql, python, databricks</p></li>
  <li><p> Experience with modern cloud development and design concepts; software development lifecycle; multi-developer code versioning and conflict resolution; planning, design, and problem resolution enterprise data applications / solutions</p></li>
  <li><p> Demonstrated ability in developing a culture that embraces innovation, and challenges existing paradigms</p></li>
  <li><p> 5+ years of experience in an Enterprise Data Management or Data Engineering role</p></li>
  <li><p> 3+ of hands on experience in building metadata driven data pipelines using Azure Data Factory, Databricks / Spark for Cloud Datalake</p></li>
  <li><p> 5+ years hands on experience with using one or more of the following for data analysis and wrangling Databricks, Python / PySpark, Jupyter Notebooks</p></li>
  <li><p> Expert level SQL knowledge on databases such as but not limited to Snowflake, Netezza, Oracle, Sql Server, MySQL, Teradata</p></li>
  <li><p> 3+ years of hands on experience on one or more of big data technologies such as Cloudera Hadoop, Pivotal, Vertica, MapR is a plus</p></li>
  <li><p> Experience working in a multi developer environment and hands on experience in using either azure devops or gitlab</p></li>
  <li><p> Preferably experienced in SLA driven Production Data Pipeline or Quality support</p></li>
  <li><p> Experience or strong understanding of the traditional enterprise ETL platforms such as IBM Datastage, Informatica, Pentaho, Ab Initio etc.</p></li>
  <li><p> Functional knowledge of some of the following technologies - Terraform, Azure CLI, PowerShell, Containerization (Kubernetes, Docker)</p></li>
  <li><p> Functional knowledge of one or more Reporting tools such as PowerBI, Tableau, OBIEE</p></li>
  <li><p> Team player with excellent communication skills, ability to communicate with the customer directly and able to explain the status of the deliverables in scrum calls</p></li>
  <li><p> Ability to implement Agile methodologies and work in an Agile DevOps environment</p></li>
 </ul>
 <p><b> Our Company</b></p>
 <p> Abercrombie &amp; Fitch Co. (A&amp;F Co.) is a global retailer of five iconic, omnichannel lifestyle brands catering to the kid through millennial customer: Abercrombie &amp; Fitch, abercrombie kids, Hollister, Gilly Hicks and Social Tourist. At A&amp;F Co., we&#x2019;re here for our associates, customers and communities on the journey to being and becoming who they are &#x2013; and because no journey is the same, we strive to create an inclusive culture, where everyone is free to share ideas.</p>
 <p><b> Our Values</b></p>
 <p> We lead with purpose and always put our people first, which is evidenced by our Great Place to Work&#x2122; Certification, as well as being a 2021 recipient of Fortune&#x2019;s Best Workplaces in Retail, and named a Best Place to Work for LGBTQ+ Equality by the Human Rights Campaign for 16 consecutive years. We&#x2019;re proud to offer equitable compensation and benefits, including flexibility and competitive Paid Time Off, as well as education and engagement events, including various Associate Resource Groups, volunteer opportunities and additional time off to give back to our global communities.</p>
 <p><b> What You&apos;ll Get</b></p>
 <p> As an Abercrombie &amp; Fitch Co. (A&amp;F Co.) associate, you&#x2019;ll be eligible to participate in a variety of benefit programs designed to fit you and your lifestyle. A&amp;F is committed to providing simple, competitive, and comprehensive benefits that align with our Company&#x2019;s culture and values, but most importantly &#x2013; with you! We also provide competitive incentives to reward the commitment our associates have for moving our global business forward:</p>
 <ul>
  <li> Incentive Bonus Program</li>
  <li> Paid Time Off and Work From Anywhere Flexibility</li>
  <li> Paid Volunteer Day per Year, allowing you to give back to your community</li>
  <li> Merchandise Discount</li>
  <li> Medical, Dental and Vision Insurance Available</li>
  <li> Life and Disability Insurance</li>
  <li> Associate Assistance Program</li>
  <li> Paid Parental and Adoption Leave</li>
  <li> Access to Carrot to support your unique parenthood journey</li>
  <li> Access to Headspace dedicated to creating healthier, happier lives from the inside out</li>
  <li> 401(K) Savings Plan with Company Match</li>
  <li> Opportunities for Career Advancement, we believe in promoting from within</li>
  <li> A Global Team of People Who&apos;ll Celebrate you for Being YOU</li>
 </ul>
</div> 
<br> 
<div>
 Additional Information
 <p><br> ABERCROMBIE &amp; FITCH CO. IS AN EQUAL OPPORTUNITY EMPLOYER</p>
 <p><i> Notice (For Colorado, New York, California and Washington): The recruiting pay range for this position is &#x24;110,000 - &#x24;136,000. Factors that may be used to determine your actual salary may include your specific skills, your years of experience, your work location, comparison to other employees in similar or related roles, or market demands. The range may be modified in the future.</i></p>
</div>",https://jobs.smartrecruiters.com/AbercrombieAndFitchCo/743999936426242-senior-engineer-data-insights-remote-,651209e06d4f7358,,Full-time,,,"Columbus, OH","Senior Engineer, Data Insights (Remote)",7 days ago,2023-10-11T13:38:28.113Z,3.5,5399.0,"$110,000 - $136,000 a year",2023-10-18T13:38:28.122Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=651209e06d4f7358&from=jasx&tk=1hd1g3s5pjfnq801&vjs=3
95,Lincoln Financial,"Date: Oct 5, 2023 
 Primary Location: Radnor, PA, US 
 Company: Lincoln Financial 
 
  
   Alternate Locations: Work from Home
  
   
   
  
   Work Arrangement:
   
  
   Hybrid/Flexible : Work at home and use the office as appropriate for in-person collaboration.
  
   
   
  
   Relocation assistance: is not available for this opportunity.
  
   
   
  
   Requisition #: 72284
  
   
   
  
   
    
     
       The Role at a Glance
     
    
    
    
      
     
      This is a great opportunity to join the growing Life IT organization at Lincoln Financial Group to build next generation application solutions to help our customers achieve their financial goals and objectives. This position will consult/analyze and deliver on more complex assignments/projects for your assigned area(s) of application development responsibility to build out a new data and analytics platform on AWS to integrate with a number of back-end applications. You will also act as a resource and develop more complex innovative business solutions by creating new applications, modifying existing applications and providing post production support (as necessary). You will deliver more complex business application software solutions following the system development life cycle process.
     
      
      
    
   
   
    
     
       What you'll be doing
     
    
    
    
      
     
      Participates in analysis, design, and ETL development as part of Agile / Scrum Develop Team 
      Architect, implement and support big data initiatives for the enterprise (using AWS, Informatica and Python to further these objectives) 
      Lead other developers, solution designers and quality analysts on project led efforts to add new data sources to the Life data platform 
      Understands data mapping and data modeling methodologies including normal form, star, and snowflake to reduce data redundancy and improve data integrity. 
      Liaise with internal Lincoln business partners on requirements, design, testing and production topics in order to create solution proposals and develop code. 
      Performs technical tasks including estimating, analysis, technical requirements, design, construction and unit & integration testing following SDLC. 
      Assist analytical teams with the design and implementation of Data solutions and systems, including integration with Operational Datastores and Data Warehouses, both on-premise and in the Cloud. 
      Develop, enhance, and support Informatica workflows and processes using Informatica Power Center for the extraction and transformation of data in UNIX/Oracle, Windows/MS SQL Server and Aurora/postgres database environments. 
      Maintains knowledge on current and emerging developments/trends for assigned area(s) of responsibility, assesses the impact, and collaborates with Scrum Team and Leadership to incorporate new trends and developments in current and future solutions 
      Participates and enhances organizational initiatives by positively influencing and supporting change management and/or departmental/enterprise initiatives within assigned area(s) of responsibility 
      Identifies and directs the implementation of process improvements that significantly improve quality across the team, department and/or business unit for his/her assigned area(s) of responsibility 
      Provides expertise to team members and applicable internal/external stakeholders on complex assignments/projects for his/her assigned area(s) of responsibility 
      Provides direction on complex assignments, projects, and/or initiatives to build and enhance the capability of his/her assigned area(s) of responsibility 
     
    
   
   
    
     
      What we’re looking for
     
    
    
    
      
     
      4 Year/Bachelor's degree or equivalent work experience (4 years of experience in lieu of Bachelors)_Minimum Required in Computer Science, Computer Information Systems, Information Systems, Information Technology or Computer Engineering or equivalent work experience 
      3+ years developing data movement and engineering applications and worked on integrating disparate systems using ETL following SDLC and/or Agile methodologies 
      3+ years of experience in ETL application development that directly aligns with the specific responsibilities for this position 
      3+ years utilizing Structured Query Language optimization on platforms including Redshift, Oracle, Postgres in order to build and manage large scale data warehouses 
      3+ years utilizing programming languages such as Python in order to program and develop software 
      3+ years of experience in Informatica Big Data Management Solution on AWS Cloud 
      3+ years of experience working on Elastic Map Reduce using AWS Cloud services to process massive amount using Informatica Big Data Management solution 
      3+ years of experience in Informatica PowerCenter (or comparable tool) developing ETL mappings using Designer, Workflow Manager, Workflow Monitor, and Repository Manager 
      3+ years of experience with Architecture Design and Data Modeling 
      3+ years of experience writing Unix/Linux or Windows Scripts in tools such as PERL, Shell script, Python, etc. 
      2+ years of experience in scheduling jobs using Autosys (or comparable distributed scheduler) 
      3+ years of experience in creating complex technical specifications from business requirements/specifications 
      3+ years of experience as a Sr Developer leading/mentoring other Developers on the team 
      Big Data – NO SQL Modeling, Hive, HBase, Pig, Cassandra, MongoDB, Redshift utilizing AWS Services and integrating data on cloud is a plus
     
     
       
     
     
      #DICE
     
    
   
  
   
   
  
   What’s it like to work here?
   
  
   At Lincoln Financial Group, we love what we do. We make meaningful contributions each and every day to empower our customers to take charge of their lives. Working alongside dedicated and talented colleagues, we build fulfilling careers and stronger communities through a company that values our unique perspectives, insights and contributions and invests in programs that empower each of us to take charge of our own future.
  
   
   
  
   What’s in it for YOU:
   
  
   
    
     A clearly defined career framework to help you successfully manage your career
     
   
    
     Leadership development and virtual training opportunities
     
   
    
     PTO/parental leave
     
   
    
     Competitive 401K and employee benefits
     
   
    
     Free financial counseling, health coaching and employee assistance program
     
   
    
     Tuition assistance program
     
   
    
     A leadership team that prioritizes your health and well-being; offering a remote work environment and flexible work hybrid situations
     
   
    
     Effective productivity/technology tools and training
     
  
  
  
   Pay Range: $102,301 - $140,000
  
   
   
  
   Actual base pay could vary based on non-discriminatory factors including but not limited to work experience, education, location, licensure requirements, proficiency and qualifications required for the role. The base pay is just one component of Lincoln’s total rewards package for employees. In addition, the role may be eligible for the Annual Incentive Program, which is discretionary and based on the performance of the company, business unit and individual. Other rewards may include long-term incentives, sales incentives and Lincoln’s standard benefits package.
  
   
   
  
   About The Company
   
  
   Lincoln Financial Group provides advice and solutions that help people take charge of their financial lives with confidence and optimism. Today, approximately 16 million customers trust our retirement, insurance and wealth protection expertise to help address their lifestyle, savings and income goals, and guard against long-term care expenses.
  
   
  
    Headquartered in Radnor, Pennsylvania, Lincoln Financial Group is the marketing name for Lincoln National Corporation (NYSE:LNC) and its affiliates. The company had $290 billion in end-of-period account balances net of reinsurance as of March 31, 2023.
  
   
   
  
   Lincoln Financial Group is a committed corporate citizen included on major sustainability indices including the Dow Jones Sustainability Index North America and ranks among Newsweek’s Most Responsible Companies. Dedicated to diversity, equity and inclusion, we are included on transparency benchmarking tools such as the Corporate Equality Index, the Disability Equality Index and the Bloomberg Gender-Equality Index. Committed to providing our employees with flexible work arrangements, we were named to FlexJobs’ list of the Top 100 Companies to Watch for Remote Jobs in 2022. With a long and rich legacy of acting ethically, telling the truth and speaking up for what is right, Lincoln was recognized as one of Ethisphere’s 2022 World’s Most Ethical Companies®. We create opportunities for early career talent through our intern development program, which ranks among WayUp and Yello’s annual list of Top 100 Internship Programs.
  
   
   
  
   Lincoln is committed to creating a diverse and inclusive environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
  
   
   
  
   Follow us on Facebook, Twitter, LinkedIn, and Instagram.
  
   
   
  
   Be Aware of Fraudulent Recruiting Activities
   
  
   If you are interested in a career at Lincoln, we encourage you to review our current openings and apply on our website. Lincoln values the privacy and security of every applicant and urges all applicants to diligently protect their sensitive personal information from scams targeting job seekers. These scams can take many forms including fake employment applications, bogus interviews and falsified offer letters.
   
  
   Lincoln will not ask applicants to provide their social security numbers, date of birth, bank account information or other sensitive information in job applications. Additionally, our recruiters do not communicate with applicants through free e-mail accounts (Gmail, Yahoo, Hotmail) or conduct interviews utilizing video chat rooms. We will never ask applicants to provide payment during the hiring process or extend an offer without conducting a phone, live video or in-person interview. Please contact Lincoln's fraud team at fraudhotline@lfg.com if you encounter a recruiter or see a job opportunity that seems suspicious.
  
   
   
  
   Additional Information
   
  
   This position may be subject to Lincoln’s Political Contribution Policy. An offer of employment may be contingent upon disclosing to Lincoln the details of certain political contributions. Lincoln may decline to extend an offer or terminate employment for this role if it determines political contributions made could have an adverse impact on Lincoln’s current or future business interests, misrepresentations were made, or for failure to fully disclose applicable political contributions and or fundraising activities.
  
   
   
  
   Any unsolicited resumes/candidate profiles submitted through our web site or to personal e-mail accounts of employees of Lincoln Financial Group are considered property of Lincoln Financial Group and are not subject to payment of agency fees.
  
   
   
  
   Lincoln Financial Group (“LFG”) is an Equal Opportunity employer and, as such, is committed in policy and practice to recruit, hire, compensate, train and promote, in all job classifications, without regard to race, color, religion, sex (including pregnancy), age, national origin, disability, sexual orientation, gender identity and expression, Veteran status, or genetic information. Applicants are evaluated on the basis of job qualifications. If you are a person with a disability that impedes your ability to express your interest for a position through our online application process, or require TTY/TDD assistance, contact us by calling 260-455-2558.","<p></p>
<div>
 <p><b>Date: </b>Oct 5, 2023 </p>
 <p><b>Primary Location:</b> Radnor, PA, US </p>
 <p><b>Company: </b>Lincoln Financial </p>
 <div>
  <div>
   <b>Alternate Locations:</b> Work from Home
  </div>
  <br> 
  <div></div> 
  <div>
   <b>Work Arrangement:</b>
  </div> 
  <div>
   Hybrid/Flexible : Work at home and use the office as appropriate for in-person collaboration.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>Relocation assistance:</b> is not available for this opportunity.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>Requisition #:</b> 72284
  </div>
  <br> 
  <div></div> 
  <div>
   <div>
    <div>
     <div>
      <b> The Role at a Glance</b>
     </div>
    </div>
    <div></div>
    <div>
     <br> 
     <div>
      This is a great opportunity to join the growing Life IT organization at Lincoln Financial Group to build next generation application solutions to help our customers achieve their financial goals and objectives. This position will consult/analyze and deliver on more complex assignments/projects for your assigned area(s) of application development responsibility to build out a new data and analytics platform on AWS to integrate with a number of back-end applications. You will also act as a resource and develop more complex innovative business solutions by creating new applications, modifying existing applications and providing post production support (as necessary). You will deliver more complex business application software solutions following the system development life cycle process.
     </div>
     <br> 
     <div></div> 
    </div>
   </div>
   <div>
    <div>
     <div>
      <b> What you&apos;ll be doing</b>
     </div>
    </div>
    <div></div>
    <div>
     <br> 
     <ul>
      <li>Participates in analysis, design, and ETL development as part of Agile / Scrum Develop Team</li> 
      <li>Architect, implement and support big data initiatives for the enterprise (using AWS, Informatica and Python to further these objectives)</li> 
      <li>Lead other developers, solution designers and quality analysts on project led efforts to add new data sources to the Life data platform</li> 
      <li>Understands data mapping and data modeling methodologies including normal form, star, and snowflake to reduce data redundancy and improve data integrity.</li> 
      <li>Liaise with internal Lincoln business partners on requirements, design, testing and production topics in order to create solution proposals and develop code.</li> 
      <li>Performs technical tasks including estimating, analysis, technical requirements, design, construction and unit &amp; integration testing following SDLC.</li> 
      <li>Assist analytical teams with the design and implementation of Data solutions and systems, including integration with Operational Datastores and Data Warehouses, both on-premise and in the Cloud.</li> 
      <li>Develop, enhance, and support Informatica workflows and processes using Informatica Power Center for the extraction and transformation of data in UNIX/Oracle, Windows/MS SQL Server and Aurora/postgres database environments.</li> 
      <li>Maintains knowledge on current and emerging developments/trends for assigned area(s) of responsibility, assesses the impact, and collaborates with Scrum Team and Leadership to incorporate new trends and developments in current and future solutions</li> 
      <li>Participates and enhances organizational initiatives by positively influencing and supporting change management and/or departmental/enterprise initiatives within assigned area(s) of responsibility</li> 
      <li>Identifies and directs the implementation of process improvements that significantly improve quality across the team, department and/or business unit for his/her assigned area(s) of responsibility</li> 
      <li>Provides expertise to team members and applicable internal/external stakeholders on complex assignments/projects for his/her assigned area(s) of responsibility</li> 
      <li>Provides direction on complex assignments, projects, and/or initiatives to build and enhance the capability of his/her assigned area(s) of responsibility<br> </li>
     </ul>
    </div>
   </div>
   <div>
    <div>
     <div>
      <b>What we&#x2019;re looking for</b>
     </div>
    </div>
    <div></div>
    <div>
     <br> 
     <ul>
      <li>4 Year/Bachelor&apos;s degree or equivalent work experience (4 years of experience in lieu of Bachelors)_Minimum Required in Computer Science, Computer Information Systems, Information Systems, Information Technology or Computer Engineering or equivalent work experience</li> 
      <li>3+ years developing data movement and engineering applications and worked on integrating disparate systems using ETL following SDLC and/or Agile methodologies</li> 
      <li>3+ years of experience in ETL application development that directly aligns with the specific responsibilities for this position</li> 
      <li>3+ years utilizing Structured Query Language optimization on platforms including Redshift, Oracle, Postgres in order to build and manage large scale data warehouses</li> 
      <li>3+ years utilizing programming languages such as Python in order to program and develop software</li> 
      <li>3+ years of experience in Informatica Big Data Management Solution on AWS Cloud</li> 
      <li>3+ years of experience working on Elastic Map Reduce using AWS Cloud services to process massive amount using Informatica Big Data Management solution</li> 
      <li>3+ years of experience in Informatica PowerCenter (or comparable tool) developing ETL mappings using Designer, Workflow Manager, Workflow Monitor, and Repository Manager</li> 
      <li>3+ years of experience with Architecture Design and Data Modeling</li> 
      <li>3+ years of experience writing Unix/Linux or Windows Scripts in tools such as PERL, Shell script, Python, etc.</li> 
      <li>2+ years of experience in scheduling jobs using Autosys (or comparable distributed scheduler)</li> 
      <li>3+ years of experience in creating complex technical specifications from business requirements/specifications</li> 
      <li>3+ years of experience as a Sr Developer leading/mentoring other Developers on the team</li> 
      <li>Big Data &#x2013; NO SQL Modeling, Hive, HBase, Pig, Cassandra, MongoDB, Redshift utilizing AWS Services and integrating data on cloud is a plus</li>
     </ul>
     <div>
      <br> 
     </div>
     <div>
      #DICE
     </div>
    </div>
   </div>
  </div>
  <br> 
  <div></div> 
  <div>
   <b>What&#x2019;s it like to work here?</b>
  </div> 
  <div>
   At Lincoln Financial Group, we love what we do. We make meaningful contributions each and every day to empower our customers to take charge of their lives. Working alongside dedicated and talented colleagues, we build fulfilling careers and stronger communities through a company that values our unique perspectives, insights and contributions and invests in programs that empower each of us to take charge of our own future.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>What&#x2019;s in it for YOU:</b>
  </div> 
  <ul>
   <li>
    <div>
     A clearly defined career framework to help you successfully manage your career
    </div> </li>
   <li>
    <div>
     Leadership development and virtual training opportunities
    </div> </li>
   <li>
    <div>
     PTO/parental leave
    </div> </li>
   <li>
    <div>
     Competitive 401K and employee benefits
    </div> </li>
   <li>
    <div>
     Free financial counseling, health coaching and employee assistance program
    </div> </li>
   <li>
    <div>
     Tuition assistance program
    </div> </li>
   <li>
    <div>
     A leadership team that prioritizes your health and well-being; offering a remote work environment and flexible work hybrid situations
    </div> </li>
   <li>
    <div>
     Effective productivity/technology tools and training
    </div><br> </li>
  </ul>
  <div></div>
  <div>
   <b>Pay Range:</b> &#x24;102,301 - &#x24;140,000
  </div>
  <br> 
  <div></div> 
  <div>
   Actual base pay could vary based on non-discriminatory factors including but not limited to work experience, education, location, licensure requirements, proficiency and qualifications required for the role. The base pay is just one component of Lincoln&#x2019;s total rewards package for employees. In addition, the role may be eligible for the Annual Incentive Program, which is discretionary and based on the performance of the company, business unit and individual. Other rewards may include long-term incentives, sales incentives and Lincoln&#x2019;s standard benefits package.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>About The Company</b>
  </div> 
  <div>
   Lincoln Financial Group provides advice and solutions that help people take charge of their financial lives with confidence and optimism. Today, approximately 16 million customers trust our retirement, insurance and wealth protection expertise to help address their lifestyle, savings and income goals, and guard against long-term care expenses.
  </div>
  <br> 
  <div>
   <br> Headquartered in Radnor, Pennsylvania, Lincoln Financial Group is the marketing name for Lincoln National Corporation (NYSE:LNC) and its affiliates. The company had &#x24;290 billion in end-of-period account balances net of reinsurance as of March 31, 2023.
  </div>
  <br> 
  <div></div> 
  <div>
   Lincoln Financial Group is a committed corporate citizen included on major sustainability indices including the Dow Jones Sustainability Index North America and ranks among Newsweek&#x2019;s Most Responsible Companies. Dedicated to diversity, equity and inclusion, we are included on transparency benchmarking tools such as the Corporate Equality Index, the Disability Equality Index and the Bloomberg Gender-Equality Index. Committed to providing our employees with flexible work arrangements, we were named to FlexJobs&#x2019; list of the Top 100 Companies to Watch for Remote Jobs in 2022. With a long and rich legacy of acting ethically, telling the truth and speaking up for what is right, Lincoln was recognized as one of Ethisphere&#x2019;s 2022 World&#x2019;s Most Ethical Companies&#xae;. We create opportunities for early career talent through our intern development program, which ranks among WayUp and Yello&#x2019;s annual list of Top 100 Internship Programs.
  </div>
  <br> 
  <div></div> 
  <div>
   Lincoln is committed to creating a diverse and inclusive environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
  </div>
  <br> 
  <div></div> 
  <div>
   Follow us on Facebook, Twitter, LinkedIn, and Instagram.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>Be Aware of Fraudulent Recruiting Activities</b>
  </div> 
  <div>
   If you are interested in a career at Lincoln, we encourage you to review our current openings and apply on our website. Lincoln values the privacy and security of every applicant and urges all applicants to diligently protect their sensitive personal information from scams targeting job seekers. These scams can take many forms including fake employment applications, bogus interviews and falsified offer letters.
  </div> 
  <div>
   Lincoln will not ask applicants to provide their social security numbers, date of birth, bank account information or other sensitive information in job applications. Additionally, our recruiters do not communicate with applicants through free e-mail accounts (Gmail, Yahoo, Hotmail) or conduct interviews utilizing video chat rooms. We will never ask applicants to provide payment during the hiring process or extend an offer without conducting a phone, live video or in-person interview. Please contact Lincoln&apos;s fraud team at fraudhotline@lfg.com if you encounter a recruiter or see a job opportunity that seems suspicious.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>Additional Information</b>
  </div> 
  <div>
   This position may be subject to Lincoln&#x2019;s Political Contribution Policy. An offer of employment may be contingent upon disclosing to Lincoln the details of certain political contributions. Lincoln may decline to extend an offer or terminate employment for this role if it determines political contributions made could have an adverse impact on Lincoln&#x2019;s current or future business interests, misrepresentations were made, or for failure to fully disclose applicable political contributions and or fundraising activities.
  </div>
  <br> 
  <div></div> 
  <div>
   Any unsolicited resumes/candidate profiles submitted through our web site or to personal e-mail accounts of employees of Lincoln Financial Group are considered property of Lincoln Financial Group and are not subject to payment of agency fees.
  </div>
  <br> 
  <div></div> 
  <div>
   Lincoln Financial Group (&#x201c;LFG&#x201d;) is an Equal Opportunity employer and, as such, is committed in policy and practice to recruit, hire, compensate, train and promote, in all job classifications, without regard to race, color, religion, sex (including pregnancy), age, national origin, disability, sexual orientation, gender identity and expression, Veteran status, or genetic information. Applicants are evaluated on the basis of job qualifications. If you are a person with a disability that impedes your ability to express your interest for a position through our online application process, or require TTY/TDD assistance, contact us by calling 260-455-2558.
  </div>
 </div>
</div>
<p></p>",https://jobs.lincolnfinancial.com/job/Radnor-Sr_-ETL-Data-Software-Engineer-%28Remote%29-PA/1073013100/?feedId=176100&utm_source=Indeed&utm_campaign=LFG_Indeed,7dba33f9aebd660e,,Internship,,,"Radnor, PA",Sr. ETL Data Software Engineer (Remote),11 days ago,2023-10-07T13:38:26.138Z,3.5,1273.0,"$102,301 - $140,000 a year",2023-10-18T13:38:26.141Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=7dba33f9aebd660e&from=jasx&tk=1hd1g40sjjm7l801&vjs=3
96,Compact Information Systems LLC,"Description: 
  About Deep Sync
  Our parent company, Compact Information Systems LLC, is considered a pioneer of the data industry and was originally founded in 1988 as a mailing list company for direct marketers and print shops. Thirty-five years later, and combining the strength of our sister brands – AccuData Integrated Marketing, AlumniFinder, ASL Marketing, College Bound Selection Service (CBSS), Deep Sync Labs and HomeData – we have grown to become some of the foremost data suppliers in the U.S.
  Today, we are Deep Sync. A company that powers agencies and brands with unmatched audience insights, unsurpassed reach, and unrivaled expertise by combining the industry’s most comprehensive data with easy-to-activate solutions. We provide billions of privacy-first data connections annually to thousands of customers. Learn more about us here.
  Position Overview
  Position Overview: We are looking for a senior level Data Engineer with a strong background in data engineering and a solid understanding of data science principles. The ideal candidate will play a critical role in designing, developing, and maintaining our data infrastructure, while also adding expertise to enable advanced analytics and machine learning initiatives.
  Key Responsibilities:
 
   Data Pipeline Development: 
   
    Design, implement, and maintain scalable data pipelines to collect, process, and store data from various sources. 
    Ensure data quality, accuracy, and consistency throughout the pipeline. 
   
  Data Modeling: 
   
    Design and implement data models for predictive analytics, machine learning, and data exploration. 
    Optimize data structures and storage to support efficient querying and analysis. 
   
  Data Integration: 
   
    Work closely with cross-functional teams to integrate data from diverse sources, including databases, APIs, and external data providers. 
    Develop and maintain ETL processes to transform and enrich raw data into actionable insights. 
   
  Performance Tuning: 
   
    Monitor and optimize the performance of data pipelines and databases to meet business requirements. 
    Identify and resolve bottlenecks and performance issues. 
   
  Continuous Learning and mentoring: 
   
    Stay up-to-date with the latest advancements in data engineering and data science technologies. 
    Share knowledge and mentor junior team members. 
   
 Requirements: 
  Requirements :
 
   5+ years experience in SQL Query Design, SQL Performance Tuning and Query Optimization
   5+ years of relevant experience in Data Warehouse Design,Data Warehouse Technical Architectures, Development and Implementation
   5+ years of relevant experience in ETL Development, ETL Implementation, Unit Testing, Troubleshooting and Support of ETL Processes
   2+ years of relevant experience with the application of Data Science principles and data modeling.
 
  Knowledge and Skills:
 
   Proficiency in SQL Query Design and Implementation
   Strong Experience with Relational Data Warehouse Systems 
   
    Data Warehouse Management Systems 
    Optimization by Indexing, Partitioning and Denormalization 
   
  Strong Ability to build and optimize data sets, ‘big data’ data pipelines and architecture
   Knowledge of data science concepts, machine learning algorithms, and statistical analysis.
   Programming skills in languages such as Python, Java, or C# required.
   Strong analytical and problem-solving skills
 
  Location:
 
   Position may be located in Redmond, Washington, we will consider remote candidates.
 
  Salary:
 
   The annualized salary range for this senior role is $130,000 - $150,000, commensurate with experience and expertise.","<div>
 Description: 
 <p><b> About Deep Sync</b></p>
 <p> Our parent company, Compact Information Systems LLC, is considered a pioneer of the data industry and was originally founded in 1988 as a mailing list company for direct marketers and print shops. Thirty-five years later, and combining the strength of our sister brands &#x2013; AccuData Integrated Marketing, AlumniFinder, ASL Marketing, College Bound Selection Service (CBSS), Deep Sync Labs and HomeData &#x2013; we have grown to become some of the foremost data suppliers in the U.S.</p>
 <p> Today, we are Deep Sync. A company that powers agencies and brands with unmatched audience insights, unsurpassed reach, and unrivaled expertise by combining the industry&#x2019;s most comprehensive data with easy-to-activate solutions. We provide billions of privacy-first data connections annually to thousands of customers. Learn more about us here.</p>
 <p><b> Position Overview</b></p>
 <p> Position Overview: We are looking for a senior level Data Engineer with a strong background in data engineering and a solid understanding of data science principles. The ideal candidate will play a critical role in designing, developing, and maintaining our data infrastructure, while also adding expertise to enable advanced analytics and machine learning initiatives.</p>
 <p><b> Key Responsibilities:</b></p>
 <ul>
  <li> Data Pipeline Development: 
   <ul>
    <li>Design, implement, and maintain scalable data pipelines to collect, process, and store data from various sources.</li> 
    <li>Ensure data quality, accuracy, and consistency throughout the pipeline.</li> 
   </ul></li>
  <li>Data Modeling: 
   <ul>
    <li>Design and implement data models for predictive analytics, machine learning, and data exploration.</li> 
    <li>Optimize data structures and storage to support efficient querying and analysis.</li> 
   </ul></li>
  <li>Data Integration: 
   <ul>
    <li>Work closely with cross-functional teams to integrate data from diverse sources, including databases, APIs, and external data providers.</li> 
    <li>Develop and maintain ETL processes to transform and enrich raw data into actionable insights.</li> 
   </ul></li>
  <li>Performance Tuning: 
   <ul>
    <li>Monitor and optimize the performance of data pipelines and databases to meet business requirements.</li> 
    <li>Identify and resolve bottlenecks and performance issues.</li> 
   </ul></li>
  <li>Continuous Learning and mentoring: 
   <ul>
    <li>Stay up-to-date with the latest advancements in data engineering and data science technologies.</li> 
    <li>Share knowledge and mentor junior team members.</li> 
   </ul></li>
 </ul>Requirements: 
 <p><b> Requirements :</b></p>
 <ul>
  <li> 5+ years experience in SQL Query Design, SQL Performance Tuning and Query Optimization</li>
  <li> 5+ years of relevant experience in Data Warehouse Design,Data Warehouse Technical Architectures, Development and Implementation</li>
  <li> 5+ years of relevant experience in ETL Development, ETL Implementation, Unit Testing, Troubleshooting and Support of ETL Processes</li>
  <li> 2+ years of relevant experience with the application of Data Science principles and data modeling.</li>
 </ul>
 <p><b> Knowledge and Skills:</b></p>
 <ul>
  <li> Proficiency in SQL Query Design and Implementation</li>
  <li> Strong Experience with Relational Data Warehouse Systems 
   <ul>
    <li>Data Warehouse Management Systems</li> 
    <li>Optimization by Indexing, Partitioning and Denormalization</li> 
   </ul></li>
  <li>Strong Ability to build and optimize data sets, &#x2018;big data&#x2019; data pipelines and architecture</li>
  <li> Knowledge of data science concepts, machine learning algorithms, and statistical analysis.</li>
  <li> Programming skills in languages such as Python, Java, or C# required.</li>
  <li> Strong analytical and problem-solving skills</li>
 </ul>
 <p><b> Location:</b></p>
 <ul>
  <li> Position may be located in Redmond, Washington, we will consider remote candidates.</li>
 </ul>
 <p><b> Salary:</b></p>
 <ul>
  <li> The annualized salary range for this senior role is &#x24;130,000 - &#x24;150,000, commensurate with experience and expertise.</li>
 </ul>
</div>",https://recruiting.paylocity.com/recruiting/jobs/Details/2002466/Compact-Information-Systems-LLC/Data-EngineerData-Scientist?source=Indeed_Feed,e6243931b07ddf9f,,,,,"7120 185th Ave NE, Redmond, WA 98052",Data Engineer/Data Scientist,8 days ago,2023-10-10T13:38:31.763Z,,,"$130,000 - $150,000 a year",2023-10-18T13:38:31.841Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e6243931b07ddf9f&from=jasx&tk=1hd1g3s5pjfnq801&vjs=3
97,Murmuration,"Who We Are
  Murmuration is a nonprofit organization focused on leveraging civic engagement to drive greater equity. We provide sophisticated tools, data, strategic guidance, and programmatic support to help our partner organizations increase civic engagement and marshal support to drive change at the community level. Our best-in-class data and easy-to-use tools have been used by hundreds of organizations to make informed decisions about who they need to reach and how to achieve and sustain impact – and to put those decisions into action. 
 Note: At Murmuration, we are committed to becoming an even more diverse, equitable, and inclusive workplace. To this end, all staff members are expected to actively participate in DEI (diversity, equity, inclusion) programming. 
 About the Position
  We are looking for an innovative Data Engineer who will build and support key components within our data infrastructure with a specific focus on the data pipelines that power our products. This individual will work within our Data Engineering team, partnering with Data Managers and Data Scientists to manage the ongoing delivery of our key data sets for our analytical and product use cases. This individual must be able to understand data requirements and will also be responsible for providing continuous refinement and improvements to our data pipelines. The Data Team is a highly collaborative, friendly, and hard-working group, and we are looking for team members who embody those values. 
 The Data Engineer will report to our Senior Data Engineer. 
 What You’ll Do:
  
  Design, develop, and maintain data pipelines using tools and technologies, such as Dagster and Airflow for orchestration, and Snowflake, AWS, and MongoDB for datastores; 
  Ensure pipelines are scalable, reliable, and fault-tolerant; 
  Be responsible for managing data from various sources, such as third party data providers, data collected, or data created internally; 
  Ensure data is ingested in a timely and efficient manner, with processes to manage data quality and integrity; 
  Transform and cleanse raw data into a structured and usable format; 
  Implement monitoring and alerting processes to detect, communicate, and address issues in data pipelines; 
  Implement data quality checks and validation processes to ensure data accuracy, completeness, and consistency; 
  Continuously optimize data pipelines for better performance and cost efficiency; 
  Maintain comprehensive and up-to-date documentation for data pipelines, including data lineage, dependencies, and configurations; 
  Ensure documentation is up-to-date and accessible to team members; 
  Provide support for data-related issues, including investigating and resolving pipeline failures; 
  Respond to ad-hoc data requests and troubleshoot data-related problems; 
  Collaborate with data scientists, analysts, and other stakeholders to understand their data requirements and deliver data in a usable format; and 
  Work closely with other data engineers to align data pipelines with overall data architecture strategies. 
 
 Requirements
  What You Should Have:
  
  Education and/or experience in Computer Science, Computer Engineering, or relevant field; 
  A minimum of 3 years’ experience working with large scale databases/cloud databases using SQL and Python; 
  Strong organizational and analytical abilities; 
  Strong problem-solving skills; 
  Strong written and verbal communication skills; 
  Familiarity with Data Orchestration Tools (Dagster, Airflow); 
  Familiarity with Snowflake and AWS (primarily S3, EC2, ECS); 
  Experience working flexibly within smaller teams; and 
  Practical knowledge of software development lifecycle (SDLC). 
 
 What You Could Have:
  
  Familiarity with Voter File Data; 
  Experience with or interest in political data; and 
  Experience within a support team providing technical support to other data functions (e.g., Data Scientists, Data Managers, etc.) 
 
 Talented Data Engineers come from all walks of life and careers. If you are passionate about civic engagement and technology, please apply, even if you do not check every box!
  Benefits
  Location and Compensation
  The Data Engineer is a full-time, salaried position with a comprehensive benefits package. It is based anywhere in the U.S. The salary range for this position is $100,000 - $130,000 and is commensurate with experience. 
 Our Culture of Care
  We work hard to create a culture of care to ensure that our staff are best equipped to lead happy, healthy, and balanced lives. To that end, we offer a comprehensive benefits package which includes:
  
  Health, vision, and dental insurance with 100% of premiums covered for you and qualifying family members; 
  Retirement benefits with a 4% employer match; 
  A flexible unlimited PTO plan; 
  Generous paid parental leave; 
  Pre-tax commuter benefits; 
  A company laptop; 
  A flexible remote work environment; 
  A home office setup stipend for all new employees; 
  Monthly reimbursement for remote work expenses; 
  A yearly professional development fund; 
  Mental health and wellness benefits through Calm and Better Help; and 
  Yearly in-person staff retreats; and 
  A welcoming culture that celebrates diversity, equity, and inclusion. 
 
 An Equal-Opportunity Employer with a Commitment to Diversity
  Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.","<div>
 <p><b>Who We Are</b></p>
 <p> Murmuration is a nonprofit organization focused on leveraging civic engagement to drive greater equity. We provide sophisticated tools, data, strategic guidance, and programmatic support to help our partner organizations increase civic engagement and marshal support to drive change at the community level. Our best-in-class data and easy-to-use tools have been used by hundreds of organizations to make informed decisions about who they need to reach and how to achieve and sustain impact &#x2013; and to put those decisions into action.</p> 
 <p>Note: At Murmuration, we are committed to becoming an even more diverse, equitable, and inclusive workplace. To this end, all staff members are expected to actively participate in DEI (diversity, equity, inclusion) programming.</p> 
 <p><b>About the Position</b></p>
 <p> We are looking for an innovative Data Engineer who will build and support key components within our data infrastructure with a specific focus on the data pipelines that power our products. This individual will work within our Data Engineering team, partnering with Data Managers and Data Scientists to manage the ongoing delivery of our key data sets for our analytical and product use cases. This individual must be able to understand data requirements and will also be responsible for providing continuous refinement and improvements to our data pipelines. The Data Team is a highly collaborative, friendly, and hard-working group, and we are looking for team members who embody those values. </p>
 <p>The Data Engineer will report to our Senior Data Engineer.</p> 
 <p><b>What You&#x2019;ll Do:</b></p>
 <ul> 
  <li>Design, develop, and maintain data pipelines using tools and technologies, such as Dagster and Airflow for orchestration, and Snowflake, AWS, and MongoDB for datastores;</li> 
  <li>Ensure pipelines are scalable, reliable, and fault-tolerant;</li> 
  <li>Be responsible for managing data from various sources, such as third party data providers, data collected, or data created internally;</li> 
  <li>Ensure data is ingested in a timely and efficient manner, with processes to manage data quality and integrity;</li> 
  <li>Transform and cleanse raw data into a structured and usable format;</li> 
  <li>Implement monitoring and alerting processes to detect, communicate, and address issues in data pipelines;</li> 
  <li>Implement data quality checks and validation processes to ensure data accuracy, completeness, and consistency;</li> 
  <li>Continuously optimize data pipelines for better performance and cost efficiency;</li> 
  <li>Maintain comprehensive and up-to-date documentation for data pipelines, including data lineage, dependencies, and configurations;</li> 
  <li>Ensure documentation is up-to-date and accessible to team members;</li> 
  <li>Provide support for data-related issues, including investigating and resolving pipeline failures;</li> 
  <li>Respond to ad-hoc data requests and troubleshoot data-related problems;</li> 
  <li>Collaborate with data scientists, analysts, and other stakeholders to understand their data requirements and deliver data in a usable format; and</li> 
  <li>Work closely with other data engineers to align data pipelines with overall data architecture strategies.</li> 
 </ul>
 <p><b>Requirements</b></p>
 <p><b> What You </b><b><i>Should</i></b><b> Have:</b></p>
 <ul> 
  <li>Education and/or experience in Computer Science, Computer Engineering, or relevant field;</li> 
  <li>A minimum of 3 years&#x2019; experience working with large scale databases/cloud databases using SQL and Python;</li> 
  <li>Strong organizational and analytical abilities;</li> 
  <li>Strong problem-solving skills;</li> 
  <li>Strong written and verbal communication skills;</li> 
  <li>Familiarity with Data Orchestration Tools (Dagster, Airflow);</li> 
  <li>Familiarity with Snowflake and AWS (primarily S3, EC2, ECS);</li> 
  <li>Experience working flexibly within smaller teams; and</li> 
  <li>Practical knowledge of software development lifecycle (SDLC).</li> 
 </ul>
 <p><b>What You </b><b><i>Could</i></b><b> Have:</b></p>
 <ul> 
  <li>Familiarity with Voter File Data;</li> 
  <li>Experience with or interest in political data; and</li> 
  <li>Experience within a support team providing technical support to other data functions (e.g., Data Scientists, Data Managers, etc.)</li> 
 </ul>
 <p>Talented Data Engineers come from all walks of life and careers. If you are passionate about civic engagement and technology, please apply, even if you do not check every box!</p>
 <p><b> Benefits</b></p>
 <p><b> Location and Compensation</b></p>
 <p> The Data Engineer is a full-time, salaried position with a comprehensive benefits package. It is based anywhere in the U.S. The salary range for this position is &#x24;100,000 - &#x24;130,000 and is commensurate with experience.</p> 
 <p><b>Our Culture of Care</b></p>
 <p> We work hard to create a culture of care to ensure that our staff are best equipped to lead happy, healthy, and balanced lives. To that end, we offer a comprehensive benefits package which includes:</p>
 <ul> 
  <li>Health, vision, and dental insurance with 100% of premiums covered for you and qualifying family members;</li> 
  <li>Retirement benefits with a 4% employer match;</li> 
  <li>A flexible unlimited PTO plan;</li> 
  <li>Generous paid parental leave;</li> 
  <li>Pre-tax commuter benefits;</li> 
  <li>A company laptop;</li> 
  <li>A flexible remote work environment;</li> 
  <li>A home office setup stipend for all new employees;</li> 
  <li>Monthly reimbursement for remote work expenses;</li> 
  <li>A yearly professional development fund;</li> 
  <li>Mental health and wellness benefits through Calm and Better Help; and</li> 
  <li>Yearly in-person staff retreats; and</li> 
  <li>A welcoming culture that celebrates diversity, equity, and inclusion.</li> 
 </ul>
 <p><b>An Equal-Opportunity Employer with a Commitment to Diversity</b></p>
 <p> Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.</p>
</div>",https://apply.workable.com/murmuration/j/1CEFDBC383,1f74a01d7a395286,,Full-time,,,Remote,Data Engineer,11 days ago,2023-10-07T13:38:43.155Z,,,"$100,000 - $130,000 a year",2023-10-18T13:38:43.160Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=1f74a01d7a395286&from=jasx&tk=1hd1g3s5pjfnq801&vjs=3
98,Ascension,"Details
 
   
 
 
  Department: Data Delivery Governance
   Schedule: Full time
   Location: Remote
 
  Benefits 
 
  Paid time off (PTO)
   Various health insurance options & wellness plans
   Retirement benefits including employer match plans
   Long-term & short-term disability
   Employee assistance programs (EAP)
   Parental leave & adoption assistance
   Tuition reimbursement
   Ways to give back to your community
 
 
 
   As a military friendly organization, Ascension promotes career flexibility and offers many benefits to help support the well-being of our military families, spouses, veterans and reservists. Our associates are empowered to apply their military experience and unique perspective to their civilian career with Ascension.
 
 
  
    Please note, benefits and benefits eligibility can vary by position, exclusions may apply for some roles (for example: PRN, Short-Term Option, etc.). Connect with your Talent Advisor today for additional specifics.
  
 
  Responsibilities 
 
  Responsible for construction and development of ""large-scale cloud data processing systems"" The Data Engineer must have considerable expertise in data warehousing and the job requires proven coding expertise with Python, Java, SQL, and Spark languages. Must be able to implement enterprise cloud data architecture designs, and will work closely with the rest of the scrum team and internal business partners to identify, evaluate, design, and implement large scale data solutions, structured and unstructured, public and proprietary data. The Data Engineer will work iteratively on the cloud platform to design, develop and implement scalable, high performance solutions that offer measurable business value to customers.
 
  
 
 
   Proficient in multiple programming languages, frameworks, domains, and tools.
   Coding skills in Scala
   Experience with gcp platform development tools Pub/sub, cloud storage, big table, big query, data flow, data proc, and composer desired.
   Strong Linux/Unix background and hands on knowledge.
   Knowledge in Hadoop and cloud platforms and surrounding ecosystems.
   Experience with web services and APIs as in RESTful and SOAP.
   Ability to document designs and concepts
   API Orchestration and Choreography for consumer apps
   Well rounded technical expertise in Apache packages and Hybrid cloud architectures
   Pipeline creation and automation for Data Acquisition
   Metadata extraction pipeline design and creation between raw and finally transformed datasets
   Quality control metrics data collection on data acquisition pipelines
   Able to collaborate with scrum team including scrum master, product owner, data analysts, Quality Assurance, business owners, and data architecture to produce the best possible end products
   Experience contributing to and leveraging jira and confluence.
   Strong experience working with real time streaming applications and batch style large scale distributed computing applications using tools like Spark, Kafka, Flume, pubsub, and airflow.
   Ability to work with different file formats like Avro, Parquet, and JSON.
   Managing and scheduling batch jobs.
   Hands on experience in Analysis, Design, Coding and Testing phases of Software Development Life Cycle (SDLC).
 
  Requirements 
 
  Education:
 
 
   High School diploma equivalency with 2 years of cumulative experience OR Associate's degree/Bachelor's degree with 1 year of experience OR 5 years of applicable cumulative job specific experience required. 2 years of leadership or management experience preferred.
 
  Additional Preferences 
 
  5+ years of experience in an engineering role using Python, Java, Spark, and SQL.
   Some of the minimum experience requirement may be met with Masters or other advanced degree
   Cloud Experience Required
   Coding experience with Python, Java, Spark, and SQL
   Strong Linux/Unix background and hands on knowledge.
   Past experience with big data technologies including HDFS, Spark, Impala, Hive,
   Experience with Shell scripting and bash.
   Experience with version control platform github
   Experience unit testing code.
   Experience with development ecosystem including Jenkins, Artifactory, CI/CD, and Terraform.
   Works on problems of diverse scope and complexity ranging from moderate to substantial
   Assists senior professionals in determining methods and procedures for new tasks
   Leads basic or moderately complex projects/activities on semi-regular basis
   Must possess excellent written and verbal communication skills
   Ability to understand and analyze complex data sets
   Exercises independent judgment on basic or moderately complex issues regarding job and related tasks
   Makes recommendations to management on new processes, tools and techniques, or development of new products and services
   Makes decisions regarding daily priorities for a work group; provides guidance to and/or assists staff on non-routine or escalated issues
   Decisions have a moderate impact on operations within a department
   Works under minimal supervision, uses independent judgment requiring analysis of variable factors
   Requires little instruction on day-to-day work and general direction on more complex tasks and projects
   Collaborates with senior professionals in the development of methods, techniques and analytical approach
   Ability to advise management on approaches to optimize for data platform success.
   Able to effectively communicate highly technical information to numerous audiences, including management, the user community, and less-experienced staff.
   Consistently communicate on status of project deliverables
   Consistently provide work effort estimates to management to assist in setting priorities
   Deliver timely work in accordance with estimates
   Solve problems as they arise and communicate potential roadblocks to manage expectations
   Adhere strictly to all security policies
 
 
 
   #LI-Remote #AscensionTechnologies
 
  Why Join Our Team 
 
  When you join Ascension, you join a team of over 150,000 individuals across the country committed to a Mission of serving others and providing compassionate, personalized care to all. Our inclusive culture, continuing education programs, career coaches and benefit offerings are just a few of the resources and tools that team members can use to create a rewarding career path. In fact, Ascension spent nearly $46 million in tuition assistance alone to support associate growth and development. If you are looking for a career where you can grow and make a difference in your community, we invite you to join our team today.
 
  Equal Employment Opportunity Employer 
 
  Ascension will provide equal employment opportunities (EEO) to all associates and applicants for employment regardless of race, color, religion, national origin, citizenship, gender, sexual orientation, gender identification or expression, age, disability, marital status, amnesty, genetic information, carrier status or any other legally protected status or status as a covered veteran in accordance with applicable federal, state and local laws.
 
 
 
   For further information, view the EEO Know Your Rights (English) poster or EEO Know Your Rights (Spanish) poster.
 
 
 
   Pay Non-Discrimination Notice
 
 
 
   Please note that Ascension will make an offer of employment only to individuals who have applied for a position using our official application. Be on alert for possible fraudulent offers of employment. Ascension will not solicit money or banking information from applicants.
 
  E-Verify Statement 
 
  This employer participates in the Electronic Employment Verification Program. Please click the E-Verify link below for more information.
   
   E-Verify","<div>
 <h3 class=""jobSectionHeader""><b>Details</b></h3>
 <div>
  <br> 
 </div>
 <ul>
  <li><b>Department: </b>Data Delivery Governance</li>
  <li><b> Schedule: </b>Full time</li>
  <li><b> Location: </b>Remote</li>
 </ul>
 <h3 class=""jobSectionHeader""><b><br> Benefits </b></h3>
 <div>
  Paid time off (PTO)
  <br> Various health insurance options &amp; wellness plans
  <br> Retirement benefits including employer match plans
  <br> Long-term &amp; short-term disability
  <br> Employee assistance programs (EAP)
  <br> Parental leave &amp; adoption assistance
  <br> Tuition reimbursement
  <br> Ways to give back to your community
 </div>
 <div></div>
 <div>
  <br> As a military friendly organization, Ascension promotes career flexibility and offers many benefits to help support the well-being of our military families, spouses, veterans and reservists. Our associates are empowered to apply their military experience and unique perspective to their civilian career with Ascension.
 </div>
 <div>
  <ul>
   <li><br> <i>Please note, benefits and benefits eligibility can vary by position, exclusions may apply for some roles (for example: PRN, Short-Term Option, etc.). Connect with your Talent Advisor today for additional specifics.</i></li>
  </ul>
 </div>
 <h3 class=""jobSectionHeader""><b><br> Responsibilities </b></h3>
 <div>
  Responsible for construction and development of &quot;large-scale cloud data processing systems&quot; The Data Engineer must have considerable expertise in data warehousing and the job requires proven coding expertise with Python, Java, SQL, and Spark languages. Must be able to implement enterprise cloud data architecture designs, and will work closely with the rest of the scrum team and internal business partners to identify, evaluate, design, and implement large scale data solutions, structured and unstructured, public and proprietary data. The Data Engineer will work iteratively on the cloud platform to design, develop and implement scalable, high performance solutions that offer measurable business value to customers.
 </div>
 <br> 
 <div></div>
 <ul>
  <li> Proficient in multiple programming languages, frameworks, domains, and tools.</li>
  <li> Coding skills in Scala</li>
  <li> Experience with gcp platform development tools Pub/sub, cloud storage, big table, big query, data flow, data proc, and composer desired.</li>
  <li> Strong Linux/Unix background and hands on knowledge.</li>
  <li> Knowledge in Hadoop and cloud platforms and surrounding ecosystems.</li>
  <li> Experience with web services and APIs as in RESTful and SOAP.</li>
  <li> Ability to document designs and concepts</li>
  <li> API Orchestration and Choreography for consumer apps</li>
  <li> Well rounded technical expertise in Apache packages and Hybrid cloud architectures</li>
  <li> Pipeline creation and automation for Data Acquisition</li>
  <li> Metadata extraction pipeline design and creation between raw and finally transformed datasets</li>
  <li> Quality control metrics data collection on data acquisition pipelines</li>
  <li> Able to collaborate with scrum team including scrum master, product owner, data analysts, Quality Assurance, business owners, and data architecture to produce the best possible end products</li>
  <li> Experience contributing to and leveraging jira and confluence.</li>
  <li> Strong experience working with real time streaming applications and batch style large scale distributed computing applications using tools like Spark, Kafka, Flume, pubsub, and airflow.</li>
  <li> Ability to work with different file formats like Avro, Parquet, and JSON.</li>
  <li> Managing and scheduling batch jobs.</li>
  <li> Hands on experience in Analysis, Design, Coding and Testing phases of Software Development Life Cycle (SDLC).</li>
 </ul>
 <h3 class=""jobSectionHeader""><b><br> Requirements </b></h3>
 <div>
  Education:
 </div>
 <ul>
  <li> High School diploma equivalency with 2 years of cumulative experience OR Associate&apos;s degree/Bachelor&apos;s degree with 1 year of experience OR 5 years of applicable cumulative job specific experience required. 2 years of leadership or management experience preferred.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b><br> Additional Preferences </b></h3>
 <ul>
  <li>5+ years of experience in an engineering role using Python, Java, Spark, and SQL.</li>
  <li> Some of the minimum experience requirement may be met with Masters or other advanced degree</li>
  <li> Cloud Experience Required</li>
  <li> Coding experience with Python, Java, Spark, and SQL</li>
  <li> Strong Linux/Unix background and hands on knowledge.</li>
  <li> Past experience with big data technologies including HDFS, Spark, Impala, Hive,</li>
  <li> Experience with Shell scripting and bash.</li>
  <li> Experience with version control platform github</li>
  <li> Experience unit testing code.</li>
  <li> Experience with development ecosystem including Jenkins, Artifactory, CI/CD, and Terraform.</li>
  <li> Works on problems of diverse scope and complexity ranging from moderate to substantial</li>
  <li> Assists senior professionals in determining methods and procedures for new tasks</li>
  <li> Leads basic or moderately complex projects/activities on semi-regular basis</li>
  <li> Must possess excellent written and verbal communication skills</li>
  <li> Ability to understand and analyze complex data sets</li>
  <li> Exercises independent judgment on basic or moderately complex issues regarding job and related tasks</li>
  <li> Makes recommendations to management on new processes, tools and techniques, or development of new products and services</li>
  <li> Makes decisions regarding daily priorities for a work group; provides guidance to and/or assists staff on non-routine or escalated issues</li>
  <li> Decisions have a moderate impact on operations within a department</li>
  <li> Works under minimal supervision, uses independent judgment requiring analysis of variable factors</li>
  <li> Requires little instruction on day-to-day work and general direction on more complex tasks and projects</li>
  <li> Collaborates with senior professionals in the development of methods, techniques and analytical approach</li>
  <li> Ability to advise management on approaches to optimize for data platform success.</li>
  <li> Able to effectively communicate highly technical information to numerous audiences, including management, the user community, and less-experienced staff.</li>
  <li> Consistently communicate on status of project deliverables</li>
  <li> Consistently provide work effort estimates to management to assist in setting priorities</li>
  <li> Deliver timely work in accordance with estimates</li>
  <li> Solve problems as they arise and communicate potential roadblocks to manage expectations</li>
  <li> Adhere strictly to all security policies</li>
 </ul>
 <div></div>
 <div>
  <br> #LI-Remote #AscensionTechnologies
 </div>
 <h3 class=""jobSectionHeader""><b><br> Why Join Our Team </b></h3>
 <div>
  When you join Ascension, you join a team of over 150,000 individuals across the country committed to a Mission of serving others and providing compassionate, personalized care to all. Our inclusive culture, continuing education programs, career coaches and benefit offerings are just a few of the resources and tools that team members can use to create a rewarding career path. In fact, Ascension spent nearly &#x24;46 million in tuition assistance alone to support associate growth and development. If you are looking for a career where you can grow and make a difference in your community, we invite you to join our team today.
 </div>
 <h3 class=""jobSectionHeader""><b><br> Equal Employment Opportunity Employer </b></h3>
 <div>
  Ascension will provide equal employment opportunities (EEO) to all associates and applicants for employment regardless of race, color, religion, national origin, citizenship, gender, sexual orientation, gender identification or expression, age, disability, marital status, amnesty, genetic information, carrier status or any other legally protected status or status as a covered veteran in accordance with applicable federal, state and local laws.
 </div>
 <div></div>
 <div>
  <br> For further information, view the EEO Know Your Rights (English) poster or EEO Know Your Rights (Spanish) poster.
 </div>
 <div></div>
 <div>
  <br> Pay Non-Discrimination Notice
 </div>
 <div></div>
 <div>
  <br> Please note that Ascension will make an offer of employment only to individuals who have applied for a position using our official application. Be on alert for possible fraudulent offers of employment. Ascension will not solicit money or banking information from applicants.
 </div>
 <h3 class=""jobSectionHeader""><b><br> E-Verify Statement </b></h3>
 <div>
  This employer participates in the Electronic Employment Verification Program. Please click the E-Verify link below for more information.
  <br> 
  <br> E-Verify
 </div>
</div>",https://www.indeed.com/rc/clk?jk=3203987de4a37c72&atk=&xpse=SoBU67I3JhoYWyzCmh0LbzkdCdPP,3203987de4a37c72,,Full-time,,,"Irving, TX 75063",Data Engineer - Sr. Specialist,12 days ago,2023-10-06T13:39:05.134Z,3.6,8458.0,"From $97,677 a year",2023-10-18T13:39:05.135Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=3203987de4a37c72&from=jasx&tk=1hd1g5e7qk7bn800&vjs=3
99,"Stark Dev, LLC","This position is open for United States Citizens/ GC/GC EAD/H4 EAD/TN VISA HOLDERS.
Must be a TEKsystems former.
Duration
12
Duration Unit
Month(s)
W2 Contract
We are looking for TEKsystems formers here with 15+ years of experience. Azure Datafactory, Azure Databricks, PySpark, Python are must haves.
Req Description
Job Title
Sr. Data Engineer
Top Skills Details
1) 8+ years of Data Engineering, must be strong with Enterprise Data Warehousing Concepts2) Pyspark, Python3) Azure Data Factory (ADF) and Databricks4) Agile/Scrum5) Must have a consulting mindset, be intellectually curious, ask the right questions, proactive in their approach and communication
DescriptionPosition: Senior Data EngineerOpen Positions: 1Placement Type: 12 month contract to startWorksite Location: 100% Remote
Job Type: Contract
Pay: From $75.00 per hour
Experience level:

 10 years

Schedule:

 Monday to Friday

Experience:

 Data Engineering: 10 years (Required)
 Enterprise Data Warehousing Concepts: 8 years (Required)
 Pyspark, Python: 8 years (Required)
 Azure Data Factory (ADF) and Databricks: 8 years (Required)
 Agile/Scrum: 8 years (Required)
 Snowflake: 3 years (Preferred)
 Kafka: 2 years (Preferred)
 complex SQL queries: 5 years (Required)
 Elastic Search: 2 years (Preferred)
 TEKsystems: 2 years (Required)
 Azure Data factory/ Azure Databricks: 8 years (Required)

Work Location: Remote","<p>This position is open for <b>United States Citizens/ GC/GC EAD/H4 EAD/TN VISA HOLDERS.</b></p>
<p><b>Must be a TEKsystems former.</b></p>
<p><b>Duration</b></p>
<p><b>12</b></p>
<p><b>Duration Unit</b></p>
<p><b>Month(s)</b></p>
<p><b>W2 Contract</b></p>
<p><b>We are looking for TEKsystems formers here with 15+ years of experience. Azure Datafactory, Azure Databricks, PySpark, Python are must haves.</b></p>
<p><b>Req Description</b></p>
<p><b>Job Title</b></p>
<p><b>Sr. Data Engineer</b></p>
<p><b>Top Skills Details</b></p>
<p><b>1) 8+ years of Data Engineering, must be strong with Enterprise Data Warehousing Concepts</b><br><b>2) Pyspark, Python</b><br><b>3) Azure Data Factory (ADF) and Databricks</b><br><b>4) Agile/Scrum</b><br><b>5) Must have a consulting mindset, be intellectually curious, ask the right questions, proactive in their approach and communication</b></p>
<p><b>Description</b><br><b>Position: Senior Data Engineer</b><br><b>Open Positions: 1</b><br><b>Placement Type: 12 month contract to start</b><br><b>Worksite Location: 100% Remote</b></p>
<p>Job Type: Contract</p>
<p>Pay: From &#x24;75.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Engineering: 10 years (Required)</li>
 <li>Enterprise Data Warehousing Concepts: 8 years (Required)</li>
 <li>Pyspark, Python: 8 years (Required)</li>
 <li>Azure Data Factory (ADF) and Databricks: 8 years (Required)</li>
 <li>Agile/Scrum: 8 years (Required)</li>
 <li>Snowflake: 3 years (Preferred)</li>
 <li>Kafka: 2 years (Preferred)</li>
 <li>complex SQL queries: 5 years (Required)</li>
 <li>Elastic Search: 2 years (Preferred)</li>
 <li>TEKsystems: 2 years (Required)</li>
 <li>Azure Data factory/ Azure Databricks: 8 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,117f6275ba0a1edd,,Contract,,,Remote,Sr. Data Engineer / W2,18 days ago,2023-09-30T13:39:16.161Z,,,From $75 an hour,2023-10-18T13:39:16.164Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=117f6275ba0a1edd&from=jasx&tk=1hd1g5htglell801&vjs=3
100,CareFirst BlueCross BlueShield,"Resp & Qualifications 
 Purpose: The Lead Data Engineer/Analyst is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on leading the development of solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company.  Essential Functions:
 
   Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability.
   Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform.
   Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, De-normalized Data Structures, OLAP, and Data Warehousing concepts.
   Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ELT/ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Enforces the implementation of best practices for data auditing, scalability, reliability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
   Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
 
  Qualifications  Education: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field. In lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.
  Experience: 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance.
  Preferred Qualifications: Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.
  Salary Range: $105,408 - $209,352
  Salary Range Disclaimer 
 The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate's work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case's facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).
  Department 
 Department: Finance Data Systems & Decision)
  Equal Employment Opportunity 
 CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
  Where To Apply 
 Please visit our website to apply: www.carefirst.com/careers
  Federal Disc/Physical Demand 
 Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
  PHYSICAL DEMANDS:
  The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
  Sponsorship in US 
 Must be eligible to work in the U.S. without Sponsorship
  #LI-CB1","<div>
 <p><b>Resp &amp; Qualifications</b> </p>
 <p><b>Purpose</b>:<br> The Lead Data Engineer/Analyst is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on leading the development of solutions and helping transform the company&apos;s platforms deliver data-driven, meaningful insights and value to company.<br> <br> <b>Essential Functions</b>:</p>
 <ul>
  <li> Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability.</li>
  <li> Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform.</li>
  <li> Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, De-normalized Data Structures, OLAP, and Data Warehousing concepts.</li>
  <li> Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ELT/ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.</li>
  <li> Enforces the implementation of best practices for data auditing, scalability, reliability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.</li>
  <li> Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.</li>
  <li> Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.</li>
 </ul>
 <p><b><br> Qualifications</b><br> <br> <b>Education:</b> Bachelor&apos;s Degree in Computer Science, Information Technology or Engineering or related field. In lieu of a Bachelor&apos;s degree, an additional 4 years of relevant work experience is required in addition to the required work experience.</p>
 <p><b> Experience:</b> 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance.</p>
 <p><b><br> Preferred Qualifications: </b>Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.</p>
 <p><br> Salary Range: &#x24;105,408 - &#x24;209,352</p>
 <p><b> Salary Range Disclaimer</b> </p>
 <p>The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate&apos;s work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case&apos;s facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).</p>
 <p><b> Department</b> </p>
 <p><b>Department: </b>Finance Data Systems &amp; Decision)</p>
 <p><b> Equal Employment Opportunity</b> </p>
 <p>CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.</p>
 <p><b> Where To Apply</b> </p>
 <p>Please visit our website to apply: www.carefirst.com/careers</p>
 <p><b> Federal Disc/Physical Demand</b> </p>
 <p>Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.</p>
 <p><b> PHYSICAL DEMANDS:</b></p>
 <p> The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.</p>
 <p><b> Sponsorship in US</b> </p>
 <p>Must be eligible to work in the U.S. without Sponsorship</p>
 <p> #LI-CB1</p>
</div>",https://carefirstcareers.ttcportals.com/jobs/13427494-lead-data-engineer-slash-analyst-remote?tm_job=18889-1A&tm_event=view&tm_company=2380,fe0fc5669a01f689,,Full-time,,,"1501 South Clinton Street, Baltimore, MD 21224",Lead Data Engineer / Analyst (Remote),15 days ago,2023-10-03T13:39:18.780Z,3.8,732.0,"$105,408 - $209,352 a year",2023-10-18T13:39:18.786Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=fe0fc5669a01f689&from=jasx&tk=1hd1g5htglell801&vjs=3
101,OCHIN,"Description: 
  Make a difference at OCHIN
  OCHIN provides leading-edge technology, data analytics, research, and support services to nearly 1,000 community health care sites, reaching nearly 6 million patients nationally. We believe that every individual, no matter their race, ethnicity, background, or zip code, should have fair opportunity to achieve their full health potential. Our work addresses differences in health that are systemic, avoidable, and unjust. We partner, learn, innovate, and advocate, in order to close the gap in health for individuals and communities negatively impacted by racism or other structural inequities.
  At OCHIN, we value the unique perspectives and experiences of every individual and work hard to maintain a culture of belonging.
  Founded in Oregon in 2000, OCHIN employs a growing virtual workforce of more than 1,000 diverse professionals, working remotely across 48 states. We offer a generous compensation package and are committed to supporting our employees’ entire well-being by fostering a healthy work-life balance and equitable opportunity for professional advancement. We are curious, collaborative learners who strive to live our values everyday: learning, heart, belonging and impact. OCHIN is excited to support our continued national expansion and the increasing demand for our innovative tools and services by welcoming new talent to our growing team.
  Position Overview
  The Research Data Engineer will provide high-level professional and technical skills in support of designing, building, and maintaining data pipelines, databases, and cloud platforms to support the needs of the OCHIN Research team.
  In this role, you will be collaborating with an innovative, collaborative team of people moving exciting projects forward and working to improve systems and processes along the way.
 
  Essential Duties
 
   Performing day to day management of on-premises, cloud, and hybrid research databases and database platforms including the Research Data Warehouse
   Integrating and transforming health-related data from a variety of sources and formats such as EHRs, geospatial, claims, and census into analyzable formats for research
   Building and maintaining datasets and data marts
   Monitor and maintain data pipelines proactively to ensure high service availability
   In partnership with Research Data Science staff and leadership, assist with scoping and designing new research data pipelines and platforms to optimize research data solutions
   Create scripts and programs to automate data operations
   Preparing and maintaining technical documentation and metadata
   Providing technical/consultative services to internal and external research partners, investigators, and other research personnel
   Performing other duties as requested by the research team
  Requirements: 
 
  A Master’s level degree in Informatics, Computer Science or related discipline. Equivalent knowledge and skills obtained through a combination of education, training, and experience may meet this requirement.
   At least 5 years of experience in database development and administration in a healthcare and/or health research setting
   At least 3 years’ experience with data warehousing, including ETL techniques
   Strong technical proficiency with SQL required
   High technical proficiency with Microsoft SQL Server, including the ability to create and edit complex queries and T-SQL scripts including dynamic SQL, required; experience with SSIS
   Strong working knowledge of standard desktop computing software packages (word processing, spreadsheets, presentation software, Internet browsers, etc.).
   Strong analytical and problem-solving skills
   Experience with cloud and/or hybrid cloud/on-premises database architectures preferred
   Knowledge of specialized and complex statistical modeling and/or machine learning techniques preferred
 
  Base Pay Overview
  The typical offer range for this role is minimum to midpoint, ($98,819 - $128,465) with the midpoint representing the average pay in a national market scope for this position. Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.
  Work Location and Travel Requirements
  The typical offer range for this role is minimum to midpoint, with the midpoint representing the average pay in a national market scope for this position. Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.
 
   Ability to work independently and efficiently from a home office environment
   High Speed Internet Service
   It is a requirement that employees work in a distraction free workplace
 
  We offer a comprehensive range of benefits. See our website for details: https://ochin.org/employment-openings
  Equal Opportunity Statement
  OCHIN is proud to be an equal opportunity employer. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills for the benefit of our staff, our mission, and the communities we serve.
  As an Equal Opportunity and Affirmative Action employer, OCHIN, Inc. does not discriminate on the basis of race, ethnicity, sex, gender identity, sexual orientation, religion, marital or civil union status, age, disability status, veteran status, or any other protected characteristics. All aspects of employment are based on merit, performance, and business needs.
  COVID-19 Vaccination Requirement
  To keep our colleagues, members, and communities safe, OCHIN requires all employees—including remote employees, contractors, interns, and new hires—to be vaccinated with a COVID-19 vaccine, as supported by state and federal public health officials, as a condition of employment. All new hires are required to provide proof of full vaccination or receive approval for a medical or religious exemption before their hire date.","<div>
 Description: 
 <p><b> Make a difference at OCHIN</b></p>
 <p> OCHIN provides leading-edge technology, data analytics, research, and support services to nearly 1,000 community health care sites, reaching nearly 6 million patients nationally. We believe that every individual, no matter their race, ethnicity, background, or zip code, should have fair opportunity to achieve their full health potential. Our work addresses differences in health that are systemic, avoidable, and unjust. We partner, learn, innovate, and advocate, in order to close the gap in health for individuals and communities negatively impacted by racism or other structural inequities.</p>
 <p><b><br> At OCHIN, we value the unique perspectives and experiences of every individual and work hard to maintain a culture of belonging.</b></p>
 <p> Founded in Oregon in 2000, OCHIN employs a growing virtual workforce of more than 1,000 diverse professionals, working remotely across 48 states. We offer a generous compensation package and are committed to supporting our employees&#x2019; entire well-being by fostering a healthy work-life balance and equitable opportunity for professional advancement. We are curious, collaborative learners who strive to live our values everyday: learning, heart, belonging and impact. OCHIN is excited to support our continued national expansion and the increasing demand for our innovative tools and services by welcoming new talent to our growing team.</p>
 <p><b> Position Overview</b></p>
 <p> The <b>Research Data Engineer </b>will provide high-level professional and technical skills in support of designing, building, and maintaining data pipelines, databases, and cloud platforms to support the needs of the OCHIN Research team.</p>
 <p> In this role, you will be collaborating with an innovative, collaborative team of people moving exciting projects forward and working to improve systems and processes along the way.</p>
 <p></p>
 <p><b><br> Essential Duties</b></p>
 <ul>
  <li> Performing day to day management of on-premises, cloud, and hybrid research databases and database platforms including the Research Data Warehouse</li>
  <li> Integrating and transforming health-related data from a variety of sources and formats such as EHRs, geospatial, claims, and census into analyzable formats for research</li>
  <li> Building and maintaining datasets and data marts</li>
  <li> Monitor and maintain data pipelines proactively to ensure high service availability</li>
  <li> In partnership with Research Data Science staff and leadership, assist with scoping and designing new research data pipelines and platforms to optimize research data solutions</li>
  <li> Create scripts and programs to automate data operations</li>
  <li> Preparing and maintaining technical documentation and metadata</li>
  <li> Providing technical/consultative services to internal and external research partners, investigators, and other research personnel</li>
  <li> Performing other duties as requested by the research team</li>
 </ul> Requirements: 
 <ul>
  <li>A Master&#x2019;s level degree in Informatics, Computer Science or related discipline. Equivalent knowledge and skills obtained through a combination of education, training, and experience may meet this requirement.</li>
  <li> At least 5 years of experience in database development and administration in a healthcare and/or health research setting</li>
  <li> At least 3 years&#x2019; experience with data warehousing, including ETL techniques</li>
  <li> Strong technical proficiency with SQL required</li>
  <li> High technical proficiency with Microsoft SQL Server, including the ability to create and edit complex queries and T-SQL scripts including dynamic SQL, required; experience with SSIS</li>
  <li> Strong working knowledge of standard desktop computing software packages (word processing, spreadsheets, presentation software, Internet browsers, etc.).</li>
  <li> Strong analytical and problem-solving skills</li>
  <li> Experience with cloud and/or hybrid cloud/on-premises database architectures preferred</li>
  <li> Knowledge of specialized and complex statistical modeling and/or machine learning techniques preferred</li>
 </ul>
 <p><b> Base Pay Overview</b></p>
 <p><b> The typical offer range for this role is minimum to midpoint, (&#x24;98,819 - &#x24;128,465) with the midpoint representing the average pay in a national market scope for this position.</b> Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.</p>
 <p><b> Work Location and Travel Requirements</b></p>
 <p> The typical offer range for this role is minimum to midpoint, with the midpoint representing the average pay in a national market scope for this position. Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.</p>
 <ul>
  <li> Ability to work independently and efficiently from a home office environment</li>
  <li> High Speed Internet Service</li>
  <li> It is a requirement that employees work in a distraction free workplace</li>
 </ul>
 <p> We offer a comprehensive range of benefits. See our website for details: https://ochin.org/employment-openings</p>
 <p><b> Equal Opportunity Statement</b></p>
 <p><i> OCHIN is proud to be an equal opportunity employer. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills for the benefit of our staff, our mission, and the communities we serve.</i></p>
 <p><i> As an Equal Opportunity and Affirmative Action employer, OCHIN, Inc. does not discriminate on the basis of race, ethnicity, sex, gender identity, sexual orientation, religion, marital or civil union status, age, disability status, veteran status, or any other protected characteristics. All aspects of employment are based on merit, performance, and business needs.</i></p>
 <p><b> COVID-19 Vaccination Requirement</b></p>
 <p><i> To keep our colleagues, members, and communities safe, OCHIN requires all employees&#x2014;including remote employees, contractors, interns, and new hires&#x2014;to be vaccinated with a COVID-19 vaccine, as supported by state and federal public health officials, as a condition of employment. All new hires are required to provide proof of full vaccination or receive approval for a medical or religious exemption before their hire date.</i></p>
</div>",https://recruiting.paylocity.com/recruiting/jobs/Details/1974445/OCHIN/RESEARCH-DATA-ENGINEER-REMOTE?source=Indeed_Feed&source=Indeed,d2dcb51303834420,,Full-time,,,Remote,RESEARCH DATA ENGINEER (REMOTE),19 days ago,2023-09-29T13:39:25.661Z,3.4,31.0,"$98,819 - $158,111 a year",2023-10-18T13:39:25.676Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d2dcb51303834420&from=jasx&tk=1hd1g5htglell801&vjs=3
102,Phantom,"Our crypto wallet is used by millions of people and provides a single, convenient solution for managing all your accounts and tokens across Solana, Ethereum, and Polygon. As a data engineer at Phantom you will see a direct correlation between your work, company growth, and our users' satisfaction. Beyond this, you will work with some of the brightest minds in the web3 space, and you'll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few web3 companies can match. 
  
  Snowflake / dbt / Rudderstack / Real-time Data Streaming / Python / SQL 
 
 Responsibilities 
 
  Architect: Design, build and launch extremely efficient and reliable data pipelines (ETL) to move data across a number of platforms including third party analytics, frontend & backend systems. 
  Educate teams: Use your data and analytics experience to see what's missing, and identify and address gaps in their existing systems and processes. 
  Partnership: Partner with stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions 
  Data: Manage the delivery of high-impact dashboards, tools, and data visualizations 
 
 Qualifications 
 
  5+ years of experience in SQL or similar languages, and development experience in at least one language (Python, Javascript, etc.) 
  5+ years of experience in the data warehouse space, custom ETL design, implementation, and maintenance. 
  Experience in leading data-driven projects from definition through interpretation and execution 
  Experience with data architecture, data modeling, schema design, and software development 
  Experience working with cloud analytics platforms and tools, specifically Snowflake, dbt, and Rudderstack 
  Bonus: Experience with blockchain or cryptocurrencies, real-time data streaming, and startup environments. 
  This role is fully remote; however, we're only open to candidates based in US and EU time zones. 
  
 The target base salary for this role will range between $150,000 to $250,000 with the addition of equity and benefits. This is determined by a few factors including your skillset, prior relevant experience, quality of interviews and market factors at the point in time of offer.","<div>
 <p>Our crypto wallet is used by millions of people and provides a single, convenient solution for managing all your accounts and tokens across Solana, Ethereum, and Polygon. As a data engineer at Phantom you will see a direct correlation between your work, company growth, and our users&apos; satisfaction. Beyond this, you will work with some of the brightest minds in the web3 space, and you&apos;ll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few web3 companies can match.</p> 
 <ul> 
  <li>Snowflake / dbt / Rudderstack / Real-time Data Streaming / Python / SQL</li> 
 </ul>
 <h1 class=""jobSectionHeader""><b>Responsibilities</b></h1> 
 <ul>
  <li><b>Architect:</b> Design, build and launch extremely efficient and reliable data pipelines (ETL) to move data across a number of platforms including third party analytics, frontend &amp; backend systems.</li> 
  <li><b>Educate teams:</b> Use your data and analytics experience to see what&apos;s missing, and identify and address gaps in their existing systems and processes.</li> 
  <li><b>Partnership:</b> Partner with stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions</li> 
  <li><b>Data:</b> Manage the delivery of high-impact dashboards, tools, and data visualizations</li> 
 </ul>
 <h1 class=""jobSectionHeader""><b>Qualifications</b></h1> 
 <ul>
  <li>5+ years of experience in SQL or similar languages, and development experience in at least one language (Python, Javascript, etc.)</li> 
  <li>5+ years of experience in the data warehouse space, custom ETL design, implementation, and maintenance.</li> 
  <li>Experience in leading data-driven projects from definition through interpretation and execution</li> 
  <li>Experience with data architecture, data modeling, schema design, and software development</li> 
  <li>Experience working with cloud analytics platforms and tools, specifically Snowflake, dbt, and Rudderstack</li> 
  <li>Bonus: Experience with blockchain or cryptocurrencies, real-time data streaming, and startup environments.</li> 
  <li>This role is fully remote; however, we&apos;re only open to candidates based in US and EU time zones.</li> 
 </ul> 
 <p>The target base salary for this role will range between &#x24;150,000 to &#x24;250,000 with the addition of equity and benefits. This is determined by a few factors including your skillset, prior relevant experience, quality of interviews and market factors at the point in time of offer.</p>
</div>",https://boards.greenhouse.io/phantom45/jobs/4047954005?gh_jid=4047954005&gh_src=5698beae5us,26945eb7798bb1ff,,,,,"518 Castro Street, San Francisco, CA 94114",Senior Data Engineer,15 days ago,2023-10-03T13:39:18.914Z,,,"$150,000 - $250,000 a year",2023-10-18T13:39:18.922Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=26945eb7798bb1ff&from=jasx&tk=1hd1g5htglell801&vjs=3
103,Mission Cloud,"As a Data Engineer, Planning & Analysis, you will report to the Vice President, Planning & Analysis. You will build and maintain Mission Cloud’s internal data lake and its data connections to Mission Cloud’s Business Intelligence Platform. You will collaborate across multiple departments to understand data sources and reporting needs, and business problems. You will use your technical expertise to automate and simplify reporting and data analysis requirements for teams at Mission Cloud.
 
 
 
   This position is 100% remote.
  
 Responsibilities 
 
  Build and maintain Mission Cloud’s internal data lake
   Establish connections from the data lake to business intelligence tools, including Amazon Quicksight
   Configure AWS services to support ETL and data warehousing
   Build and maintain the process for syncing the data lake with Amazon Quicksight
   Create operational efficiencies and improve data governance by establishing a singular data access point and source of truth for Mission Cloud
   Build repeatable, efficient processes to help teams at Mission Cloud become self-sufficient and able to accurately measure, monitor, and forecast their key performance indicators (KPIs)
   Creation of data visualizations and dashboards that effectively communicate key business insights and tell a compelling story
   Exercises judgment in selecting methods, evaluating, adapting of complex techniques and evaluation criteria for obtaining results pertaining to data warehousing.
 
  Requirements
 
   Ability to work in a business intelligence / data engineering role, preferably in the technology industry
   Design & implementation experience with distributed applications
   Exhibit advanced wide-ranging experience, using in-depth professional knowledge of database architectures and data pipeline development 
  Ability to handle unstructured, and semi-structured data, working in a data lake environment
   Experience building data lakes in Redshift
   Ability to translate data into insightful dashboards in Amazon Quicksight or a similar tool
   Expertise in designing and implementing AWS-based solutions, including services like Amazon Redshift, AWS Glue, Lambda, AWS Athena, AWS QuickSight
   Demonstrated proficiency in data analytics, data transformation (ETL/ELT), data integration, data warehousing, data lake architecture, and relevant AWS services.
   Experience working with extracting and ingesting data from various data sources using AWS Glue, AWS AppFlow, Lambda, Step Functions, Event Bridge
   Experience working with technologies like Amazon CDK, Cloud Formation, Terraform or equivalent
   Experience working with AWS Cloud Watch, Cloud Trail, Secrets Manager, KMS
   Ability to work across departments and manage multiple stakeholder needs and requests at one time
   Ability to manage and execute overlapping projects of varying durations and complexities
   Knowledge of Finance, Mathematics, Statistics or related field
   AWS Certification (required within 6 months of hire)
 
  Benefits
 
   Access to health, vision and dental insurance with options 100% covered by Mission Cloud for employee and their dependents
   Flexible Spending Accounts (Healthcare & Dependent Care)
   Generous Paid Time Off (FlexPTO, parental leave, volunteering time off)
   Reproductive health benefits
   Pet insurance
   401k matching program
   Life insurance paid by Mission Cloud
   Monthly flex stipend
   Monthly cell phone stipend
   Home office expense benefit
   An internal department dedicated to helping team members on their career path
   Inclusive work environment with several Employee Resource Groups
 
 
   Placement within the range is determined by a variety of factors, including but not limited to knowledge, skills, and ability as evaluated during the interview process. Range: $110000-$139,435
 
 
   Commitment to Diversity and Inclusion
 
 
 
   We are committed to diversity and inclusion. We value every individual’s unique story, experience, and perspective. We aim to amplify the voices of our team members and our community to create a safe, empathetic, and inclusive environment where everyone can contribute to one’s authentic self. Mission Cloud makes every effort to ensure that all employees are compensated fairly regardless of gender, ethnicity, race, or past salary history. We understand that fair compensation practices establish that diversity, fair hiring processes, and fair pay are part of who we are as a company and maintain positive employee morale. We use market data to define salary ranges for each role and regularly review compensation adjustments as needed based on salary range updates.
 
 
 
   Mission Cloud is an Equal Opportunity Employer and participant in the U.S. Federal E-Verify program. Mission Cloud will consider qualified applicants with criminal histories in a manner consistent with The Los Angeles Fair Chance Initiative for Hiring Ordinance.
 
 
 
   About Mission Cloud
 
 
 
   Mission Cloud is an Amazon Web Services (AWS) Premier Consulting Partner and MSP. Clients depend on us to expertly and securely architect, migrate, manage, and optimize their cloud environments.
 
 
   Mission Cloud’s team of AWS Certified Solutions Architects and DevOps Engineers are ready to help you harness the full power of the AWS cloud to transform your business and operations.","<div>
 <div>
  As a Data Engineer, Planning &amp; Analysis, you will report to the Vice President, Planning &amp; Analysis. You will build and maintain Mission Cloud&#x2019;s internal data lake and its data connections to Mission Cloud&#x2019;s Business Intelligence Platform. You will collaborate across multiple departments to understand data sources and reporting needs, and business problems. You will use your technical expertise to automate and simplify reporting and data analysis requirements for teams at Mission Cloud.
 </div>
 <div></div>
 <div>
  <br> This position is 100% remote.
 </div> 
 <h3 class=""jobSectionHeader""><b>Responsibilities </b></h3>
 <ul>
  <li>Build and maintain Mission Cloud&#x2019;s internal data lake</li>
  <li> Establish connections from the data lake to business intelligence tools, including Amazon Quicksight</li>
  <li> Configure AWS services to support ETL and data warehousing</li>
  <li> Build and maintain the process for syncing the data lake with Amazon Quicksight</li>
  <li> Create operational efficiencies and improve data governance by establishing a singular data access point and source of truth for Mission Cloud</li>
  <li> Build repeatable, efficient processes to help teams at Mission Cloud become self-sufficient and able to accurately measure, monitor, and forecast their key performance indicators (KPIs)</li>
  <li> Creation of data visualizations and dashboards that effectively communicate key business insights and tell a compelling story</li>
  <li> Exercises judgment in selecting methods, evaluating, adapting of complex techniques and evaluation criteria for obtaining results pertaining to data warehousing.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Requirements</b></h3>
 <ul>
  <li> Ability to work in a business intelligence / data engineering role, preferably in the technology industry</li>
  <li> Design &amp; implementation experience with distributed applications</li>
  <li> Exhibit advanced wide-ranging experience, using in-depth professional knowledge of database architectures and data pipeline development </li>
  <li>Ability to handle unstructured, and semi-structured data, working in a data lake environment</li>
  <li> Experience building data lakes in Redshift</li>
  <li> Ability to translate data into insightful dashboards in Amazon Quicksight or a similar tool</li>
  <li> Expertise in designing and implementing AWS-based solutions, including services like Amazon Redshift, AWS Glue, Lambda, AWS Athena, AWS QuickSight</li>
  <li> Demonstrated proficiency in data analytics, data transformation (ETL/ELT), data integration, data warehousing, data lake architecture, and relevant AWS services.</li>
  <li> Experience working with extracting and ingesting data from various data sources using AWS Glue, AWS AppFlow, Lambda, Step Functions, Event Bridge</li>
  <li> Experience working with technologies like Amazon CDK, Cloud Formation, Terraform or equivalent</li>
  <li> Experience working with AWS Cloud Watch, Cloud Trail, Secrets Manager, KMS</li>
  <li> Ability to work across departments and manage multiple stakeholder needs and requests at one time</li>
  <li> Ability to manage and execute overlapping projects of varying durations and complexities</li>
  <li> Knowledge of Finance, Mathematics, Statistics or related field</li>
  <li> AWS Certification (required within 6 months of hire)</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Benefits</b></h3>
 <ul>
  <li> Access to health, vision and dental insurance with options 100% covered by Mission Cloud for employee and their dependents</li>
  <li> Flexible Spending Accounts (Healthcare &amp; Dependent Care)</li>
  <li> Generous Paid Time Off (FlexPTO, parental leave, volunteering time off)</li>
  <li> Reproductive health benefits</li>
  <li> Pet insurance</li>
  <li> 401k matching program</li>
  <li> Life insurance paid by Mission Cloud</li>
  <li> Monthly flex stipend</li>
  <li> Monthly cell phone stipend</li>
  <li> Home office expense benefit</li>
  <li> An internal department dedicated to helping team members on their career path</li>
  <li> Inclusive work environment with several Employee Resource Groups</li>
 </ul>
 <div>
   Placement within the range is determined by a variety of factors, including but not limited to knowledge, skills, and ability as evaluated during the interview process. Range: &#x24;110000-&#x24;139,435
 </div>
 <div>
  <b> Commitment to Diversity and Inclusion</b>
 </div>
 <div></div>
 <div>
  <br> We are committed to diversity and inclusion. We value every individual&#x2019;s unique story, experience, and perspective. We aim to amplify the voices of our team members and our community to create a safe, empathetic, and inclusive environment where everyone can contribute to one&#x2019;s authentic self. Mission Cloud makes every effort to ensure that all employees are compensated fairly regardless of gender, ethnicity, race, or past salary history. We understand that fair compensation practices establish that diversity, fair hiring processes, and fair pay are part of who we are as a company and maintain positive employee morale. We use market data to define salary ranges for each role and regularly review compensation adjustments as needed based on salary range updates.
 </div>
 <div></div>
 <div>
  <br> Mission Cloud is an Equal Opportunity Employer and participant in the U.S. Federal E-Verify program. Mission Cloud will consider qualified applicants with criminal histories in a manner consistent with The Los Angeles Fair Chance Initiative for Hiring Ordinance.
 </div>
 <div></div>
 <div>
  <b><br> About Mission Cloud</b>
 </div>
 <div></div>
 <div>
  <br> Mission Cloud is an Amazon Web Services (AWS) Premier Consulting Partner and MSP. Clients depend on us to expertly and securely architect, migrate, manage, and optimize their cloud environments.
 </div>
 <div>
   Mission Cloud&#x2019;s team of AWS Certified Solutions Architects and DevOps Engineers are ready to help you harness the full power of the AWS cloud to transform your business and operations.
 </div>
</div>",https://jobs.lever.co/missioncloud/5d3925fe-0239-4f4e-b728-0f20b27f004f?lever-source=Indeed,60ee2be8d11b8703,,Full-time,,,Remote,"Senior Data Engineer, Planning & Analysis",18 days ago,2023-09-30T13:39:38.973Z,4.0,2.0,"$130,000 - $156,958 a year",2023-10-18T13:39:38.987Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=60ee2be8d11b8703&from=jasx&tk=1hd1g5htglell801&vjs=3
104,Chartboost,"Who we are: 
  Chartboost is the leading in-app monetization and programmatic advertising platform. We reach a global audience of over 700 million monthly active users and process over 2.7 trillion monthly advertising auctions. The Chartboost SDK is one the most widely integrated mobile ad SDKs and through the Chartboost Exchange, Ad Network, DSP and other services, we empower mobile app developers to build businesses, while connecting advertisers to highly engaged audiences. 
  
 Chartboost is hiring accomplished data engineers to join our team, helping us build and maintain a data platform that supports diverse uses. This includes data analysis, exploration, aggregation, user modeling, and scalable training systems. As part of the Data Team you will be a key player in our small, nimble, internationally-distributed team and drive significant impact all across the company's data technology and business. 
  We currently use Python, Java, and Scala to develop tools with Spark, Kafka, Airflow, MySQL, Druid, Spinnaker, and Kubernetes. We run in both GCP and AWS, but primarily work with Dataproc, Dataflow, BigQuery and Big Table. You will have the opportunity to join us in exploring new technologies and use them to design, deploy and operate highly performant systems. 
  In this role you are expected to be comfortable working to high standards as a professional data engineer, dealing with huge amounts of business-critical data (PBs), and to contribute across a full spectrum of responsibilities from architecture to ops. 
  Impact you will make: 
  
  Develop high-quality reliable data pipelines that convert data streams into valuable information 
  Design, implement and deploy both real time and batch data processing pipelines for internal and external customers 
  Develop tools to monitor, debug, analyze and operate our data infrastructure 
  Design and implement data technologies that can scale for hundreds of millions of users 
  Collaborate with our product and business teams to deliver valuable new features and functions 
  
 Who you are: 
  
  BS in Computer Science or related technical discipline or equivalent experience 
  2+ years of professional experience in data engineering environments 
  2+ years of experience with SQL and programming in any of Python/Java/Scala or similar HLL 
  Experience with data pipelines processing larger than 10TB of data is a plus 
  Experience working in cloud environments, ideally with GCP or AWS 
  Strong experience in improving performance of queries and data jobs and scaling the system for exponential growth in data volumes and traffic 
  Expert debugging skills and enthusiasm for automation to deliver high-quality reliable systems 
  Comfortable with modern development tools such as Git and Confluence and working in a distributed agile team environment with both high autonomy and regular collaboration 
  
 
 Perks: 
  
  Comprehensive medical, dental and vision insurance 
  Restricted Stock Units (RSUs) - you will have the potential of RSUs depending on the level/role 
  401(k) plan with match through Fidelity 
  Catered lunches and fully stocked kitchens 
  Commuter Program 
  Flex Vacation – personal time to refresh your mind/body/soul, spend time with loved ones and celebrate life events. There is no accrual or specific limit to the amount of time an employee may use 
  
 
 More about us: 
  We are proud of the product we've built and appreciate the impact it has on other people's businesses and lives. We want to be surrounded by people who are always finding opportunities to try something new and grow. We love data and anything that helps drive intelligent decisions and always design with the user in mind. Sounds like a fit? Join us, and be part of the team that will change the future of mobile gaming! 
  We are an equal opportunity employer — we celebrate diversity and are committed to creating an inclusive environment for all employees and make our hiring decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. 
  California Residents, please review the Chartboost California Employment Candidate Privacy Notice before submitting any personal information. 
  
 The pay range for this position in California at the start of employment is expected to be between $108,720.00 and $135,900.00 per year. For applicants based in New York City and New Jersey at the start of employment is expected to be between $108,720.00 and $135,900.00 per year. Furthermore, the pay range for this position for applicants based in Washington at the start of employment is expected to be between $96,640.00 and $120,800.00 per year. 
  However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations.  Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards and eligibility to participate in our 401(K) plan, in addition to a full range of medical, dental, vision, and basic life insurance. Employees will also receive 13 paid holidays per calendar year, unlimited discretionary time off, and will receive 10 sick days per calendar year. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an ""at-will position"" and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors","<div>
 <p><b>Who we are:</b></p> 
 <p> Chartboost is the leading in-app monetization and programmatic advertising platform. We reach a global audience of over 700 million monthly active users and process over 2.7 trillion monthly advertising auctions. The Chartboost SDK is one the most widely integrated mobile ad SDKs and through the Chartboost Exchange, Ad Network, DSP and other services, we empower mobile app developers to build businesses, while connecting advertisers to highly engaged audiences.</p> 
 <p></p> 
 <p>Chartboost is hiring accomplished data engineers to join our team, helping us build and maintain a data platform that supports diverse uses. This includes data analysis, exploration, aggregation, user modeling, and scalable training systems. As part of the Data Team you will be a key player in our small, nimble, internationally-distributed team and drive significant impact all across the company&apos;s data technology and business.</p> 
 <p> We currently use Python, Java, and Scala to develop tools with Spark, Kafka, Airflow, MySQL, Druid, Spinnaker, and Kubernetes. We run in both GCP and AWS, but primarily work with Dataproc, Dataflow, BigQuery and Big Table. You will have the opportunity to join us in exploring new technologies and use them to design, deploy and operate highly performant systems.</p> 
 <p> In this role you are expected to be comfortable working to high standards as a professional data engineer, dealing with huge amounts of business-critical data (PBs), and to contribute across a full spectrum of responsibilities from architecture to ops.</p> 
 <p><b> Impact you will make:</b></p> 
 <ul> 
  <li>Develop high-quality reliable data pipelines that convert data streams into valuable information</li> 
  <li>Design, implement and deploy both real time and batch data processing pipelines for internal and external customers</li> 
  <li>Develop tools to monitor, debug, analyze and operate our data infrastructure</li> 
  <li>Design and implement data technologies that can scale for hundreds of millions of users</li> 
  <li>Collaborate with our product and business teams to deliver valuable new features and functions</li> 
 </ul> 
 <p><b>Who you are:</b></p> 
 <ul> 
  <li>BS in Computer Science or related technical discipline or equivalent experience</li> 
  <li>2+ years of professional experience in data engineering environments</li> 
  <li>2+ years of experience with SQL and programming in any of Python/Java/Scala or similar HLL</li> 
  <li>Experience with data pipelines processing larger than 10TB of data is a plus</li> 
  <li>Experience working in cloud environments, ideally with GCP or AWS</li> 
  <li>Strong experience in improving performance of queries and data jobs and scaling the system for exponential growth in data volumes and traffic</li> 
  <li>Expert debugging skills and enthusiasm for automation to deliver high-quality reliable systems</li> 
  <li>Comfortable with modern development tools such as Git and Confluence and working in a distributed agile team environment with both high autonomy and regular collaboration</li> 
 </ul> 
 <p></p>
 <p><b>Perks:</b></p> 
 <ul> 
  <li>Comprehensive medical, dental and vision insurance</li> 
  <li>Restricted Stock Units (RSUs) - you will have the potential of RSUs depending on the level/role</li> 
  <li>401(k) plan with match through Fidelity</li> 
  <li>Catered lunches and fully stocked kitchens</li> 
  <li>Commuter Program</li> 
  <li>Flex Vacation &#x2013; personal time to refresh your mind/body/soul, spend time with loved ones and celebrate life events. There is no accrual or specific limit to the amount of time an employee may use</li> 
 </ul> 
 <p></p>
 <p><b>More about us:</b></p> 
 <p> We are proud of the product we&apos;ve built and appreciate the impact it has on other people&apos;s businesses and lives. We want to be surrounded by people who are always finding opportunities to try something new and grow. We love data and anything that helps drive intelligent decisions and always design with the user in mind. Sounds like a fit? Join us, and be part of the team that will change the future of mobile gaming!</p> 
 <p> We are an equal opportunity employer &#x2014; we celebrate diversity and are committed to creating an inclusive environment for all employees and make our hiring decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.</p> 
 <p> California Residents, please review the Chartboost California Employment Candidate Privacy Notice before submitting any personal information.</p> 
 <p></p> 
 <p><i>The pay range for this position in </i><b><i>California</i></b><i> at the start of employment is expected to be between </i><i>&#x24;108,720.00</i><i> and </i><i>&#x24;135,900.00 </i><i>per year. </i><i>For applicants based in </i><b><i>New York City </i></b><i>and </i><b><i>New Jersey</i></b><i> at the start of employment is expected to be between </i><i>&#x24;108,720.00</i><i> and </i><i>&#x24;135,900.00</i><i> per year. Furthermore, the pay range for this position for applicants based in </i><b><i>Washington</i></b><i> at the start of employment is expected to be between </i><i>&#x24;96,640.00</i><i> and </i><i>&#x24;120,800.00</i><i> per year.</i></p> 
 <p><i> However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations.</i><br> <br> <i>Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards and eligibility to participate in our 401(K) plan, in addition to a full range of medical, dental, vision, and basic life insurance. Employees will also receive 13 paid holidays per calendar year, unlimited discretionary time off, and will receive 10 sick days per calendar year. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an &quot;at-will position&quot; and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors</i></p>
</div>
<p></p>",https://boards.greenhouse.io/chartboost/jobs/5404685?gh_src=fc304f791us,afe7bc4e3d93a349,,,,,"San Francisco, CA",Data Engineer,18 days ago,2023-09-30T13:39:31.718Z,4.3,3.0,"$96,640 - $120,800 a year",2023-10-18T13:39:31.738Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=afe7bc4e3d93a349&from=jasx&tk=1hd1g5htglell801&vjs=3
105,G Associates LLC,"Title: Sr Data Engineer
Location: Remote
Length: Contract-To-Hire
WHAT YOU'LL NEED:

 Bachelor’s degree in computer science, Management information systems (MIS) or related degree / experience commensurate to a degree preferred
 5+ years of hands-on experience implementing, maintaining, and supporting data management solutions including program/project delivery
 7+ years of experience with SQL & T-SQL code development - experience with Snowflake preferred
 5+ years of experience with Python – A MUST HAVE!
 4+ years of experience with Databricks
 4+ years of experience designing, building and deploying solutions with Azure Data Factory – A MUST HAVE !
 4+ years of experience with logical modeling for visualization tools (Tableau, Power BI, Sigma)
 4+ years of Experience with Data lake technologies including ADLS Gen. 2, AWS S3, AWS Glue preferred
 Need to differentiate between a Senior Data Engineer versus a Data Analyst (e.g., BI) … don’t want an Analyst
 Needs to be heavy Python (libraries include Pandas) and have worked with very large data sets; Understands OOP (Object-Oriented Programming language).
 Candidates need to own the entire process creation (business requirements through production). Also need to be comfortable wearing multiple hats (ie., jumping off the Innovation team to help with something on the Run team (production/maintenance).
 Must be able and comfortable with mentoring other Data Engineers
 Senior enough to know how to deal with ambiguity … they are creating a totally new data warehousing environment, moving to Azure Data Lake. Be able to jump in and build pipelines …

Job Type: Full-time
Pay: $83,492.68 - $135,000.00 per year
Experience level:

 9 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Azure Data Lake: 7 years (Required)
 SQL: 7 years (Required)
 Data Engineer: 10 years (Required)
 Databricks: 7 years (Required)

Work Location: Remote","<p><b>Title: Sr Data Engineer</b></p>
<p><b>Location: Remote</b></p>
<p><b>Length: Contract-To-Hire</b></p>
<p><b>WHAT YOU&apos;LL NEED:</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, Management information systems (MIS) or related degree / experience commensurate to a degree preferred</li>
 <li>5+ years of hands-on experience implementing, maintaining, and supporting data management solutions including program/project delivery</li>
 <li>7+ years of experience with SQL &amp; T-SQL code development - experience with Snowflake preferred</li>
 <li>5+ years of experience with Python &#x2013; A MUST HAVE!</li>
 <li>4+ years of experience with Databricks</li>
 <li>4+ years of experience designing, building and deploying solutions with Azure Data Factory &#x2013; A MUST HAVE !</li>
 <li>4+ years of experience with logical modeling for visualization tools (Tableau, Power BI, Sigma)</li>
 <li>4+ years of Experience with Data lake technologies including ADLS Gen. 2, AWS S3, AWS Glue preferred</li>
 <li>Need to differentiate between a Senior Data Engineer versus a Data Analyst (e.g., BI) &#x2026; don&#x2019;t want an Analyst</li>
 <li>Needs to be heavy Python (libraries include Pandas) and have worked with very large data sets; Understands OOP (Object-Oriented Programming language).</li>
 <li>Candidates need to own the entire process creation (business requirements through production). Also need to be comfortable wearing multiple hats (ie., jumping off the Innovation team to help with something on the Run team (production/maintenance).</li>
 <li>Must be able and comfortable with mentoring other Data Engineers</li>
 <li>Senior enough to know how to deal with ambiguity &#x2026; they are creating a totally new data warehousing environment, moving to Azure Data Lake. Be able to jump in and build pipelines &#x2026;</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;83,492.68 - &#x24;135,000.00 per year</p>
<p>Experience level:</p>
<ul>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Azure Data Lake: 7 years (Required)</li>
 <li>SQL: 7 years (Required)</li>
 <li>Data Engineer: 10 years (Required)</li>
 <li>Databricks: 7 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,ead9ad56f5aa8781,,Full-time,,,Remote,Sr. Azure Data Engineer,Just posted,2023-10-19T13:57:38.428Z,,,"$83,493 - $135,000 a year",2023-10-19T13:57:38.433Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=ead9ad56f5aa8781&from=jasx&tk=1hd43k4db21ci002&vjs=3
106,Vail Resorts,"As a leading mountain resort operator with over 40 resorts in sixteen states and four countries. We exist to create an Experience of a Lifetime for our employees, so they can, in turn, provide and Experience of a Lifetime for our guests. We are looking for leaders, innovators, creators, and ambitious professionals to join our talented team. If you’re ready to pursue your fullest potential, we want to get to know you!
  Many of our Corporate function teams can now live and work in any of the states in which Vail Resorts currently operates* – enabling flexible remote work alongside a commitment to building and maintaining strong culture both in person and virtually. If you’re ready to pursue your fullest potential, we want to get to know you. Find your purpose with us at www.vailresortscareers.com.
 
  Job Summary:
  The Enterprise Data Engineering Team at Vail Resorts is on a journey to redefine how data is ingested, modeled, and surfaced to our key stakeholders across the enterprise. This team is at the forefront of creating a modern data estate on which the foundation of our core businesses will operate. We are looking for a passionate and driven Senior Data Engineer to become an important part of our fast-paced, high-energy, and innovative culture. The ideal candidate will have experience in Azure, Databricks, ETL processes, Python, SQL, Jira and Github. In addition to these skills, the candidate should also have experience in data lake house architecture, data modeling and migration from on-premise enterprise data warehousing to data lake.
  As a Senior Data Engineer, you will be responsible for designing and implementing data pipelines that are scalable, reliable, and efficient. You will work closely with analytics teams, data scientist and other stakeholders to understand their requirements and design solutions that meet their needs.
 
  Job Specifications:
 
   Outlet: Corporate
   Expected Pay Range: $99,900 - $135,120 + annual bonus
   Shift & Schedule Availability: Full Time / Year Round
 
 
   Other Specifics: Remote
 
 
  Job Responsibilities:
 
   Design and implement data pipelines using Azure and Databricks.
   Develop ETL/ELT processes to extract load data into Databricks Lakehouse using PySpark/Python/Scala and Delta Live Tables.
   Experience orchestrating and monitoring workflows.
   Work with data scientists/data analysts to understand their requirements and design solutions that meet their needs.
   Develop and maintain Python scripts to automate data processing tasks.
   Write complex SQL queries.
   Optimize database performance by tuning queries and indexes.
   Monitor database performance and troubleshoot issues as they arise.
   Other duties as assigned
 
 
  Job Requirements:
 
   Bachelor’s degree in Computer Science or a related field.
   5+ years of experience in data engineering.
   Experience with Azure and Databricks.
   Strong knowledge of ETL processes.
   Proficiency in Python, SQL, Jira and Github.
   Experience with big data technologies such as Hadoop, Spark, or Kafka is a plus.
   Experience with Jira and Github.
   Experience in data lake house architecture.
   Experience ingesting data from Event Hubs is a plus.
   Experience in data modeling is a plus. Experience in migration from on-premise enterprise data warehousing to data lake. 
 
 
 The expected Total Compensation for this role is $99,900 - $135,120 + annual bonus. Individual compensation decisions are based on a variety of factors.
  The perks include a free ski pass, and a set of benefits including...
 
   Medical, Dental, Vision insurance, and a 401(k) retirement plan
   Hourly employees are generally eligible for accrued Paid Time Off (PTO) and Sick Time. Salaried employees are generally eligible for Flexible Time Off (FTO)
   Paid Parental Leave for eligible mothers and fathers
   Healthcare & Dependent Care Flexible Spending Accounts
   Life, AD&D, and disability insurance
 
 
  Reach Your Peak at Vail Resorts. At Vail Resorts, our team is made whole by the brave, passionate individuals who ambitiously push boundaries and challenge the status quo. Whether you’re looking for seasonal work or the career of a lifetime, join us today to reach your peak.
 
   Remote work is currently permitted from British Columbia and the 16 U.S. states in which we currently operate. This includes: California, Colorado, Indiana, Michigan, Minnesota, Missouri, New Hampshire, New York, Nevada, Ohio, Pennsylvania, Utah, Vermont, Washington State, Wisconsin, and Wyoming. Please note that the ability to work remotely, and the particulars related to such work, are subject to change at any time; and, accordingly, the Company reserves the right to change its policies and/or require in-person/in-office work at any time in its sole discretion.
 
 
  Vail Resorts is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other status protected by applicable law.
 
  Requisition ID 498620 Reference Date: 10/18/2023 Job Code Function: Information Systems","<p></p>
<div>
 <p>As a leading mountain resort operator with over 40 resorts in sixteen states and four countries. We exist to create an <i>Experience of a Lifetime</i> for our employees, so they can, in turn, provide and <i>Experience of a Lifetime</i> for our guests. We are looking for leaders, innovators, creators, and ambitious professionals to join our talented team. If you&#x2019;re ready to pursue your fullest potential, we want to get to know you!</p>
 <p><br> Many of our Corporate function teams can now live and work in any of the states in which Vail Resorts currently operates* &#x2013; enabling flexible remote work alongside a commitment to building and maintaining strong culture both in person and virtually. If you&#x2019;re ready to pursue your fullest potential, we want to get to know you. Find your purpose with us at www.vailresortscareers.com.</p>
 <p></p>
 <p><b><br> Job Summary:</b></p>
 <p> The Enterprise Data Engineering Team at Vail Resorts is on a journey to redefine how data is ingested, modeled, and surfaced to our key stakeholders across the enterprise. This team is at the forefront of creating a modern data estate on which the foundation of our core businesses will operate.<br> We are looking for a passionate and driven Senior Data Engineer to become an important part of our fast-paced, high-energy, and innovative culture. The ideal candidate will have experience in Azure, Databricks, ETL processes, Python, SQL, Jira and Github. In addition to these skills, the candidate should also have experience in data lake house architecture, data modeling and migration from on-premise enterprise data warehousing to data lake.</p>
 <p><br> As a Senior Data Engineer, you will be responsible for designing and implementing data pipelines that are scalable, reliable, and efficient. You will work closely with analytics teams, data scientist and other stakeholders to understand their requirements and design solutions that meet their needs.</p>
 <p></p>
 <p><b><br> Job Specifications:</b></p>
 <ul>
  <li> Outlet: Corporate</li>
  <li> Expected Pay Range: &#x24;99,900 - &#x24;135,120 + annual bonus</li>
  <li> Shift &amp; Schedule Availability: Full Time / Year Round</li>
 </ul>
 <ul>
  <li> Other Specifics: Remote</li>
 </ul>
 <p></p>
 <p><b><br> Job Responsibilities:</b></p>
 <ul>
  <li> Design and implement data pipelines using Azure and Databricks.</li>
  <li> Develop ETL/ELT processes to extract load data into Databricks Lakehouse using PySpark/Python/Scala and Delta Live Tables.</li>
  <li> Experience orchestrating and monitoring workflows.</li>
  <li> Work with data scientists/data analysts to understand their requirements and design solutions that meet their needs.</li>
  <li> Develop and maintain Python scripts to automate data processing tasks.</li>
  <li> Write complex SQL queries.</li>
  <li> Optimize database performance by tuning queries and indexes.</li>
  <li> Monitor database performance and troubleshoot issues as they arise.</li>
  <li> Other duties as assigned</li>
 </ul>
 <p></p>
 <p><b><br> Job Requirements:</b></p>
 <ul>
  <li> Bachelor&#x2019;s degree in Computer Science or a related field.</li>
  <li> 5+ years of experience in data engineering.</li>
  <li> Experience with Azure and Databricks.</li>
  <li> Strong knowledge of ETL processes.</li>
  <li> Proficiency in Python, SQL, Jira and Github.</li>
  <li> Experience with big data technologies such as Hadoop, Spark, or Kafka is a plus.</li>
  <li> Experience with Jira and Github.</li>
  <li> Experience in data lake house architecture.</li>
  <li> Experience ingesting data from Event Hubs is a plus.</li>
  <li> Experience in data modeling is a plus.</li> Experience in migration from on-premise enterprise data warehousing to data lake. 
 </ul>
 <p></p>
 <p>The expected Total Compensation for this role is &#x24;99,900 - &#x24;135,120 + annual bonus. Individual compensation decisions are based on a variety of factors.</p>
 <p><br> The perks include a free ski pass, and a set of benefits including...</p>
 <ul>
  <li> Medical, Dental, Vision insurance, and a 401(k) retirement plan</li>
  <li> Hourly employees are generally eligible for accrued Paid Time Off (PTO) and Sick Time. Salaried employees are generally eligible for Flexible Time Off (FTO)</li>
  <li> Paid Parental Leave for eligible mothers and fathers</li>
  <li> Healthcare &amp; Dependent Care Flexible Spending Accounts</li>
  <li> Life, AD&amp;D, and disability insurance</li>
 </ul>
 <p></p>
 <p><b><br> Reach Your Peak at Vail Resorts. </b>At Vail Resorts, our team is made whole by the brave, passionate individuals who ambitiously push boundaries and challenge the status quo. Whether you&#x2019;re looking for seasonal work or the career of a lifetime, join us today to reach your peak.</p>
 <ul>
  <li><br> <b><i>Remote work is currently permitted from British Columbia and the 16 U.S. states in which we currently operate. This includes: California, Colorado, Indiana, Michigan, Minnesota, Missouri, New Hampshire, New York, Nevada, Ohio, Pennsylvania, Utah, Vermont, Washington State, Wisconsin, and Wyoming. Please note that the ability to work remotely, and the particulars related to such work, are subject to change at any time; and, accordingly, the Company reserves the right to change its policies and/or require in-person/in-office work at any time in its sole discretion.</i></b></li>
 </ul>
 <p></p>
 <p><i><br> Vail Resorts is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other status protected by applicable law.</i></p>
 <p></p>
 <p><i><br> Requisition ID 498620</i><br> <i>Reference Date: 10/18/2023</i><br> <i>Job Code Function: Information Systems</i></p>
</div>",https://jobs.vailresortscareers.com/corporate/job/Remote-Senior-Analyst-Data-Engineer-%28Remote%29-Remo/1088673400/?feedId=367000&rx_campaign=indeed0&rx_ch=jobp4p&rx_group=283690&rx_job=498620&rx_medium=cpc&rx_r=none&rx_source=Indeed&rx_ts=20231019T080103Z&rx_vp=cpc&sponsored=ppsa&utm_source=Indeed&utm_medium=organic&rx_p=G8ESN2HWY2&rx_viewer=77810b4c6e8711ee9e4d770833722a7216099a4bc60047c2bc5e1e92037b3eb0,de1f69845d4ad405,,Full-time,,,Remote,Senior Analyst Data Engineer (Remote),Just posted,2023-10-19T13:57:33.750Z,3.8,2276.0,"$99,900 - $135,120 a year",2023-10-19T13:57:33.752Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=de1f69845d4ad405&from=jasx&tk=1hd43k4db21ci002&vjs=3
107,"Crossover Health Management Services, Inc.","About Crossover Health
 
  Crossover Health is creating the future of health as it should be. A national, team-based medical group with a focus on wellbeing and prevention that extends beyond traditional sick care, the company delivers an entirely new model of healthcare—Primary Health—built on the foundation of trusted relationships, an interdisciplinary care team approach, and outcomes-based payment. Crossover’s Primary Health model integrates primary care, physical medicine, mental health, health coaching, care navigation and more, and delivers care in surround-sound—in-person, virtually and via asynchronous messaging. Together we are building a community of members that embraces healthcare as a proactive part of their lifestyle.
 
  Job Summary
  Crossover’s Data Engineer is responsible for managing and developing data sources for analytics at scale. This is a critical role for supporting Crossover’s growing analytics team, and serves as the connection between Crossover’s Product and Technology teams, Data Science team, and data infrastructure vendors. In this role, the successful candidate will build out new data sources within the enterprise data warehouse, guide data modeling efforts for new and existing projects, and manage data ingress and egress between Crossover teams, clients, partners, and vendors. The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data.
 
  Job Responsibilities
 
   Develop and maintain data sources within Crossover’s enterprise data warehouse (inclusive of our current vendor and/or future data infrastructure)
   Assist with recommendations for data architecture, data storage, data integration, data quality, and data models
   Contribute to design sessions based on technical requirements, and build data models to clean and transform datasets for use by Crossover’s Data Science and Analytics teams
   Assist with ETL, ELT, and reverse-ETL design and development initiatives including data analysis, source-target mapping, data profiling, change data capture, QA testing, and performance tuning to guarantee quality and repeatability of data model results
   Create and maintain data model standards, including MDM (Master Data Management) and codebase standardization
   Migrate Enterprise Workloads to Snowflake using industry standard methodologies
   Automate and deploy as well as build CI/CD pipelines to support cloud based workload
   Design, deliver cloud native, hybrid, and multi-cloud Workloads
   Invest in documentation, including all system design, architecture and ongoing changes
   Design and support production job schedules, including alerting, monitoring, break fixes, and performance tuning
   Build solutions that are automated, scalable, and sustainable while minimizing defects and technical debt
   Assist stakeholders including analytics, design, product, and executive teams with data-related technical issues
   Ability to work independently with little instruction or direct oversight
   Perform other duties as assigned
 
 
  Minimum Qualifications
 
   Bachelors in Computer Science or Data Engineering, related degree, or equivalent professional experience
   3+ years relevant work experience within a complex, dynamic environment, with preference for experience with clinical healthcare data
   3+ years architecting , implementing, and supporting data infrastructure and topologies
 
 
   Experience building and operating highly available, distributed systems of extraction, ingestion, and processing of large data sets across a variety of applications (OLTP, OLAP and DSS)
   3+ years Experience with Data warehousing, methodologies, modeling techniques, design patterns, and technologies.
   Experience with data migration tools and deploying cloudbase solutions
   Experience in writing advanced SQL (DML & DDL), including Stored Procedures, Indexes, user defined functions, windows functions, correlated subqueries and CTE's, and related data query and management technology
   Coding ability in R, Python, and Shell Scripting to build and deploy Pipelines
   Working knowledge of Git, or similar collaborative code management software
   Experience with data integration tools such as FiveTran, DBT, Informatica, Matillion, or similar ETL/ELT tools
   Experience with Snowflake’s data platform
 
 
  Preferred Qualifications
 
   Masters in Computer Science, Data Engineering, or related degree
   Healthcare data acquisition, ingestion, processing, and analytics knowledge highly preferred
   Previous experience with health informatics, taxonomies, terminologies, and code sets
   Knowledge and understanding of product features: IAAS, PAAS and SAAS solutions
   Experience with healthcare claims data, formats, and analytics
   Experience with Health Catalyst’s data and analytics platform
   Experience with Tableau Cloud administration
   Experience with Master Data Management
   Understand Cloud Ecosystem
 
  The base pay range for this position is $91,428.00 to $105,143.00 per year. Pay range may vary depending on work location, applicable knowledge, skills, and experience. This position will be eligible for an annual bonus opportunity and comprehensive benefits package that includes Medical Insurance, Dental Insurance, Vision Insurance, Short- and Long-Term Disability, Life Insurance, Paid Time Off and 401K.
 
  Crossover Health is committed to Equal Employment Opportunity regardless of race, color, national origin, gender, sexual orientation, age, religion, veteran status, disability, history of disability or perceived disability. If you need assistance or an accommodation due to a disability, you may email us at careers@crossoverhealth.com.
 
  To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.
  #LI-Remote","<div>
 <p><b>About Crossover Health</b></p>
 <p></p>
 <p> Crossover Health is creating the future of health as it should be. A national, team-based medical group with a focus on wellbeing and prevention that extends beyond traditional sick care, the company delivers an entirely new model of healthcare&#x2014;Primary Health&#x2014;built on the foundation of trusted relationships, an interdisciplinary care team approach, and outcomes-based payment. Crossover&#x2019;s Primary Health model integrates primary care, physical medicine, mental health, health coaching, care navigation and more, and delivers care in surround-sound&#x2014;in-person, virtually and via asynchronous messaging. Together we are building a community of members that embraces healthcare as a proactive part of their lifestyle.</p>
 <p></p>
 <p><b> Job Summary</b></p>
 <br> Crossover&#x2019;s Data Engineer is responsible for managing and developing data sources for analytics at scale. This is a critical role for supporting Crossover&#x2019;s growing analytics team, and serves as the connection between Crossover&#x2019;s Product and Technology teams, Data Science team, and data infrastructure vendors. In this role, the successful candidate will build out new data sources within the enterprise data warehouse, guide data modeling efforts for new and existing projects, and manage data ingress and egress between Crossover teams, clients, partners, and vendors. The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data.
 <p></p>
 <p><b> Job Responsibilities</b></p>
 <ul>
  <li><p> Develop and maintain data sources within Crossover&#x2019;s enterprise data warehouse (inclusive of our current vendor and/or future data infrastructure)</p></li>
  <li><p> Assist with recommendations for data architecture, data storage, data integration, data quality, and data models</p></li>
  <li><p> Contribute to design sessions based on technical requirements, and build data models to clean and transform datasets for use by Crossover&#x2019;s Data Science and Analytics teams</p></li>
  <li><p> Assist with ETL, ELT, and reverse-ETL design and development initiatives including data analysis, source-target mapping, data profiling, change data capture, QA testing, and performance tuning to guarantee quality and repeatability of data model results</p></li>
  <li><p> Create and maintain data model standards, including MDM (Master Data Management) and codebase standardization</p></li>
  <li><p> Migrate Enterprise Workloads to Snowflake using industry standard methodologies</p></li>
  <li><p> Automate and deploy as well as build CI/CD pipelines to support cloud based workload</p></li>
  <li><p> Design, deliver cloud native, hybrid, and multi-cloud Workloads</p></li>
  <li><p> Invest in documentation, including all system design, architecture and ongoing changes</p></li>
  <li><p> Design and support production job schedules, including alerting, monitoring, break fixes, and performance tuning</p></li>
  <li><p> Build solutions that are automated, scalable, and sustainable while minimizing defects and technical debt</p></li>
  <li><p> Assist stakeholders including analytics, design, product, and executive teams with data-related technical issues</p></li>
  <li><p> Ability to work independently with little instruction or direct oversight</p></li>
  <li><p> Perform other duties as assigned</p></li>
 </ul>
 <p></p>
 <p><b> Minimum Qualifications</b></p>
 <ul>
  <li><p> Bachelors in Computer Science or Data Engineering, related degree, or equivalent professional experience</p></li>
  <li><p> 3+ years relevant work experience within a complex, dynamic environment, with preference for experience with clinical healthcare data</p></li>
  <li><p> 3+ years architecting , implementing, and supporting data infrastructure and topologies</p></li>
 </ul>
 <ul>
  <li><p> Experience building and operating highly available, distributed systems of extraction, ingestion, and processing of large data sets across a variety of applications (OLTP, OLAP and DSS)</p></li>
  <li><p> 3+ years Experience with Data warehousing, methodologies, modeling techniques, design patterns, and technologies.</p></li>
  <li><p> Experience with data migration tools and deploying cloudbase solutions</p></li>
  <li><p> Experience in writing advanced SQL (DML &amp; DDL), including Stored Procedures, Indexes, user defined functions, windows functions, correlated subqueries and CTE&apos;s, and related data query and management technology</p></li>
  <li><p> Coding ability in R, Python, and Shell Scripting to build and deploy Pipelines</p></li>
  <li><p> Working knowledge of Git, or similar collaborative code management software</p></li>
  <li><p> Experience with data integration tools such as FiveTran, DBT, Informatica, Matillion, or similar ETL/ELT tools</p></li>
  <li><p> Experience with Snowflake&#x2019;s data platform</p></li>
 </ul>
 <p></p>
 <p><b> Preferred Qualifications</b></p>
 <ul>
  <li><p> Masters in Computer Science, Data Engineering, or related degree</p></li>
  <li><p> Healthcare data acquisition, ingestion, processing, and analytics knowledge highly preferred</p></li>
  <li><p> Previous experience with health informatics, taxonomies, terminologies, and code sets</p></li>
  <li><p> Knowledge and understanding of product features: IAAS, PAAS and SAAS solutions</p></li>
  <li><p> Experience with healthcare claims data, formats, and analytics</p></li>
  <li><p> Experience with Health Catalyst&#x2019;s data and analytics platform</p></li>
  <li><p> Experience with Tableau Cloud administration</p></li>
  <li><p> Experience with Master Data Management</p></li>
  <li><p> Understand Cloud Ecosystem</p></li>
 </ul>
 <p></p> The base pay range for this position is &#x24;91,428.00 to &#x24;105,143.00 per year. Pay range may vary depending on work location, applicable knowledge, skills, and experience. This position will be eligible for an annual bonus opportunity and comprehensive benefits package that includes Medical Insurance, Dental Insurance, Vision Insurance, Short- and Long-Term Disability, Life Insurance, Paid Time Off and 401K.
 <p></p>
 <p> Crossover Health is committed to Equal Employment Opportunity regardless of race, color, national origin, gender, sexual orientation, age, religion, veteran status, disability, history of disability or perceived disability. If you need assistance or an accommodation due to a disability, you may email us at careers@crossoverhealth.com.</p>
 <p></p>
 <p><b> To all recruitment agencies</b>: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.</p>
 <p></p> #LI-Remote
</div>",https://crossoverhealth.wd1.myworkdayjobs.com/Careers/job/Remote-USA/Data-Engineer_R23_774,fa2210df4da3bf9a,,Full-time,,,"101 West Avenida Vista Hermosa, San Clemente, CA 92672",Data Engineer,Today,2023-10-19T13:57:51.131Z,3.4,46.0,"$91,428 - $105,143 a year",2023-10-19T13:57:51.135Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=fa2210df4da3bf9a&from=jasx&tk=1hd43k4db21ci002&vjs=3
108,GTECH LLC,"Job Title : Azure Data Engineer
Job Location: Remote
Responsibilities

 Understand business requirements.
 Understand source systems, source data and source data formats that are available on-prem / cloud.
 Design and build data ingestion pipeline.
 Design and build complex data processing pipelines.
 Work with relevant stakeholders to assist with data-related technical issues and support their data needs.
 Build programs for data quality checks.
 Provide operational support.
 Work with data architecture, data governance and data analytics. teams to ensure pipelines adhere to enterprise standards, usability, and performance.
 Involve in System Testing, UAT, code deployment activities.
 Coordinate with offshore team on regular basis

Primary Skills

 Azure Databricks
 PySpark,
 Scala
 Snowflake (at least for 1 resource)

Secondary Skills

 ADF
 CICD
 Airflow
 SQL
 Cloud Databases
 Understanding of Agile methodologies

Job Type: Full-time
Salary: $110,000.00 - $120,000.00 per year
Experience level:

 5 years

Schedule:

 8 hour shift

Work Location: Remote","<p><b>Job Title : Azure Data Engineer</b></p>
<p><b>Job Location: Remote</b></p>
<p><b>Responsibilities</b></p>
<ul>
 <li><b>Understand business requirements.</b></li>
 <li><b>Understand source systems, source data and source data formats that are available on-prem / cloud.</b></li>
 <li><b>Design and build data ingestion pipeline.</b></li>
 <li><b>Design and build complex data processing pipelines.</b></li>
 <li><b>Work with relevant stakeholders to assist with data-related technical issues and support their data needs.</b></li>
 <li><b>Build programs for data quality checks.</b></li>
 <li><b>Provide operational support.</b></li>
 <li><b>Work with data architecture, data governance and data analytics. teams to ensure pipelines adhere to enterprise standards, usability, and performance.</b></li>
 <li><b>Involve in System Testing, UAT, code deployment activities.</b></li>
 <li><b>Coordinate with offshore team on regular basis</b></li>
</ul>
<p><b>Primary Skills</b></p>
<ul>
 <li><b>Azure Databricks</b></li>
 <li><b>PySpark,</b></li>
 <li><b>Scala</b></li>
 <li><b>Snowflake (at least for 1 resource)</b></li>
</ul>
<p><b>Secondary Skills</b></p>
<ul>
 <li><b>ADF</b></li>
 <li><b>CICD</b></li>
 <li><b>Airflow</b></li>
 <li><b>SQL</b></li>
 <li><b>Cloud Databases</b></li>
 <li><b>Understanding of Agile methodologies</b></li>
</ul>
<p>Job Type: Full-time</p>
<p>Salary: &#x24;110,000.00 - &#x24;120,000.00 per year</p>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,647e801f97a01cca,,Full-time,,,Remote,Azure Data Engineer,Today,2023-10-19T13:57:56.683Z,,,"$110,000 - $120,000 a year",2023-10-19T13:57:56.684Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=647e801f97a01cca&from=jasx&tk=1hd43k4db21ci002&vjs=3
109,Uplight,"The Position
  Do you dream about creating a more sustainable future? At Uplight, we are motivating energy users and providers to accelerate the clean energy ecosystem. We’re working with over 90 of the world’s leading electric and gas utilities to build the future platform for grid and energy management. Uplight delivers personalized experiences that customers have now come to expect – improving satisfaction, increasing revenue, reducing the cost to serve, and contributing to carbon reduction goals. We are a certified B Corp, enabling us to put our values into action by not only making decisions for the benefit of our shareholders, but also for our customers, environment, employees, and community.
  We are seeking a Data Operations Engineer to join our team and help us achieve our ambitious goals for our business and for the planet.
  We will consider this role for continued remote work with some work/travel to our offices in Denver, Boulder, Boston, Chicago or Vancouver.
  What you get to do:
  We are searching for a Data Operations Engineer with expertise in building and supporting big data systems that help utility companies and people reduce their energy usage and carbon footprint. You’ll work with our data ingestion team, platform team SRE and other stakeholders to ensure that our systems operate efficiently by resolving issues and proactively implementing stability and automation improvements. A focus for this position will be hands-on usage of a data mocking or masking tool to ensure that private customer data remains secure.
  Skills and experience are necessary, but we hire on value alignment first, so if you feel you would be a good fit with us, still consider applying.  You should:
 
   Be excited to work with talented, committed people in a fast-paced environment.
   Be passionate about energy consumption and making a difference in the world.
   Be capable of prioritizing your work to adhere to a deliverable schedule and ensure successful delivery that exceeds expectations.
   Be ready, able and willing to collaborate with a partner or colleague to help solve problems.
   Have a strong eye for detail and quality of your code.
   Not be afraid to dig into hard problems, and enjoy experimenting to come up with simple, pragmatic solutions to solve them.
 
  What you bring to Uplight:
 
   You are an accomplished developer with 2-5+ years of professional experience.
   Google Cloud Platform (Preferred), AWS or other cloud service provider experience.
   Understanding of the agile software development lifecycle including: scoping, detailed design, effort estimation, implementation, debugging, maintenance and support.
   Demonstrable knowledge of how to write effective unit and functional tests.
   BS in Computer Science / Engineering or demonstrable industry experience.
 
  Bonus Points:
 
   Experience using a data masking or mocking tool such as Tonic or Mockaroo.
   Experience with Apache Airflow, Matillion, Python, Spark or other data engineering tools.
   Experience building data pipelines or ETL/ELT processes.
   Experience with cloud infrastructure and SaaS environments.
 
  What makes working at Uplight amazing:
  In addition to all the standard medical and dental benefits, that kick in Day 1, we are:
 
   Proud to be over 500+ rebels with an important cause by helping to create a more sustainable planet.
   Committed to the environment, our employees, and our communities
   Focused on career growth by following defined career ladders
   Committed to taking our work and mission seriously and.... we love to laugh!
 
  We also provide:
 
   401k program with company matching
   Medical, vision, and dental insurance
   Monthly wellness stipend
   Peer to peer recognition program
   Management by objectives bonus program
   Innovative flexible time off policy
   Exceptionally collaborative and cool office spaces (once we reopen them)
 
  Salary Range: $115,000 to $135,000
  In accordance with the Colorado Equal Pay for Equal Work Act, the approximate annual base compensation range is listed above. The actual offer, reflecting the total compensation package and benefits, will be determined by a number of factors including the applicant's experience, knowledge, skills, and abilities, as well as internal equity among our team.
  Uplight provides equal employment opportunities to all employees and applicants and prohibits discrimination and harassment of any type without regard to race (including hair texture and hairstyles), color, religion (including head coverings), age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.","<div>
 <p><b>The Position</b></p>
 <p> Do you dream about creating a more sustainable future? At Uplight, we are motivating energy users and providers to accelerate the clean energy ecosystem. We&#x2019;re working with over 90 of the world&#x2019;s leading electric and gas utilities to build the future platform for grid and energy management. Uplight delivers personalized experiences that customers have now come to expect &#x2013; improving satisfaction, increasing revenue, reducing the cost to serve, and contributing to carbon reduction goals. We are a certified B Corp, enabling us to put our values into action by not only making decisions for the benefit of our shareholders, but also for our customers, environment, employees, and community.</p>
 <p> We are seeking a Data Operations Engineer to join our team and help us achieve our ambitious goals for our business and for the planet.</p>
 <p><i> We will consider this role for continued remote work with some work/travel to our offices in Denver, Boulder, Boston, Chicago or Vancouver.</i></p>
 <p><b> What you get to do:</b></p>
 <p> We are searching for a Data Operations Engineer with expertise in building and supporting big data systems that help utility companies and people reduce their energy usage and carbon footprint. You&#x2019;ll work with our data ingestion team, platform team SRE and other stakeholders to ensure that our systems operate efficiently by resolving issues and proactively implementing stability and automation improvements. A focus for this position will be hands-on usage of a data mocking or masking tool to ensure that private customer data remains secure.</p>
 <p><i> Skills and experience are necessary, but we hire on value alignment first, so if you feel you would be a good fit with us, still consider applying.</i><br> <br> You should:</p>
 <ul>
  <li> Be excited to work with talented, committed people in a fast-paced environment.</li>
  <li> Be passionate about energy consumption and making a difference in the world.</li>
  <li> Be capable of prioritizing your work to adhere to a deliverable schedule and ensure successful delivery that exceeds expectations.</li>
  <li> Be ready, able and willing to collaborate with a partner or colleague to help solve problems.</li>
  <li> Have a strong eye for detail and quality of your code.</li>
  <li> Not be afraid to dig into hard problems, and enjoy experimenting to come up with simple, pragmatic solutions to solve them.</li>
 </ul>
 <p><b> What you bring to Uplight:</b></p>
 <ul>
  <li> You are an accomplished developer with 2-5+ years of professional experience.</li>
  <li> Google Cloud Platform <i>(Preferred),</i> AWS or other cloud service provider experience.</li>
  <li> Understanding of the agile software development lifecycle including: scoping, detailed design, effort estimation, implementation, debugging, maintenance and support.</li>
  <li> Demonstrable knowledge of how to write effective unit and functional tests.</li>
  <li> BS in Computer Science / Engineering or demonstrable industry experience.</li>
 </ul>
 <p><b> Bonus Points:</b></p>
 <ul>
  <li> Experience using a data masking or mocking tool such as Tonic or Mockaroo.</li>
  <li> Experience with Apache Airflow, Matillion, Python, Spark or other data engineering tools.</li>
  <li> Experience building data pipelines or ETL/ELT processes.</li>
  <li> Experience with cloud infrastructure and SaaS environments.</li>
 </ul>
 <p><b> What makes working at Uplight amazing:</b></p>
 <p> In addition to all the standard medical and dental benefits, that kick in Day 1, we are:</p>
 <ul>
  <li> Proud to be over 500+ rebels with an important cause by helping to create a more sustainable planet.</li>
  <li> Committed to the environment, our employees, and our communities</li>
  <li> Focused on career growth by following defined career ladders</li>
  <li> Committed to taking our work and mission seriously and.... we love to laugh!</li>
 </ul>
 <p> We also provide:</p>
 <ul>
  <li> 401k program with company matching</li>
  <li> Medical, vision, and dental insurance</li>
  <li> Monthly wellness stipend</li>
  <li> Peer to peer recognition program</li>
  <li> Management by objectives bonus program</li>
  <li> Innovative flexible time off policy</li>
  <li> Exceptionally collaborative and cool office spaces (once we reopen them)</li>
 </ul>
 <p> Salary Range: &#x24;115,000 to &#x24;135,000</p>
 <p> In accordance with the Colorado Equal Pay for Equal Work Act, the approximate annual base compensation range is listed above. The actual offer, reflecting the total compensation package and benefits, will be determined by a number of factors including the applicant&apos;s experience, knowledge, skills, and abilities, as well as internal equity among our team.</p>
 <p><i> Uplight provides equal employment opportunities to all employees and applicants and prohibits discrimination and harassment of any type without regard to race (including hair texture and hairstyles), color, religion (including head coverings), age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.</i></p>
</div>
<p></p>",https://jobs.jobvite.com/careers/uplight/job/ok7Hjfwb?__jvst=Job+Board&__jvsd=Indeed,21025ed51ea9aa9f,,Full-time,,,Remote,Data Engineer,Today,2023-10-19T13:57:50.228Z,4.2,6.0,"$115,000 - $135,000 a year",2023-10-19T13:57:50.232Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=21025ed51ea9aa9f&from=jasx&tk=1hd43k4db21ci002&vjs=3
112,OTSI,"OTSI (Object Technology Solutions, Inc) has an immediate opening Senior Data Engineer at Overland Park, KS (Remote) and it’s a Long Term Contract position.
This would be W2 position and consultant should go for F2F interview in Overland Park, KS location. (so please apply who are ready to work on W2 and go for a F2F interview)
Job Description:
· 7+ Year of experience as a Data Engineer with below skills
· Big Data,
· Spark,
· HIVE
· AWS
· Kubernetes
· Data Bricks
About us:
OTSI is a leading global technology company offering solutions, consulting, and managed services for businesses worldwide since 1999. OTSI serves clients from its 15 offices across 6 countries around the globe with a “Follow-the-Sun” model. Headquartered in Overland Park, Kansas, we have a strong presence in North America, Central America, and Asia-Pacific with a Global Delivery Center based in India. These strategic locations offer our customers the competitive advantages of onshore, nearshore, and offshore engagement and delivery options, with 24/7 support. OTSI works with 100+ enterprise customers, of which many are Fortune ranked, OTSI focuses on industry segments such as Banking, Financial Services & Insurance, Healthcare & Life Sciences, Energy & Utilities, Communications & Media Entertainment, Engineering & Telecom, Retail & Consumer Services, Hi-tech, Manufacturing, Engineering, transport logistics, Government, Defense & PSUs.
Our Center of Excellence:
· Data & Analytics
· Digital Transformation
· QA & Automation
· Enterprise Applications
· Disruptive Technologies
Job Type: Contract
Pay: $50.00 - $58.00 per hour
Experience level:

 7 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Data Engineer: 7 years (Preferred)
 Spark: 5 years (Preferred)
 AWS: 5 years (Preferred)
 DATA BRICKS: 5 years (Preferred)
 Kubernetes: 5 years (Preferred)

Work Location: Remote","<p><b>OTSI (Object Technology Solutions, Inc)</b> has an immediate opening <b>Senior Data Engineer </b>at <b>Overland Park, KS (Remote) </b>and it&#x2019;s a <b>Long Term Contract </b>position.</p>
<p><b>This would be W2 position and consultant should go for F2F interview in Overland Park, KS location. (so please apply who are ready to work on W2 and go for a F2F interview)</b></p>
<p><b>Job Description:</b></p>
<p>&#xb7; 7+ Year of experience as a Data Engineer with below skills</p>
<p>&#xb7; Big Data,</p>
<p>&#xb7; Spark,</p>
<p>&#xb7; HIVE</p>
<p>&#xb7; AWS</p>
<p>&#xb7; Kubernetes</p>
<p>&#xb7; Data Bricks</p>
<p><b>About us:</b></p>
<p>OTSI is a leading global technology company offering solutions, consulting, and managed services for businesses worldwide since 1999. OTSI serves clients from its 15 offices across 6 countries around the globe with a &#x201c;Follow-the-Sun&#x201d; model. Headquartered in Overland Park, Kansas, we have a strong presence in North America, Central America, and Asia-Pacific with a Global Delivery Center based in India. These strategic locations offer our customers the competitive advantages of onshore, nearshore, and offshore engagement and delivery options, with 24/7 support. OTSI works with 100+ enterprise customers, of which many are Fortune ranked, OTSI focuses on industry segments such as Banking, Financial Services &amp; Insurance, Healthcare &amp; Life Sciences, Energy &amp; Utilities, Communications &amp; Media Entertainment, Engineering &amp; Telecom, Retail &amp; Consumer Services, Hi-tech, Manufacturing, Engineering, transport logistics, Government, Defense &amp; PSUs.</p>
<p>Our Center of Excellence:</p>
<p>&#xb7; Data &amp; Analytics</p>
<p>&#xb7; Digital Transformation</p>
<p>&#xb7; QA &amp; Automation</p>
<p>&#xb7; Enterprise Applications</p>
<p>&#xb7; Disruptive Technologies</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;50.00 - &#x24;58.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Engineer: 7 years (Preferred)</li>
 <li>Spark: 5 years (Preferred)</li>
 <li>AWS: 5 years (Preferred)</li>
 <li>DATA BRICKS: 5 years (Preferred)</li>
 <li>Kubernetes: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,f5777b4056e4e7e0,,Contract,,,"Overland Park, KS",Data Engineer (Only W2),Today,2023-10-19T13:58:49.036Z,3.5,23.0,$50 - $58 an hour,2023-10-19T13:58:49.037Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=f5777b4056e4e7e0&from=jasx&tk=1hd43l8mokhqg800&vjs=3
115,CareFirst BlueCross BlueShield,"Resp & Qualifications 
 PURPOSE:  The Lead Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on leading the development of solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company.  ESSENTIAL FUNCTIONS:
 
   Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability.
   Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Informatica, Snowflake, and Azure SQL.
   Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, De-normalized Data Structures, OLAP, and Data Warehousing concepts.
   Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ELT/ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Enforces the implementation of best practices for data auditing, scalability, reliability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
   Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
 
  SUPERVISORY RESPONSIBILITY: Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.  QUALIFICATIONS:  Education Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.  Experience: 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.  Knowledge, Skills and Abilities (KSAs)
 
   Knowledge and understanding of Informatica including Cloud version (IICS).
   Knowledge and understanding of Cloud Platforms (ie. Azure).
   Knowledge and understanding of Cloud Databases (ie. Snowflake, Azure SQL).
   Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python).
   Knowledge and understanding of database design and implementation concepts.
   Knowledge and understanding of data exchange formats.
   Knowledge and understanding of data movement concepts.
   Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
   Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
   Able to effectively provide direction to and lead technical teams. Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging. 
 
  Salary Range: $105,408 - $209,352 
 Salary Range Disclaimer 
 The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate's work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case's facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).
  Department 
 Department: ODS/ETL Members
  Equal Employment Opportunity 
 CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
  Where To Apply 
 Please visit our website to apply: www.carefirst.com/careers
  Federal Disc/Physical Demand 
 Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
  PHYSICAL DEMANDS:
  The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
  Sponsorship in US 
 Must be eligible to work in the U.S. without Sponsorship
  #LI-LD1","<div>
 <p><b>Resp &amp; Qualifications</b> </p>
 <p><b>PURPOSE: </b><br> The Lead Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on leading the development of solutions and helping transform the company&apos;s platforms deliver data-driven, meaningful insights and value to company.<br> <br> <b>ESSENTIAL FUNCTIONS:</b></p>
 <ul>
  <li> Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability.</li>
  <li> Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Informatica, Snowflake, and Azure SQL.</li>
  <li> Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, De-normalized Data Structures, OLAP, and Data Warehousing concepts.</li>
  <li> Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ELT/ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.</li>
  <li> Enforces the implementation of best practices for data auditing, scalability, reliability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.</li>
  <li> Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.</li>
  <li> Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.</li>
 </ul>
 <p><b><br> SUPERVISORY RESPONSIBILITY:</b><br> Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.<br> <br> <b>QUALIFICATIONS:</b><br> <br> <b>Education Level: </b>Bachelor&apos;s Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor&apos;s degree, an additional 4 years of relevant work experience is required in addition to the required work experience.<br> <br> <b>Experience: </b>8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.<br> <br> <b>Knowledge, Skills and Abilities (KSAs)</b></p>
 <ul>
  <li> Knowledge and understanding of Informatica including Cloud version (IICS).</li>
  <li> Knowledge and understanding of Cloud Platforms (ie. Azure).</li>
  <li> Knowledge and understanding of Cloud Databases (ie. Snowflake, Azure SQL).</li>
  <li> Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python).</li>
  <li> Knowledge and understanding of database design and implementation concepts.</li>
  <li> Knowledge and understanding of data exchange formats.</li>
  <li> Knowledge and understanding of data movement concepts.</li>
  <li> Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.</li>
  <li> Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.</li>
  <li> Able to effectively provide direction to and lead technical teams.</li> Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging. 
 </ul>
 <p><br> <b>Salary Range:</b> &#x24;105,408 - &#x24;209,352<br> </p>
 <p><b>Salary Range Disclaimer</b> </p>
 <p>The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate&apos;s work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case&apos;s facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).</p>
 <p><b> Department</b> </p>
 <p><b>Department: </b>ODS/ETL Members</p>
 <p><b> Equal Employment Opportunity</b> </p>
 <p>CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.</p>
 <p><b> Where To Apply</b> </p>
 <p>Please visit our website to apply: www.carefirst.com/careers</p>
 <p><b> Federal Disc/Physical Demand</b> </p>
 <p>Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.</p>
 <p><b> PHYSICAL DEMANDS:</b></p>
 <p> The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.</p>
 <p><b> Sponsorship in US</b> </p>
 <p>Must be eligible to work in the U.S. without Sponsorship</p>
 <p> #LI-LD1</p>
</div>",https://carefirstcareers.ttcportals.com/jobs/13495531-lead-data-engineer-remote?tm_job=18952-1A&tm_event=view&tm_company=2380,227119efc149cadc,,Full-time,,,"840 1st Street NE, Washington, DC 20002",Lead Data Engineer (Remote),Today,2023-10-19T13:58:48.154Z,3.8,732.0,"$105,408 - $209,352 a year",2023-10-19T13:58:48.156Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=227119efc149cadc&from=jasx&tk=1hd43l8mokhqg800&vjs=3
116,Global IT Resources,"This position is responsible for the design, specifications, coding, testing, implementation, maintenance, documentation, debugging, and troubleshooting of data warehouse solutions. Its work includes analyzing source system data, designing dimensional data models, developing ETLs, and creating data visualization products such as reports and dashboards. The position plays a critical role in requirement gathering, project plan planning, technical architecture design, go-live implementation, and ongoing system support. In addition to these duties, the position is required to perform any other tasks assigned to support the department’s function.
A data engineer is responsible for designing, developing, testing, documenting, and maintaining data warehouse and analytics architecture to meet data and analytics needs.
· Collaborate with business and technology partners to gather business requirements and create architectural designs.
· Use business domain knowledge to profile data and determine the best approaches to extract data into data warehouses.
· Apply the Kimball dimensional data modeling technique to build and enhance the enterprise data warehouse.
· Design, develop, test, document, and support new and existing ETL processes using Microsoft SQL Server Integration Services (SSIS), Microsoft Azure Data Factory, IBM InfoSphere DataStage, or other tools.
· Demonstrate proficiency in SQL programming and performance tuning.
· Develop business intelligence products, such as reports and dashboards, using Tableau or other data visualization tools.
· Document business rules and metadata in the data dictionary and keep the information updated.
· Possess excellent communication skills and the ability to articulate system designs and patterns to varying levels of leadership.
· Participate in project planning, including scoping backlogs and determining estimates.
· Assume on-call duties to support the operation of reporting and analytics systems.
· Maintain appropriate business domain knowledge in healthcare, education, research, finance, or business administration.
Job Types: Temporary, Contract, Full-time
Pay: $70.00 - $80.00 per hour
Expected hours: 40 per week
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Vision insurance

Schedule:

 Day shift
 Monday to Friday

Education:

 Bachelor's (Required)

Experience:

 T-SQL: 3 years (Required)
 ETL: 3 years (Required)
 Tableau: 3 years (Required)
 Power BI: 3 years (Required)
 SSIS: 3 years (Required)
 Healthcare: 2 years (Required)

License/Certification:

 EPIC Clarity Certification (Preferred)
 EPIC Cadoodle Certification (Preferred)

Work Location: Remote","<p>This position is responsible for the design, specifications, coding, testing, implementation, maintenance, documentation, debugging, and troubleshooting of data warehouse solutions. Its work includes analyzing source system data, designing dimensional data models, developing ETLs, and creating data visualization products such as reports and dashboards. The position plays a critical role in requirement gathering, project plan planning, technical architecture design, go-live implementation, and ongoing system support. In addition to these duties, the position is required to perform any other tasks assigned to support the department&#x2019;s function.</p>
<p>A data engineer is responsible for designing, developing, testing, documenting, and maintaining data warehouse and analytics architecture to meet data and analytics needs.</p>
<p>&#xb7; Collaborate with business and technology partners to gather business requirements and create architectural designs.</p>
<p>&#xb7; Use business domain knowledge to profile data and determine the best approaches to extract data into data warehouses.</p>
<p>&#xb7; Apply the Kimball dimensional data modeling technique to build and enhance the enterprise data warehouse.</p>
<p>&#xb7; Design, develop, test, document, and support new and existing ETL processes using Microsoft SQL Server Integration Services (SSIS), Microsoft Azure Data Factory, IBM InfoSphere DataStage, or other tools.</p>
<p>&#xb7; Demonstrate proficiency in SQL programming and performance tuning.</p>
<p>&#xb7; Develop business intelligence products, such as reports and dashboards, using Tableau or other data visualization tools.</p>
<p>&#xb7; Document business rules and metadata in the data dictionary and keep the information updated.</p>
<p>&#xb7; Possess excellent communication skills and the ability to articulate system designs and patterns to varying levels of leadership.</p>
<p>&#xb7; Participate in project planning, including scoping backlogs and determining estimates.</p>
<p>&#xb7; Assume on-call duties to support the operation of reporting and analytics systems.</p>
<p>&#xb7; Maintain appropriate business domain knowledge in healthcare, education, research, finance, or business administration.</p>
<p>Job Types: Temporary, Contract, Full-time</p>
<p>Pay: &#x24;70.00 - &#x24;80.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Required)</li>
</ul>
<p>Experience:</p>
<ul>
 <li>T-SQL: 3 years (Required)</li>
 <li>ETL: 3 years (Required)</li>
 <li>Tableau: 3 years (Required)</li>
 <li>Power BI: 3 years (Required)</li>
 <li>SSIS: 3 years (Required)</li>
 <li>Healthcare: 2 years (Required)</li>
</ul>
<p>License/Certification:</p>
<ul>
 <li>EPIC Clarity Certification (Preferred)</li>
 <li>EPIC Cadoodle Certification (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,15fd4f1c42b1c5c0,,Temporary,Full-time,Contract,Remote,Data Engineer BI,Today,2023-10-19T13:58:58.422Z,4.0,4.0,$70 - $80 an hour,2023-10-19T13:58:58.424Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=15fd4f1c42b1c5c0&from=jasx&tk=1hd43l8mokhqg800&vjs=3
117,Western Governors University,"The salary range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
 
 
 
   At WGU, it is not typical for an individual to be hired at or near the top of the range for their role, and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is:
 
  Pay Range: $127,700.00 - $191,500.00
 
   If you’re passionate about building a better future for individuals, communities, and our country—and you’re committed to working hard to play your part in building that future—consider WGU as the next step in your career.
 
 
 
   Driven by a mission to expand access to higher education through online, competency-based degree programs, WGU is also committed to being a great place to work for a diverse workforce of student-focused professionals. The university has pioneered a new way to learn in the 21st century, one that has received praise from academic, industry, government, and media leaders. Whatever your role, working for WGU gives you a part to play in helping students graduate, creating a better tomorrow for themselves and their families.
 
 
   Current WGU employees should submit an internal application before 10/27/2023 to be considered.
 
 
 
   The Staff Data Engineer should be agnostic to tools and should be able to supervise, design, architect and code using Apache Spark and other cloud technologies. The position will supervise and design how data will flow through hybrid data environments comprised of open-source big data platforms and traditional database systems. The core responsibility for this position includes supervision of data engineering technical aspects, design of data and system architecture for the Data Lake and data warehouse, supervision of the technical aspects of a data engineering team and projects encompassing dimensional and normalized data modeling. The Staff Data Engineer will improve technical standards in the environment ensuring optimal use of data warehouse and other data stores to solve business problems. They will serve as the lead engineer and go to person for all aspects of the data engineer team including solution architecture of data systems.
 
 
 
   Essential Functions and Responsibilities:
 
 
  
   
     Supervise work on cloud technologies and architect scalable and performant Data Lake systems.
   
  
   
     Establish design and methodology for database build processes.
   
  
   
     Supervise the architecture and design of complete data model solutions.
   
  
   
     Supervise necessary data protection and security processes.
   
  
   
     Create and design extract processes for data access layer.
   
  
   
     Translate business problems/information requirements accurately to logical/physical data models aligning with customers’ data architecture standards.
   
  
   
     Supervise and perform research and analysis to find solutions for complex business problems.
   
  
   
     Monitor job performance and fine tune Spark SQL queries as appropriate on a regular basis.
   
  
   
     Supervise the profiling of data, the publishing of data profiles and corrective actions if required to ensure data quality.
   
  
   
     Supervise and perform documentation / reverse engineering / analysis of data mapping using data integration code/tools.
   
  
   
     Work with APIs for data wrangling and integrations with other systems data in the EDW.
   
  
   
     Perform impact analysis using Data Integration/Data Virtualization tool repositories, DB data dictionary, UNIX scripts and frontend code on versioning systems.
   
  
   
     Analyze / research data on multiple platforms as wells as multiple heterogeneous databases including custom developed databases.
   
  
   
     Positively impact projects by completing tasks assigned on time.
   
  
   
     Communicate technical and domain knowledge as it relates to work, to both technical and non-technical audiences.
   
  
   
     Ingest and transform structured, semi-structured, and unstructured data from sources including relational databases, NoSQL, external APIs, JSON, XML, delimited files, and more.
   
  
   
     Support business and functional requirements and translate these requirements into robust, scalable, solutions.
   
  
   
     Collaborate with engineers to help adopt best practices in data system creation, data integrity, test design, analysis, validation, and documentation.
   
  
   
     Help continually improve ongoing reporting and analysis processes, automate, or simplify self-service modelling and production support for customers.
   
  
   
     Performs other related duties as assigned.
   
 
 
 
   Knowledge, Skill and Abilities:
 
 
  
   
     Expertise with analytical reporting tools, preferably Cognos and Tableau.
   
  
   
     Mastery in code based ETL/ ELT tools for importing and exporting data across disparate systems.
   
  
   
     Expertise in analytic skills related to working with unstructured datasets.
   
  
   
     Use of industry best practices for code development, testing, implementation and documentation.
   
  
   
     Ability to evaluate and prioritize work based on the organization’s needs.
   
  
   
     Ability to supervise cross team projects to accomplish data integrations and pipelines.
   
  
   
     Supervisory abilities for data engineering team with respect to technical design and architecture.
   
  
   
     Excellent verbal & written communication, along with technical documentation
   
  
   
     Ability to work and deliver in a team environment
   
  
   
     Ability to manage the use of tools like Jira, Confluence, GitHub
   
  
   
     Architect and Develop processes for audit of Data Integrity
   
  
   
     Ability to mentor Associate/Senior/Data Engineer in data pipeline architecture and coding standards
   
  
   
     Supervise Validation and testing to analyze and debug issues
   
  
   
     Mastery of AWS cloud technologies, REST API, and HTML5
   
  
   
     Mastery of relational SQL and NoSQL databases
   
  
   
     Mastery with object-oriented/object function scripting languages: Python, Java, Scala
   
  
   
     Mastery of big data tools: Hadoop, Spark, Kafka, Databricks, etc.
   
 
 
 
   Competencies:
 
 
   Organizational or Student Impact:
 
 
  
   
     Recommends and implements changes in technical/business processes; identifies areas for improvement.
   
  
   
     Helps lead/coordinate extremely complex technical projects and programs and leads development and implementation of innovative solutions for specialized technical issues.
   
  
   
     Works proactively; identifies and helps prevent/ solve problems that may cross disciplines.
   
  
   
     Fully understands and quantifies project risks with impact. Identifies, generates, and implements innovative solutions.
   
 
 
   Problem Solving & Decision Making:
 
 
  
   
     This individual accomplishes goals and objectives independently.
   
  
   
     Builds and leads teams, influencing decisions and results.
   
  
   
     Uses discretion to fully scope, design, and implement solutions to complex technical problems.
   
  
   
     The individual provides regular technical advice and direction to technical teams and management.
   
  
   
     Models and helps set high standards for effective interactions with internal and external individuals.
   
 
 
   Communication & Influence:
 
 
  
   
     Communicates with parties within and outside of their job function and typically has responsibilities for communicating with parties external to the organization.
   
  
   
     Works to influence others to accept and understand new concepts, practices, and approaches. Requires ability to communicate with executive leadership regarding matters of significant importance to the organization.
   
  
   
     This individual may conduct briefings with senior leaders within the technical function.
   
 
 
   Leadership:
 
 
  
   
     Frequently responsible for providing guidance, coaching, and training to other employees across the Company within the area of expertise.
   
  
   
     Responsible for managing large, complex project initiatives or strategically important solutions to the organization, involving large cross-functional teams.
   
  
   
     May have direct reports but generally fewer than three.
   
 
 
 
   Job Qualifications:
 
 
  
   
     M. S. in Business, Management Information Systems, Computer Science, or a related field, or an equivalent combination of experience and training.
   
  
   
     Seven or more years of experience as a Data Engineer, Data Integration, Big Data, or Business Intelligence, Software Engineer
   
 
 
 
   Preferred Qualifications:
 
 
  
   
     Strong experience with distance education and distance learning students is preferred.
   
  
   
     Higher Education domain knowledge
   
 
 
 
   #LI-REMOTE
 
 
   #LI-ZARD
 
 
   As an equal opportunity employer, WGU recognizes that our strength lies in our people. We are committed to diversity.","<div>
 <div>
  The salary range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
 </div>
 <div></div>
 <div>
   At WGU, it is not typical for an individual to be hired at or near the top of the range for their role, and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is:
 </div>
 <br> Pay Range: &#x24;127,700.00 - &#x24;191,500.00
 <div>
  <br> If you&#x2019;re passionate about building a better future for individuals, communities, and our country&#x2014;and you&#x2019;re committed to working hard to play your part in building that future&#x2014;consider WGU as the next step in your career.
 </div>
 <div></div>
 <div>
   Driven by a mission to expand access to higher education through online, competency-based degree programs, WGU is also committed to being a great place to work for a diverse workforce of student-focused professionals. The university has pioneered a new way to learn in the 21st century, one that has received praise from academic, industry, government, and media leaders. Whatever your role, working for WGU gives you a part to play in helping students graduate, creating a better tomorrow for themselves and their families.
 </div>
 <div>
   Current WGU employees should submit an internal application before 10/27/2023 to be considered.
 </div>
 <div></div>
 <div>
   The Staff Data Engineer should be agnostic to tools and should be able to supervise, design, architect and code using Apache Spark and other cloud technologies. The position will supervise and design how data will flow through hybrid data environments comprised of open-source big data platforms and traditional database systems. The core responsibility for this position includes supervision of data engineering technical aspects, design of data and system architecture for the Data Lake and data warehouse, supervision of the technical aspects of a data engineering team and projects encompassing dimensional and normalized data modeling. The Staff Data Engineer will improve technical standards in the environment ensuring optimal use of data warehouse and other data stores to solve business problems. They will serve as the lead engineer and go to person for all aspects of the data engineer team including solution architecture of data systems.
 </div>
 <div></div>
 <div>
   Essential Functions and Responsibilities:
 </div>
 <ul>
  <li>
   <div>
     Supervise work on cloud technologies and architect scalable and performant Data Lake systems.
   </div></li>
  <li>
   <div>
     Establish design and methodology for database build processes.
   </div></li>
  <li>
   <div>
     Supervise the architecture and design of complete data model solutions.
   </div></li>
  <li>
   <div>
     Supervise necessary data protection and security processes.
   </div></li>
  <li>
   <div>
     Create and design extract processes for data access layer.
   </div></li>
  <li>
   <div>
     Translate business problems/information requirements accurately to logical/physical data models aligning with customers&#x2019; data architecture standards.
   </div></li>
  <li>
   <div>
     Supervise and perform research and analysis to find solutions for complex business problems.
   </div></li>
  <li>
   <div>
     Monitor job performance and fine tune Spark SQL queries as appropriate on a regular basis.
   </div></li>
  <li>
   <div>
     Supervise the profiling of data, the publishing of data profiles and corrective actions if required to ensure data quality.
   </div></li>
  <li>
   <div>
     Supervise and perform documentation / reverse engineering / analysis of data mapping using data integration code/tools.
   </div></li>
  <li>
   <div>
     Work with APIs for data wrangling and integrations with other systems data in the EDW.
   </div></li>
  <li>
   <div>
     Perform impact analysis using Data Integration/Data Virtualization tool repositories, DB data dictionary, UNIX scripts and frontend code on versioning systems.
   </div></li>
  <li>
   <div>
     Analyze / research data on multiple platforms as wells as multiple heterogeneous databases including custom developed databases.
   </div></li>
  <li>
   <div>
     Positively impact projects by completing tasks assigned on time.
   </div></li>
  <li>
   <div>
     Communicate technical and domain knowledge as it relates to work, to both technical and non-technical audiences.
   </div></li>
  <li>
   <div>
     Ingest and transform structured, semi-structured, and unstructured data from sources including relational databases, NoSQL, external APIs, JSON, XML, delimited files, and more.
   </div></li>
  <li>
   <div>
     Support business and functional requirements and translate these requirements into robust, scalable, solutions.
   </div></li>
  <li>
   <div>
     Collaborate with engineers to help adopt best practices in data system creation, data integrity, test design, analysis, validation, and documentation.
   </div></li>
  <li>
   <div>
     Help continually improve ongoing reporting and analysis processes, automate, or simplify self-service modelling and production support for customers.
   </div></li>
  <li>
   <div>
     Performs other related duties as assigned.
   </div></li>
 </ul>
 <div></div>
 <div>
   Knowledge, Skill and Abilities:
 </div>
 <ul>
  <li>
   <div>
     Expertise with analytical reporting tools, preferably Cognos and Tableau.
   </div></li>
  <li>
   <div>
     Mastery in code based ETL/ ELT tools for importing and exporting data across disparate systems.
   </div></li>
  <li>
   <div>
     Expertise in analytic skills related to working with unstructured datasets.
   </div></li>
  <li>
   <div>
     Use of industry best practices for code development, testing, implementation and documentation.
   </div></li>
  <li>
   <div>
     Ability to evaluate and prioritize work based on the organization&#x2019;s needs.
   </div></li>
  <li>
   <div>
     Ability to supervise cross team projects to accomplish data integrations and pipelines.
   </div></li>
  <li>
   <div>
     Supervisory abilities for data engineering team with respect to technical design and architecture.
   </div></li>
  <li>
   <div>
     Excellent verbal &amp; written communication, along with technical documentation
   </div></li>
  <li>
   <div>
     Ability to work and deliver in a team environment
   </div></li>
  <li>
   <div>
     Ability to manage the use of tools like Jira, Confluence, GitHub
   </div></li>
  <li>
   <div>
     Architect and Develop processes for audit of Data Integrity
   </div></li>
  <li>
   <div>
     Ability to mentor Associate/Senior/Data Engineer in data pipeline architecture and coding standards
   </div></li>
  <li>
   <div>
     Supervise Validation and testing to analyze and debug issues
   </div></li>
  <li>
   <div>
     Mastery of AWS cloud technologies, REST API, and HTML5
   </div></li>
  <li>
   <div>
     Mastery of relational SQL and NoSQL databases
   </div></li>
  <li>
   <div>
     Mastery with object-oriented/object function scripting languages: Python, Java, Scala
   </div></li>
  <li>
   <div>
     Mastery of big data tools: Hadoop, Spark, Kafka, Databricks, etc.
   </div></li>
 </ul>
 <div></div>
 <div>
   Competencies:
 </div>
 <div>
   Organizational or Student Impact:
 </div>
 <ul>
  <li>
   <div>
     Recommends and implements changes in technical/business processes; identifies areas for improvement.
   </div></li>
  <li>
   <div>
     Helps lead/coordinate extremely complex technical projects and programs and leads development and implementation of innovative solutions for specialized technical issues.
   </div></li>
  <li>
   <div>
     Works proactively; identifies and helps prevent/ solve problems that may cross disciplines.
   </div></li>
  <li>
   <div>
     Fully understands and quantifies project risks with impact. Identifies, generates, and implements innovative solutions.
   </div></li>
 </ul>
 <div>
   Problem Solving &amp; Decision Making:
 </div>
 <ul>
  <li>
   <div>
     This individual accomplishes goals and objectives independently.
   </div></li>
  <li>
   <div>
     Builds and leads teams, influencing decisions and results.
   </div></li>
  <li>
   <div>
     Uses discretion to fully scope, design, and implement solutions to complex technical problems.
   </div></li>
  <li>
   <div>
     The individual provides regular technical advice and direction to technical teams and management.
   </div></li>
  <li>
   <div>
     Models and helps set high standards for effective interactions with internal and external individuals.
   </div></li>
 </ul>
 <div>
   Communication &amp; Influence:
 </div>
 <ul>
  <li>
   <div>
     Communicates with parties within and outside of their job function and typically has responsibilities for communicating with parties external to the organization.
   </div></li>
  <li>
   <div>
     Works to influence others to accept and understand new concepts, practices, and approaches. Requires ability to communicate with executive leadership regarding matters of significant importance to the organization.
   </div></li>
  <li>
   <div>
     This individual may conduct briefings with senior leaders within the technical function.
   </div></li>
 </ul>
 <div>
   Leadership:
 </div>
 <ul>
  <li>
   <div>
     Frequently responsible for providing guidance, coaching, and training to other employees across the Company within the area of expertise.
   </div></li>
  <li>
   <div>
     Responsible for managing large, complex project initiatives or strategically important solutions to the organization, involving large cross-functional teams.
   </div></li>
  <li>
   <div>
     May have direct reports but generally fewer than three.
   </div></li>
 </ul>
 <div></div>
 <div>
   Job Qualifications:
 </div>
 <ul>
  <li>
   <div>
     M. S. in Business, Management Information Systems, Computer Science, or a related field, or an equivalent combination of experience and training.
   </div></li>
  <li>
   <div>
     Seven or more years of experience as a Data Engineer, Data Integration, Big Data, or Business Intelligence, Software Engineer
   </div></li>
 </ul>
 <div></div>
 <div>
   Preferred Qualifications:
 </div>
 <ul>
  <li>
   <div>
     Strong experience with distance education and distance learning students is preferred.
   </div></li>
  <li>
   <div>
     Higher Education domain knowledge
   </div></li>
 </ul>
 <div></div>
 <div>
   #LI-REMOTE
 </div>
 <div>
   #LI-ZARD
 </div>
 <div>
  <br> As an equal opportunity employer, WGU recognizes that our strength lies in our people. We are committed to diversity.
 </div>
</div>",https://wgu.wd5.myworkdayjobs.com/en-US/External/job/United-States---Remote/Staff-Data-Engineer_JR-016081,6d6399c51c6d5573,,Full-time,,,Remote,Staff Data Engineer,Today,2023-10-19T13:58:54.937Z,3.6,498.0,"$127,700 - $191,500 a year",2023-10-19T13:58:54.940Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=6d6399c51c6d5573&from=jasx&tk=1hd43l8mokhqg800&vjs=3
118,MST Solutions,"Role/Responsibilities

 Support transition of media data from RAPP (current vendor)
 Hands on technical role working with data and providing expertise on how best to transition data
 ETL, Python, AWS, SQL

Top 3-5 Skills
ETL
AWS – Apache Airflow, Kafka, Streaming Data, etc.
Python
SQL – Validate data policy checks
Modeling experience is a plus
Job Requirements:
Desired soft skills

 Hands on
 Flexible, must be able to attend meetings
 Good verbal and written communication as they will have to give their recommendations on data transfer to leadership team
 This will be a high visibility role

Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience:

 ETL: 1 year (Preferred)
 AWS: 1 year (Preferred)
 Redshift: 1 year (Preferred)

Work Location: Remote","<p>Role/Responsibilities</p>
<ul>
 <li>Support transition of media data from RAPP (current vendor)</li>
 <li>Hands on technical role working with data and providing expertise on how best to transition data</li>
 <li>ETL, Python, AWS, SQL</li>
</ul>
<p>Top 3-5 Skills</p>
<p>ETL</p>
<p>AWS &#x2013; Apache Airflow, Kafka, Streaming Data, etc.</p>
<p>Python</p>
<p>SQL &#x2013; Validate data policy checks</p>
<p>Modeling experience is a plus</p>
<p><b>Job Requirements:</b></p>
<p>Desired soft skills</p>
<ul>
 <li>Hands on</li>
 <li>Flexible, must be able to attend meetings</li>
 <li>Good verbal and written communication as they will have to give their recommendations on data transfer to leadership team</li>
 <li>This will be a high visibility role</li>
</ul>
<p>Job Type: Contract</p>
<p>Salary: &#x24;55.00 - &#x24;60.00 per hour</p>
<p>Experience:</p>
<ul>
 <li>ETL: 1 year (Preferred)</li>
 <li>AWS: 1 year (Preferred)</li>
 <li>Redshift: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,34d9149b4a61c486,,Contract,,,Remote,Sr Data Engineer - Only W2,Today,2023-10-19T13:59:10.446Z,3.8,5.0,$55 - $60 an hour,2023-10-19T13:59:10.448Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=34d9149b4a61c486&from=jasx&tk=1hd43l8mokhqg800&vjs=3
119,Booz Allen Hamilton,"Job Description 
  
 
 
  
   
    
     
      
       
        
         Location: 
        
        
         Springfield,VA,US 
        
       
       
        
         Remote Work: 
        
        
         Yes 
        
       
       
        
         Job Number: 
        
        
         R0182529
        
       
      
     
    
    
    
      
     
       
      
     
    
    
    
     
      
       
        
         Azure Data Engineer, Lead
          The Opportunity:
          The DHS Cube program is seeking an experienced Data Engineer for its data management, data integration, and business intelligence program. This high-visibility mission support data program is managed within Headquarters for the DHS and our numerous client stakeholders. Over the next year and beyond, this program will be looking to take on new challenges, including modernizing their business intelligence technologies through a move to cloud services, improving the data quality through the development of confidence scorecards, and development of a data governance or data standards prototype.
         
          As part of this role, our Azure ETL Engineer will be responsible for moving ETL pipelines from one framework to a new framework. The role will also perform ETL testing and troubleshooting, including testing the unit data models, system performance testing, uploading, downloading, or querying speed tests, and data flow validations. In this role, you will also be building repeatable documentation assets to enable implementation teams to take advantage of pre-built assets, identifying data storage requirements, determining the storage needs of the customer, and ongoing development of a repository of ETL knowledge and utilizing that to enhance the quality, speed, and productivity of the team. As a senior member of the team, you’ll also be supporting more mid-level engineers with the testing and QA of their code along with an expectation of enhanced code velocity, design thinking, and the ability to work independently and provide feedback to the Data Architect on required design updates throughout the process.
         
          Join us. The world can’t wait.
         
          You Have: 
         
          8+ years of experience in a role encompassing industry standard ETL development techniques
           8+ years of experience with data integration, and database technologies, including Oracle, Postgres, Cosmos, or SQL
           3+ years of experience with Azure Cloud SaaS solutions and managed services serverless technologies, including Azure Data Factory, Synapse Analytics, Logic Apps, or ADLS
           Experience with scripting and basic programming, including JavaScript, shell script, or Python
           Experience with data analysis and profiling of source data while developing or building robust ETL processes
           Knowledge of disparate data sources and targets
           Knowledge of data validation, cleansing, transformation, consolidation, de-duplication, aggregation, de-aggregation, and enrichment
           Knowledge of API development and testing
           DHS Suitability
           Bachelor’s degree
         
         
          Nice If You Have:
         
           Experience with Agile and Scrum methodologies
           Experience with Azure Platform CI/CD or DevOps
           Experience with reporting tools, including Tableau or Power BI
           Experience with industry standard ETL tools such as SQL, scripting languages, data modelling techniques, or relational and NoSQL database engineering and configuration, including document store, Azure Cosmos, or AWS DynamoDB
           Knowledge of data transactional requirements, business logic pertaining to commit and rollback cycles, and how to implement to preserve the integrity of related data elements in government, financial and similar systems
           Ability to build ETL processes that apply to data migration and data integration scenarios to know their key differences
           Ability to adapt to a rapidly changing product and respond strategically to client needs
           Ability to balance multiple efforts simultaneously and meet strict deadlines
           Ability to have a desire for learning and development, and a passion for exploratory analysis or exploratory learning
         
         
          Vetting: 
         Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client; DHS suitability is required.
         
          Create Your Career:
         
          Grow With Us
          Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
         
          A Place Where You Belong
          Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll develop your community in no time.
         
          Support Your Well-Being
          Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
         
          Your Candidate Journey
          At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
         
          Compensation
          At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
          Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $93,300.00 to $212,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
         
          Work Model Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
         
           If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
           If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
         
         
          EEO Commitment
          We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.","<div>
 <div>
  <div>
   <h2 class=""jobSectionHeader""><b>Job Description</b></h2> 
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Location: 
        </div>
        <div>
         Springfield,VA,US 
        </div>
       </div>
       <div>
        <div>
         Remote Work: 
        </div>
        <div>
         Yes 
        </div>
       </div>
       <div>
        <div>
         Job Number: 
        </div>
        <div>
         R0182529
        </div>
       </div>
      </div>
     </div>
    </div>
    <p></p>
    <div>
     <br> 
     <div>
      <div> 
      </div>
     </div>
    </div>
    <div></div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Azure Data Engineer, Lead
         <p><b> The Opportunity:</b></p>
         <p> The DHS Cube program is seeking an experienced Data Engineer for its data management, data integration, and business intelligence program. This high-visibility mission support data program is managed within Headquarters for the DHS and our numerous client stakeholders. Over the next year and beyond, this program will be looking to take on new challenges, including modernizing their business intelligence technologies through a move to cloud services, improving the data quality through the development of confidence scorecards, and development of a data governance or data standards prototype.</p>
         <p></p>
         <p> As part of this role, our Azure ETL Engineer will be responsible for moving ETL pipelines from one framework to a new framework. The role will also perform ETL testing and troubleshooting, including testing the unit data models, system performance testing, uploading, downloading, or querying speed tests, and data flow validations. In this role, you will also be building repeatable documentation assets to enable implementation teams to take advantage of pre-built assets, identifying data storage requirements, determining the storage needs of the customer, and ongoing development of a repository of ETL knowledge and utilizing that to enhance the quality, speed, and productivity of the team. As a senior member of the team, you&#x2019;ll also be supporting more mid-level engineers with the testing and QA of their code along with an expectation of enhanced code velocity, design thinking, and the ability to work independently and provide feedback to the Data Architect on required design updates throughout the process.</p>
         <p></p>
         <p> Join us. The world can&#x2019;t wait.</p>
         <p></p>
         <p><b> You Have: </b></p>
         <ul>
          <li>8+ years of experience in a role encompassing industry standard ETL development techniques</li>
          <li> 8+ years of experience with data integration, and database technologies, including Oracle, Postgres, Cosmos, or SQL</li>
          <li> 3+ years of experience with Azure Cloud SaaS solutions and managed services serverless technologies, including Azure Data Factory, Synapse Analytics, Logic Apps, or ADLS</li>
          <li> Experience with scripting and basic programming, including JavaScript, shell script, or Python</li>
          <li> Experience with data analysis and profiling of source data while developing or building robust ETL processes</li>
          <li> Knowledge of disparate data sources and targets</li>
          <li> Knowledge of data validation, cleansing, transformation, consolidation, de-duplication, aggregation, de-aggregation, and enrichment</li>
          <li> Knowledge of API development and testing</li>
          <li> DHS Suitability</li>
          <li> Bachelor&#x2019;s degree</li>
         </ul>
         <p></p>
         <p><b> Nice If You Have:</b></p>
         <ul>
          <li> Experience with Agile and Scrum methodologies</li>
          <li> Experience with Azure Platform CI/CD or DevOps</li>
          <li> Experience with reporting tools, including Tableau or Power BI</li>
          <li> Experience with industry standard ETL tools such as SQL, scripting languages, data modelling techniques, or relational and NoSQL database engineering and configuration, including document store, Azure Cosmos, or AWS DynamoDB</li>
          <li> Knowledge of data transactional requirements, business logic pertaining to commit and rollback cycles, and how to implement to preserve the integrity of related data elements in government, financial and similar systems</li>
          <li> Ability to build ETL processes that apply to data migration and data integration scenarios to know their key differences</li>
          <li> Ability to adapt to a rapidly changing product and respond strategically to client needs</li>
          <li> Ability to balance multiple efforts simultaneously and meet strict deadlines</li>
          <li> Ability to have a desire for learning and development, and a passion for exploratory analysis or exploratory learning</li>
         </ul>
         <p></p>
         <p><b> Vetting: </b></p>
         <p>Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client; DHS suitability is required.</p>
         <p></p>
         <p><b> Create Your Career:</b></p>
         <p></p>
         <p><b> Grow With Us</b></p>
         <p> Your growth matters to us&#x2014;that&#x2019;s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.</p>
         <p></p>
         <p><b> A Place Where You Belong</b></p>
         <p> Diverse perspectives cultivate collective ingenuity. Booz Allen&#x2019;s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you&#x2019;ll develop your community in no time.</p>
         <p></p>
         <p><b> Support Your Well-Being</b></p>
         <p> Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we&#x2019;ll support you as you pursue a balanced, fulfilling life&#x2014;at work and at home.</p>
         <p></p>
         <p><b> Your Candidate Journey</b></p>
         <p> At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we&#x2019;ve compiled a list of resources so you&#x2019;ll know what to expect as we forge a connection with you during your journey as a candidate with us.</p>
         <p></p>
         <p><b> Compensation</b></p>
         <p> At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen&#x2019;s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.</p>
         <p></p> Salary at Booz Allen is determined by various factors, including but not limited to location, the individual&#x2019;s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is &#x24;93,300.00 to &#x24;212,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen&#x2019;s total compensation package for employees.
         <p></p>
         <p><b> Work Model</b><br> Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.</p>
         <ul>
          <li> If this position is listed as remote or hybrid, you&#x2019;ll periodically work from a Booz Allen or client site facility.</li>
          <li> If this position is listed as onsite, you&#x2019;ll work with colleagues and clients in person, as needed for the specific role.</li>
         </ul>
         <p></p>
         <p><b> EEO Commitment</b></p>
         <p> We&#x2019;re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change &#x2013; no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.</p>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<p></p>",https://careers.boozallen.com/jobs/JobDetail/Springfield-Azure-Data-Engineer-Lead-R0182529/86836?source=JB-14400,bcb3fa3a95822e7e,,,,,"Springfield, VA","Azure Data Engineer, Lead",Today,2023-10-19T13:59:04.074Z,3.9,2512.0,"$93,300 - $212,000 a year",2023-10-19T13:59:04.076Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=bcb3fa3a95822e7e&from=jasx&tk=1hd43l8mokhqg800&vjs=3
120,MJH Life Sciences,"At MJH Life Sciences our success is measured by your success! If you set your standards high and want to contribute to a winning team, we’ll provide you with every opportunity to help grow our company and your career. Our associates come from all backgrounds, sharing one key quality: determination to succeed. We value being Service Focused, having a Passion for Winning, Innovation, Respect, Integrity, and Teamwork. Nothing means more to us than hiring people with these attributes. If you believe you’re right for the job, this is the place to prove it!
Job Title: Senior Data & Analytics Engineer
Reports to: Data Analytics Manager
As a Senior Data & Analytics Engineer at MJH, you will model data and work closely with data architects, data analysts, data product managers, data scientists, and other analytics engineers to provide clean, accurate datasets for use by the team and business stakeholders. You will spend your time transforming, testing, deploying, and documenting data. You will be the bridge between the raw data our Data Integrations team lands in the data lake and our analytical and business stakeholders. As the person most directly responsible for materializing critical data for end users, you will apply software engineering best practices, including version control and continuous integration to the analytics code base.
Job Responsibilities

 Assist the Data Analytics Manager in developing a highly scalable data warehouse and analytics platform that will assist with improved business outcomes.
 Assist with defining and developing the enterprise data architecture and best practices around processes, standards, methodologies, and data modeling.
 Collaborate with stakeholders to understand user needs and use cases to develop clear and compelling data models that support interactive and dynamic visualizations.
 Use dbt and Snowflake to iteratively deliver usable data models that enable analyst workflows and reverse ETL processes.
 Implement test-driven processes to ensure data accuracy.
 Assist project management in developing requirements and delivery expectations.
 When necessary, provide documentation and training to stakeholders to foster increased understanding and utilization of the delivered functionality.
 Become a subject matter expert on core business rules and systems.
 Be a champion of data privacy and quality.
 Perform other duties as assigned.

Desired Skills and Experience

 8+ years of experience as an analytics engineer, data engineer, data analyst, or ETL developer.
 3 + years of experience in Snowflake with exposure to administration
 2 + years of experience in DBT with advanced level like using jinja templates and creating macros.
 3+ years of experience in GitHub implementing CICD pipelines for data engineering.
 Experience with any reporting tool like Tableau.
 Advanced proficiency in SQL and data modeling strategies.
 Previous experience with ETL/ELT principles and data warehousing concepts.
 Great team player with the ability to interact and communicate with all levels of management.
 Excellent organizational, prioritization, and independent decision-making skills.
 Communicate clearly using excellent written and verbal skills.
 Experience with Segment, Fivetran, and Tableau is a plus.
 Familiarity with modeling strategies such as snowflake, star, OBT, and relational is preferred.
 Experience working in an Agile environment is preferred.
 dbt certification would be strongly preferred.
 Continuous learning mindset.
 Bachelor’s degree required.

Job Type: Full-time
COVID-19 considerations:Team is working remotely while we evaluate return to work scenarios.
#LI-REMOTE
MJH Life Sciences provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. All employees of MJH Life Sciences are employed “At Will.” This means that either the employee or the Company is free to end the employment relationship at any time, for any reason, with or without cause and with or without notice.
Job Type: Full-time
Pay: $130,000.00 - $145,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Referral program
 Vision insurance

Compensation package:

 Bonus opportunities
 Yearly bonus

Experience level:

 6 years

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Education:

 Bachelor's (Required)

Work Location: Remote","<p>At MJH Life Sciences our success is measured by your success! If you set your standards high and want to contribute to a winning team, we&#x2019;ll provide you with every opportunity to help grow our company and your career. Our associates come from all backgrounds, sharing one key quality: determination to succeed. We value being Service Focused, having a Passion for Winning, Innovation, Respect, Integrity, and Teamwork. Nothing means more to us than hiring people with these attributes. If you believe you&#x2019;re right for the job, this is the place to prove it!</p>
<p>Job Title: Senior Data &amp; Analytics Engineer</p>
<p>Reports to: Data Analytics Manager</p>
<p>As a Senior Data &amp; Analytics Engineer at MJH, you will model data and work closely with data architects, data analysts, data product managers, data scientists, and other analytics engineers to provide clean, accurate datasets for use by the team and business stakeholders. You will spend your time transforming, testing, deploying, and documenting data. You will be the bridge between the raw data our Data Integrations team lands in the data lake and our analytical and business stakeholders. As the person most directly responsible for materializing critical data for end users, you will apply software engineering best practices, including version control and continuous integration to the analytics code base.</p>
<p>Job Responsibilities</p>
<ul>
 <li>Assist the Data Analytics Manager in developing a highly scalable data warehouse and analytics platform that will assist with improved business outcomes.</li>
 <li>Assist with defining and developing the enterprise data architecture and best practices around processes, standards, methodologies, and data modeling.</li>
 <li>Collaborate with stakeholders to understand user needs and use cases to develop clear and compelling data models that support interactive and dynamic visualizations.</li>
 <li>Use dbt and Snowflake to iteratively deliver usable data models that enable analyst workflows and reverse ETL processes.</li>
 <li>Implement test-driven processes to ensure data accuracy.</li>
 <li>Assist project management in developing requirements and delivery expectations.</li>
 <li>When necessary, provide documentation and training to stakeholders to foster increased understanding and utilization of the delivered functionality.</li>
 <li>Become a subject matter expert on core business rules and systems.</li>
 <li>Be a champion of data privacy and quality.</li>
 <li>Perform other duties as assigned.</li>
</ul>
<p>Desired Skills and Experience</p>
<ul>
 <li>8+ years of experience as an analytics engineer, data engineer, data analyst, or ETL developer.</li>
 <li>3 + years of experience in Snowflake with exposure to administration</li>
 <li>2 + years of experience in DBT with advanced level like using jinja templates and creating macros.</li>
 <li>3+ years of experience in GitHub implementing CICD pipelines for data engineering.</li>
 <li>Experience with any reporting tool like Tableau.</li>
 <li>Advanced proficiency in SQL and data modeling strategies.</li>
 <li>Previous experience with ETL/ELT principles and data warehousing concepts.</li>
 <li>Great team player with the ability to interact and communicate with all levels of management.</li>
 <li>Excellent organizational, prioritization, and independent decision-making skills.</li>
 <li>Communicate clearly using excellent written and verbal skills.</li>
 <li>Experience with Segment, Fivetran, and Tableau is a plus.</li>
 <li>Familiarity with modeling strategies such as snowflake, star, OBT, and relational is preferred.</li>
 <li>Experience working in an Agile environment is preferred.</li>
 <li>dbt certification would be strongly preferred.</li>
 <li>Continuous learning mindset.</li>
 <li>Bachelor&#x2019;s degree required.</li>
</ul>
<p>Job Type: Full-time</p>
<p>COVID-19 considerations:<br>Team is working remotely while we evaluate return to work scenarios.</p>
<p>#LI-REMOTE</p>
<p>MJH Life Sciences provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. All employees of MJH Life Sciences are employed &#x201c;At Will.&#x201d; This means that either the employee or the Company is free to end the employment relationship at any time, for any reason, with or without cause and with or without notice.</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;130,000.00 - &#x24;145,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Referral program</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Bonus opportunities</li>
 <li>Yearly bonus</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>6 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Required)</li>
</ul>
<p>Work Location: Remote</p>",,afa1c3c5c9790b59,,Full-time,,,Hybrid remote,Senior Data and Analytics Engineer,Today,2023-10-19T13:59:18.125Z,2.6,8.0,"$130,000 - $145,000 a year",2023-10-19T13:59:18.127Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=afa1c3c5c9790b59&from=jasx&tk=1hd43k4db21ci002&vjs=3
121,CyberCoders,"Data Engineer 
 
  Job Title: Senior Data Engineer
   
  Remote: Yes, 100% Remote
   
  Job Type: Direct Hire
   
  Hours: Full-Time
   
  Base Salary Range: $140-185k (base) / year
   
   
  Given the clients work with government contracts, you must be an active US Citizen to apply. You will need to obtain a security clearance after hire, but do not need to have one currently. US CITIZENSHIP REQUIRED is for all Government Clearance. Thank you.
   
   Our team is looking for an experienced Senior Data Engineer to join us in working on top secret DOD cleared projects. Ideally someone with experience in time series and sensor data collecting with the ability to program in Python or R. We are building out a new data engineering team and this role would support our existing 4 data scientists in their efforts by building data pipelines, shaping data, etl, etc.
 
  What You Need for this Position
 
  Data Engineering / Cloud Engineering / Database Management Experience
  Incredible SQL Skills (MySQL / SQLite / Oracle)
  Programming experience (Python or R)
 
  
  Bonus Point For:
  
 
  Sensor Data / Imaging Data / Time Series Data
  IOT 
  Azure Gov / AWS
  Government Clearance
 
  What's In It for You
 
  Generous Base Salary
  401k (+match)
  Flexible Remote Schedule
  Health / Dental / Vision
  Vacation / PTO
  14 Paid holidays + Holiday break around new year / end of year
 
 
   So, if you are a Data Engineer with experience, please apply today!
 
 
   Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Hanna Frauen
 
 
 Applicants must be authorized to work in the U.S.
  CyberCoders is proud to be an Equal Opportunity Employer
  
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
  
  
 Your Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.","<div>
 Data Engineer 
 <div>
  <b>Job Title:</b> Senior Data Engineer
  <br> 
  <b>Remote:</b> Yes, 100% Remote
  <br> 
  <b>Job Type:</b> Direct Hire
  <br> 
  <b>Hours:</b> Full-Time
  <br> 
  <b>Base Salary Range:</b> &#x24;140-185k (base) / year
  <br> 
  <br> 
  <b>Given the clients work with government contracts, you must be an active US Citizen to apply. You will need to obtain a security clearance after hire, but do not need to have one currently. US CITIZENSHIP REQUIRED is for all Government Clearance. Thank you.</b>
  <br> 
  <br> Our team is looking for an experienced Senior Data Engineer to join us in working on top secret DOD cleared projects. Ideally someone with experience in time series and sensor data collecting with the ability to program in Python or R. We are building out a new data engineering team and this role would support our existing 4 data scientists in their efforts by building data pipelines, shaping data, etl, etc.
 </div>
 <h4 class=""jobSectionHeader""><b> What You Need for this Position</b></h4>
 <ul>
  <li>Data Engineering / Cloud Engineering / Database Management Experience</li>
  <li>Incredible SQL Skills (MySQL / SQLite / Oracle)</li>
  <li>Programming experience (Python or R)</li>
 </ul>
 <br> 
 <b> Bonus Point For:</b>
 <br> 
 <ul>
  <li>Sensor Data / Imaging Data / Time Series Data</li>
  <li>IOT </li>
  <li>Azure Gov / AWS</li>
  <li>Government Clearance</li>
 </ul>
 <h4 class=""jobSectionHeader""><b> What&apos;s In It for You</b></h4>
 <ul>
  <li>Generous Base Salary</li>
  <li>401k (+match)</li>
  <li>Flexible Remote Schedule</li>
  <li>Health / Dental / Vision</li>
  <li>Vacation / PTO</li>
  <li>14 Paid holidays + Holiday break around new year / end of year</li>
 </ul>
 <div>
   So, if you are a Data Engineer with experience, please apply today!
 </div>
 <div>
   Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Hanna Frauen
 </div>
 <ul></ul>
 <p>Applicants must be authorized to work in the U.S.</p>
 <b><br> CyberCoders is proud to be an Equal Opportunity Employer</b>
 <br> 
 <br> All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
 <br> 
 <br> 
 <b>Your Right to Work</b> &#x2013; In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
</div>",https://www.cybercoders.com/jobs/details/717927/?ad=recruiticsindeed,a4031cd344d4043a,,Full-time,,,"Raleigh, NC 27603",Data Engineer,Today,2023-10-19T13:59:04.681Z,3.9,83.0,"$145,000 - $185,000 a year",2023-10-19T13:59:04.684Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CpFJQzrgRR8WqXWK1qKKEqALWJw739KlKqr2H-MSI4ehQhkyt7GrkYFAdLCKqicVASi5yZbI4YMJT6zbNaVJQ9VpWTQVt8WNxM99zI9hhByKIns9pnYPaa26fTVfAngNXV5jjAnsIsJcpzAxA-ypRPi1v5DPjTXTN38MCAn2CXvF3yj9sLIp67z0lfd2fFK_BEes-36nURsAHwgBUduNjzH3NLqZ58B4QDR0b1-N2TBhb0sg7InzPSEV4iLNwvPHFyFVZ7nl1ouF5ty-RHCQdmWz52oYpldH-SmzUyXAnvLHs79mwyXwQxXYG3TBHt1K9wb0lEIEqC6QnXWMrpQrt-tL0kpwsuFxIIXAr2cMH1K06v2z5k8SnQhTO4rbSrFsblw_v0B1Fs0L_w7YiSk05vkPdnglSEqdZCZhaKWS0HzJxGvUapwTH4aJHMbt-BJZUuNRShHCeLnJ93EfkWJiCJ-DbLcKmo0PMHepeu6lN3tzXBRLpnbrbEsWJ_lg9-4fABnYD2BLHVm1QB7lyWJY07MWLSUqE1Z2Pw_sJBGq0DbLbG3FRqVw0AIBzwwu9_YCWU_s0fs_BcuU9T1oYoFjQc9Ro584fw0ROeHATs1_ieC6vaSS4qjwiFz098y6jDWHo0EZNuNS0A0v1WGLmjv2zOYI8e4ohiQushAUeW0YyYKGl-24MCGpc8mDsVdlYETYbrdd1R57vQ1gHnzHY0cPvxRhfjg7uIoZuJfKmKEuBiEnpRTxGu6esfwssMrV9oQalEgbQHKPWOfZuu4h2CpttPURbozbT63OlLEEixy9Ibn0NKWOK3dlDo1fYL-M4HXmBcOPFicn97UyjcM4CWm-ueeQmMPzoKTQy_9i4sRgh3vEPIgJdugN63M2SxNRJgoezjB97X3sQl2KVZz0hwIRxb0CCcHZATKQGzax65lhdRfeUPKf9SMQPWoC4rYf6uViefA6DnqwBrykEX67v7BwXjry0ZVgPBCmkY-FDcajXuGIKsVWlf6qpVwtz1k5OIJrKPQ3F8txGwrpKlE_yK2T_4yW5oy7bXo8bgAJzVvtivEGap4nyqeM7F88fu5YlmICq0B4_PfGRuza2XFzxvR0i46NCGwPIzh1oM6wRFZxhYcTsppa_Xacgg1YxF9vmINVNPQm1C6oZtLQVeABvR3ej2jubo90_xWMQFqyO4iD3nGvtP6UTOWVPYw5-AgRhM_JY%3D&xkcb=SoAB-_M3JkPawbyit50ObzkdCdPP&p=4&fvj=0&vjs=3&jsa=4254&tk=1hd43l8mokhqg800&from=jasx&wvign=1
125,Cognizant Technology Solutions,"We are Cognizant Artificial Intelligence 
  Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them. 
  With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks. 
  *You must be legally authorized to work in United States without the need of employer sponsorship, now or at any time in the future * 
  This is a remote position open to any qualified applicant in the United States 
 Job Title: Data Engineer (OLTP) 
  Requirement is for a Data Engineer (OLTP) who would be working on Data Engineering Database Design Data Modelling ETL/ELT pipeline Data Migration for OLTP Application. 
 
   Understand existing data model & create target data models for cloud applications as part of modernization efforts. 
 
 
   Developing Spark/Scala jobs for large Application Services in the Big Data and Data Warehouse space. 
 
 
   Collaborate and review new data models with application architects application teams and leadership for execution. 
 
 
   Responsible for the resolution of technical issues Design unit test cases and perform unit testing support SIT/UAT activities.
  
 
  Experience : 6 to 9yrs.
  
 
  Technical Skills : Data Engineering (OLTP) Data Modelling (OLTP) Dimensional modeling Database Design Data Migration Domain, ETL Validation, Databricks
  
 
  Roles & Responsibilities : 
 
 
   Design and development of solution for extraction transformation and loading of data in new database 
 
 
   Create transition state data strategy for dual system (transition state) 
 
 
   Design develop enhance and integrate PBM data subjects into a normalize relational model
  
 
   Deploy and maintain OLTP Relational and Data Warehouses/Data Marts Models
  
 
   Strong understanding and experience building ETL/ETL solutions for both structured and unstructured data sets 
 
 
   Experience in SQL and NoSQL databases 
 
 
   Experience in data migration programs from legacy to Cloud big data applications.
  
 
   Experience with Healthcare or Pharmacy business domain 
 
 
   6+ years Data Engineering Enterprise Data Architecture Data Modeling Solutions Designing Data Analysis and BI Solutions Delivery Data Warehouse implementation
  
 
   6+ years in conducting Data Analysis Data Profiling and Data Discovery on dispersed operational source systems
  
  Salary and Other Compensation: 
 The annual salary for this position is between $125,000.00 – $140,000.00 depending on experience and other qualifications of the successful candidate. 
  This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans. 
  Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements: 
 
  Medical/Dental/Vision/Life Insurance 
  Paid holidays plus Paid Time Off 
  401(k) plan and contributions 
  Long-term/Short-term Disability 
  Paid Parental Leave 
  Employee Stock Purchase Plan 
  
 Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law. 
 #LI-DC1 #CB #Ind123
  Employee Status : Full Time Employee
  Shift : Day Job
  Travel : No
  Job Posting : Oct 17 2023
 
 
   About Cognizant
  Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 
  Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
 
  Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
  If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.","<div>
 <p><b>We are Cognizant Artificial Intelligence</b></p> 
 <p> Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them.</p> 
 <p> With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks.</p> 
 <p> *<b>You must be legally authorized to work in United States without the need of employer sponsorship, now or at any time in the future *</b></p> 
 <p> This is a <b>remote position</b> open to any qualified applicant in the United States </p>
 <p><b>Job Title: </b><b>Data Engineer (OLTP</b>)</p> 
 <p> Requirement is for a Data Engineer (OLTP) who would be working on Data Engineering Database Design Data Modelling ETL/ELT pipeline Data Migration for OLTP Application. </p>
 <ul>
  <li> Understand existing data model &amp; create target data models for cloud applications as part of modernization efforts. </li>
 </ul>
 <ul>
  <li> Developing Spark/Scala jobs for large Application Services in the Big Data and Data Warehouse space. </li>
 </ul>
 <ul>
  <li> Collaborate and review new data models with application architects application teams and leadership for execution. </li>
 </ul>
 <ul>
  <li> Responsible for the resolution of technical issues Design unit test cases and perform unit testing support SIT/UAT activities.</li>
 </ul> 
 <ul>
  <li><b>Experience : 6 to 9yrs.</b></li>
 </ul> 
 <ul>
  <li><b>Technical Skills : Data Engineering (OLTP)</b><b> </b><b>Data Modelling (OLTP)</b><b> </b><b>Dimensional modeling</b><b> </b><b>Database Design</b><b> </b><b>Data Migration Domain, ETL Validation, Databricks</b></li>
 </ul> 
 <ul>
  <li><b>Roles &amp; Responsibilities : </b></li>
 </ul>
 <ul>
  <li> Design and development of solution for extraction transformation and loading of data in new database </li>
 </ul>
 <ul>
  <li> Create transition state data strategy for dual system (transition state) </li>
 </ul>
 <ul>
  <li> Design develop enhance and integrate PBM data subjects into a normalize relational model</li>
 </ul> 
 <ul>
  <li> Deploy and maintain OLTP Relational and Data Warehouses/Data Marts Models</li>
 </ul> 
 <ul>
  <li> Strong understanding and experience building ETL/ETL solutions for both structured and unstructured data sets </li>
 </ul>
 <ul>
  <li> Experience in SQL and NoSQL databases </li>
 </ul>
 <ul>
  <li> Experience in data migration programs from legacy to Cloud big data applications.</li>
 </ul> 
 <ul>
  <li> <b>Experience with Healthcare or Pharmacy business domain</b> </li>
 </ul>
 <ul>
  <li> 6+ years Data Engineering Enterprise Data Architecture Data Modeling Solutions Designing Data Analysis and BI Solutions Delivery Data Warehouse implementation</li>
 </ul> 
 <ul>
  <li> 6+ years in conducting Data Analysis Data Profiling and Data Discovery on dispersed operational source systems</li>
 </ul> 
 <p><b> Salary and Other Compensation</b>: </p>
 <p>The annual salary for this position is between &#x24;125,000.00 &#x2013; &#x24;140,000.00 depending on experience and other qualifications of the successful candidate.</p> 
 <p> This position is also eligible for Cognizant&#x2019;s discretionary annual incentive program, based on performance and subject to the terms of Cognizant&#x2019;s applicable plans.</p> 
 <p><b> Benefits</b>: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements: </p>
 <ul>
  <li>Medical/Dental/Vision/Life Insurance</li> 
  <li>Paid holidays plus Paid Time Off</li> 
  <li>401(k) plan and contributions</li> 
  <li>Long-term/Short-term Disability</li> 
  <li>Paid Parental Leave</li> 
  <li>Employee Stock Purchase Plan</li> 
 </ul> 
 <p><b>Disclaimer: </b>The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law. </p>
 <p><b>#LI-DC1 #CB #Ind123</b></p>
 <p><b> Employee Status : </b>Full Time Employee</p>
 <p><b> Shift : </b>Day Job</p>
 <p><b> Travel : </b>No</p>
 <p><b> Job Posting : </b>Oct 17 2023</p>
 <p></p>
 <div>
  <b> About Cognizant</b>
 </div> Cognizant (Nasdaq-100: CTSH) is one of the world&apos;s leading professional services companies, transforming clients&apos; business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 <p></p>
 <p> Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.</p>
 <p></p>
 <p> Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.</p>
 <p> If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.</p>
</div>
<p></p>",https://click.appcast.io/track/hr8ggcb-org?cs=hqw&jg=6mnw&ittk=HYBKE1YVGF,69367ead0238ca43,,Full-time,,,"Chicago, IL 60601",Data Engineer (OLTP),1 day ago,2023-10-18T13:59:48.706Z,3.9,15970.0,"$125,000 - $140,000 a year",2023-10-19T13:59:48.709Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=69367ead0238ca43&from=jasx&tk=1hd43nlmik6qs800&vjs=3
150,ServiceNow,"Company Description
  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. 
 With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. 
 Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. 
 Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.
  Job Description
  As a Senior Staff Data and Software Engineer, you will be responsible for developing and implementing cutting-edge technical solutions that align with our organization's business objectives. You will work closely with stakeholders to understand their needs, assess existing systems and infrastructure, and design robust and scalable data and mircroservices solutions that drive innovation and efficiency. Your role will require a combination of technical expertise, strategic thinking, and effective communication to bridge the gap between business and technology.
 
  Key Responsibilities:
 
   Solution Design: Collaborate with business leaders, project managers, and technical teams to understand requirements and design holistic technical solutions that address current and future needs.
   Architecture Planning: Develop and maintain technology roadmaps, ensuring alignment with organizational goals and industry best practices.
   Technical Leadership: Provide technical leadership and guidance to development teams, ensuring adherence to architectural standards and best practices.
   Risk Assessment: Identify and evaluate technical risks and propose mitigation strategies to ensure project success and data security.
   Documentation: Create and maintain comprehensive architecture documentation, including diagrams, guidelines, and standards for development teams to follow.
   Vendor Evaluation: Assess and recommend third-party tools, products, and services that can enhance our technical solutions.
   Prototyping: Develop proof-of-concept and prototype solutions to validate architectural decisions and demonstrate feasibility.
   Performance Optimization: Continuously monitor and analyze system performance, identifying areas for improvement and optimizing existing solutions.
   Security and Compliance: Ensure that solutions comply with industry regulations and security standards, and proactively address security vulnerabilities.
   Collaboration: Foster collaboration and effective communication between cross-functional teams, promoting a culture of innovation and excellence.
 
  
  Qualifications
  Qualifications:
 
   Bachelor's degree in Computer Science, Information Technology, or related field (Master's degree preferred).
   Proven experience as a Lead Engineer and Solution Architect or a similar role.
   Strong knowledge of enterprise architecture principles and best practices.
   Proficiency in designing and implementing solutions using various technologies and platforms.
   Excellent problem-solving and analytical skills.
   Outstanding communication and interpersonal abilities.
   Project management skills and experience in managing complex technical projects.
   Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus.
 
  Preferred Skills:
 
   Cloud computing expertise (e.g., AWS, Azure, Google Cloud Platform).
   Knowledge of DevOps practices and tools.
   Familiarity with microservices architecture, expertise a plus.
   Familiarity with graph databases, expertise a plus.
   Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes).
   Strong understanding of data architecture and database technologies.
   Knowledge of cybersecurity best practices.
   Excellent presentation and facilitation skills.
 
  #DTjobs
  For positions in the Bay Area, we offer a base pay of $184,700 - $323,300, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.
  Additional Information
  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. 
 At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. 
 If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. 
 For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. 
 Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.
  
 From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license. 
 Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.","<div>
 <b>Company Description</b>
 <p><br> At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can&#x2019;t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.</p> 
 <p>With more than 7,700+ customers, we serve approximately 85% of the Fortune 500&#xae;, and we&apos;re proud to be one of FORTUNE 100 Best Companies to Work For&#xae; and World&apos;s Most Admired Companies&#x2122;.</p> 
 <p>Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.</p> 
 <p>Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.</p>
 <b><br> Job Description</b>
 <p><br> As a Senior Staff Data and Software Engineer, you will be responsible for developing and implementing cutting-edge technical solutions that align with our organization&apos;s business objectives. You will work closely with stakeholders to understand their needs, assess existing systems and infrastructure, and design robust and scalable data and mircroservices solutions that drive innovation and efficiency. Your role will require a combination of technical expertise, strategic thinking, and effective communication to bridge the gap between business and technology.</p>
 <p></p>
 <p><b><br> Key Responsibilities:</b></p>
 <ul>
  <li><b> Solution Design:</b> Collaborate with business leaders, project managers, and technical teams to understand requirements and design holistic technical solutions that address current and future needs.</li>
  <li><b> Architecture Planning:</b> Develop and maintain technology roadmaps, ensuring alignment with organizational goals and industry best practices.</li>
  <li><b> Technical Leadership:</b> Provide technical leadership and guidance to development teams, ensuring adherence to architectural standards and best practices.</li>
  <li><b> Risk Assessment:</b> Identify and evaluate technical risks and propose mitigation strategies to ensure project success and data security.</li>
  <li><b> Documentation:</b> Create and maintain comprehensive architecture documentation, including diagrams, guidelines, and standards for development teams to follow.</li>
  <li><b> Vendor Evaluation:</b> Assess and recommend third-party tools, products, and services that can enhance our technical solutions.</li>
  <li><b> Prototyping:</b> Develop proof-of-concept and prototype solutions to validate architectural decisions and demonstrate feasibility.</li>
  <li><b> Performance Optimization:</b> Continuously monitor and analyze system performance, identifying areas for improvement and optimizing existing solutions.</li>
  <li><b> Security and Compliance:</b> Ensure that solutions comply with industry regulations and security standards, and proactively address security vulnerabilities.</li>
  <li><b> Collaboration:</b> Foster collaboration and effective communication between cross-functional teams, promoting a culture of innovation and excellence.</li>
 </ul>
 <br> 
 <b> Qualifications</b>
 <p><b><br> Qualifications:</b></p>
 <ul>
  <li> Bachelor&apos;s degree in Computer Science, Information Technology, or related field (Master&apos;s degree preferred).</li>
  <li> Proven experience as a Lead Engineer and Solution Architect or a similar role.</li>
  <li> Strong knowledge of enterprise architecture principles and best practices.</li>
  <li> Proficiency in designing and implementing solutions using various technologies and platforms.</li>
  <li> Excellent problem-solving and analytical skills.</li>
  <li> Outstanding communication and interpersonal abilities.</li>
  <li> Project management skills and experience in managing complex technical projects.</li>
  <li> Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus.</li>
 </ul>
 <p><b> Preferred Skills:</b></p>
 <ul>
  <li> Cloud computing expertise (e.g., AWS, Azure, Google Cloud Platform).</li>
  <li> Knowledge of DevOps practices and tools.</li>
  <li> Familiarity with microservices architecture, expertise a plus.</li>
  <li> Familiarity with graph databases, expertise a plus.</li>
  <li> Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes).</li>
  <li> Strong understanding of data architecture and database technologies.</li>
  <li> Knowledge of cybersecurity best practices.</li>
  <li> Excellent presentation and facilitation skills.</li>
 </ul>
 <p> #DTjobs</p>
 <p> For positions in the Bay Area, we offer a base pay of &#x24;184,700 - &#x24;323,300, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.</p>
 <b><br> Additional Information</b>
 <p><br> ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.</p> 
 <p>At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.</p> 
 <p>If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.</p> 
 <p>For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.</p> 
 <p>Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.</p>
 <p><br> </p>
 <p>From Fortune. &#xa9; 2022 Fortune Media IP Limited All rights reserved. Used under license.</p> 
 <p>Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.</p>
</div>",https://careers.servicenow.com/careers/jobs/743999938047893EXT?lang=en-us&trid=35ab2906-b356-4a56-8472-f60d30d2e2f0,226571fd23411b51,,Full-time,,,"176 N Racine Ave, Chicago, IL 60607",Senior Staff Data and Services Software Engineer,Today,2023-10-19T14:01:59.759Z,3.7,239.0,"$184,700 - $323,300 a year",2023-10-19T14:01:59.761Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=226571fd23411b51&from=jasx&tk=1hd43l8mokhqg800&vjs=3
153,Info Origin,"Job Title : Data Engineer
 Duration : 6+ Months (C2H)
 Location : Armonk, NY (Remote)
 Interview : Webcam/Skype

Role Description:
We seek a data engineer to revolutionize North American accident and Health reporting and business decision processes. Responsibilities include crafting and maintaining data pipelines, enhancing data quality, and implementing agile methodologies for project execution.
Responsibilities:
The role involves developing data pipelines for various Accident and Health workstreams, applying ETL methodologies, and ensuring data quality. You'll build performance dashboards, generate automated reports, and work with SQL and Python for data manipulation. Additionally, you'll create end-to-end data solutions, utilize Apache Spark, and document pipeline processes and data assets.
Requirements:

 Bachelor's degree in Computer Science, Data Science, or a related quantitative field.
 5+ years of data engineering experience.
 Proficiency in Python and SQL, along with knowledge of data technologies like Hadoop, Spark, Java, Python, and more.
 Familiarity with relational databases.
 Strong attention to detail and analytical skills.
 Effective communication and organizational abilities.
 Self-reliant and collaborative.
 Capability to drive organizational transformations smoothly.

Job Type: Contract
Salary: $60.00 - $70.00 per hour
Schedule:

 8 hour shift

Work Location: Remote","<ul>
 <li><b>Job Title : Data Engineer</b></li>
 <li><b>Duration : 6+ Months (C2H)</b></li>
 <li><b>Location : Armonk, NY (Remote)</b></li>
 <li><b>Interview : Webcam/Skype</b></li>
</ul>
<p><b>Role Description:</b></p>
<p>We seek a data engineer to revolutionize North American accident and Health reporting and business decision processes. Responsibilities include crafting and maintaining data pipelines, enhancing data quality, and implementing agile methodologies for project execution.</p>
<p><b>Responsibilities:</b></p>
<p>The role involves developing data pipelines for various Accident and Health workstreams, applying ETL methodologies, and ensuring data quality. You&apos;ll build performance dashboards, generate automated reports, and work with SQL and Python for data manipulation. Additionally, you&apos;ll create end-to-end data solutions, utilize Apache Spark, and document pipeline processes and data assets.</p>
<p><b>Requirements:</b></p>
<ul>
 <li>Bachelor&apos;s degree in Computer Science, Data Science, or a related quantitative field.</li>
 <li>5+ years of data engineering experience.</li>
 <li>Proficiency in Python and SQL, along with knowledge of data technologies like Hadoop, Spark, Java, Python, and more.</li>
 <li>Familiarity with relational databases.</li>
 <li>Strong attention to detail and analytical skills.</li>
 <li>Effective communication and organizational abilities.</li>
 <li>Self-reliant and collaborative.</li>
 <li>Capability to drive organizational transformations smoothly.</li>
</ul>
<p>Job Type: Contract</p>
<p>Salary: &#x24;60.00 - &#x24;70.00 per hour</p>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,4539c1f9e485ed4e,,Contract,,,Remote,Data Engineer (W2 Only),5 days ago,2023-10-14T14:02:27.487Z,,,$60 - $70 an hour,2023-10-19T14:02:27.488Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=4539c1f9e485ed4e&from=jasx&tk=1hd43sbgnkiai800&vjs=3
160,P3 Adaptive,"P3 Adaptive (https://p3adaptive.com/) has been the pioneer of data empowerment since 2010. Our mission is to re-shape the entire analytics industry taking it from the current lumbering and friction-filled incarnation and turning it into the nimble and revolutionary movement it's always supposed to have been.
We are actively hiring data engineers with an intermediate or better working knowledge of Microsoft Azure Data and Analytics Services. If you are experienced in creating solutions with Azure Data Factory, Azure Synapse, Azure Data Lake, and Azure SQL Server, you would be a great fit for this position.As a P3 Adaptive Data Engineer, you'll be intimately involved every day providing remote development for organizations in diverse industries that are working to adopt this new and improved way of thinking.Why Work for P3 Adaptive?Overall, what we do is really cool and really important. As part of the P3 Adaptive team, you will work with the most exciting platforms in Microsoft's history to solve data issues across multiple business segments and industries. You will work with some of the brightest minds and most passionate people in the industry. You will make a difference from your first day and you won't stop there. With P3 Adaptive, you will have both the resources and the support you need to grow your career. If that isn't enough, you will also have direct access to leadership, the opportunity for advancement, and access to top-tier ongoing training.Finally, we're not your typical 9 to 5. P3 Adaptive is respectful of the need for work/life balance. With flexible schedules and the permanent ability to work remotely, you can make your schedule suit your personal needs.Job Duties:

 Support the execution of Power BI projects, working alongside expert Principal Consultants and Solution Architects.
 Create Data Storage Solutions with SQL Server and Data Lakes.
 Develop ETL Pipelines with Azure Data Factory.
 Provision Azure Subscriptions and Resources.
 Develop Automation Solutions using languages such as PowerShell and Python

Qualifications:

 Exceptional communication skills
 Excellent time management skills – multitasking is critical
 Experienced in Project Management
 Intermediate or better knowledge of T-SQL for DDL and DML applications.
 Experience with Azure Active Directory Security Groups and Role-Based Access Controls
 Experience with SSIS, SSAS preferred
 Experience with PowerShell and Python preferred
 Ability to connect with our clients
 Insatiable curiosity and love of learning
 Travel Optional

The ideal candidate will have:

 Prior Professional Services or Consulting

Benefits:

 Remote
 Dental, life, medical, vision, and 401k
 ST/LT Disability Insurance
 Generous PTO policy
 Continued training to level up your Power Skills
 Supplemental benefits available:
 Accident Insurance
 Critical Illness Insurance
 Hospital Indemnity
 Health Care FSA
 Dependent Day Care FSA
 Employee Assistance Program
 Paid parental leave
 Hardware and software allowance
 Personalized incentives

Do you meet the criteria? Are you ready to take control of your career?P3 Adaptive is proud to be an equal opportunity employer committed to diversity and inclusion. We consider all applications from suitably qualified persons regardless of their race, sex, disability, religion/belief, sexual orientation, or age. All employment decisions are decided on the basis of qualifications, merit, and business need.
Job Type: Full-time
Pay: $110,000.00 - $140,000.00 per year
Schedule:

 8 hour shift
 Choose your own hours
 Monday to Friday

Work Location: Remote","<p><b>P3 Adaptive</b> (https://p3adaptive.com/) has been the pioneer of data empowerment since 2010. Our mission is to re-shape the entire analytics industry taking it from the current lumbering and friction-filled incarnation and turning it into the nimble and revolutionary movement it&apos;s always supposed to have been.</p>
<p>We are actively hiring data engineers with an intermediate or better working knowledge of Microsoft Azure Data and Analytics Services. If you are experienced in creating solutions with Azure Data Factory, Azure Synapse, Azure Data Lake, and Azure SQL Server, you would be a great fit for this position.<br>As a P3 Adaptive Data Engineer, you&apos;ll be intimately involved every day providing remote development for organizations in diverse industries that are working to adopt this new and improved way of thinking.<br><b>Why Work for P3 Adaptive?</b><br>Overall, what we do is really cool and really important. As part of the P3 Adaptive team, you will work with the most exciting platforms in Microsoft&apos;s history to solve data issues across multiple business segments and industries. You will work with some of the brightest minds and most passionate people in the industry. You will make a difference from your first day and you won&apos;t stop there. With P3 Adaptive, you will have both the resources and the support you need to grow your career. If that isn&apos;t enough, you will also have direct access to leadership, the opportunity for advancement, and access to top-tier ongoing training.<br>Finally, we&apos;re not your typical 9 to 5. P3 Adaptive is respectful of the need for work/life balance. With flexible schedules and the permanent ability to work remotely, you can make your schedule suit your personal needs.<br><b>Job Duties:</b></p>
<ul>
 <li>Support the execution of Power BI projects, working alongside expert Principal Consultants and Solution Architects.</li>
 <li>Create Data Storage Solutions with SQL Server and Data Lakes.</li>
 <li>Develop ETL Pipelines with Azure Data Factory.</li>
 <li>Provision Azure Subscriptions and Resources.</li>
 <li>Develop Automation Solutions using languages such as PowerShell and Python</li>
</ul>
<p><b>Qualifications</b>:</p>
<ul>
 <li>Exceptional communication skills</li>
 <li>Excellent time management skills &#x2013; multitasking is critical</li>
 <li>Experienced in Project Management</li>
 <li>Intermediate or better knowledge of T-SQL for DDL and DML applications.</li>
 <li>Experience with Azure Active Directory Security Groups and Role-Based Access Controls</li>
 <li>Experience with SSIS, SSAS preferred</li>
 <li>Experience with PowerShell and Python preferred</li>
 <li>Ability to connect with our clients</li>
 <li>Insatiable curiosity and love of learning</li>
 <li>Travel Optional</li>
</ul>
<p><b>The ideal candidate will have:</b></p>
<ul>
 <li>Prior Professional Services or Consulting</li>
</ul>
<p><b>Benefits</b>:</p>
<ul>
 <li>Remote</li>
 <li>Dental, life, medical, vision, and 401k</li>
 <li>ST/LT Disability Insurance</li>
 <li>Generous PTO policy</li>
 <li>Continued training to level up your Power Skills</li>
 <li>Supplemental benefits available:</li>
 <li>Accident Insurance</li>
 <li>Critical Illness Insurance</li>
 <li>Hospital Indemnity</li>
 <li>Health Care FSA</li>
 <li>Dependent Day Care FSA</li>
 <li>Employee Assistance Program</li>
 <li>Paid parental leave</li>
 <li>Hardware and software allowance</li>
 <li>Personalized incentives</li>
</ul>
<p><b>Do you meet the criteria? Are you ready to take control of your career?</b><br><i>P3 Adaptive is proud to be an equal opportunity employer committed to diversity and inclusion. We consider all applications from suitably qualified persons regardless of their race, sex, disability, religion/belief, sexual orientation, or age. All employment decisions are decided on the basis of qualifications, merit, and business need.</i></p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;110,000.00 - &#x24;140,000.00 per year</p>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Choose your own hours</li>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,45cf7e5b6edba0bb,,Full-time,,,Remote,Principal Azure Data Engineer,5 days ago,2023-10-14T14:03:29.150Z,5.0,3.0,"$110,000 - $140,000 a year",2023-10-19T14:03:29.152Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=45cf7e5b6edba0bb&from=jasx&tk=1hd43uovekcmr800&vjs=3
172,Kforce,"RESPONSIBILITIES:
 Kforce has a client in Cincinnati, OH that is seeking a Level 3 Data Engineer to join their team on a 6-month contract to hire basis. This consultant can work fully remote. 

 
Summary:
 
 
 Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms 
 The Data Engineer will define high-level migration plans to address the gaps between the current and future state 
 Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions 
 Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement 
 Interpreting data, analyzing results using statistical techniques 
 Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality 
 The Data Engineer will be acquiring data from primary or secondary data sources and maintaining databases 
 Promote the reuse of data assets, including the management of the data catalog for reference 
 
REQUIREMENTS:
 
 
 Extensive knowledge of data principles, patterns, processes, and practices 
 Any experience defining evolutionary data solutions and underlying technologies 
 Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy 
 Experience with SQL database design, data modeling 
 Demonstrated written and oral communication skills 
 Basic understanding of network and data security architecture 
 Strong knowledge of industry trends 
 Knowledge in a minimum of two of the following technical disciplines: data warehousing, data management, analytics development, data science, application programming interfaces (APIs), data integration, cloud, servers and storage, and database management 
 Experience building cost effective and performance driven solutions using elastic architectures in Microsoft Azure leveraging Cosmos, Azure Data Factory, Azure Synapse and Databricks Platforms 
 Experience with SQL and NoSQL applications on Big Data Platforms 
 Experience with SSAS Tabular models, Power BI, Dataflows and DAX 
 Experience with Azure Data Platform stack: Azure Data Lake, Azure Synapse, Data Factory and Databricks 
 Experience with Python, Spark and SQL 
 Any experience with streaming technologies like Kafka, IBM MQ and EventHub 
 Experience with Google cloud Platform is a plus 
 
Able to perform the following duties:
 
 
 Analyze and organize raw data 
 Build data systems and pipelines 
 Evaluate business needs and objectives 
 Interpret trends and patterns 
 Conduct complex data analysis and report on results 
 The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future. 

 We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave. 

 
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law. 

 This job is not eligible for bonuses, incentives or commissions. 

 Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.","<b>RESPONSIBILITIES:</b>
<br> Kforce has a client in Cincinnati, OH that is seeking a Level 3 Data Engineer to join their team on a 6-month contract to hire basis. This consultant can work fully remote. 
<br>
<br> 
<b>Summary:</b>
<br> 
<ul> 
 <li>Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms</li> 
 <li>The Data Engineer will define high-level migration plans to address the gaps between the current and future state</li> 
 <li>Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions</li> 
 <li>Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement</li> 
 <li>Interpreting data, analyzing results using statistical techniques</li> 
 <li>Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality</li> 
 <li>The Data Engineer will be acquiring data from primary or secondary data sources and maintaining databases</li> 
 <li>Promote the reuse of data assets, including the management of the data catalog for reference</li> 
</ul> 
<b>REQUIREMENTS:</b>
<br> 
<ul> 
 <li>Extensive knowledge of data principles, patterns, processes, and practices</li> 
 <li>Any experience defining evolutionary data solutions and underlying technologies</li> 
 <li>Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy</li> 
 <li>Experience with SQL database design, data modeling</li> 
 <li>Demonstrated written and oral communication skills</li> 
 <li>Basic understanding of network and data security architecture</li> 
 <li>Strong knowledge of industry trends</li> 
 <li>Knowledge in a minimum of two of the following technical disciplines: data warehousing, data management, analytics development, data science, application programming interfaces (APIs), data integration, cloud, servers and storage, and database management</li> 
 <li>Experience building cost effective and performance driven solutions using elastic architectures in Microsoft Azure leveraging Cosmos, Azure Data Factory, Azure Synapse and Databricks Platforms</li> 
 <li>Experience with SQL and NoSQL applications on Big Data Platforms</li> 
 <li>Experience with SSAS Tabular models, Power BI, Dataflows and DAX</li> 
 <li>Experience with Azure Data Platform stack: Azure Data Lake, Azure Synapse, Data Factory and Databricks</li> 
 <li>Experience with Python, Spark and SQL</li> 
 <li>Any experience with streaming technologies like Kafka, IBM MQ and EventHub</li> 
 <li>Experience with Google cloud Platform is a plus</li> 
</ul> 
<b>Able to perform the following duties:</b>
<br> 
<ul> 
 <li>Analyze and organize raw data</li> 
 <li>Build data systems and pipelines</li> 
 <li>Evaluate business needs and objectives</li> 
 <li>Interpret trends and patterns</li> 
 <li>Conduct complex data analysis and report on results</li> 
</ul> The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future. 
<br>
<br> We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability &amp; ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave. 
<br>
<br> 
<b>Note:</b> Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce&apos;s sole discretion unless and until paid and may be modified in its discretion consistent with the law. 
<br>
<br> This job is not eligible for bonuses, incentives or commissions. 
<br>
<br> Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.",https://www.kforce.com/Jobs/job.aspx?job=1696~EQG~2092232T1~99&id=2128&utm_source=Indeed&utm_medium=PPC&utm_campaign=Indeed-PPC,308081f84698f671,,Contract,,,"Cincinnati, OH 45202",Data Engineer Level 3,6 days ago,2023-10-13T14:04:06.060Z,3.8,1879.0,$65.45 - $88.55 an hour,2023-10-19T14:04:06.062Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0C5IatSLh_Ak1q39eQQoPIxD737RW9NeiYGvIRXkrLjEBkC4LI6KweFMaB7igpdWMnsfRNLm1L2W8cPynQETfxrnswXwUz4DZXw4wzfB0OLKSrFPihfr1OM4maBMIYGBI3H9c6yqOrdJ66gAQt039hSSqL5ecKg-NpGiIIoTi_VN_JTHfg9E7I7GzPN-5w0CR9ru4g7gZUhDAh-r8zgCO_Oc_hnM8zcbzwpeoWBaPoxPMZc74lOQN7-3vT2v2LvwEA5ZeLMMci97VSugvfgvcH2FI32rUmQPv7YEvq8_eWL4R7nBibXozPzOUG43LFltBJkWZ9aTpekVs05N23Q0o8Vg-IquasFPHlkPH9TWQUCSd141JTCmmxwDPBZFNiL8BaZoyqxTH1Kv9I6wJ9e31r7iXga5wYzeYLGTFd9SqPMtL0TJMcYMv1QXHRjIkL7d4vUbgMrLOs45Ma4Ulk5H4s1dzkyQhwd0lZJPyUx-y9_TxZf-HyJkd0TRzPayI5L6suq2mWxRwc-9SAUHRo514reIGcbpCkvjltdIfwWy-ViQXVuGeXME293jf5Nri55BtQHPel8ILKKJEbuv8I-puJrQUvkxx68HonsS6m9QRyAmIHiXfVUDCmcBEsR13nKPDeciEcaGb4VZUnt-s-wfC30iFXHtZeRsOscqv9GyuF-KbyveAzLoDA_kxmowVH9OevfyX_bjz9OPTEIPv2nDcL4&xkcb=SoD8-_M3JkAPCWRkDx0FbzkdCdPP&p=14&fvj=0&vjs=3&jsa=2411&tk=1hd4406f5kf31800&from=jasx&wvign=1
181,LiveRamp,"LiveRamp is the data collaboration platform of choice for the world’s most innovative companies. A groundbreaking leader in consumer privacy, data ethics, and foundational identity, LiveRamp is setting the new standard for building a connected customer view with unmatched clarity and context while protecting precious brand and consumer trust. LiveRamp offers complete flexibility to collaborate wherever data lives to support the widest range of data collaboration use cases—within organizations, between brands, and across its premier global network of top-quality partners.
 
 
 
   Hundreds of global innovators, from iconic consumer brands and tech giants to banks, retailers, and healthcare leaders turn to LiveRamp to build enduring brand and business value by deepening customer engagement and loyalty, activating new partnerships, and maximizing the value of their first-party data while staying on the forefront of rapidly evolving compliance and privacy requirements.
 
 
 
   LiveRamp is the leading data connectivity platform. We believe connected data has the power to change the world. Our platform powers insights and experiences centered around the needs of real people, and in ways that keep the Internet open for all. LiveRampers thrive on building together with curiosity and humility—and have a good bit of fun along the way. We’re always looking for smart, kind, and creative people to grow our team and impact.
   
   Mission: LiveRamp makes it safe and easy for businesses to use data effectively.
 
 
   The Activations Back End team is responsible for the bulk of big data processing that powers LiveRamp’s primary product (a product that delivers several hundred million in annual recurring revenue). Our systems process tens of thousands of batch requests per day, ranging in size from GBs to tens of TBs, and provide detailed monitoring, statistics, error recovery, and resiliency to power this core product.
   
   At LiveRamp big data processing is not just for analytics to uncover business insights. Our product fundamentally is a big data product, as we transform, make useful, and transport massive datasets to hundreds of integrations. In doing so, we enable many of the most successful companies on the planet.
   
   You will:
 
 
   Work collaboratively with a small team of engineers to reinvent a system already processing petabytes of data every day.
   Play a key role in transforming Activations Back End systems to being cloud-agnostic, multi-regional, and fundamentally more extensible
   Building new pipelines and products with SQL on cloud data warehouses
   Help shape the future of our big data technology platform.
   Infuse industry best practices into our team processes and development practices.
   Assist in architectural design and implementation of our systems and interfaces.
   Develop our data processing pipelines using technology such as Spark, Dataproc, and Kubernetes.
   Research and experiment strategies to optimize the performance and efficiency of petabytes of data processing.
   Foster a positive environment of integrity, empowerment, initiative, and teamwork.
   Provide operational support for our team’s production systems.
   Share the remarkable work of our Activations team with the world through tech talks, blog posts, whitepapers, and conference participation.
   Learn and grow in your role and career.
 
 
   Your team will:
 
 
   Rearchitect its platform to power our products in multiple clouds and regions, as well as to take advantage of cutting-edge cloud data warehouses, like Snowflake and SingleStore, which unlocks many new exciting possibilities.
   Deliver 1-> N Global deployments, Novel Uses of Cloud Data Warehouses, 100x Processing Speedups, and other Paradigm shifts.
   Play an important role in improving LiveRamp’s foundational products
 
 
   About you: :
 
 
   3+ years of experience writing and deploying high-quality production code.
   Have made significant and meaningful development contributions to team projects involving distributed systems operating at scale.
   Excitement to learn and improve, comfort with ambiguity
   Communication skills to share highly technical information with technical and non-technical teammates.
   Deeply inquisitive. Always asking why we do things and how we can do them better.
   Thrive in a collaborative environment.
   Demonstrated ability to build sustainable technical solutions that verifiably meet specific, well-defined business requirements.
   Are you the type of person that likes running 200TB spark shuffles, 100x-ing job times, and or having an insight that saves $1 million in recurring cloud infrastructure cost? Call us
   Experience architecting and improving system performance on cloud data warehouses
   Experience writing and managing complex SQL queries
 
 
   Benefits:
 
 
   People: Work with talented, collaborative, and friendly people who love what they do.
   Fun: We host in-person and virtual events such as game nights, happy hours, camping trips, and sports leagues.
   Work/Life Harmony: Flexible paid time off, paid holidays, options for working from home, and paid parental leave.
   Comprehensive Benefits Package: Medical, dental, vision, life, and disability. Plus, mental health support (via Talkspace), flexible time off, parental leave, family forming benefits, and a flexible lifestyle and wellbeing reimbursement program (up to $375 per quarter, U.S. LiveRampers)
   Savings: Our 401K matching plan—1:1 match up to 6% of salary—helps you plan ahead. Also Employee Stock Purchase Plan - 15% discount off purchase price of LiveRamp stock (U.S. LiveRampers)
 
 
 
   The approximate annual base compensation range is $130,000 to $190,000 The actual offer, reflecting the total compensation package and benefits, will be determined by a number of factors including the applicant's experience, knowledge, skills, and abilities, geography, as well as internal equity among our team.
 
 
 
   More about us:
 
 
   LiveRamp’s mission is to connect data in ways that matter, and doing so starts with our people. We know that inspired teams enlist people from a blend of backgrounds and experiences. And we know that individuals do their best when they not only bring their full selves to work but feel like they truly belong. Connecting LiveRampers to new ideas and one another is one of our guiding principles—one that informs how we hire, train, and grow our global team across nine countries and four continents. Click 
  
   here 
  
  to learn more about Diversity, Inclusion, & Belonging (DIB) at LiveRamp.","<div>
 <div>
  LiveRamp is the data collaboration platform of choice for the world&#x2019;s most innovative companies. A groundbreaking leader in consumer privacy, data ethics, and foundational identity, LiveRamp is setting the new standard for building a connected customer view with unmatched clarity and context while protecting precious brand and consumer trust. LiveRamp offers complete flexibility to collaborate wherever data lives to support the widest range of data collaboration use cases&#x2014;within organizations, between brands, and across its premier global network of top-quality partners.
 </div>
 <div></div>
 <div>
   Hundreds of global innovators, from iconic consumer brands and tech giants to banks, retailers, and healthcare leaders turn to LiveRamp to build enduring brand and business value by deepening customer engagement and loyalty, activating new partnerships, and maximizing the value of their first-party data while staying on the forefront of rapidly evolving compliance and privacy requirements.
 </div>
 <div></div>
 <div>
  <br> LiveRamp is the leading data connectivity platform. We believe connected data has the power to change the world. Our platform powers insights and experiences centered around the needs of real people, and in ways that keep the Internet open for all. LiveRampers thrive on building together with curiosity and humility&#x2014;and have a good bit of fun along the way. We&#x2019;re always looking for smart, kind, and creative people to grow our team and impact.
  <br> 
  <br> Mission: LiveRamp makes it safe and easy for businesses to use data effectively.
 </div>
 <div>
  <br> The Activations Back End team is responsible for the bulk of big data processing that powers LiveRamp&#x2019;s primary product (a product that delivers several hundred million in annual recurring revenue). Our systems process tens of thousands of batch requests per day, ranging in size from GBs to tens of TBs, and provide detailed monitoring, statistics, error recovery, and resiliency to power this core product.
  <br> 
  <br> At LiveRamp big data processing is not just for analytics to uncover business insights. Our product fundamentally is a big data product, as we transform, make useful, and transport massive datasets to hundreds of integrations. In doing so, we enable many of the most successful companies on the planet.
  <br> 
  <br> You will:
 </div>
 <ul>
  <li> Work collaboratively with a small team of engineers to reinvent a system already processing petabytes of data every day.</li>
  <li> Play a key role in transforming Activations Back End systems to being cloud-agnostic, multi-regional, and fundamentally more extensible</li>
  <li> Building new pipelines and products with SQL on cloud data warehouses</li>
  <li> Help shape the future of our big data technology platform.</li>
  <li> Infuse industry best practices into our team processes and development practices.</li>
  <li> Assist in architectural design and implementation of our systems and interfaces.</li>
  <li> Develop our data processing pipelines using technology such as Spark, Dataproc, and Kubernetes.</li>
  <li> Research and experiment strategies to optimize the performance and efficiency of petabytes of data processing.</li>
  <li> Foster a positive environment of integrity, empowerment, initiative, and teamwork.</li>
  <li> Provide operational support for our team&#x2019;s production systems.</li>
  <li> Share the remarkable work of our Activations team with the world through tech talks, blog posts, whitepapers, and conference participation.</li>
  <li> Learn and grow in your role and career.</li>
 </ul>
 <div>
  <br> Your team will:
 </div>
 <ul>
  <li> Rearchitect its platform to power our products in multiple clouds and regions, as well as to take advantage of cutting-edge cloud data warehouses, like Snowflake and SingleStore, which unlocks many new exciting possibilities.</li>
  <li> Deliver 1-&gt; N Global deployments, Novel Uses of Cloud Data Warehouses, 100x Processing Speedups, and other Paradigm shifts.</li>
  <li> Play an important role in improving LiveRamp&#x2019;s foundational products</li>
 </ul>
 <div>
  <br> About you: :
 </div>
 <ul>
  <li> 3+ years of experience writing and deploying high-quality production code.</li>
  <li> Have made significant and meaningful development contributions to team projects involving distributed systems operating at scale.</li>
  <li> Excitement to learn and improve, comfort with ambiguity</li>
  <li> Communication skills to share highly technical information with technical and non-technical teammates.</li>
  <li> Deeply inquisitive. Always asking why we do things and how we can do them better.</li>
  <li> Thrive in a collaborative environment.</li>
  <li> Demonstrated ability to build sustainable technical solutions that verifiably meet specific, well-defined business requirements.</li>
  <li> Are you the type of person that likes running 200TB spark shuffles, 100x-ing job times, and or having an insight that saves &#x24;1 million in recurring cloud infrastructure cost? Call us</li>
  <li> Experience architecting and improving system performance on cloud data warehouses</li>
  <li> Experience writing and managing complex SQL queries</li>
 </ul>
 <div>
   Benefits:
 </div>
 <ul>
  <li> People: Work with talented, collaborative, and friendly people who love what they do.</li>
  <li> Fun: We host in-person and virtual events such as game nights, happy hours, camping trips, and sports leagues.</li>
  <li> Work/Life Harmony: Flexible paid time off, paid holidays, options for working from home, and paid parental leave.</li>
  <li> Comprehensive Benefits Package: Medical, dental, vision, life, and disability. Plus, mental health support (via Talkspace), flexible time off, parental leave, family forming benefits, and a flexible lifestyle and wellbeing reimbursement program (up to &#x24;375 per quarter, U.S. LiveRampers)</li>
  <li> Savings: Our 401K matching plan&#x2014;1:1 match up to 6% of salary&#x2014;helps you plan ahead. Also Employee Stock Purchase Plan - 15% discount off purchase price of LiveRamp stock (U.S. LiveRampers)</li>
 </ul>
 <div></div>
 <div>
  <i> The approximate annual base compensation range is &#x24;130,000 to &#x24;190,000 The actual offer, reflecting the total compensation package and benefits, will be determined by a number of factors including the applicant&apos;s experience, knowledge, skills, and abilities, geography, as well as internal equity among our team.</i>
 </div>
 <div></div>
 <div>
  <i><br> More about us:</i>
 </div>
 <div>
  <i> LiveRamp&#x2019;s mission is to connect data in ways that matter, and doing so starts with our people. We know that inspired teams enlist people from a blend of backgrounds and experiences. And we know that individuals do their best when they not only bring their full selves to work but feel like they truly belong. Connecting LiveRampers to new ideas and one another is one of our guiding principles&#x2014;one that informs how we hire, train, and grow our global team across nine countries and four continents. Click </i>
  <div>
   <i>here </i>
  </div>
  <i>to learn more about Diversity, Inclusion, &amp; Belonging (DIB) at LiveRamp.</i>
 </div>
</div>
<div></div>",https://liveramp.wd5.myworkdayjobs.com/en-US/LiveRampCareers/job/Remote---US/Senior-Software-Engineer--Big-Data-Activation_JR010020,97f5548d6f257ea6,,Full-time,,,Remote,"Senior Software Engineer, Big Data Activation",30+ days ago,2023-09-19T14:05:03.131Z,3.3,4.0,"$130,000 - $190,000 a year",2023-10-19T14:05:03.133Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=97f5548d6f257ea6&from=jasx&tk=1hd440jnukecp800&vjs=3
183,Rise Technical Recruitment Limited,"Data Engineer (AWS)  $100,000 + 401k + PTO + Medical + Dental + Vision New York - Fully Remote
  
 Are you a Data Engineer looking for an exciting new opportunity where you will progress in your technical skills while growing in your career?
  On offer is an exciting opportunity to join an innovative health tech development team. You'll be at the forefront of expanding and optimizing their data infrastructure, playing a vital role in transforming healthcare with cutting-edge technology.
   This company is at the pinnacle of revolutionizing healthcare with data-driven solutions. You'll create and maintain data pipeline architecture, work with big data technologies while collaborating with your team enabling them to leverage data effectively.
  This role would suit an experienced Data Engineer looking for a chance to advance their career in data engineering, expand on their technical knowledge and be part of a team to transform technology based healthcare.
  The Person
 
  3+ Years Experience in Data Engineering
  ETL pipeline experience
  Experience using SQL, Python, Data Tools and AWS
  Knowledge of Postgres, Go and C++ is beneficial
  Degree in Computer Science
  Great Communicator and team player
 
  
 The Role
 
  Develop and maintain data pipelines, including ETL.
  Automate tasks, optimize data delivery, and build scalable data infrastructure using SQL and AWS. 
  Construct analytics tools for actionable insights by collaborating with cross-functional teams. 
  Ensure data security, develop data tools for analytics, and enhance data system functionality.","<div>
 <p><b>Data Engineer (AWS) </b><br> <b>&#x24;100,000 + 401k + PTO + Medical + Dental + Vision</b><br> <b>New York - Fully Remote</b></p>
 <p> </p>
 <p>Are you a Data Engineer looking for an exciting new opportunity where you will progress in your technical skills while growing in your career?</p>
 <p><br> On offer is an exciting opportunity to join an innovative health tech development team. You&apos;ll be at the forefront of expanding and optimizing their data infrastructure, playing a vital role in transforming healthcare with cutting-edge technology.</p>
 <p><br> <br> This company is at the pinnacle of revolutionizing healthcare with data-driven solutions. You&apos;ll create and maintain data pipeline architecture, work with big data technologies while collaborating with your team enabling them to leverage data effectively.</p>
 <p><br> This role would suit an experienced Data Engineer looking for a chance to advance their career in data engineering, expand on their technical knowledge and be part of a team to transform technology based healthcare.</p>
 <p><b><br> The Person</b></p>
 <ul>
  <li>3+ Years Experience in Data Engineering</li>
  <li>ETL pipeline experience</li>
  <li>Experience using SQL, Python, Data Tools and AWS</li>
  <li>Knowledge of Postgres, Go and C++ is beneficial</li>
  <li>Degree in Computer Science</li>
  <li>Great Communicator and team player</li>
 </ul>
 <p> </p>
 <p><b>The Role</b></p>
 <ul>
  <li>Develop and maintain data pipelines, including ETL.</li>
  <li>Automate tasks, optimize data delivery, and build scalable data infrastructure using SQL and AWS. </li>
  <li>Construct analytics tools for actionable insights by collaborating with cross-functional teams. </li>
  <li>Ensure data security, develop data tools for analytics, and enhance data system functionality.</li>
 </ul>
</div>",https://www.aplitrak.com/?adid=U29waGlhLldlZWRvbi4wMjY5OS40NzA3QHJpc2V0ZWNobmljYWxyZWNydWl0bWVudC5hcGxpdHJhay5jb20,f35bff95778bf3eb,,Full-time,,,"New York, NY",Data Engineer,6 days ago,2023-10-13T14:05:16.321Z,3.7,26.0,"$100,000 a year",2023-10-19T14:05:16.323Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BlIR6L0eizDKDqkzeZRfLume_DxC2-xIBuckbPXhGlgQ01ftrjl3y36AU3ugZ3jwBaOnc34sn8XbNBVs2KCizg5GI3EZLfAopY5lohbSAwYZAP-cvdW_21imXJW-WA7S8h4lF0k8hETfEB7KuKsUPPSI4RMom8llcUQ7IXNslTkazNARLJv2L8GPEJlc_wIpeOyszi9Oe9PwEkz-0tE9eA6WFoqp8NjXfH17HlrFZ8An4QbGfRIolI4Rm_0ODmDugT_N4nUsrF7Pqlf7Tkbsbp64rmElHqTSvlbZ7I32BNapb-Rsv3e3D4eQ2C4pOrOlH_jL2FxzchsDb2_xlBC4eo4ALbh3IYdlvGC68pJhEE770KHVK0CjNqd78BKg-OSkdPdUkd_N4nUbrBYC-tokQ-8nmTUq0CghuoFbXsXwtUMJgtc_fMHwpQeMuf96Aupvi0xIlqHicQb_wiJKNwdD7IvpxA5KYNS41_2ovoHg6M7pDg1y8kSzKSuQ0clD57ypI4HC9mRxrFA1DX9UMhK833KsXe79ehpeVsda7rMTsh-oWIyR6k8PU3li4S_r18J9W0MciUajHKj8eppNTVLquqF4QGZjb-MODNmMep-DxKUel9PpLqXCTf1PZEnaMfcLr1wMswK1yqTZSvj64NW1J_s7j7NobXUsmQJgweEQL4XhnZcw6aImuQr3VpobaBzU8%3D&xkcb=SoAZ-_M3JkANkHxcpR0BbzkdCdPP&p=10&fvj=0&vjs=3&jsa=4439&tk=1hd440i722a6o002&from=jasx&wvign=1
199,ServiceNow,"Company Description
  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. 
 With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. 
 Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. 
 Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.
  Job Description
  What you get to do in this role:
 
   Develop re-usable, consistent, best in class dimensional data models to power use cases across the GTM spectrum - sales, partner, marketing and customer
   Develop scalable automated ETL/ELT jobs in Snowflake, to build and maintain these data models
   Use machine learning techniques to develop predictive metrics (Python is a must) and test those in an Azure ML environment
   Write proficient SQL queries and Python code to stand up master data models from raw data sources across disparate systems
   Identify any data integrity issues and deep dive to find root cause
   Enforce company data policies and procedures to ensure data quality and reduce discrepancies
   Secure approvals for data access based on business needs
   Train analysts and data scientists alike on available data sources
   Ensure very large databases and compute clusters operate optimally
   Implement and maintaining database structures and governance
   Develop / maintaining documentation on databases and production tables
   Collaborate across the company’s multiple data teams to meet analytics deliverables
 
  
  Qualifications
  To be successful in this role you have:
 
   5+ years of core data engineering position with advanced experience in SQL and Snowflake is a must
   Advanced scripting skills : R, SAS, Python
   Experience with developing ML models is a plus
   Experience with Azure ML, Azure ML Pipelines, Databricks is a must
   Sales GTM domain expertise in an Enterprise SaaS company is a must
   Data Science Expertise
   Effective problem solving and analytical skills. Ability to manage multiple projects and report simultaneously across different stakeholders
   Structured thinking with ability to easily break down ambiguous problems and propose impactful data modeling designs
   Passion for analyzing large and complex data sets and converting them into the information which drive business decisions
   Attention to detail, organization and effective verbal/written communication skills
   Proven track record in rolling out self-service analytics solutions (e.g. Tableau Server Ask Data, etc)
   Solid decision making, negotiation, and persuasion skills, often in ambiguous situations
   Must be able to work in fast paced environment and be able to adapt to changing requirements.
   Understanding of technology development projects and the full technology development lifecycle
 
 
  JV20
  #DTjobs
  For positions in the Bay Area, we offer a base pay of $133,300 - $226,700, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.
  Additional Information
  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. 
 At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. 
 If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. 
 For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. 
 Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.
  
 From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license. 
 Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.","<div>
 <b>Company Description</b>
 <p><br> At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can&#x2019;t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.</p> 
 <p>With more than 7,700+ customers, we serve approximately 85% of the Fortune 500&#xae;, and we&apos;re proud to be one of FORTUNE 100 Best Companies to Work For&#xae; and World&apos;s Most Admired Companies&#x2122;.</p> 
 <p>Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.</p> 
 <p>Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.</p>
 <b><br> Job Description</b>
 <p><b><br> What you get to do in this role:</b></p>
 <ul>
  <li> Develop re-usable, consistent, best in class dimensional data models to power use cases across the GTM spectrum - sales, partner, marketing and customer</li>
  <li> Develop scalable automated ETL/ELT jobs in Snowflake, to build and maintain these data models</li>
  <li> Use machine learning techniques to develop predictive metrics (Python is a must) and test those in an Azure ML environment</li>
  <li> Write proficient SQL queries and Python code to stand up master data models from raw data sources across disparate systems</li>
  <li> Identify any data integrity issues and deep dive to find root cause</li>
  <li> Enforce company data policies and procedures to ensure data quality and reduce discrepancies</li>
  <li> Secure approvals for data access based on business needs</li>
  <li> Train analysts and data scientists alike on available data sources</li>
  <li> Ensure very large databases and compute clusters operate optimally</li>
  <li> Implement and maintaining database structures and governance</li>
  <li> Develop / maintaining documentation on databases and production tables</li>
  <li> Collaborate across the company&#x2019;s multiple data teams to meet analytics deliverables</li>
 </ul>
 <br> 
 <b> Qualifications</b>
 <p><b><br> To be successful in this role you have:</b></p>
 <ul>
  <li> 5+ years of core data engineering position with advanced experience in SQL and Snowflake is a must</li>
  <li> Advanced scripting skills : R, SAS, Python</li>
  <li> Experience with developing ML models is a plus</li>
  <li> Experience with Azure ML, Azure ML Pipelines, Databricks is a must</li>
  <li> Sales GTM domain expertise in an Enterprise SaaS company is a must</li>
  <li> Data Science Expertise</li>
  <li> Effective problem solving and analytical skills. Ability to manage multiple projects and report simultaneously across different stakeholders</li>
  <li> Structured thinking with ability to easily break down ambiguous problems and propose impactful data modeling designs</li>
  <li> Passion for analyzing large and complex data sets and converting them into the information which drive business decisions</li>
  <li> Attention to detail, organization and effective verbal/written communication skills</li>
  <li> Proven track record in rolling out self-service analytics solutions (e.g. Tableau Server Ask Data, etc)</li>
  <li> Solid decision making, negotiation, and persuasion skills, often in ambiguous situations</li>
  <li> Must be able to work in fast paced environment and be able to adapt to changing requirements.</li>
  <li> Understanding of technology development projects and the full technology development lifecycle</li>
 </ul>
 <p></p>
 <p><br> JV20</p>
 <p> #DTjobs</p>
 <p> For positions in the Bay Area, we offer a base pay of &#x24;133,300 - &#x24;226,700, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.</p>
 <b><br> Additional Information</b>
 <p><br> ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.</p> 
 <p>At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.</p> 
 <p>If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.</p> 
 <p>For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.</p> 
 <p>Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.</p>
 <p><br> </p>
 <p>From Fortune. &#xa9; 2022 Fortune Media IP Limited All rights reserved. Used under license.</p> 
 <p>Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.</p>
</div>",https://careers.servicenow.com/careers/jobs/743999935473343EXT?lang=en-us&trid=35ab2906-b356-4a56-8472-f60d30d2e2f0,d81d17fc1c3957ba,,Full-time,,,"4810 Eastgate Mall, San Diego, CA 92121",Senior Data Engineer,12 days ago,2023-10-07T14:06:30.667Z,3.7,239.0,"$133,300 - $226,700 a year",2023-10-19T14:06:30.669Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d81d17fc1c3957ba&from=jasx&tk=1hd443vdui469801&vjs=3
204,BlueSky Technology Solutions,"We have an immediate need for a Senior Data Engineer with experience supporting Health Plan Financial Data Reporting. This permanent position is 100% remote and offers competitive compensation (that will depend on level of experience). Excellent benefits package.
Target Skills and Experience:

 4+ years of SAS programming experience with extensive macro utilization. One or more SAS certifications desired.
 Strong Data Engineering experience with proven track record in SAS usage of designing, developing, and maintaining data extract/transformation jobs, analytical reporting, and automated programming with reusable and SAS macro driven components
 Strong experience as Data Analyst or Data Engineer within a Health Plan/Payor environment supporting Financial reporting.
 Prior Clinical and Claims data reporting experience.
 4+ years of SQL technical experience preferably with multiple Data Base Management systems. Strong understanding of indexed and efficient table joins.
 3+ years of Tableau dashboard development experience.

Job Type: Full-time
Pay: $105,000.00 - $125,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Life insurance
 Paid time off
 Retirement plan
 Vision insurance

Schedule:

 8 hour shift

Education:

 Bachelor's (Required)

Experience:

 SQL: 4 years (Preferred)
 Data Analytics/Data Engineering: 4 years (Preferred)
 SAS programming: 4 years (Preferred)
 Health Plan Financial Reporting: 3 years (Preferred)
 SAS Macro utilization: 3 years (Preferred)

Work Location: Remote","<p>We have an immediate need for a Senior Data Engineer with experience supporting Health Plan Financial Data Reporting. This permanent position is 100% remote and offers competitive compensation (that will depend on level of experience). Excellent benefits package.</p>
<p><b>Target Skills and Experience:</b></p>
<ul>
 <li>4+ years of SAS programming experience with extensive macro utilization. One or more SAS certifications desired.</li>
 <li>Strong Data Engineering experience with proven track record in SAS usage of designing, developing, and maintaining data extract/transformation jobs, analytical reporting, and automated programming with reusable and SAS macro driven components</li>
 <li>Strong experience as Data Analyst or Data Engineer within a Health Plan/Payor environment supporting Financial reporting.</li>
 <li>Prior Clinical and Claims data reporting experience.</li>
 <li>4+ years of SQL technical experience preferably with multiple Data Base Management systems. Strong understanding of indexed and efficient table joins.</li>
 <li>3+ years of Tableau dashboard development experience.</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;105,000.00 - &#x24;125,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Required)</li>
</ul>
<p>Experience:</p>
<ul>
 <li>SQL: 4 years (Preferred)</li>
 <li>Data Analytics/Data Engineering: 4 years (Preferred)</li>
 <li>SAS programming: 4 years (Preferred)</li>
 <li>Health Plan Financial Reporting: 3 years (Preferred)</li>
 <li>SAS Macro utilization: 3 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,30b006781edd63e6,,Full-time,,,Remote,Senior Data Engineer - Health Plan Financial Reporting - 100% Remote,8 days ago,2023-10-11T14:07:05.263Z,,,"$105,000 - $125,000 a year",2023-10-19T14:07:05.267Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=30b006781edd63e6&from=jasx&tk=1hd445crtikef800&vjs=3
207,Murmuration,"About Murmuration
  Murmuration is a nonprofit organization focused on leveraging civic engagement to drive greater equity. We provide sophisticated tools, data, strategic guidance, and programmatic support to help our partner organizations increase civic engagement and marshal support to drive change at the community level. Our best-in-class data and easy-to-use tools have been used by hundreds of organizations to make informed decisions about who they need to reach and how to achieve and sustain impact – and to put those decisions into action.
  About the Role
  As a QA Engineer, Data Validation on the Platform Engineering team, you’ll be expected to participate in all aspects of designing, building, and implementing software testing within the organization. As part of an agile development team, this may include developing test strategies, drawing up test documents, identifying faults, and reviewing QA reports. A successful candidate will bring strong analytical ability and the ability to deliver results within a fast-moving, agile environment. 
 Note: At Murmuration, we are committed to becoming an even more diverse, equitable, and inclusive workplace. To this end, all staff members are expected to actively participate in DEIB (diversity, equity, inclusion, belonging) programming. 
 What You’ll Do:
  
  Develop automated tests for Web and Data Applications; 
  Extend and maintain existing test frameworks; 
  Plan, document, review, and execute testing using the most appropriate methodology and tools; 
  Collaborate with project team members on requirements specifications for test plan creation and execution; 
  Support an agile data pipeline development environment with frequent deployments to production; 
 
 
  Work closely with architects, developers, and product managers to define testing strategies; and 
  Advocate for necessary changes to improve end-user experience. 
 
 Requirements
  What You Should Have:
  
  3+ years of QA experience with a knowledge of testing methodologies, industry tools, and best practices; 
  Experience working with automation testing tools; 
  Experience working with Test Management platforms such as TestRail; 
  Experience with AWS cloud services (S3, Lambda, etc.); 
  Experience with Linux command line tools; 
  Experience using Git repository management solutions (Bitbucket, GitHub, etc.); 
  Experience with QA processes and deliverables in an Agile/Scrum environment; 
  Proficient in defect tracking and analyzing root cause utilizing tools (Jira, etc.); 
  Outstanding communication, organizational and interpersonal skills; and 
  Excellent problem solving, critical thinking, and debugging skills. 
 
 What You Could Have:
  
  Experience with data testing; 
  Experience working with both SQL and NoSQL (MongoDB) databases; 
  Experience with Python or other scripting language; 
  Experience with Pandas for Python (https://pandas.pydata.org/); 
  Experience with Docker; and 
  Experience working with deployment pipelines; 
 
 Talented QA Engineers, Data Validation come from all walks of life and careers. If you are passionate about civic engagement and technology, please apply, even if you do not check every box!
  Location and Compensation
  The QA Engineer, Data Validation is a full-time, salaried position with a comprehensive benefits package. It is based anywhere in the U.S. The salary range for this position is $85,000 - $105,000 and is commensurate with experience.
  Benefits
  Our Culture of Care
  We work hard to create a culture of care to ensure that our staff are best equipped to lead happy, healthy, and balanced lives. To that end, we offer a comprehensive benefits package which includes:
  
  Health, vision, and dental insurance with 100% of premiums covered for you and qualifying family members; 
  Retirement benefits with a 4% employer match; 
  A flexible unlimited PTO plan; 
  Generous paid parental leave; 
  Pre-tax commuter benefits; 
  A company laptop; 
  A flexible remote work environment; 
  A home office setup stipend for all new employees; 
  Monthly reimbursement for remote work expenses; 
  A yearly professional development fund; 
  Mental health and wellness benefits through Calm and Better Help; 
  Yearly in-person staff retreats; and 
  A welcoming culture that celebrates diversity, equity, and inclusion. 
 
 An Equal-Opportunity Employer with a Commitment to Diversity
  Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.","<div>
 <p><b>About Murmuration</b></p>
 <p> Murmuration is a nonprofit organization focused on leveraging civic engagement to drive greater equity. We provide sophisticated tools, data, strategic guidance, and programmatic support to help our partner organizations increase civic engagement and marshal support to drive change at the community level. Our best-in-class data and easy-to-use tools have been used by hundreds of organizations to make informed decisions about who they need to reach and how to achieve and sustain impact &#x2013; and to put those decisions into action.</p>
 <p><b> About the Role</b></p>
 <p> As a QA Engineer, Data Validation on the Platform Engineering team, you&#x2019;ll be expected to participate in all aspects of designing, building, and implementing software testing within the organization. As part of an agile development team, this may include developing test strategies, drawing up test documents, identifying faults, and reviewing QA reports. A successful candidate will bring strong analytical ability and the ability to deliver results within a fast-moving, agile environment.</p> 
 <p>Note: At Murmuration, we are committed to becoming an even more diverse, equitable, and inclusive workplace. To this end, all staff members are expected to actively participate in DEIB (diversity, equity, inclusion, belonging) programming.</p> 
 <p><b>What You&#x2019;ll Do:</b></p>
 <ul> 
  <li>Develop automated tests for Web and Data Applications;</li> 
  <li>Extend and maintain existing test frameworks;</li> 
  <li>Plan, document, review, and execute testing using the most appropriate methodology and tools;</li> 
  <li>Collaborate with project team members on requirements specifications for test plan creation and execution;</li> 
  <li>Support an agile data pipeline development environment with frequent deployments to production;</li> 
 </ul>
 <ul>
  <li>Work closely with architects, developers, and product managers to define testing strategies; and</li> 
  <li>Advocate for necessary changes to improve end-user experience.</li> 
 </ul>
 <p><b>Requirements</b></p>
 <p><b> What You </b><b><i>Should</i></b><b> Have:</b></p>
 <ul> 
  <li>3+ years of QA experience with a knowledge of testing methodologies, industry tools, and best practices;</li> 
  <li>Experience working with automation testing tools;</li> 
  <li>Experience working with Test Management platforms such as TestRail;</li> 
  <li>Experience with AWS cloud services (S3, Lambda, etc.);</li> 
  <li>Experience with Linux command line tools;</li> 
  <li>Experience using Git repository management solutions (Bitbucket, GitHub, etc.);</li> 
  <li>Experience with QA processes and deliverables in an Agile/Scrum environment;</li> 
  <li>Proficient in defect tracking and analyzing root cause utilizing tools (Jira, etc.);</li> 
  <li>Outstanding communication, organizational and interpersonal skills; and</li> 
  <li>Excellent problem solving, critical thinking, and debugging skills.</li> 
 </ul>
 <p><b>What You </b><b><i>Could</i></b><b> Have:</b></p>
 <ul> 
  <li>Experience with data testing;</li> 
  <li>Experience working with both SQL and NoSQL (MongoDB) databases;</li> 
  <li>Experience with Python or other scripting language;</li> 
  <li>Experience with Pandas for Python (https://pandas.pydata.org/);</li> 
  <li>Experience with Docker; and</li> 
  <li>Experience working with deployment pipelines;</li> 
 </ul>
 <p>Talented QA Engineers, Data Validation come from all walks of life and careers. If you are passionate about civic engagement and technology, please apply, even if you do not check every box!</p>
 <p> Location and Compensation</p>
 <p> The QA Engineer, Data Validation is a full-time, salaried position with a comprehensive benefits package. It is based anywhere in the U.S. The salary range for this position is &#x24;85,000 - &#x24;105,000 and is commensurate with experience.</p>
 <p><b> Benefits</b></p>
 <p><b> Our Culture of Care</b></p>
 <p> We work hard to create a culture of care to ensure that our staff are best equipped to lead happy, healthy, and balanced lives. To that end, we offer a comprehensive benefits package which includes:</p>
 <ul> 
  <li>Health, vision, and dental insurance with 100% of premiums covered for you and qualifying family members;</li> 
  <li>Retirement benefits with a 4% employer match;</li> 
  <li>A flexible unlimited PTO plan;</li> 
  <li>Generous paid parental leave;</li> 
  <li>Pre-tax commuter benefits;</li> 
  <li>A company laptop;</li> 
  <li>A flexible remote work environment;</li> 
  <li>A home office setup stipend for all new employees;</li> 
  <li>Monthly reimbursement for remote work expenses;</li> 
  <li>A yearly professional development fund;</li> 
  <li>Mental health and wellness benefits through Calm and Better Help;</li> 
  <li>Yearly in-person staff retreats; and</li> 
  <li>A welcoming culture that celebrates diversity, equity, and inclusion.</li> 
 </ul>
 <p><b>An Equal-Opportunity Employer with a Commitment to Diversity</b></p>
 <p> Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.</p>
</div>",https://www.indeed.com/rc/clk?jk=86374b3b92cb99ca&atk=&xpse=SoBr67I3JkAStWQxg50JbzkdCdPP,86374b3b92cb99ca,,Full-time,,,"New York, NY","QA Engineer, Data Validation",12 days ago,2023-10-07T14:07:55.448Z,,,"$85,000 - $105,000 a year",2023-10-19T14:07:55.452Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=86374b3b92cb99ca&from=jasx&tk=1hd4474bsitkv800&vjs=3
210,Oncora Medical,"About us:
Oncora is an oncology software and data company dedicated to helping physicians and scientists collect and use real-world data to improve outcomes for cancer patients. Our machine learning algorithms, which are deployed in active clinical environments, accurately predict oncology outcomes such as unplanned hospitalization, survival, and recurrence. Our software products include: a clinical workflow and data entry system for oncology clinical care, a data warehouse that leverages connections to other healthcare software systems such as EMRs, PACS, to amass real-world, regulatory-grade oncology data, a machine learning platform to train and validate predictive models of key oncology events, a machine learning API to power external software tools, and a virtual clinical trial platform that allows pharma and device companies to leverage automated medical image analysis to advance new technologies in the fight to cure cancer. We work with world-leading cancer centers such as MD Anderson and Northwell Health, global device companies such as Varian Medical Systems, and innovative biopharma companies. Our team is mission-driven to its core.
About the role:
We are looking for an experienced engineer to join our mission driven team to help develop our data platform that integrates and transforms multiple imperfect and messy healthcare data sources into clean, usable data so that we can learn from every cancer patient.
As a core member of our Platform and Integrations team, you will play a vital role in designing and building our core data platform and helping scale it to serve additional hospitals. You will lead major improvements to our data infrastructure and apply your skills in software development, architecture and data modeling to get things done.
We are a small team trying to tackle a very large problem, so we need teammates that are ultimately accountable to themselves and continuously push themselves, the product and the organization forward.
What you will be doing:

 Designing and implementing improvements to our data extraction and transformation processes to increase performance and stability
 Architecting our data warehousing and reporting capabilities to support real-time analysis of tens of thousands of patients representing millions of data points
 Incrementally evolving our platform architecture and infrastructure without disrupting operations
 Overseeing and monitoring our existing data platforms for stability, performance and accuracy
 Working with Product and Engineering to define, document, and build transformations to extract intelligence from multiple incomplete and siloed data sources
 Building pipelines to de-identify and consolidate cross-institutional data to fuel predictive analytics, research, and clinical trials
 Building visibility into all aspects of our data platform (workflow status, system health, data lineage, etc.)

About you:

 Demonstrated experience independently leading complex software projects
 A strong base of software engineering experience, typically 5-8+ years, with a good portion of that leading the development of data platforms, pipelines or data-intensive projects
 Deep understanding of multiple database technologies, for example relational, document, key/value, columnar, etc (we use Postgres, MongoDB, Redis, and ElasticSearch)
 Experience building asynchronous and distributed systems (we use RabbitMQ)
 Fluency with a functional or imperative language (we use Python)
 A focus on writing understandable, testable, and maintainable code
 Familiarity with modern containerized environments (we use Docker & Kubernetes)

Bonuses:

 Experience with healthcare data standards and integrations (HL7, FHIR, DICOM, etc.)

Compensation, Benefits, and Perks:

 Salary: $145k-180k plus equity compensation
 401k, health and dental insurance, flexible vacation, paid parental leave
 eBooks, online courses, home office budget
 Events: happy hours, team dinners, conversations with oncologists
 Work with smart, passionate people on a product that will have a direct impact on the lives of cancer patients

What to expect in the hiring process:

 Introductory phone call (15-30 minutes zoom call)
 Phone interview with CTO/CEO (60 min zoom call)
 Virtual onsite, including a pair programming session, engineering team meet, and co-founder meet (90-120 minute zoom call)
 Final stages, potential follow-up interviews, and offer discussions

Job Type: Full-time
Pay: $120,000.00 - $185,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Flexible schedule
 Health insurance
 Paid time off
 Parental leave

Experience level:

 4 years

Schedule:

 Monday to Friday

Work Location: Remote","<p><b>About us:</b></p>
<p>Oncora is an oncology software and data company dedicated to helping physicians and scientists collect and use real-world data to improve outcomes for cancer patients. Our machine learning algorithms, which are deployed in active clinical environments, accurately predict oncology outcomes such as unplanned hospitalization, survival, and recurrence. Our software products include: a clinical workflow and data entry system for oncology clinical care, a data warehouse that leverages connections to other healthcare software systems such as EMRs, PACS, to amass real-world, regulatory-grade oncology data, a machine learning platform to train and validate predictive models of key oncology events, a machine learning API to power external software tools, and a virtual clinical trial platform that allows pharma and device companies to leverage automated medical image analysis to advance new technologies in the fight to cure cancer. We work with world-leading cancer centers such as MD Anderson and Northwell Health, global device companies such as Varian Medical Systems, and innovative biopharma companies. Our team is mission-driven to its core.</p>
<p><b>About the role:</b></p>
<p>We are looking for an experienced engineer to join our mission driven team to help develop our data platform that integrates and transforms multiple imperfect and messy healthcare data sources into clean, usable data so that we can learn from every cancer patient.</p>
<p>As a core member of our Platform and Integrations team, you will play a vital role in designing and building our core data platform and helping scale it to serve additional hospitals. You will lead major improvements to our data infrastructure and apply your skills in software development, architecture and data modeling to get things done.</p>
<p>We are a small team trying to tackle a very large problem, so we need teammates that are ultimately accountable to themselves and continuously push themselves, the product and the organization forward.</p>
<p><b>What you will be doing:</b></p>
<ul>
 <li>Designing and implementing improvements to our data extraction and transformation processes to increase performance and stability</li>
 <li>Architecting our data warehousing and reporting capabilities to support real-time analysis of tens of thousands of patients representing millions of data points</li>
 <li>Incrementally evolving our platform architecture and infrastructure without disrupting operations</li>
 <li>Overseeing and monitoring our existing data platforms for stability, performance and accuracy</li>
 <li>Working with Product and Engineering to define, document, and build transformations to extract intelligence from multiple incomplete and siloed data sources</li>
 <li>Building pipelines to de-identify and consolidate cross-institutional data to fuel predictive analytics, research, and clinical trials</li>
 <li>Building visibility into all aspects of our data platform (workflow status, system health, data lineage, etc.)</li>
</ul>
<p><b>About you:</b></p>
<ul>
 <li>Demonstrated experience independently leading complex software projects</li>
 <li>A strong base of software engineering experience, typically 5-8+ years, with a good portion of that leading the development of data platforms, pipelines or data-intensive projects</li>
 <li>Deep understanding of multiple database technologies, for example relational, document, key/value, columnar, etc (we use Postgres, MongoDB, Redis, and ElasticSearch)</li>
 <li>Experience building asynchronous and distributed systems (we use RabbitMQ)</li>
 <li>Fluency with a functional or imperative language (we use Python)</li>
 <li>A focus on writing understandable, testable, and maintainable code</li>
 <li>Familiarity with modern containerized environments (we use Docker &amp; Kubernetes)</li>
</ul>
<p><b>Bonuses:</b></p>
<ul>
 <li>Experience with healthcare data standards and integrations (HL7, FHIR, DICOM, etc.)</li>
</ul>
<p><b>Compensation, Benefits, and Perks:</b></p>
<ul>
 <li>Salary: &#x24;145k-180k plus equity compensation</li>
 <li>401k, health and dental insurance, flexible vacation, paid parental leave</li>
 <li>eBooks, online courses, home office budget</li>
 <li>Events: happy hours, team dinners, conversations with oncologists</li>
 <li>Work with smart, passionate people on a product that will have a direct impact on the lives of cancer patients</li>
</ul>
<p><b>What to expect in the hiring process:</b></p>
<ul>
 <li>Introductory phone call (15-30 minutes zoom call)</li>
 <li>Phone interview with CTO/CEO (60 min zoom call)</li>
 <li>Virtual onsite, including a pair programming session, engineering team meet, and co-founder meet (90-120 minute zoom call)</li>
 <li>Final stages, potential follow-up interviews, and offer discussions</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;120,000.00 - &#x24;185,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Parental leave</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>4 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,72b83dc7d8aad2ef,,Full-time,,,Remote,Senior Software Engineer (Data Platform),Today,2023-10-20T12:33:10.016Z,,,"$120,000 - $185,000 a year",2023-10-20T12:33:10.172Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=72b83dc7d8aad2ef&from=jasx&tk=1hd6h6bheiolg800&vjs=3
211,TEKletics,"BUSINESS INTELLIGENCE (ETL)
Job Responsibilities:The successful candidate will work with high level BI-ETL developers to process data from various source systems into an Enterprise Data Warehouse.
Primary duties include, but are not limited to:

 Building/interpreting design documents
 Building/interpreting mapping documents
 Building/completing test documents
 Participate in technical design reviews and code reviews
 Understand and adhere to ETL best practices
 Develop and troubleshoot ETL code
 Participate in peer review sessions

Required Skills: Comfortable working in a team environment  Strong work ethic*

 Excellent time management
 Excellent oral/written communication
 Prior experience using MS Office (word, excel, outlook)

Preferred Skills: Fundamental understanding of database concepts * Experience with UNIX scripting

 Experience with Object-Oriented Programming
 ETL experience
 Hadoop Experience
 Leadership Experience

Job Type: Full-time
Pay: $15.00 - $30.00 per hour
Benefits:

 Paid time off

Schedule:

 8 hour shift

COVID-19 considerations:All employees are currently working remotely
Experience:

 SQL: 1 year (Preferred)
 Python: 1 year (Preferred)
 Azure: 1 year (Preferred)

Work Location: Remote","<p><b>BUSINESS INTELLIGENCE (ETL)</b></p>
<p><b>Job Responsibilities:</b>The successful candidate will work with high level BI-ETL developers to process data from various source systems into an Enterprise Data Warehouse.</p>
<p><b>Primary duties include, but are not limited to:</b></p>
<ul>
 <li>Building/interpreting design documents</li>
 <li>Building/interpreting mapping documents</li>
 <li>Building/completing test documents</li>
 <li>Participate in technical design reviews and code reviews</li>
 <li>Understand and adhere to ETL best practices</li>
 <li>Develop and troubleshoot ETL code</li>
 <li>Participate in peer review sessions</li>
</ul>
<p><b>Required Skills:</b> <b>Comfortable working in a team environment </b> Strong work ethic<br>*</p>
<ul>
 <li>Excellent time management</li>
 <li>Excellent oral/written communication</li>
 <li>Prior experience using MS Office (word, excel, outlook)</li>
</ul>
<p>Preferred Skills: Fundamental understanding of database concepts * Experience with UNIX scripting</p>
<ul>
 <li>Experience with Object-Oriented Programming</li>
 <li>ETL experience</li>
 <li>Hadoop Experience</li>
 <li>Leadership Experience</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;15.00 - &#x24;30.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Paid time off</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>COVID-19 considerations:<br>All employees are currently working remotely</p>
<p>Experience:</p>
<ul>
 <li>SQL: 1 year (Preferred)</li>
 <li>Python: 1 year (Preferred)</li>
 <li>Azure: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,5de691f5f49787f9,,Full-time,,,Remote,Junior Data Engineer,Today,2023-10-20T12:33:15.222Z,2.5,2.0,$15 - $30 an hour,2023-10-20T12:33:15.224Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=5de691f5f49787f9&from=jasx&tk=1hd6h6bheiolg800&vjs=3
212,"Aflac, Incorporated","Salary Range: $55,000 - $140,000
 
  We’ve Got You Under Our Wing
  We are the duck. We develop and empower our people, cultivate relationships, give back to our community, and celebrate every success along the way. We do it all…The Aflac Way.
 
  Aflac, a Fortune 500 company, is an industry leader in voluntary insurance products that pay cash directly to policyholders and one of America's best-known brands. Aflac has been recognized as Fortune’s 50 Best Workplaces for Diversity and as one of World’s Most Ethical Companies by Ethisphere.com.
  
 Our business is about being there for people in need. So, ask yourself, are you the duck? If so, there’s a home, and a flourishing career for you at Aflac.
 
  Work Designation. Depending on your location within the continental US, this role may be hybrid or remote. 
 
  If you live within 50 miles of the Aflac offices located in Columbus, GA or Columbia, SC, this role will be hybrid. This means you will be expected to work in the office for at least 60% of the work week. You will work from your home (within the continental US) for the remaining portion of the work week. Details of this schedule will be discussed with your leadership.
   If you live more than 50 miles from the Aflac offices located in Columbus, GA or Columbia, SC, this role will be remote. This means you will be expected to work from your home, within the continental US. If the role is remote, there may be occasions that you are requested to come to the office based on business need. Any requests to come to the office would be communicated with you in advance.
 
 
  What does it take to be successful at Aflac?
 
   Acting with Integrity
   Communicating Effectively
   Pursuing Self-Development
   Serving Customers
   Supporting Change
   Supporting Organizational Goals
   Working with Diverse Populations
 
 
  What does it take to be successful in this role?
 
  AWS Data Platform - Cloud infrastructure, Datalake/Cloud Formation, Automation, CI/CD 
  Amazon Cloud Data Storage – S3, RedShift, DynamoDB, NoSQL 
  ETL Tools – AWS Glue, Informatica Suite, SSIS, Infoworks 
  SQL & Relational Databases – SQL Server, Teradata, MS Access, HIVE, HBase 
  XML 
  XSLT 
  .NET Framework 
  C# 
  Java 
  JavaScript 
  jQuery 
  LINQ 
  MVC Framework 
  ASPX 
  Angular.js 
  Bootstrap 
  Knockout 
  Business Intelligence 
  ETL Techniques 
  Data Modeling 
  Data Warehousing/Business Intelligence 
  Meta Data Repository 
  MS SQL Server
 
 
  Education & Experience Required
 
   Bachelor's Degree In Programming/systems or computer science, or related field
   Four or more years of programming experience
   Experience and understanding of multiple programming languages and applicable applications including SQL and ETL
   Experienced in Cloud data storage and consumption models such as S3 Buckets, Lake Formation, RedShift, Dynamo DB
   Experienced in working with compute engines such as Spark, EMR, Data bricks, Snowflake etc.
 
  Or an equivalent combination of education and experience
 
  Principal Duties & Responsibilities
 
  Works under minimum supervisor to exercise independent decision making; Creates processes which initiate the ETL or Batch cycle; develops streaming processes for extracted data loading to destination database, including on-the-fly processing where extract and transformation phase to no go to persistent storage; Performs data profiling of source data in order to identify data quality issues and anomalies, business knowledge embedded in data; natural keys, and meta data information
 
  
 
  Build repeatable, automated and sustainable Extract, Transform and Load (ETL) processes leveraging platforms such as AWS cloud native – AWS Glue, DMS, Informatica, Infoworks, Hadoop, Spark processing Engines
 
  
 
  Creates data validation rule on source data to confirm the data has correct and/or expected values; Writes alternate workflow steps or reports back to the source for further analysis and correction of incorrect record(s) when validation rules are not passed
 
  
 
  Develops processes to be applied to extracted source data to move to target state; Writes data cleansing functions to get data to proper prunes data set to include only fields needed; translates source code values to target value; Standardizes free form values to codes; Derives new values through calculations on existing fields; Merges data from multiple in order to generate on consolidated source for the target
 
  
 
  Sorts and Aggregates records into rollup where multiple records are represented; Creates surrogate-key values to use in place of multiple natural keys; Turns multiple columns into multiple rows or vice–versa (Transposing or Pivoting); Splits multi-valued column data into multiple columns; Disaggregates repeating columns into separate detail table(s); Creates lookup tables; Looks up and validates reference information as part of data validation
 
  
 
  Creates and applies data validation step process in order to perform partial, full or no record’s rejection; Writes processes which handle exceptions and/or move records exceptions to alternate Transform step(s)
 
  
 
  Develops processes which load the transformed data into end target systems (database, file, application, etc.); may apply different techniques based on business needs including inserting new data into target; Over write existing data with cumulative information; Updates existing data at some frequency; Creates data validation steps in this layer to ensure loaded data
 
  
 
  Creates process cleanup after complex ETL processes which release resources used to run ETL; Creates processes to archive data
 
  
 
  Participates in project collaboration meeting with clients, business analysts, and team members in order to analyze and clarify business requirements; Translates business requirements into detailed technical specifications
 
  
 
  Works with project teams to define and design scope for each project; Creates unit test cases to ensure the application meets the needs of the business
 
  
 
  Ensures proper configuration management and change controls are implemented; Provides technical assistance and cross training to other team members
 
  
 
  Designs and implements technology best practices, guidelines and repeatable processes; Prepares and presents status updates on various projects
 
  
 
  Performs other duties as required
 
 
  Total Rewards
  This compensation range is specific to the job level and takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to: education, experience, licensure, certifications, geographic location, and internal equity. The range has been created in good faith based on information known to Aflac at the time of the posting. Compensation decisions are dependent on the circumstances of each case. This salary range does not include any potential incentive pay or benefits, however, such information will be provided separately when appropriate. The salary range for this position is $55,000 to $140,000.
 
  In addition to the base salary, we offer an array of benefits to meet your needs including medical, dental, and vision coverage, prescription drug coverage, health care flexible spending, dependent care flexible spending, Aflac supplemental policies (Accident, Cancer, Critical Illness and Hospital Indemnity offered at no costs to employee), 401(k) plans, annual bonuses, and an opportunity to purchase company stock. On an annual basis, you’ll also be offered 11 paid holidays, up to 20 days PTO to be used for any reason, and, if eligible, state mandated sick leave (Washington employees accrue 1 hour sick leave for every 40 hours worked) and other leaves of absence, if eligible, when needed to support your physical, financial, and emotional well-being. Aflac complies with all applicable leave laws, including, but not limited to sick and safe leave, and adoption and parental leave, in all states and localities.","<div>
 <p><b>Salary Range: </b>&#x24;55,000 - &#x24;140,000</p>
 <p></p>
 <p><b><br> We&#x2019;ve Got You Under Our Wing</b></p>
 <p> We are the duck. We develop and empower our people, cultivate relationships, give back to our community, and celebrate every success along the way. We do it all&#x2026;<i>The Aflac Way</i>.</p>
 <p></p>
 <p><br> Aflac, a Fortune 500 company, is an industry leader in voluntary insurance products that pay cash directly to policyholders and one of America&apos;s best-known brands. Aflac has been recognized as Fortune&#x2019;s 50 Best Workplaces for Diversity and as one of World&#x2019;s Most Ethical Companies by Ethisphere.com.</p>
 <p><br> </p>
 <p>Our business is about being there for people in need. So, ask yourself, are you the duck? If so, there&#x2019;s a home, and a flourishing career for you at Aflac.</p>
 <p></p>
 <p><b><br> Work Designation.</b> Depending on your location within the continental US, this role may be <b>hybrid</b> or <b>remote. </b></p>
 <ul>
  <li>If you live <i>within 50 miles</i> of the Aflac offices located in Columbus, GA or Columbia, SC, this role will be <b>hybrid.</b><b> </b>This means you will be expected to work in the office for at least 60% of the work week. You will work from your home (within the continental US) for the remaining portion of the work week. Details of this schedule will be discussed with your leadership.</li>
  <li> If you live <i>more than 50 miles</i> from the Aflac offices located in Columbus, GA or Columbia, SC, this role will be <b>remote.</b> This means you will be expected to work from your home, within the continental US. If the role is remote, there may be occasions that you are requested to come to the office based on business need. Any requests to come to the office would be communicated with you in advance.</li>
 </ul>
 <p></p>
 <p><b><br> What does it take to be successful at Aflac?</b></p>
 <ul>
  <li> Acting with Integrity</li>
  <li> Communicating Effectively</li>
  <li> Pursuing Self-Development</li>
  <li> Serving Customers</li>
  <li> Supporting Change</li>
  <li> Supporting Organizational Goals</li>
  <li> Working with Diverse Populations</li>
 </ul>
 <p></p>
 <p><b><br> What does it take to be successful in this role?</b></p>
 <ul>
  <li>AWS Data Platform - Cloud infrastructure, Datalake/Cloud Formation, Automation, CI/CD </li>
  <li>Amazon Cloud Data Storage &#x2013; S3, RedShift, DynamoDB, NoSQL </li>
  <li>ETL Tools &#x2013; AWS Glue, Informatica Suite, SSIS, Infoworks </li>
  <li>SQL &amp; Relational Databases &#x2013; SQL Server, Teradata, MS Access, HIVE, HBase </li>
  <li>XML </li>
  <li>XSLT </li>
  <li>.NET Framework </li>
  <li>C# </li>
  <li>Java </li>
  <li>JavaScript </li>
  <li>jQuery </li>
  <li>LINQ </li>
  <li>MVC Framework </li>
  <li>ASPX </li>
  <li>Angular.js </li>
  <li>Bootstrap </li>
  <li>Knockout </li>
  <li>Business Intelligence </li>
  <li>ETL Techniques </li>
  <li>Data Modeling </li>
  <li>Data Warehousing/Business Intelligence </li>
  <li>Meta Data Repository </li>
  <li>MS SQL Server</li>
 </ul>
 <p></p>
 <p><b><br> Education &amp; Experience Required</b></p>
 <ul>
  <li> Bachelor&apos;s Degree In Programming/systems or computer science, or related field</li>
  <li> Four or more years of programming experience</li>
  <li> Experience and understanding of multiple programming languages and applicable applications including SQL and ETL</li>
  <li> Experienced in Cloud data storage and consumption models such as S3 Buckets, Lake Formation, RedShift, Dynamo DB</li>
  <li> Experienced in working with compute engines such as Spark, EMR, Data bricks, Snowflake etc.</li>
 </ul>
 <p><i> Or an equivalent combination of education and experience</i></p>
 <p></p>
 <p><b><br> Principal Duties &amp; Responsibilities</b></p>
 <ul>
  <li>Works under minimum supervisor to exercise independent decision making; Creates processes which initiate the ETL or Batch cycle; develops streaming processes for extracted data loading to destination database, including on-the-fly processing where extract and transformation phase to no go to persistent storage; Performs data profiling of source data in order to identify data quality issues and anomalies, business knowledge embedded in data; natural keys, and meta data information</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Build repeatable, automated and sustainable Extract, Transform and Load (ETL) processes leveraging platforms such as AWS cloud native &#x2013; AWS Glue, DMS, Informatica, Infoworks, Hadoop, Spark processing Engines</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Creates data validation rule on source data to confirm the data has correct and/or expected values; Writes alternate workflow steps or reports back to the source for further analysis and correction of incorrect record(s) when validation rules are not passed</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Develops processes to be applied to extracted source data to move to target state; Writes data cleansing functions to get data to proper prunes data set to include only fields needed; translates source code values to target value; Standardizes free form values to codes; Derives new values through calculations on existing fields; Merges data from multiple in order to generate on consolidated source for the target</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Sorts and Aggregates records into rollup where multiple records are represented; Creates surrogate-key values to use in place of multiple natural keys; Turns multiple columns into multiple rows or vice&#x2013;versa (Transposing or Pivoting); Splits multi-valued column data into multiple columns; Disaggregates repeating columns into separate detail table(s); Creates lookup tables; Looks up and validates reference information as part of data validation</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Creates and applies data validation step process in order to perform partial, full or no record&#x2019;s rejection; Writes processes which handle exceptions and/or move records exceptions to alternate Transform step(s)</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Develops processes which load the transformed data into end target systems (database, file, application, etc.); may apply different techniques based on business needs including inserting new data into target; Over write existing data with cumulative information; Updates existing data at some frequency; Creates data validation steps in this layer to ensure loaded data</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Creates process cleanup after complex ETL processes which release resources used to run ETL; Creates processes to archive data</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Participates in project collaboration meeting with clients, business analysts, and team members in order to analyze and clarify business requirements; Translates business requirements into detailed technical specifications</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Works with project teams to define and design scope for each project; Creates unit test cases to ensure the application meets the needs of the business</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Ensures proper configuration management and change controls are implemented; Provides technical assistance and cross training to other team members</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Designs and implements technology best practices, guidelines and repeatable processes; Prepares and presents status updates on various projects</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Performs other duties as required</li>
 </ul>
 <p></p>
 <p><b><br> Total Rewards</b></p>
 <p><i> This compensation range is specific to the job level and takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to: education, experience, licensure, certifications, geographic location, and internal equity. The range has been created in good faith based on information known to Aflac at the time of the posting. Compensation decisions are dependent on the circumstances of each case. This salary range does not include any potential incentive pay or benefits, however, such information will be provided separately when appropriate. The salary range for this position is &#x24;55,000 to &#x24;140,000.</i></p>
 <p></p>
 <p><i><br> In addition to the base salary, we offer an array of benefits to meet your needs including medical, dental, and vision coverage, prescription drug coverage, health care flexible spending, dependent care flexible spending, Aflac supplemental policies (Accident, Cancer, Critical Illness and Hospital Indemnity offered at no costs to employee), 401(k) plans, annual bonuses, and an opportunity to purchase company stock. On an annual basis, you&#x2019;ll also be offered 11 paid holidays, up to 20 days PTO to be used for any reason, and, if eligible, state mandated sick leave (Washington employees accrue 1 hour sick leave for every 40 hours worked) and other leaves of absence, if eligible, when needed to support your physical, financial, and emotional well-being. Aflac complies with all applicable leave laws, including, but not limited to sick and safe leave, and adoption and parental leave, in all states and localities.</i></p>
</div>
<p></p>",https://careers.aflac.com/job/Remote-Data-Engineer-%28AWSETL%29-OR-31999/1089155300/?feedId=342200&utm_source=Indeed&utm_campaign=Aflac_Indeed,6b0246ca2a37effe,,,,,Remote,Data Engineer (AWS/ETL),Today,2023-10-20T12:33:11.625Z,3.5,4233.0,"$55,000 - $140,000 a year",2023-10-20T12:33:13.028Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=6b0246ca2a37effe&from=jasx&tk=1hd6h6bheiolg800&vjs=3
213,Elite Quests LLC,"Required Qualifications
· Bachelor’s degree in a related field or relevant work experience.
· Minimum 3 years of experience using Azure Data Lake, Data Factory and/or Data Bricks.
· Minimum 3 years of experience working with SAP technologies such as Hana or BW.
· Minimum 3 years of experience working in an IT environment independently and in teams.
· Solid understanding and application of business concepts, procedures and practices.
Preferred Qualifications
· Experience with DevOps Version Control and Storyboarding.
· Experience with data and reporting using data from ERP systems such as SAP.
· Experience with regulatory tools for document archiving and document control.
· Ability to perform in a regulated and/or validated environment.
· General knowledge of a variety of alternatives and their impact on business.
· Experience with other reporting tools such as Snowflake, BOBJ, Tableau
Job Type: Contract
Pay: $35.34 per hour
Experience level:

 3 years

Schedule:

 8 hour shift

Experience:

 Informatica: 1 year (Preferred)
 Data lake: 3 years (Required)
 Azure Data Lake: 3 years (Required)

Work Location: Remote","<p><b>Required Qualifications</b></p>
<p>&#xb7; Bachelor&#x2019;s degree in a related field or relevant work experience.</p>
<p>&#xb7; Minimum 3 years of experience using Azure Data Lake, Data Factory and/or Data Bricks.</p>
<p>&#xb7; Minimum 3 years of experience working with SAP technologies such as Hana or BW.</p>
<p>&#xb7; Minimum 3 years of experience working in an IT environment independently and in teams.</p>
<p>&#xb7; Solid understanding and application of business concepts, procedures and practices.</p>
<p><b>Preferred Qualifications</b></p>
<p>&#xb7; Experience with DevOps Version Control and Storyboarding.</p>
<p>&#xb7; Experience with data and reporting using data from ERP systems such as SAP.</p>
<p>&#xb7; Experience with regulatory tools for document archiving and document control.</p>
<p>&#xb7; Ability to perform in a regulated and/or validated environment.</p>
<p>&#xb7; General knowledge of a variety of alternatives and their impact on business.</p>
<p>&#xb7; Experience with other reporting tools such as Snowflake, BOBJ, Tableau</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;35.34 per hour</p>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>Data lake: 3 years (Required)</li>
 <li>Azure Data Lake: 3 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,350077b572417b1d,,Contract,,,Remote,Azure Data Engineer (W2 only),Today,2023-10-20T12:33:21.914Z,,,$35.34 an hour,2023-10-20T12:33:21.918Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=350077b572417b1d&from=jasx&tk=1hd6h6bheiolg800&vjs=3
214,Liberty Mutual,"Pay Philosophy
  The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role.
  Description 
 
  Under direct supervision, responsible for the analysis, development, and execution of data solutions of low to moderate complexity that assists with the information lifecycle needs of an organization
   Assists with collecting, integrating, and analyzing organizational data with the purpose of drawing conclusions from that information
   Develops, constructs, tests, and maintains data architectures for data platform, database, analytical, reporting, or data science systems
   Recognizes opportunities to improve data reliability, quality, and efficiency and may make recommendations where appropriate 
  Designs and develops low complexity programs and tools to support ingestion, curation and provisioning of enterprise data to achieve analytics or reporting
   Builds and designs data models and data architecture that improve accessibility, efficiency, governance and quality of data
   Recognizes opportunities to improve data quality
   Assists with aspects of deployment of data solutions
   Helps identify possible process improvements that address technology gaps within a single business process of low to moderate complexity
   Analyzes and prepare low to moderately complex technology enabled recommendations to address gaps within a single business process
   Performs other projects and duties as assigned
   Telecommuting permitted up to 100%
 
  Qualifications
  The position requires a Bachelor’s degree, or foreign equivalent, in Electrical Engineering, or a related technical or business field plus two (2) years of experience in the job offered or a Associate Data Engineer-related occupation. Position also requires demonstrable experience with each of the following:
 
   New and emerging technologies including AWS SDK, and Docker/Kubernetes
   IT concepts, strategies and methodologies
   IT architectures and technical standards
   Business function and business operations
   Design and development tools
   Layered systems architectures and shared data engineering concepts
   Agile data engineering concepts and processes
   Applying customer requirements, including drawing out unforeseen implications and making recommendations for design, the ability to define design reasoning, understanding potential impacts of design requirements
   Telecommuting permitted up to 100%
 
  To apply, please visit https://jobs.libertymutualgroup.com/, select “Search Jobs,” enter job requisition #2023-61263 in the “Job ID or Keywords” field, and submit resume. Alternatively, you may apply by submitting a resume via e-mail to RecruitLM@LibertyMutual.com. Reference requisition number in subject of e-mail.
  About Us
  **This position may have in-office requirements depending on candidate location.**
 
  At Liberty Mutual, our purpose is to help people embrace today and confidently pursue tomorrow. That’s why we provide an environment focused on openness, inclusion, trust and respect. Here, you’ll discover our expansive range of roles, and a workplace where we aim to help turn your passion into a rewarding profession.
 
  Liberty Mutual has proudly been recognized as a “Great Place to Work” by Great Place to Work® US for the past several years. We were also selected as one of the “100 Best Places to Work in IT” on IDG’s Insider Pro and Computerworld’s 2020 list. For many years running, we have been named by Forbes as one of America’s Best Employers for Women and one of America’s Best Employers for New Graduates—as well as one of America’s Best Employers for Diversity. To learn more about our commitment to diversity and inclusion please visit: https://jobs.libertymutualgroup.com/diversity-equity-inclusion/
 
  We value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits
 
  Liberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran’s status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.","<div>
 <b>Pay Philosophy</b>
 <p> The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role.</p>
 <b><br> Description </b>
 <ul>
  <li>Under direct supervision, responsible for the analysis, development, and execution of data solutions of low to moderate complexity that assists with the information lifecycle needs of an organization</li>
  <li> Assists with collecting, integrating, and analyzing organizational data with the purpose of drawing conclusions from that information</li>
  <li> Develops, constructs, tests, and maintains data architectures for data platform, database, analytical, reporting, or data science systems</li>
  <li> Recognizes opportunities to improve data reliability, quality, and efficiency and may make recommendations where appropriate </li>
  <li>Designs and develops low complexity programs and tools to support ingestion, curation and provisioning of enterprise data to achieve analytics or reporting</li>
  <li> Builds and designs data models and data architecture that improve accessibility, efficiency, governance and quality of data</li>
  <li> Recognizes opportunities to improve data quality</li>
  <li> Assists with aspects of deployment of data solutions</li>
  <li> Helps identify possible process improvements that address technology gaps within a single business process of low to moderate complexity</li>
  <li> Analyzes and prepare low to moderately complex technology enabled recommendations to address gaps within a single business process</li>
  <li> Performs other projects and duties as assigned</li>
  <li> Telecommuting permitted up to 100%</li>
 </ul>
 <b> Qualifications</b>
 <p> The position requires a Bachelor&#x2019;s degree, or foreign equivalent, in Electrical Engineering, or a related technical or business field plus two (2) years of experience in the job offered or a Associate Data Engineer-related occupation. Position also requires demonstrable experience with each of the following:</p>
 <ul>
  <li> New and emerging technologies including AWS SDK, and Docker/Kubernetes</li>
  <li> IT concepts, strategies and methodologies</li>
  <li> IT architectures and technical standards</li>
  <li> Business function and business operations</li>
  <li> Design and development tools</li>
  <li> Layered systems architectures and shared data engineering concepts</li>
  <li> Agile data engineering concepts and processes</li>
  <li> Applying customer requirements, including drawing out unforeseen implications and making recommendations for design, the ability to define design reasoning, understanding potential impacts of design requirements</li>
  <li> Telecommuting permitted up to 100%</li>
 </ul>
 <p> To apply, please visit https://jobs.libertymutualgroup.com/, select &#x201c;Search Jobs,&#x201d; enter job requisition #2023-61263 in the &#x201c;Job ID or Keywords&#x201d; field, and submit resume. Alternatively, you may apply by submitting a resume via e-mail to RecruitLM@LibertyMutual.com. Reference requisition number in subject of e-mail.</p>
 <b> About Us</b>
 <p> **This position may have in-office requirements depending on candidate location.**</p>
 <p></p>
 <p><br> At Liberty Mutual, our purpose is to help people embrace today and confidently pursue tomorrow. That&#x2019;s why we provide an environment focused on openness, inclusion, trust and respect. Here, you&#x2019;ll discover our expansive range of roles, and a workplace where we aim to help turn your passion into a rewarding profession.</p>
 <p></p>
 <p><br> Liberty Mutual has proudly been recognized as a &#x201c;Great Place to Work&#x201d; by Great Place to Work&#xae; US for the past several years. We were also selected as one of the &#x201c;100 Best Places to Work in IT&#x201d; on IDG&#x2019;s Insider Pro and Computerworld&#x2019;s 2020 list. For many years running, we have been named by Forbes as one of America&#x2019;s Best Employers for Women and one of America&#x2019;s Best Employers for New Graduates&#x2014;as well as one of America&#x2019;s Best Employers for Diversity. To learn more about our commitment to diversity and inclusion please visit: https://jobs.libertymutualgroup.com/diversity-equity-inclusion/</p>
 <p></p>
 <p><br> We value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits</p>
 <p></p>
 <p><br> Liberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran&#x2019;s status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.</p>
</div>",https://app.eightfold.ai/careers/job/618494050518?domain=libertymutual.com&utm_source=indeed&microsite=libertymutual.com&mode=job&iis=Job+Board&iisn=Indeed+Organic,ae83164544567281,,Full-time,,,"Columbus, OH",Associate Data Engineer,Today,2023-10-20T12:33:18.129Z,3.5,5353.0,"$79,602 - $114,700 a year",2023-10-20T12:33:18.132Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=ae83164544567281&from=jasx&tk=1hd6h6bheiolg800&vjs=3
215,eCom Solutions Inc,"Job Title: Sr. Lead Data Engineer
Location: Remote
Duration: Long Term
Pay Rate: BOE
Job Description Summary :
Recognized subject matter expert. Apply advanced statistical data modeling techniques to large data sets to create actionable business insights- Use statistical analysis software packages (SAS, SPSS, etc.-) and business intelligence and analytics platforms to create dashboards and reporting capabilities- Design, develop and deploy algorithms through statistical programming that: support complex business decision making, manage large amounts of data and create visualization and insights. Manage major/complex large projects and coaches, review and delegate work to entry/intermediate profession.
MUST HAVE's:
​Over all 12-15 years of experience as a Data Engineer.
TOP 3 Skills:
1. Snowflake
2. SQL
3. Data Modelling
Expertise in ETL tools (SAP BODS, Informatica or similar tools)
MAJOR JOB DUTIES AND RESPONSIBILITIES
o Collaborate with the Analytical team to prototype and prove the viability of data solutions
o Design of all aspects of data solutions including modeling, developing, technical documentation, data diagrams and data dictionaries.
o Lead development and execution of data solutions involving domain specific analytics.
o Provide expertise in the development of standards, architectural governance, design patterns, and practices, evaluate best applicable solutions for different use cases
o Work cross-functionally with product owners, business stakeholders, and management.
o Direct people management and leadership experience (budget management, interview, hire, performance management, promotion, etc.)
o Suggest how to design and optimize data architecture for consumption, utilization, and analytics on the Data Warehouse
o Analyze data patterns and optimize data processing. Suggest best practice, technology, and process improvements
o Comfortable with rapid prototyping and disciplined software development processes.
o Realize and communicate trends related to Data technologies
o Create centralized and streamlined Datawarehouse support processes. Provide effective training to team members
o Performs other duties as assigned
QUALIFICATIONS (Education/Training, Experience and Certifications)
o Bachelor’s Degree in Engineering (computer science or other equivalent majors). Prefer advanced degree
o 12+ years of architecture or engineering experience in building data pipelines, ETLs, data platforms, data products, distributed data systems or AI/ML, Master Data,
Data Quality, Cloud Technologies, etc.
o 12+ year of experience with SQL, shell is a plus
o 3+ plus years of job-related experience in programming languages such as Scala, Python, or similar
KNOWLEDGE SKILLS AND ABILITIES (Those necessary to perform the job competently)
o Expertise in Cloud Data technologies (Snowflake, AWS or GCP)
o Expertise in ETL tools (SAP BODS, Informatica or similar tools)
o SAP functional domain knowledge is big plus (Supply Chain, Finance etc.)
o Experience in data governance, data security, data compliance etc.
o Large scale projects, process and industry experience is big plus
o Strong organizational skills
o Proven team motivator who can develop talent
o Able to coach team members.
o Ability to provide technical direction to data and analytics teams
o Excellent communication skills both verbal and written
Job Type: Contract
Pay: $65.00 - $70.00 per hour
Benefits:

 Dental insurance
 Health insurance
 Life insurance
 Vision insurance

Experience level:

 11+ years

Schedule:

 Day shift
 Monday to Friday

Experience:

 Cloud Data technologies (Snowflake, AWS or GCP): 4 years (Preferred)
 ETL tools (SAP BODS, Informatica: 4 years (Preferred)
 SAP functional domain knowledge: 1 year (Preferred)

Work Location: Remote","<p>Job Title: Sr. Lead Data Engineer</p>
<p>Location: Remote</p>
<p>Duration: Long Term</p>
<p>Pay Rate: BOE</p>
<p><b>Job Description Summary :</b></p>
<p>Recognized subject matter expert. Apply advanced statistical data modeling techniques to large data sets to create actionable business insights- Use statistical analysis software packages (SAS, SPSS, etc.-) and business intelligence and analytics platforms to create dashboards and reporting capabilities- Design, develop and deploy algorithms through statistical programming that: support complex business decision making, manage large amounts of data and create visualization and insights. Manage major/complex large projects and coaches, review and delegate work to entry/intermediate profession.</p>
<p><b>MUST HAVE&apos;s:</b></p>
<p><b>&#x200b;</b>Over all 12-15 years of experience as a Data Engineer.</p>
<p>TOP 3 Skills:</p>
<p>1. Snowflake</p>
<p>2. SQL</p>
<p>3. Data Modelling</p>
<p>Expertise in ETL tools (SAP BODS, Informatica or similar tools)</p>
<p><b>MAJOR JOB DUTIES AND RESPONSIBILITIES</b></p>
<p>o Collaborate with the Analytical team to prototype and prove the viability of data solutions</p>
<p>o Design of all aspects of data solutions including modeling, developing, technical documentation, data diagrams and data dictionaries.</p>
<p>o Lead development and execution of data solutions involving domain specific analytics.</p>
<p>o Provide expertise in the development of standards, architectural governance, design patterns, and practices, evaluate best applicable solutions for different use cases</p>
<p>o Work cross-functionally with product owners, business stakeholders, and management.</p>
<p>o Direct people management and leadership experience (budget management, interview, hire, performance management, promotion, etc.)</p>
<p>o Suggest how to design and optimize data architecture for consumption, utilization, and analytics on the Data Warehouse</p>
<p>o Analyze data patterns and optimize data processing. Suggest best practice, technology, and process improvements</p>
<p>o Comfortable with rapid prototyping and disciplined software development processes.</p>
<p>o Realize and communicate trends related to Data technologies</p>
<p>o Create centralized and streamlined Datawarehouse support processes. Provide effective training to team members</p>
<p>o Performs other duties as assigned</p>
<p><b>QUALIFICATIONS</b> (Education/Training, Experience and Certifications)</p>
<p>o Bachelor&#x2019;s Degree in Engineering (computer science or other equivalent majors). Prefer advanced degree</p>
<p>o 12+ years of architecture or engineering experience in building data pipelines, ETLs, data platforms, data products, distributed data systems or AI/ML, Master Data,</p>
<p>Data Quality, Cloud Technologies, etc.</p>
<p>o 12+ year of experience with SQL, shell is a plus</p>
<p>o 3+ plus years of job-related experience in programming languages such as Scala, Python, or similar</p>
<p><b>KNOWLEDGE SKILLS AND ABILITIES</b> (Those necessary to perform the job competently)</p>
<p>o Expertise in Cloud Data technologies (Snowflake, AWS or GCP)</p>
<p>o Expertise in ETL tools (SAP BODS, Informatica or similar tools)</p>
<p>o SAP functional domain knowledge is big plus (Supply Chain, Finance etc.)</p>
<p>o Experience in data governance, data security, data compliance etc.</p>
<p>o Large scale projects, process and industry experience is big plus</p>
<p>o Strong organizational skills</p>
<p>o Proven team motivator who can develop talent</p>
<p>o Able to coach team members.</p>
<p>o Ability to provide technical direction to data and analytics teams</p>
<p>o Excellent communication skills both verbal and written</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;65.00 - &#x24;70.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Life insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>11+ years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Cloud Data technologies (Snowflake, AWS or GCP): 4 years (Preferred)</li>
 <li>ETL tools (SAP BODS, Informatica: 4 years (Preferred)</li>
 <li>SAP functional domain knowledge: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,c323d27b302a6ce8,,Contract,,,Remote,Lead Data Engineer,Today,2023-10-20T12:33:27.145Z,,,$65 - $70 an hour,2023-10-20T12:33:27.148Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=c323d27b302a6ce8&from=jasx&tk=1hd6h6bheiolg800&vjs=3
216,Fortune logix,"Data engineer who have strong experience in DBT Cloud, 
Snowflake Cloud DB and building Finance Datamarts 
(GL, JL, AP, AR & Fixed Assets) with infor Lawson ERP. 
Remote 
and 
rate is $70/hour 
C2C.
Job Type: Contract
Pay: $65.00 - $70.00 per hour
Experience level:

 10 years

Schedule:

 Monday to Friday

Work Location: Remote","<p><b>Data engineer who have strong experience in DBT Cloud, </b></p>
<p><b>Snowflake Cloud DB and building Finance Datamarts </b></p>
<p><b>(GL, JL, AP, AR &amp; Fixed Assets) with infor Lawson ERP. </b></p>
<p><b>Remote </b></p>
<p><b>and </b></p>
<p><b>rate is &#x24;70/hour </b></p>
<p><b>C2C.</b></p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;65.00 - &#x24;70.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,0126d0e3ab279a99,,Contract,,,Remote,Data Engineer,Today,2023-10-20T12:33:28.635Z,,,$65 - $70 an hour,2023-10-20T12:33:28.637Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=0126d0e3ab279a99&from=jasx&tk=1hd6h6bheiolg800&vjs=3
217,Gridiron IT,"Seeking a Junior Data Engineer on a remote basis. Secret clearance is required. 
Overview: We are looking to immediately fill a Junior Data Engineer on our team. The ADE is one of the major pillars of MyNavy HR Transformation and serves as an enterprise-wide centralized repository that provides seamless and secure data access. This pilot’s objective is to support defining a comprehensive future-state ADE data model by informing the total number of unique data elements and to assess the ability to accelerate the data integration process using new data tools available following the Authority to Operate (ATO).
Minimum Qualifications:

 2+ years of experience with scalable ETL workflows/development, extract, cleanse, and process disparate data sources
 Secret Clearance is required.
 HS Diploma required (Bachelor's preferred)
 Experience with cleaning and transforming data utilizing Python and/or SQL, specifically complex SQL queries
 Familiarity with acquiring data from disparate data sources using APIs and SQL
 Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms

Job Type: Full-time
Pay: $48.00 - $52.00 per hour
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Vision insurance

Schedule:

 8 hour shift

Work Location: Remote","<p><b>Seeking a Junior Data Engineer on a remote basis. </b><br><b>Secret clearance is required. </b></p>
<p><b>Overview:</b> We are looking to immediately fill a Junior Data Engineer on our team. The ADE is one of the major pillars of MyNavy HR Transformation and serves as an enterprise-wide centralized repository that provides seamless and secure data access. This pilot&#x2019;s objective is to support defining a comprehensive future-state ADE data model by informing the total number of unique data elements and to assess the ability to accelerate the data integration process using new data tools available following the Authority to Operate (ATO).</p>
<p><b>Minimum Qualifications:</b></p>
<ul>
 <li>2+ years of experience with scalable ETL workflows/development, extract, cleanse, and process disparate data sources</li>
 <li>Secret Clearance is required.</li>
 <li>HS Diploma required (Bachelor&apos;s preferred)</li>
 <li>Experience with cleaning and transforming data utilizing Python and/or SQL, specifically complex SQL queries</li>
 <li>Familiarity with acquiring data from disparate data sources using APIs and SQL</li>
 <li>Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;48.00 - &#x24;52.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,45a231ffe43c577a,,Full-time,,,Remote,Jr Data Engineer - Secret Cleared,Today,2023-10-20T12:33:28.964Z,4.2,17.0,$48 - $52 an hour,2023-10-20T12:33:28.967Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=45a231ffe43c577a&from=jasx&tk=1hd6h6bheiolg800&vjs=3
218,IBR (Imagine Believe Realize),"The Senior Data Engineer must be able to meet the key criteria below:

 Location: 100% telework
 Years' Experience: 10+ years
 Education: Bachelor’s in IT related field
 Security Clearance: IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required.
 Work Authorization: Must show that applicant is legally permitted to work in the United States.
 Employment Type: Full-Time, W-2
 Key Skills:
 10+ years of IT experience focusing on enterprise data architecture and management
 Experience with Databricks required
 8+ years experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
 Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services
 Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
 Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus

Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities

 Plan, create, and maintain data architectures, ensuring alignment with business requirements
 Obtain data, formulate dataset processes, and store optimized data
 Identify problems and inefficiencies and apply solutions
 Determine tasks where manual participation can be eliminated with automation.
 Identify and optimize data bottlenecks, leveraging automation where possible
 Create and manage data lifecycle policies (retention, backups/restore, etc)
 Create, maintain, and manage ETL/ELT pipelines
 Create, maintain, and manage data transformations
 Maintain/update documentation
 Create, maintain, and manage data pipeline schedules
 Monitor data pipelines
 Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality
 Support AI/ML teams with optimizing feature engineering code
 Spark updates
 Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT
 Research existing data in the data lake to determine best sources for data
 Create, manage, and maintain ksqlDB and Kafka Streams queries/code
 Maintain and update Python-based data processing scripts executed on AWS Lambdas
 Unit tests for all the Spark, Python data processing and Lambda codes
 Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)

Qualifications

 10+ years of IT experience focusing on enterprise data architecture and management
 Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
 Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required
 Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark
 Data Lake concepts such as time travel and schema evolution and optimization
 Structured Streaming and Delta Live Tables with Databricks a bonus
 Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
 Advanced level understanding of streaming data pipelines and how they differ from batch systems
 Formalize concepts of how to handle late data, defining windows, and data freshness
 Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc
 Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.
 Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus
 Understanding of streaming data pipelines and batch systems
 Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness
 Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
 Indexing and partitioning strategy experience
 Debug, troubleshoot, design and implement solutions to complex technical issues
 Experience with large-scale, high-performance enterprise big data application deployment and solution
 Understanding how to create DAGs to define workflows
 Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required
 Architecture experience in AWS environment a bonus
 Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale (UNICORN) a bonus
 Experience with Docker, Jenkins, and CloudWatch
 Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines
 Experience working with AWS Lambdas for configuration and optimization
 Experience working with DynamoDB to query and write data
 Experience with S3
 Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus
 Familiarity with Pytest and Unittest a bonus
 Experience working with JSON and defining JSON Schemas a bonus
 Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus
 Familiarity with Schema Registry, message formats such as Avro, ORC, etc.
 Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams
 Ability to thrive in a team-based environment
 Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management

Physical Demands
Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.
Work Environment
Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.
About IBRImagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:

 Nationwide medical, dental, and vision insurance
 3 weeks of Paid Time Off and 11 Paid Federal Holidays
 401k matching
 Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees
 Flexible spending accounts and Dependent Care spending accounts
 Wellness incentives
 Reimbursement for professional development and certifications
 Training assistance opportunities

Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.
Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.”Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional contact information has been provided below:
info@teamibr.com​​​​​​​407.459.1830
Job Type: Full-time
Pay: $131,243.38 - $158,056.53 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Referral program
 Vision insurance

Experience level:

 10 years

Schedule:

 Monday to Friday

Work Location: Remote","<p><b>The Senior Data Engineer must be able to meet the key criteria below:</b></p>
<ul>
 <li><b>Location: </b>100% telework</li>
 <li><b>Years&apos; Experience: </b>10+ years</li>
 <li><b>Education: </b>Bachelor&#x2019;s in IT related field</li>
 <li><b>Security Clearance:</b> IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required.</li>
 <li><b>Work Authorization:</b> Must show that applicant is legally permitted to work in the United States.</li>
 <li><b>Employment Type:</b> Full-Time, W-2</li>
 <li><b>Key Skills:</b></li>
 <li>10+ years of IT experience focusing on enterprise data architecture and management</li>
 <li>Experience with Databricks required</li>
 <li>8+ years experience in Conceptual/Logical/Physical Data Modeling &amp; expertise in Relational and Dimensional Data Modeling</li>
 <li>Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services</li>
 <li>Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)</li>
 <li>Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus</li>
</ul>
<p><b>Overview</b></p>
<p>Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations &amp; Maintenance activities.</p>
<p><b>Responsibilities</b></p>
<ul>
 <li>Plan, create, and maintain data architectures, ensuring alignment with business requirements</li>
 <li>Obtain data, formulate dataset processes, and store optimized data</li>
 <li>Identify problems and inefficiencies and apply solutions</li>
 <li>Determine tasks where manual participation can be eliminated with automation.</li>
 <li>Identify and optimize data bottlenecks, leveraging automation where possible</li>
 <li>Create and manage data lifecycle policies (retention, backups/restore, etc)</li>
 <li>Create, maintain, and manage ETL/ELT pipelines</li>
 <li>Create, maintain, and manage data transformations</li>
 <li>Maintain/update documentation</li>
 <li>Create, maintain, and manage data pipeline schedules</li>
 <li>Monitor data pipelines</li>
 <li>Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality</li>
 <li>Support AI/ML teams with optimizing feature engineering code</li>
 <li>Spark updates</li>
 <li>Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT</li>
 <li>Research existing data in the data lake to determine best sources for data</li>
 <li>Create, manage, and maintain ksqlDB and Kafka Streams queries/code</li>
 <li>Maintain and update Python-based data processing scripts executed on AWS Lambdas</li>
 <li>Unit tests for all the Spark, Python data processing and Lambda codes</li>
 <li>Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)</li>
</ul>
<p><b>Qualifications</b></p>
<ul>
 <li>10+ years of IT experience focusing on enterprise data architecture and management</li>
 <li>Experience in Conceptual/Logical/Physical Data Modeling &amp; expertise in Relational and Dimensional Data Modeling</li>
 <li>Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required</li>
 <li>Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark</li>
 <li>Data Lake concepts such as time travel and schema evolution and optimization</li>
 <li>Structured Streaming and Delta Live Tables with Databricks a bonus</li>
 <li>Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support</li>
 <li>Advanced level understanding of streaming data pipelines and how they differ from batch systems</li>
 <li>Formalize concepts of how to handle late data, defining windows, and data freshness</li>
 <li>Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc</li>
 <li>Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.</li>
 <li>Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus</li>
 <li>Understanding of streaming data pipelines and batch systems</li>
 <li>Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness</li>
 <li>Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)</li>
 <li>Indexing and partitioning strategy experience</li>
 <li>Debug, troubleshoot, design and implement solutions to complex technical issues</li>
 <li>Experience with large-scale, high-performance enterprise big data application deployment and solution</li>
 <li>Understanding how to create DAGs to define workflows</li>
 <li>Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required</li>
 <li>Architecture experience in AWS environment a bonus</li>
 <li>Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale (UNICORN) a bonus</li>
 <li>Experience with Docker, Jenkins, and CloudWatch</li>
 <li>Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines</li>
 <li>Experience working with AWS Lambdas for configuration and optimization</li>
 <li>Experience working with DynamoDB to query and write data</li>
 <li>Experience with S3</li>
 <li>Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus</li>
 <li>Familiarity with Pytest and Unittest a bonus</li>
 <li>Experience working with JSON and defining JSON Schemas a bonus</li>
 <li>Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus</li>
 <li>Familiarity with Schema Registry, message formats such as Avro, ORC, etc.</li>
 <li>Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams</li>
 <li>Ability to thrive in a team-based environment</li>
 <li>Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management</li>
</ul>
<p><b>Physical Demands</b></p>
<p>Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.</p>
<p><b>Work Environment</b></p>
<p>Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.</p>
<p><b>About IBR</b><br>Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:</p>
<ul>
 <li>Nationwide medical, dental, and vision insurance</li>
 <li>3 weeks of Paid Time Off and 11 Paid Federal Holidays</li>
 <li>401k matching</li>
 <li>Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees</li>
 <li>Flexible spending accounts and Dependent Care spending accounts</li>
 <li>Wellness incentives</li>
 <li>Reimbursement for professional development and certifications</li>
 <li>Training assistance opportunities</li>
</ul>
<p>Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.</p>
<p>Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.&#x201d;<br>Learn more at <i>http://www.teamibr.com</i></p>
<p>If alternative methods of assistance are needed with the application process, additional contact information has been provided below:</p>
<p><i>info@teamibr.com</i><br>&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;407.459.1830</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;131,243.38 - &#x24;158,056.53 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Referral program</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,5581f8a1bea3816b,,Full-time,,,Remote,Senior Data Engineer,Today,2023-10-20T12:33:29.893Z,,,"$131,243 - $158,057 a year",2023-10-20T12:33:29.895Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=5581f8a1bea3816b&from=jasx&tk=1hd6h6bheiolg800&vjs=3
219,Headspace,"About the Staff Data Engineer at Headspace: 
  Headspace is seeking an experienced Staff Data Engineer to join our Data Products team (part of our Data Engineering org). In this role, you will be responsible for architecting and implementing a set of core data sets in our data lake. Your customers are our data consumers, including analysts, machine learning engineers and data scientists. 
  What you will do: 
  
  Design and implement mission critical data pipelines for our company 
  Help create a set of high-quality, composable data products for our data consumers 
  Write well designed, testable, performant, and efficient code that runs on Apache Spark and Delta Lake 
  Lead the development of a world-class data lake that meets the strict security, privacy, and compliance requirements of the healthcare industry 
  Collaborate with the data science and machine learning team to build data sets used for model training and development 
  Mentor and coach other engineers to build a data-first culture at the company 
  Write well designed, testable, performant, and efficient code 
  Contribute in all phases of the development lifecycle 
  
 What you will bring: 
  Required Skills: 
  
  6+ years professional software development 
  You’ve built high quality data pipelines before with comprehensive unit tests suites, data quality checks etc. 
  Has a solid grasp of building new frameworks, tools or systems. Able to bring creative technical solutions to the table and design solutions at scale. 
  Experience with Apache Spark and Delta Lake are a plus, but not required 
  Solid understanding of system topologies from machine architecture to network architecture. Ability to solve unique complex problems. 
  Ability to work independently with minimal supervision on system level projects. Identifies and corrects errors on their own. Assumes greater responsibilities and anticipates some team needs. A wide degree of creativity and latitude is expected. 
  Proposes new solutions, ideas, tools and techniques for moderately complex problems. 
  Begins to assume a lead role in team projects. Mentors and provides guidance. 
  Considers multiple approaches and recommends best technical direction including logic and reasoning to areas outside of the immediate team. 
  Proven hands-on Software Development experience, especially API and microservices architecture 
  
 Preferred Skills: 
  
  Having experience apache spark would be useful 
  Experience with data modeling 
  Python experience is a plus 
  
 Pay & Benefits: 
  The base salary range for this role is determined by a number of factors, including but not limited to skills and scope required, relevant licensure and certifications, and unique relevant experience and job-related skills. The base salary range for this role is $131,414 - $190,100. 
  At Headspace, cash salary is but one component of our Total Rewards package. We’re proud of our robust package inclusive of: base salary, stock awards, comprehensive healthcare coverage, monthly wellness stipend, retirement savings match, lifetime Headspace membership, unlimited, free mental health coaching, generous parental leave, and much more. Paid performance incentives are also included for those in eligible roles. Additional details about our Total Rewards package will be provided during the recruitment process. 
  How we feel about Diversity, Equity, Inclusion and Belonging: 
  Headspace is committed to bringing together humans from different backgrounds and perspectives, providing employees with a safe and welcoming work environment free of discrimination and harassment. We strive to create a diverse & inclusive environment where everyone can thrive, feel a sense of belonging, and do impactful work together. 
  As an equal opportunity employer, we prohibit any unlawful discrimination against a job applicant on the basis of their race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, family or parental status, disability*, age, veteran status, or any other status protected by the laws or regulations in the locations where we operate. We respect the laws enforced by the EEOC and are dedicated to going above and beyond in fostering diversity across our workplace. 
 
  Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on Headspace Health. Please inform our Talent team if you need any assistance completing any forms or to otherwise participate in the application or interview process.
  
  
 Headspace participates in the E-Verify Program. 
  Privacy Statement 
  All member records are protected according to our Privacy Policy. Further, while employees of Headspace (formerly Ginger) cannot access Headspace products/services, they will be offered benefits according to the company's benefit plan. To ensure we are adhering to best practice and ethical guidelines in the field of mental health, we take care to avoid dual relationships. A dual relationship occurs when a mental health care provider has a second, significantly different relationship with their client in addition to the traditional client-therapist relationship—including, for example, a managerial relationship. 
  As such, Headspace requests that individuals who have received coaching or clinical services at Headspace wait until their care with Headspace is complete before applying for a position. If someone with a Headspace account is hired for a position, please note their account will be deactivated and they will not be able to use Headspace services for the duration of their employment. 
  Further, if Headspace cannot find a role that fails to resolve an ethical issue associated with a dual relationship, Headspace may need to take steps to ensure ethical obligations are being adhered to, including a delayed start date or a potential leave of absence. Such steps would be taken to protect both the former member, as well as any relevant individuals from their care team, from impairment, risk of exploitation, or harm. 
  For how how we will use the personal information you provide as part of the application process, please see: https://organizations.headspace.com/page/applicant-notice.","<div>
 <p><b>About the Staff Data Engineer</b><b> at Headspace:</b></p> 
 <p><i> Headspace is seeking an experienced Staff Data Engineer to join our Data Products team (part of our Data Engineering org). In this role, you will be responsible for architecting and implementing a set of core data sets in our data lake. Your customers are our data consumers, including analysts, machine learning engineers and data scientists.</i></p> 
 <p><b> What you will do:</b></p> 
 <ul> 
  <li>Design and implement mission critical data pipelines for our company</li> 
  <li>Help create a set of high-quality, composable data products for our data consumers</li> 
  <li>Write well designed, testable, performant, and efficient code that runs on Apache Spark and Delta Lake</li> 
  <li>Lead the development of a world-class data lake that meets the strict security, privacy, and compliance requirements of the healthcare industry</li> 
  <li>Collaborate with the data science and machine learning team to build data sets used for model training and development</li> 
  <li>Mentor and coach other engineers to build a data-first culture at the company</li> 
  <li>Write well designed, testable, performant, and efficient code</li> 
  <li>Contribute in all phases of the development lifecycle</li> 
 </ul> 
 <p><b>What you will bring</b>:</p> 
 <p><b> Required Skills:</b></p> 
 <ul> 
  <li>6+ years professional software development</li> 
  <li>You&#x2019;ve built high quality data pipelines before with comprehensive unit tests suites, data quality checks etc.</li> 
  <li>Has a solid grasp of building new frameworks, tools or systems. Able to bring creative technical solutions to the table and design solutions at scale.</li> 
  <li>Experience with Apache Spark and Delta Lake are a plus, but not required</li> 
  <li>Solid understanding of system topologies from machine architecture to network architecture. Ability to solve unique complex problems.</li> 
  <li>Ability to work independently with minimal supervision on system level projects. Identifies and corrects errors on their own. Assumes greater responsibilities and anticipates some team needs. A wide degree of creativity and latitude is expected.</li> 
  <li>Proposes new solutions, ideas, tools and techniques for moderately complex problems.</li> 
  <li>Begins to assume a lead role in team projects. Mentors and provides guidance.</li> 
  <li>Considers multiple approaches and recommends best technical direction including logic and reasoning to areas outside of the immediate team.</li> 
  <li>Proven hands-on Software Development experience, especially API and microservices architecture</li> 
 </ul> 
 <p><b>Preferred Skills:</b></p> 
 <ul> 
  <li>Having experience apache spark would be useful</li> 
  <li>Experience with data modeling</li> 
  <li>Python experience is a plus</li> 
 </ul> 
 <p><b>Pay &amp; Benefits</b>:</p> 
 <p> The base salary range for this role is determined by a number of factors, including but not limited to skills and scope required, relevant licensure and certifications, and unique relevant experience and job-related skills. The base salary range for this role is <b>&#x24;131,414 - &#x24;190,100</b>.</p> 
 <p> At Headspace, cash salary is but one component of our Total Rewards package. We&#x2019;re proud of our robust package inclusive of: base salary, stock awards, comprehensive healthcare coverage, monthly wellness stipend, retirement savings match, lifetime Headspace membership, unlimited, free mental health coaching, generous parental leave, and much more. Paid performance incentives are also included for those in eligible roles. Additional details about our Total Rewards package will be provided during the recruitment process.</p> 
 <p><b> How we feel about Diversity, Equity, Inclusion and Belonging:</b></p> 
 <p> Headspace is committed to bringing together humans from different backgrounds and perspectives, providing employees with a safe and welcoming work environment free of discrimination and harassment. We strive to create a diverse &amp; inclusive environment where everyone can thrive, feel a sense of belonging, and do impactful work together.</p> 
 <p> As an equal opportunity employer, we prohibit any unlawful discrimination against a job applicant on the basis of their race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, family or parental status, disability*, age, veteran status, or any other status protected by the laws or regulations in the locations where we operate. We respect the laws enforced by the EEOC and are dedicated to going above and beyond in fostering diversity across our workplace.</p> 
 <ul>
  <li><i>Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on Headspace Health. </i><b><i>Please inform our Talent team</i></b><b><i> if you need any assistance completing any forms or to otherwise participate in the application or interview process.</i></b></li>
 </ul> 
 <p></p> 
 <p><i>Headspace participates in the </i><i>E-Verify Program</i><i>.</i></p> 
 <p><b><i> Privacy Statement</i></b></p> 
 <p><i> All member records are protected according to our</i><i> </i><i>Privacy Policy</i><i>. Further, while employees of Headspace (formerly Ginger) cannot access Headspace products/services, they will be offered benefits according to the company&apos;s benefit plan. To ensure we are adhering to best practice and ethical guidelines in the field of mental health, we take care to avoid dual relationships. A dual relationship occurs when a mental health care provider has a second, significantly different relationship with their client in addition to the traditional client-therapist relationship&#x2014;including, for example, a managerial relationship.</i></p> 
 <p><i> As such, Headspace requests that individuals who have received coaching or clinical services at Headspace wait until their care with Headspace is complete before applying for a position. If someone with a Headspace account is hired for a position, please note their account will be deactivated and they will not be able to use Headspace services for the duration of their employment.</i></p> 
 <p><i> Further, if Headspace cannot find a role that fails to resolve an ethical issue associated with a dual relationship, Headspace may need to take steps to ensure ethical obligations are being adhered to, including a delayed start date or a potential leave of absence. Such steps would be taken to protect both the former member, as well as any relevant individuals from their care team, from impairment, risk of exploitation, or harm.</i></p> 
 <p> For how how we will use the personal information you provide as part of the application process, please see: https://organizations.headspace.com/page/applicant-notice.</p>
</div>",https://boards.greenhouse.io/hs/jobs/5452319,f1db132bcf22f966,,,,,Remote,Staff Data Engineer,Today,2023-10-20T12:33:31.378Z,,,"$131,414 - $190,100 a year",2023-10-20T12:33:31.380Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=f1db132bcf22f966&from=jasx&tk=1hd6h6bheiolg800&vjs=3
220,Bright Vision Technologies,"Bright Vision Technologies has an immediate opportunity for Lead Data Engineer in Santa Clara, CA.(Remote)
  Title: Lead Data Engineer Role: Remote Location: Santa Clara, CA.
  As a consultant within the DIE team, you will work with our clients to define their digital strategy and execution roadmap, and design and implement differentiated digital solutions to help deliver measurable value.
  Must have 12+years of experience and should have worked as Lead.
  Your responsibilities in this role will include:
 
   Primary focus is on Glue, S3, Redshift, Lambda, PySpark, Spark.
   Then added skillset which could add value are AWS Step function, NoSQL DB like Dynamo DB and AWS Data Migration Service in that order of priority.
   Data engineer should have at least 2 years of relevant AWS experience with their services mentioned above.
   Experience in data security or governance and performance improvement is an added benefit
   Only focus is on AWS services and tech stack.
 
  Would you like to know more about our new opportunity? For immediate consideration, please send your resume directly to Venkata Raju at venkat.r@bvteck.com or Phone +1 (732) 298-7641 
 https://calendly.com/venkat-bvteck/15min 
 At BVTeck, we are committed to providing equal employment opportunities and fostering an inclusive work environment. We encourage applications from all qualified individuals regardless of race, ethnicity, religion, gender identity, sexual orientation, age, disability, or any other protected status. If you require accommodations during the recruitment process, please let us know.
  
 5S8nYMOotm","<div>
 <p>Bright Vision Technologies has an immediate opportunity for Lead Data Engineer in Santa Clara, CA.(Remote)</p>
 <p> Title: Lead Data Engineer<br> Role: Remote<br> Location: Santa Clara, CA.</p>
 <p> As a consultant within the DIE team, you will work with our clients to define their digital strategy and execution roadmap, and design and implement differentiated digital solutions to help deliver measurable value.</p>
 <p><b> Must have 12+years of experience and should have worked as Lead.</b></p>
 <p><br> Your responsibilities in this role will include:</p>
 <ul>
  <li> Primary focus is on Glue, S3, Redshift, Lambda, PySpark, Spark.</li>
  <li> Then added skillset which could add value are AWS Step function, NoSQL DB like Dynamo DB and AWS Data Migration Service in that order of priority.</li>
  <li> Data engineer should have at least 2 years of relevant AWS experience with their services mentioned above.</li>
  <li> Experience in data security or governance and performance improvement is an added benefit</li>
  <li> Only focus is on AWS services and tech stack.</li>
 </ul>
 <p><i> Would you like to know more about our new opportunity? For immediate consideration, please send your resume directly to Venkata Raju at venkat.r@bvteck.com or Phone +1 (732) 298-7641</i></p> 
 <p><i>https://calendly.com/venkat-bvteck/15min</i></p> 
 <p><i>At BVTeck, we are committed to providing equal employment opportunities and fostering an inclusive work environment. We encourage applications from all qualified individuals regardless of race, ethnicity, religion, gender identity, sexual orientation, age, disability, or any other protected status. If you require accommodations during the recruitment process, please let us know.</i></p>
 <p> </p>
 <p>5S8nYMOotm</p>
</div>",https://brightvisiontechnologies.applytojob.com/apply/5S8nYMOotm/Lead-Data-Engineer?source=INDE,67ac721633aee65d,,,,,Remote,Lead Data Engineer,Just posted,2023-10-20T12:33:33.694Z,,,$70 an hour,2023-10-20T12:33:33.695Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=67ac721633aee65d&from=jasx&tk=1hd6h6bheiolg800&vjs=3
222,Vidu Solutions LLC,"Required Skills:
Python, Spark/PySpark, AWS- EMR
Job Types: Contract, Full-time
Salary: $66,356.86 - $146,845.57 per year
Schedule:

 8 hour shift

Work Location: In person","<p>Required Skills:</p>
<p>Python, Spark/PySpark, AWS- EMR</p>
<p>Job Types: Contract, Full-time</p>
<p>Salary: &#x24;66,356.86 - &#x24;146,845.57 per year</p>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: In person</p>",,d51b9af24af3140d,,Full-time,Contract,,"3521 Mclean Ave, Fairfax, VA 22030",Data Engineer,1 day ago,2023-10-19T12:33:44.384Z,,,"$66,357 - $146,846 a year",2023-10-20T12:33:44.388Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d51b9af24af3140d&from=jasx&tk=1hd6h77l0ihna800&vjs=3
223,"Stark Dev, LLC","This position is open for United States Citizens/ GC/GC EAD/H4 EAD/TN VISA HOLDERS.
Rate:
$60-70
Duration:
12 months
W2 contract
Location:
Columbus Ohio and Dallas
Visa Requirements:
USC / GC / GC EAD / H4 EAD or TN visa
Description:

 8+ years of professional work experience designing and implementing data pipelines in a cloud environment is required. (E.g. Apache NiFi /Informatica BDM/Talend, etc.).
 5+ years of experience migrating/developing data solutions in the AWS cloud is required. Experience needed in Glue Catalog, AWS Glue, AWS DMS, Apache Airflow, AWS IAM, AWS RDS
 3+ years of experience building/implementing data pipelines using Databricks such as Unity Catalog, Databricks workflow, Databricks Live Table etc.
 3+ years of Hands-on object-oriented programming experience using Python (specially Py Spark) to write complex, highly optimized queries across large volumes of data.
 Knowledge or experience in Big data technologies such as Hadoop/Spark
 Knowledge or experience in Data modeling and ETL processing
 Knowledge or experience in architectural best practices in building data lakes
 Expert level knowledge of using SQL to write complex, highly optimized queries across large volumes of data.

Job Type: Contract
Pay: From $60.00 per hour
Experience level:

 5 years
 8 years

Schedule:

 Monday to Friday

Experience:

 PySpark: 3 years (Required)
 implementing data pipelines in a cloud environment: 8 years (Required)
 migrating/developing data solutions in the AWS cloud: 5 years (Required)
 building/implementing data pipelines using Databricks: 3 years (Required)
 object-oriented programming experience using Python: 3 years (Required)
 Big data technologies such as Hadoop/Spark: 2 years (Required)
 Data modeling and ETL processing: 2 years (Required)
 architectural best practices in building data lakes: 2 years (Required)
 SQL queries: 2 years (Required)
 Apache NiFi /Informatica BDM/Talend: 8 years (Required)
 AWS Glue, AWS DMS, Apache Airflow, AWS IAM, AWS RDS: 5 years (Required)
 Unity Catalog, Databricks workflow, Databricks Live Table: 3 years (Required)

Work Location: In person","<p>This position is open for <b>United States Citizens/ GC/GC EAD/H4 EAD/TN VISA HOLDERS.</b></p>
<p><b>Rate:</b></p>
<p><b>&#x24;60-70</b></p>
<p><b>Duration:</b></p>
<p><b>12 months</b></p>
<p><b>W2 contract</b></p>
<p><b>Location:</b></p>
<p><b>Columbus Ohio and Dallas</b></p>
<p><b>Visa Requirements:</b></p>
<p><b>USC / GC / GC EAD / H4 EAD or TN visa</b></p>
<p><b>Description:</b></p>
<ul>
 <li>8+ years of professional work experience designing and implementing data pipelines in a cloud environment is required. (E.g. Apache NiFi /Informatica BDM/Talend, etc.).</li>
 <li>5+ years of experience migrating/developing data solutions in the AWS cloud is required. Experience needed in Glue Catalog, AWS Glue, AWS DMS, Apache Airflow, AWS IAM, AWS RDS</li>
 <li>3+ years of experience building/implementing data pipelines using Databricks such as Unity Catalog, Databricks workflow, Databricks Live Table etc.</li>
 <li>3+ years of Hands-on object-oriented programming experience using Python (specially Py Spark) to write complex, highly optimized queries across large volumes of data.</li>
 <li>Knowledge or experience in Big data technologies such as Hadoop/Spark</li>
 <li>Knowledge or experience in Data modeling and ETL processing</li>
 <li>Knowledge or experience in architectural best practices in building data lakes</li>
 <li>Expert level knowledge of using SQL to write complex, highly optimized queries across large volumes of data.</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: From &#x24;60.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>PySpark: 3 years (Required)</li>
 <li>implementing data pipelines in a cloud environment: 8 years (Required)</li>
 <li>migrating/developing data solutions in the AWS cloud: 5 years (Required)</li>
 <li>building/implementing data pipelines using Databricks: 3 years (Required)</li>
 <li>object-oriented programming experience using Python: 3 years (Required)</li>
 <li>Big data technologies such as Hadoop/Spark: 2 years (Required)</li>
 <li>Data modeling and ETL processing: 2 years (Required)</li>
 <li>architectural best practices in building data lakes: 2 years (Required)</li>
 <li>SQL queries: 2 years (Required)</li>
 <li>Apache NiFi /Informatica BDM/Talend: 8 years (Required)</li>
 <li>AWS Glue, AWS DMS, Apache Airflow, AWS IAM, AWS RDS: 5 years (Required)</li>
 <li>Unity Catalog, Databricks workflow, Databricks Live Table: 3 years (Required)</li>
</ul>
<p>Work Location: In person</p>",,f120f0ae5b708e07,,Contract,,,"Columbus, OH",Sr. Data Engineer / W2,Today,2023-10-20T12:33:46.597Z,,,From $60 an hour,2023-10-20T12:33:46.598Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=f120f0ae5b708e07&from=jasx&tk=1hd6h75sii6mo800&vjs=3
224,Excelgens,"Job description
Key Responsibilities:

 Utilize Python 3 and SQL to perform dataset analysis and transformations.
 Work with AWS services for data storage, processing, and deployment.
 Version control and collaborate using GIT for effective code management.
 Implement ETL (Extract, Transform, Load) processes to prepare data for analysis.
 Develop and maintain a Data Lake architecture for efficient data storage and retrieval.
 Leverage PySpark to process large datasets and perform distributed computing tasks.

Qualifications:

 Strong proficiency in Python 3 and SQL for data analysis and manipulation.
 Experience working with AWS services, including S3, Redshift, and Glue.
 Proficiency with GIT for version control and collaborative coding.
 Hands-on experience with PySpark for data processing and analytics.
 Proven expertise in building ETL pipelines for data integration and transformation.
 Familiarity with data lake architecture and best practices.
 Excellent problem-solving skills and ability to work with complex datasets.

Assessment:
Candidates will be required to complete an assessment prior to the interview. The assessment will include questions on Python 3, SQL, dataset analysis, and GIT.
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Experience level:

 7 years

Experience:

 Python: 6 years (Required)
 Pyspark: 6 years (Required)
 AWS: 6 years (Required)
 Dataset Analysis: 5 years (Preferred)
 Data lake: 5 years (Preferred)

Work Location: Remote","<p><b>Job description</b></p>
<p><b>Key Responsibilities:</b></p>
<ul>
 <li>Utilize Python 3 and SQL to perform dataset analysis and transformations.</li>
 <li>Work with AWS services for data storage, processing, and deployment.</li>
 <li>Version control and collaborate using GIT for effective code management.</li>
 <li>Implement ETL (Extract, Transform, Load) processes to prepare data for analysis.</li>
 <li>Develop and maintain a Data Lake architecture for efficient data storage and retrieval.</li>
 <li>Leverage PySpark to process large datasets and perform distributed computing tasks.</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Strong proficiency in Python 3 and SQL for data analysis and manipulation.</li>
 <li>Experience working with AWS services, including S3, Redshift, and Glue.</li>
 <li>Proficiency with GIT for version control and collaborative coding.</li>
 <li>Hands-on experience with PySpark for data processing and analytics.</li>
 <li>Proven expertise in building ETL pipelines for data integration and transformation.</li>
 <li>Familiarity with data lake architecture and best practices.</li>
 <li>Excellent problem-solving skills and ability to work with complex datasets.</li>
</ul>
<p><b>Assessment:</b></p>
<p>Candidates will be required to complete an assessment prior to the interview. The assessment will include questions on Python 3, SQL, dataset analysis, and GIT.</p>
<p>Job Type: Contract</p>
<p>Salary: &#x24;60.00 - &#x24;65.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Python: 6 years (Required)</li>
 <li>Pyspark: 6 years (Required)</li>
 <li>AWS: 6 years (Required)</li>
 <li>Dataset Analysis: 5 years (Preferred)</li>
 <li>Data lake: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,11466c7b4a6cfcce,,Contract,,,Remote,Senior Data Engineer,Today,2023-10-20T12:33:52.277Z,,,$60 - $65 an hour,2023-10-20T12:33:52.279Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=11466c7b4a6cfcce&from=jasx&tk=1hd6h75sii6mo800&vjs=3
225,GTECH LLC,"Responsibilities

 Understand business requirements.
 Understand source systems, source data and source data formats that are available on-prem / cloud.
 Design and build data ingestion pipeline.
 Design and build complex data processing pipelines.
 Work with relevant stakeholders to assist with data-related technical issues and support their data needs.
 Build programs for data quality checks.
 Provide operational support.
 Work with data architecture, data governance and data analytics. teams to ensure pipelines adhere to enterprise standards, usability, and performance.
 Involve in System Testing, UAT, code deployment activities.
 Coordinate with offshore team on regular basis

Primary Skills
Azure Databricks
PySpark,
Scala
Snowflake (at least for 1 resource)
Secondary Skills
ADF
CICD
Airflow
SQL
Cloud Databases
Understanding of Agile methodologies
Experience Level
4-6 years of working experience in primary skills
Overall 8-10 years in ETL/Data Engineering
Job Type: Full-time
Salary: $83,492.68 - $130,739.06 per year
Benefits:

 401(k)
 Dental insurance
 Flexible schedule
 Health insurance
 Paid time off
 Tuition reimbursement
 Vision insurance

Experience level:

 10 years
 11+ years
 4 years
 5 years
 6 years
 7 years
 8 years
 9 years

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p>Responsibilities</p>
<ul>
 <li>Understand business requirements.</li>
 <li>Understand source systems, source data and source data formats that are available on-prem / cloud.</li>
 <li>Design and build data ingestion pipeline.</li>
 <li>Design and build complex data processing pipelines.</li>
 <li>Work with relevant stakeholders to assist with data-related technical issues and support their data needs.</li>
 <li>Build programs for data quality checks.</li>
 <li>Provide operational support.</li>
 <li>Work with data architecture, data governance and data analytics. teams to ensure pipelines adhere to enterprise standards, usability, and performance.</li>
 <li>Involve in System Testing, UAT, code deployment activities.</li>
 <li>Coordinate with offshore team on regular basis</li>
</ul>
<p>Primary Skills</p>
<p>Azure Databricks</p>
<p>PySpark,</p>
<p>Scala</p>
<p>Snowflake (at least for 1 resource)</p>
<p>Secondary Skills</p>
<p>ADF</p>
<p>CICD</p>
<p>Airflow</p>
<p>SQL</p>
<p>Cloud Databases</p>
<p>Understanding of Agile methodologies</p>
<p>Experience Level</p>
<p>4-6 years of working experience in primary skills</p>
<p>Overall 8-10 years in ETL/Data Engineering</p>
<p>Job Type: Full-time</p>
<p>Salary: &#x24;83,492.68 - &#x24;130,739.06 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Tuition reimbursement</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
 <li>4 years</li>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,d4db43c4028922ab,,Full-time,,,Remote,Azure Data Engineer,Today,2023-10-20T12:33:56.963Z,,,"$83,493 - $130,739 a year",2023-10-20T12:33:56.966Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d4db43c4028922ab&from=jasx&tk=1hd6h75sii6mo800&vjs=3
227,"Crossover Health Management Services, Inc.","About Crossover Health
 
  Crossover Health is creating the future of health as it should be. A national, team-based medical group with a focus on wellbeing and prevention that extends beyond traditional sick care, the company delivers an entirely new model of healthcare—Primary Health—built on the foundation of trusted relationships, an interdisciplinary care team approach, and outcomes-based payment. Crossover’s Primary Health model integrates primary care, physical medicine, mental health, health coaching, care navigation and more, and delivers care in surround-sound—in-person, virtually and via asynchronous messaging. Together we are building a community of members that embraces healthcare as a proactive part of their lifestyle.
 
  Job Summary
  Crossover’s Data Engineer is responsible for managing and developing data sources for analytics at scale. This is a critical role for supporting Crossover’s growing analytics team, and serves as the connection between Crossover’s Product and Technology teams, Data Science team, and data infrastructure vendors. In this role, the successful candidate will build out new data sources within the enterprise data warehouse, guide data modeling efforts for new and existing projects, and manage data ingress and egress between Crossover teams, clients, partners, and vendors. The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data.
 
  Job Responsibilities
 
   Develop and maintain data sources within Crossover’s enterprise data warehouse (inclusive of our current vendor and/or future data infrastructure)
   Assist with recommendations for data architecture, data storage, data integration, data quality, and data models
   Contribute to design sessions based on technical requirements, and build data models to clean and transform datasets for use by Crossover’s Data Science and Analytics teams
   Assist with ETL, ELT, and reverse-ETL design and development initiatives including data analysis, source-target mapping, data profiling, change data capture, QA testing, and performance tuning to guarantee quality and repeatability of data model results
   Create and maintain data model standards, including MDM (Master Data Management) and codebase standardization
   Migrate Enterprise Workloads to Snowflake using industry standard methodologies
   Automate and deploy as well as build CI/CD pipelines to support cloud based workload
   Design, deliver cloud native, hybrid, and multi-cloud Workloads
   Invest in documentation, including all system design, architecture and ongoing changes
   Design and support production job schedules, including alerting, monitoring, break fixes, and performance tuning
   Build solutions that are automated, scalable, and sustainable while minimizing defects and technical debt
   Assist stakeholders including analytics, design, product, and executive teams with data-related technical issues
   Ability to work independently with little instruction or direct oversight
   Perform other duties as assigned
 
 
  Minimum Qualifications
 
   Bachelors in Computer Science or Data Engineering, related degree, or equivalent professional experience
   3+ years relevant work experience within a complex, dynamic environment, with preference for experience with clinical healthcare data
   3+ years architecting , implementing, and supporting data infrastructure and topologies
 
 
   Experience building and operating highly available, distributed systems of extraction, ingestion, and processing of large data sets across a variety of applications (OLTP, OLAP and DSS)
   3+ years Experience with Data warehousing, methodologies, modeling techniques, design patterns, and technologies.
   Experience with data migration tools and deploying cloudbase solutions
   Experience in writing advanced SQL (DML & DDL), including Stored Procedures, Indexes, user defined functions, windows functions, correlated subqueries and CTE's, and related data query and management technology
   Coding ability in R, Python, and Shell Scripting to build and deploy Pipelines
   Working knowledge of Git, or similar collaborative code management software
   Experience with data integration tools such as FiveTran, DBT, Informatica, Matillion, or similar ETL/ELT tools
   Experience with Snowflake’s data platform
 
 
  Preferred Qualifications
 
   Masters in Computer Science, Data Engineering, or related degree
   Healthcare data acquisition, ingestion, processing, and analytics knowledge highly preferred
   Previous experience with health informatics, taxonomies, terminologies, and code sets
   Knowledge and understanding of product features: IAAS, PAAS and SAAS solutions
   Experience with healthcare claims data, formats, and analytics
   Experience with Health Catalyst’s data and analytics platform
   Experience with Tableau Cloud administration
   Experience with Master Data Management
   Understand Cloud Ecosystem
 
  The base pay range for this position is $91,428.00 to $105,143.00 per year. Pay range may vary depending on work location, applicable knowledge, skills, and experience. This position will be eligible for an annual bonus opportunity and comprehensive benefits package that includes Medical Insurance, Dental Insurance, Vision Insurance, Short- and Long-Term Disability, Life Insurance, Paid Time Off and 401K.
 
  Crossover Health is committed to Equal Employment Opportunity regardless of race, color, national origin, gender, sexual orientation, age, religion, veteran status, disability, history of disability or perceived disability. If you need assistance or an accommodation due to a disability, you may email us at careers@crossoverhealth.com.
 
  To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.
  #LI-Remote","<div>
 <p><b>About Crossover Health</b></p>
 <p></p>
 <p> Crossover Health is creating the future of health as it should be. A national, team-based medical group with a focus on wellbeing and prevention that extends beyond traditional sick care, the company delivers an entirely new model of healthcare&#x2014;Primary Health&#x2014;built on the foundation of trusted relationships, an interdisciplinary care team approach, and outcomes-based payment. Crossover&#x2019;s Primary Health model integrates primary care, physical medicine, mental health, health coaching, care navigation and more, and delivers care in surround-sound&#x2014;in-person, virtually and via asynchronous messaging. Together we are building a community of members that embraces healthcare as a proactive part of their lifestyle.</p>
 <p></p>
 <p><b> Job Summary</b></p>
 <br> Crossover&#x2019;s Data Engineer is responsible for managing and developing data sources for analytics at scale. This is a critical role for supporting Crossover&#x2019;s growing analytics team, and serves as the connection between Crossover&#x2019;s Product and Technology teams, Data Science team, and data infrastructure vendors. In this role, the successful candidate will build out new data sources within the enterprise data warehouse, guide data modeling efforts for new and existing projects, and manage data ingress and egress between Crossover teams, clients, partners, and vendors. The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data.
 <p></p>
 <p><b> Job Responsibilities</b></p>
 <ul>
  <li><p> Develop and maintain data sources within Crossover&#x2019;s enterprise data warehouse (inclusive of our current vendor and/or future data infrastructure)</p></li>
  <li><p> Assist with recommendations for data architecture, data storage, data integration, data quality, and data models</p></li>
  <li><p> Contribute to design sessions based on technical requirements, and build data models to clean and transform datasets for use by Crossover&#x2019;s Data Science and Analytics teams</p></li>
  <li><p> Assist with ETL, ELT, and reverse-ETL design and development initiatives including data analysis, source-target mapping, data profiling, change data capture, QA testing, and performance tuning to guarantee quality and repeatability of data model results</p></li>
  <li><p> Create and maintain data model standards, including MDM (Master Data Management) and codebase standardization</p></li>
  <li><p> Migrate Enterprise Workloads to Snowflake using industry standard methodologies</p></li>
  <li><p> Automate and deploy as well as build CI/CD pipelines to support cloud based workload</p></li>
  <li><p> Design, deliver cloud native, hybrid, and multi-cloud Workloads</p></li>
  <li><p> Invest in documentation, including all system design, architecture and ongoing changes</p></li>
  <li><p> Design and support production job schedules, including alerting, monitoring, break fixes, and performance tuning</p></li>
  <li><p> Build solutions that are automated, scalable, and sustainable while minimizing defects and technical debt</p></li>
  <li><p> Assist stakeholders including analytics, design, product, and executive teams with data-related technical issues</p></li>
  <li><p> Ability to work independently with little instruction or direct oversight</p></li>
  <li><p> Perform other duties as assigned</p></li>
 </ul>
 <p></p>
 <p><b> Minimum Qualifications</b></p>
 <ul>
  <li><p> Bachelors in Computer Science or Data Engineering, related degree, or equivalent professional experience</p></li>
  <li><p> 3+ years relevant work experience within a complex, dynamic environment, with preference for experience with clinical healthcare data</p></li>
  <li><p> 3+ years architecting , implementing, and supporting data infrastructure and topologies</p></li>
 </ul>
 <ul>
  <li><p> Experience building and operating highly available, distributed systems of extraction, ingestion, and processing of large data sets across a variety of applications (OLTP, OLAP and DSS)</p></li>
  <li><p> 3+ years Experience with Data warehousing, methodologies, modeling techniques, design patterns, and technologies.</p></li>
  <li><p> Experience with data migration tools and deploying cloudbase solutions</p></li>
  <li><p> Experience in writing advanced SQL (DML &amp; DDL), including Stored Procedures, Indexes, user defined functions, windows functions, correlated subqueries and CTE&apos;s, and related data query and management technology</p></li>
  <li><p> Coding ability in R, Python, and Shell Scripting to build and deploy Pipelines</p></li>
  <li><p> Working knowledge of Git, or similar collaborative code management software</p></li>
  <li><p> Experience with data integration tools such as FiveTran, DBT, Informatica, Matillion, or similar ETL/ELT tools</p></li>
  <li><p> Experience with Snowflake&#x2019;s data platform</p></li>
 </ul>
 <p></p>
 <p><b> Preferred Qualifications</b></p>
 <ul>
  <li><p> Masters in Computer Science, Data Engineering, or related degree</p></li>
  <li><p> Healthcare data acquisition, ingestion, processing, and analytics knowledge highly preferred</p></li>
  <li><p> Previous experience with health informatics, taxonomies, terminologies, and code sets</p></li>
  <li><p> Knowledge and understanding of product features: IAAS, PAAS and SAAS solutions</p></li>
  <li><p> Experience with healthcare claims data, formats, and analytics</p></li>
  <li><p> Experience with Health Catalyst&#x2019;s data and analytics platform</p></li>
  <li><p> Experience with Tableau Cloud administration</p></li>
  <li><p> Experience with Master Data Management</p></li>
  <li><p> Understand Cloud Ecosystem</p></li>
 </ul>
 <p></p> The base pay range for this position is &#x24;91,428.00 to &#x24;105,143.00 per year. Pay range may vary depending on work location, applicable knowledge, skills, and experience. This position will be eligible for an annual bonus opportunity and comprehensive benefits package that includes Medical Insurance, Dental Insurance, Vision Insurance, Short- and Long-Term Disability, Life Insurance, Paid Time Off and 401K.
 <p></p>
 <p> Crossover Health is committed to Equal Employment Opportunity regardless of race, color, national origin, gender, sexual orientation, age, religion, veteran status, disability, history of disability or perceived disability. If you need assistance or an accommodation due to a disability, you may email us at careers@crossoverhealth.com.</p>
 <p></p>
 <p><b> To all recruitment agencies</b>: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.</p>
 <p></p> #LI-Remote
</div>",https://crossoverhealth.wd1.myworkdayjobs.com/Careers/job/Remote-USA/Data-Engineer_R23_774,d7a6a168405310c5,,Full-time,,,"101 West Avenida Vista Hermosa, San Clemente, CA 92672",Data Engineer,Today,2023-10-20T12:34:04.760Z,3.4,47.0,"$91,428 - $105,143 a year",2023-10-20T12:34:04.762Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d7a6a168405310c5&from=jasx&tk=1hd6h75sii6mo800&vjs=3
228,Twin Health,"Twin Health 
   At Twin Health, we empower people to reverse, prevent and improve chronic metabolic diseases. Twin Health invented The Whole Body Digital Twin™ , a dynamic representation of each individual's unique metabolism, built from thousands of data points collected daily via non-invasive sensors and self-reported preferences. The Whole Body Digital Twin delivers a new standard of care, empowering physicians and patients to make personalized data-driven decisions. 
   Working here 
   Our team is passionate, talented, and driven by our purpose to improve the health and happiness of our members. Our culture empowers each Twin to do what's needed to create value for our customers and our company, and enjoy their experience at work. Twin Health was awarded Innovator of the Year by Employer Health Innovation Roundtable (EHIR) (out of 358 companies), named to the 2021 CB Insights Digital Health 150, and recognized by Built In's 2022 Best Places To Work Awards. In October 2021, Twin Health announced its Series C funding round of $155M, led by ICONIQ Growth, enabling us to scale services in the U.S. and globally, helping to solve the global chronic metabolic disease health crisis. We have recently announced broad and growing partnerships with premier employers, such as Blackstone and Berkshire Hathaway. We are building the company you always wished you worked for. Join us in revolutionizing healthcare and building the most impactful digital health company in the world! 
   Excited to join us and do your part in improving people's health and happiness?
 
  Opportunity 
  Twin Heath is expanding rapidly across health providers nationwide in the US as well as India. We are seeking an experienced Data Warehouse Engineer to join our growing team to own the entire process of building a data warehouse from scratch and managing ETL pipelines, working collaboratively with internal business stakeholders and customers. We are looking for candidates physically located in PST. 
  Responsibilities 
  
  Drive analysis, architecture, design, and development of Cloud based data warehouse and business intelligence solutions. 
   Design and develop data flows and models for data warehousing and self-serve reporting. 
   Design and build extensible data acquisition and integration solutions to meet business reporting and analytics needs. 
   Implement a comprehensive framework to support logging, profile and audit high volume high frequency file processing supporting multiple formats. 
   Automate and optimize existing data processing workloads to integrate with the enterprise data warehouse. 
   Understand and translate business needs into data models to support long-term, scalable, and reliable solutions. 
   Create data models for self-serve reporting. 
   Build data pipelines from systems such as CRM, ERP and internal applications with the emphasis on scalability and reliability. 
   Partner with business users, senior architects, and infrastructure engineers to form complete end-to end-solutions. 
   Drive data quality across the organization; develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking. 
   Prepare to respond to ad hoc reporting needs of business functions. 
   Work in a fast-paced environment and perform effectively in a sprint based agile development environment. 
   Collaborate with developers and analysts on technical and functional designs. 
   Analyze query performance and perform query tuning to assist development engineers in designing and optimizing queries. 
  Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time. 
  This remote opportunity is available to US based persons 
  
 Qualifications 
  
  Bachelor's degree in computers Ince or related field. 
   At least 8 year of overall experience building data warehouse solutions with solid data modeling skills. 
   At least 3 years of hands-on experience in building Data Warehouses in Snowflake or Redshift 
   At least 4 years of experience in developing data pipelines using Python, PySpark or Scala. 
   Experience with API design and development using RESTful and/or SOAP protocols. 
   At least 2 years of experience creating DBT models and in a cloud, data Warehouses platform. 
   Knowledge of advanced SQL scripting and ability to write complex queries. 
   Experience using Rivery or new age ELT tools like Fivetran, Matillion etc. 
   Understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security 
   Experience working with various file formats at scale. 
   Experience with AWS. Familiarity with AWS Athena, Glue, Data Pipeline is a plus. 
   At least 2 years of experience with Salesforce Health Cloud, Marketing Cloud and NetSuite Schemas. 
   Experience designing highly scalable ETL/ELT processes with complex data transformations, data formats including error handling and monitoring. 
   Excellent analytical, problem solving, and troubleshooting skills, and solid communication skills. 
  
 Compensation and Benefits 
  The compensation range for this position is $140,000 annually. 
  In addition, Twin has an ambitious vision to empower people to live healthier and happier lives, and to achieve this purpose, we need the very best people to enhance our cutting-edge technology and medical science, deliver the best possible care, and turn our passion into value for our members, partners and investors. We are committed to delivering an outstanding culture and experience for every Twin employee through a company based on the values of passion, talent, and trust. We offer comprehensive benefits and perks in line with these principles, as well as a high level of flexibility for every Twin 
  
  A competitive compensation package in line with leading technology companies 
  As a remote friendly company we are committed to providing opportunities for all who join to further build relationships, increase cross-functional collaboration, and celebrate our accomplishments. 
  Opportunity for equity participation 
  Unlimited vacation with manager approval 
  16 weeks of 100% paid parental leave for delivering parents; 8 weeks of 100% paid parental leave for non-delivering parents 
  100% Employer sponsored healthcare, dental, and vision for you, and 80% coverage for your family; Health Savings Account and Flexible Spending Account options 
  401k retirement savings plan 
 
 
  
   Salary range for US jobs
  
   US Salary Range
  
    $140,000—$140,000 USD","<div>
 <div>
  <p><b>Twin Health</b></p> 
  <p> At Twin Health, we empower people to reverse, prevent and improve chronic metabolic diseases. Twin Health invented The Whole Body Digital Twin&#x2122; , a dynamic representation of each individual&apos;s unique metabolism, built from thousands of data points collected daily via non-invasive sensors and self-reported preferences. The Whole Body Digital Twin delivers a new standard of care, empowering physicians and patients to make personalized data-driven decisions.</p> 
  <p><b> Working here</b></p> 
  <p> Our team is passionate, talented, and driven by our purpose to improve the health and happiness of our members. Our culture empowers each Twin to do what&apos;s needed to create value for our customers and our company, and enjoy their experience at work. Twin Health was awarded Innovator of the Year by Employer Health Innovation Roundtable (EHIR) (out of 358 companies), named to the 2021 CB Insights Digital Health 150, and recognized by Built In&apos;s 2022 Best Places To Work Awards. In October 2021, Twin Health announced its Series C funding round of &#x24;155M, led by ICONIQ Growth, enabling us to scale services in the U.S. and globally, helping to solve the global chronic metabolic disease health crisis. We have recently announced broad and growing partnerships with premier employers, such as Blackstone and Berkshire Hathaway. We are building the company you always wished you worked for. Join us in revolutionizing healthcare and building the most impactful digital health company in the world!</p> 
  <p><b> Excited to join us and do your part in improving people&apos;s health and happiness?</b></p>
 </div>
 <p><b> Opportunity</b></p> 
 <p> Twin Heath is expanding rapidly across health providers nationwide in the US as well as India. We are seeking an experienced Data Warehouse Engineer to join our growing team to own the entire process of building a data warehouse from scratch and managing ETL pipelines, working collaboratively with internal business stakeholders and customers. We are looking for candidates physically located in PST.</p> 
 <p><b> Responsibilities</b></p> 
 <ul> 
  <li>Drive analysis, architecture, design, and development of Cloud based data warehouse and business intelligence solutions.</li> 
  <li> Design and develop data flows and models for data warehousing and self-serve reporting.</li> 
  <li> Design and build extensible data acquisition and integration solutions to meet business reporting and analytics needs.</li> 
  <li> Implement a comprehensive framework to support logging, profile and audit high volume high frequency file processing supporting multiple formats.</li> 
  <li> Automate and optimize existing data processing workloads to integrate with the enterprise data warehouse.</li> 
  <li> Understand and translate business needs into data models to support long-term, scalable, and reliable solutions.</li> 
  <li> Create data models for self-serve reporting.</li> 
  <li> Build data pipelines from systems such as CRM, ERP and internal applications with the emphasis on scalability and reliability.</li> 
  <li> Partner with business users, senior architects, and infrastructure engineers to form complete end-to end-solutions.</li> 
  <li> Drive data quality across the organization; develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking.</li> 
  <li> Prepare to respond to ad hoc reporting needs of business functions.</li> 
  <li> Work in a fast-paced environment and perform effectively in a sprint based agile development environment.</li> 
  <li> Collaborate with developers and analysts on technical and functional designs.</li> 
  <li> Analyze query performance and perform query tuning to assist development engineers in designing and optimizing queries.</li> 
  <li>Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.</li> 
  <li>This remote opportunity is available to US based persons</li> 
 </ul> 
 <p><b>Qualifications</b></p> 
 <ul> 
  <li>Bachelor&apos;s degree in computers Ince or related field.</li> 
  <li> At least 8 year of overall experience building data warehouse solutions with solid data modeling skills.</li> 
  <li> At least 3 years of hands-on experience in building Data Warehouses in Snowflake or Redshift</li> 
  <li> At least 4 years of experience in developing data pipelines using Python, PySpark or Scala.</li> 
  <li> Experience with API design and development using RESTful and/or SOAP protocols.</li> 
  <li> At least 2 years of experience creating DBT models and in a cloud, data Warehouses platform.</li> 
  <li> Knowledge of advanced SQL scripting and ability to write complex queries.</li> 
  <li> Experience using Rivery or new age ELT tools like Fivetran, Matillion etc.</li> 
  <li> Understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security</li> 
  <li> Experience working with various file formats at scale.</li> 
  <li> Experience with AWS. Familiarity with AWS Athena, Glue, Data Pipeline is a plus.</li> 
  <li> At least 2 years of experience with Salesforce Health Cloud, Marketing Cloud and NetSuite Schemas.</li> 
  <li> Experience designing highly scalable ETL/ELT processes with complex data transformations, data formats including error handling and monitoring.</li> 
  <li> Excellent analytical, problem solving, and troubleshooting skills, and solid communication skills.</li> 
 </ul> 
 <p><b>Compensation and Benefits</b></p> 
 <p> The compensation range for this position is &#x24;140,000 annually.</p> 
 <p> In addition, Twin has an ambitious vision to empower people to live healthier and happier lives, and to achieve this purpose, we need the very best people to enhance our cutting-edge technology and medical science, deliver the best possible care, and turn our passion into value for our members, partners and investors. We are committed to delivering an outstanding culture and experience for every Twin employee through a company based on the values of passion, talent, and trust. We offer comprehensive benefits and perks in line with these principles, as well as a high level of flexibility for every Twin</p> 
 <ul> 
  <li>A competitive compensation package in line with leading technology companies</li> 
  <li>As a remote friendly company we are committed to providing opportunities for all who join to further build relationships, increase cross-functional collaboration, and celebrate our accomplishments.</li> 
  <li>Opportunity for equity participation</li> 
  <li>Unlimited vacation with manager approval</li> 
  <li>16 weeks of 100% paid parental leave for delivering parents; 8 weeks of 100% paid parental leave for non-delivering parents</li> 
  <li>100% Employer sponsored healthcare, dental, and vision for you, and 80% coverage for your family; Health Savings Account and Flexible Spending Account options</li> 
  <li>401k retirement savings plan</li> 
 </ul>
 <div>
  <div>
   Salary range for US jobs
  </div>
  <p><b> US Salary Range</b></p>
  <div>
    &#x24;140,000&#x2014;&#x24;140,000 USD
  </div>
 </div>
</div>",https://boards.greenhouse.io/twinhealth/jobs/5002329004?gh_src=3d8c6d624us,3b61441b9b1ff9fa,,,,,Remote,Senior Data Warehouse Engineer,Today,2023-10-20T12:34:03.090Z,3.6,9.0,"$140,000 a year",2023-10-20T12:34:03.092Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=3b61441b9b1ff9fa&from=jasx&tk=1hd6h75sii6mo800&vjs=3
230,MetroSys,"Responsibilities:
 
   Collaborate with stakeholders to understand data migration requirements, including scope, timelines, and specific data sets.
   Design and develop a comprehensive data migration plan, considering factors such as data volume, complexity, and business continuity.
   Utilize Komprise data management software to perform assessments, identify target data, and orchestrate the migration process.
   Configure and optimize Komprise settings to align with migration goals, ensuring efficient and secure data transfer.
   Conduct pre-migration validation tests to ensure data integrity and accuracy before the actual migration process.
   Monitor the data migration process in real-time, addressing any issues or discrepancies as they arise.
   Implement data validation and reconciliation procedures to verify the successful completion of the migration.
   Collaborate with cross-functional teams, including storage administrators and system engineers, to ensure seamless integration with the NetApp environment.
   Document the entire migration process, including configurations, settings, and any custom scripts or workflows used.
   Provide knowledge transfer and training to internal teams for ongoing management and maintenance of the migrated data.
 
  Requirements:
 
   Bachelor's degree in Information Technology, Computer Science, or a related field (preferred) or equivalent work experience.
   Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise.
   Strong proficiency in Komprise data management software and related tools.
   In-depth knowledge of Isilon and NetApp storage platforms, including file systems, protocols, and administration.
   Experience with scripting languages (e.g., Python, PowerShell) for automation and customization of migration processes.
   Excellent problem-solving and analytical skills, with the ability to diagnose and resolve complex data migration issues.
   Strong communication and interpersonal skills, with the ability to collaborate effectively with technical and non-technical stakeholders.
 
  
 T7ZuUtppMf","<div>
 <p>Responsibilities:</p>
 <ul>
  <li> Collaborate with stakeholders to understand data migration requirements, including scope, timelines, and specific data sets.</li>
  <li> Design and develop a comprehensive data migration plan, considering factors such as data volume, complexity, and business continuity.</li>
  <li> Utilize Komprise data management software to perform assessments, identify target data, and orchestrate the migration process.</li>
  <li> Configure and optimize Komprise settings to align with migration goals, ensuring efficient and secure data transfer.</li>
  <li> Conduct pre-migration validation tests to ensure data integrity and accuracy before the actual migration process.</li>
  <li> Monitor the data migration process in real-time, addressing any issues or discrepancies as they arise.</li>
  <li> Implement data validation and reconciliation procedures to verify the successful completion of the migration.</li>
  <li> Collaborate with cross-functional teams, including storage administrators and system engineers, to ensure seamless integration with the NetApp environment.</li>
  <li> Document the entire migration process, including configurations, settings, and any custom scripts or workflows used.</li>
  <li> Provide knowledge transfer and training to internal teams for ongoing management and maintenance of the migrated data.</li>
 </ul>
 <p> Requirements:</p>
 <ul>
  <li> Bachelor&apos;s degree in Information Technology, Computer Science, or a related field (preferred) or equivalent work experience.</li>
  <li> Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise.</li>
  <li> Strong proficiency in Komprise data management software and related tools.</li>
  <li> In-depth knowledge of Isilon and NetApp storage platforms, including file systems, protocols, and administration.</li>
  <li> Experience with scripting languages (e.g., Python, PowerShell) for automation and customization of migration processes.</li>
  <li> Excellent problem-solving and analytical skills, with the ability to diagnose and resolve complex data migration issues.</li>
  <li> Strong communication and interpersonal skills, with the ability to collaborate effectively with technical and non-technical stakeholders.</li>
 </ul>
 <p> </p>
 <p>T7ZuUtppMf</p>
</div>",https://metrosys.applytojob.com/apply/T7ZuUtppMf/Data-Migration-Engineer?source=INDE,7f7a185825ef7e9f,,,,,Remote,Data Migration Engineer,Today,2023-10-20T12:34:16.480Z,,,$60 - $80 an hour,2023-10-20T12:34:16.481Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=7f7a185825ef7e9f&from=jasx&tk=1hd6h82eqk7eu800&vjs=3
232,Dexian DISYS,"Job Title: Azure Data Engineer
Location: 100% REMOTE
Duration: 6 months (CONTRACT TO HIRE)
Skills & Experience Needed:

 Strong background in Databricks and Snowflake
 5-10 years of experience.
 Moving enterprise data.
 Deploying a Data Engineering product to Analytics.
 5+ years progressive SQL Server engineering and development experience
 ETL processes and Azure

Dexian is an Equal Opportunity Employer that recruits and hires qualified candidates without regard to race, religion, sex, sexual orientation, gender identity, age, national origin, ancestry, citizenship, disability, or veteran status.
Job Types: Contract, Full-time
Pay: $70.00 - $75.00 per hour
Schedule:

 8 hour shift

Experience:

 Databricks: 2 years (Required)
 Snowflake: 1 year (Required)
 Azure: 2 years (Required)
 Data management: 5 years (Preferred)

Work Location: Remote","<p><b>Job Title:</b> <b>Azure Data Engineer</b></p>
<p><b>Location:</b> <b>100% REMOTE</b></p>
<p><b>Duration:</b> 6 months <b>(CONTRACT TO HIRE)</b></p>
<p><b>Skills &amp; Experience Needed:</b></p>
<ul>
 <li>Strong background in Databricks and Snowflake</li>
 <li>5-10 years of experience.</li>
 <li>Moving enterprise data.</li>
 <li>Deploying a Data Engineering product to Analytics.</li>
 <li>5+ years progressive SQL Server engineering and development experience</li>
 <li>ETL processes and Azure</li>
</ul>
<p>Dexian is an Equal Opportunity Employer that recruits and hires qualified candidates without regard to race, religion, sex, sexual orientation, gender identity, age, national origin, ancestry, citizenship, disability, or veteran status.</p>
<p>Job Types: Contract, Full-time</p>
<p>Pay: &#x24;70.00 - &#x24;75.00 per hour</p>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Databricks: 2 years (Required)</li>
 <li>Snowflake: 1 year (Required)</li>
 <li>Azure: 2 years (Required)</li>
 <li>Data management: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,adae4efd7c29f52f,,Full-time,Contract,,Remote,100% REMOTE - Azure Data Engineer,2 days ago,2023-10-18T12:34:35.403Z,3.4,382.0,$70 - $75 an hour,2023-10-20T12:34:35.405Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BTYkY06FZEdAAtNWO-eDAfNklmfZymsMF6eFRONl7rAG-Z7iiNntZQmnz-piRwTeT1fa4JUmDVhBEhqSdYV3nk8vKs5zI85XL4vCJJzDBktKUc4VUEWDZGHBIHYy0Jm9yO_KXvgn9s9bOZt-E20Ubyk8NyiWr7ac5GrFFkor8CSwVYPrpJAYbyR4Gej9iTdD-if_AcTe74ykq2OUfGR7_KG8xrQO0p3hb9AwvV8mWANgODTVQ85wmsiRqJXKlxiO7Pa1VGstUFZC33AanRxt8mS-p3Pfv3ahXgUMkBcd3tShlhnEtHuhDM-QHyxSIEhDIEcBlBTo8Lv0ayPDrEoqaSx54I9tbNEknFZlX09vLzLfZrQpidr83m5ZDpgOhe4cT_QTwEOSaC6e_D54Y66f14zREOx6gXI0j-j5_vQQ8BfKeDskeQ91R2A9TrFchdzVYAqHrir4nAtSu-0j8kW-DTt91ZPqCR5L0l7ig7mFiwB7Ct_5em5vhUteppG-ALP_nIeTvUnbqlOmZnBO_3snvSROv9iRzqHa3kETlmo77ofZxz_zyAnpQ1q4vYYMLLZbMMXSuElQgonqC0nIfQSMDyI69wBO_8uvPI4hv13Zovt4dueNOUl5G-ABwjgfiRlR4%3D&xkcb=SoCm-_M3JmqsksWR7p0FbzkdCdPP&p=14&fvj=1&vjs=3&jsa=5110&tk=1hd6h8q6h2beg000&from=jasx&wvign=1
241,Resource Consulting Services,"Opening for Data EngineerLocation: RemoteDuration: 12+MonthNeed Minimum 10 Years experienceMust have Linked idMust have Healthcare experience in recent projectSkills:String data engineering experienceMust have ETL- Matillion tool experienceMust have Snowflake experienceMust have Strong SQL ExperienceBODS is good to have
Ofc :: +1 (201)-809-9207 Ext-424Email: sajjad@rconsultinginc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
People with a criminal record are encouraged to apply
Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p>Opening for Data Engineer<br>Location: Remote<br>Duration: 12+Month<br>Need Minimum 10 Years experience<br>Must have Linked id<br>Must have Healthcare experience in recent project<br>Skills:<br>String data engineering experience<br>Must have ETL- Matillion tool experience<br>Must have Snowflake experience<br>Must have Strong SQL Experience<br>BODS is good to have</p>
<p>Ofc :: +1 (201)-809-9207 Ext-424<br>Email: sajjad@rconsultinginc.com</p>
<p>Job Type: Contract</p>
<p>Salary: &#x24;65.00 - &#x24;70.00 per hour</p>
<p>People with a criminal record are encouraged to apply</p>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,efbd2611031a4f0d,,Contract,,,Remote,Data Engineer,7 days ago,2023-10-13T12:35:03.845Z,,,$65 - $70 an hour,2023-10-20T12:35:03.854Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=efbd2611031a4f0d&from=jasx&tk=1hd6h9l2djtch81e&vjs=3
249,InvoiceCloud,"About InvoiceCloud: 
   InvoiceCloud, an EngageSmart solution, is a leading provider of online bill payment services. Founded in 2009, the company has grown to be one of the leading disruptors in the cloud-based electronic bill presentment and payment (EBPP) space, helping institutions put customer experience first. By switching to InvoiceCloud, clients can improve customer engagement, loyalty, and efficiency while reducing churn and missed payments in the process. With over 50 million payments processed annually, InvoiceCloud is one of the most secure, innovative, and inclusive fintech solutions in the market. To learn more, visit www.InvoiceCloud.com.
 
  As a Data Conversion Engineer on our integrations team, you will work to convert data for InvoiceCloud clients as they transition or upgrade their core processing system, ensuring their customers' online bill paying experience remains consistent and uninterrupted. You will be responsible for analyzing the source data, designing the target data format, extracting existing data, and performing the transform & load. You will be encouraged to develop automated scripts and tools to facilitate repeatable conversions where possible. 
  As a Data Conversion Engineer You Will: 
  
  Understand how to write and troubleshoot complex SQL queries 
  Understand ETL (Extract, Transform, and Load) processes and tools 
  Create wikis for data conversion processes and tools 
  Able to handle a fast-paced environment 
  Able to multitask efficiently 
  
 What We Seek: 
  
  Bachelor's Degree preferred or equivalent combination of education and experience required 
  5 years of Experience using Microsoft Technologies including VB.NET, ASP.NET, C#, Visual Studio 
  Proficiency with SQL Server and Microsoft ETL tools 
  Machine learning and automation experience a plus 
  Self-led, capable of working with little direction 
  Skilled communicator with a collaborative spirit 
  
 
 Base Compensation Range: ($70,000.00 to $90,000.00) annually 
  Base salary is one component of total compensation. Employees may also be eligible for an annual bonus or commission and equity. Some roles may also be eligible for overtime pay. 
  The above represents the expected base compensation range for this job requisition. Ultimately, in determining your pay, we'll consider many factors including, but not limited to, skills, experience, qualifications, geographic location, and other job-related factors. 
 
 
   Benefits 
   We offer a competitive benefits program including: 
   
   Medical, dental, vision, life & disability insurance 
   401(k) plan with company match & employee stock purchase plan (ESPP) 
   Flexible Time Off (FTO), wellbeing days, paid holidays, and summer Fridays 
   Mental health resources 
   Paid parental leave & Backup Care 
   Tuition reimbursement 
   Employee Resource Groups (ERGs) 
   
  Invoice Cloud is an Equal Opportunity Employer. 
   Invoice Cloud provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. 
   This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. 
   If you have a disability under the Americans with Disabilities Act or similar law, or you require a religious accommodation, and you wish to discuss potential accommodations related to applying for employment at our company, please contact jobs@engagesmart.com. 
   Click here to review EngageSmart's Job Applicant Privacy Policy. 
   To all recruitment agencies: Invoice Cloud does not accept agency resumes. Please do not forward resumes to our job's alias, employees, or any other organization location. Invoice Cloud is not responsible for any fees related to unsolicited resumes.","<div>
 <div>
  <p><b>About InvoiceCloud</b>:</p> 
  <p> InvoiceCloud, an EngageSmart solution, is a leading provider of online bill payment services. Founded in 2009, the company has grown to be one of the leading disruptors in the cloud-based electronic bill presentment and payment (EBPP) space, helping institutions put customer experience first. By switching to InvoiceCloud, clients can improve customer engagement, loyalty, and efficiency while reducing churn and missed payments in the process. With over 50 million payments processed annually, InvoiceCloud is one of the most secure, innovative, and inclusive fintech solutions in the market. To learn more, visit www.InvoiceCloud.com.</p>
 </div>
 <p> As a Data Conversion Engineer on our integrations team, you will work to convert data for InvoiceCloud clients as they transition or upgrade their core processing system, ensuring their customers&apos; online bill paying experience remains consistent and uninterrupted. You will be responsible for analyzing the source data, designing the target data format, extracting existing data, and performing the transform &amp; load. You will be encouraged to develop automated scripts and tools to facilitate repeatable conversions where possible.</p> 
 <p><b> As a Data Conversion Engineer You Will:</b></p> 
 <ul> 
  <li>Understand how to write and troubleshoot complex SQL queries</li> 
  <li>Understand ETL (Extract, Transform, and Load) processes and tools</li> 
  <li>Create wikis for data conversion processes and tools</li> 
  <li>Able to handle a fast-paced environment</li> 
  <li>Able to multitask efficiently</li> 
 </ul> 
 <p><b>What We Seek:</b></p> 
 <ul> 
  <li>Bachelor&apos;s Degree preferred or equivalent combination of education and experience required</li> 
  <li>5 years of Experience using Microsoft Technologies including VB.NET, ASP.NET, C#, Visual Studio</li> 
  <li>Proficiency with SQL Server and Microsoft ETL tools</li> 
  <li>Machine learning and automation experience a plus</li> 
  <li>Self-led, capable of working with little direction</li> 
  <li>Skilled communicator with a collaborative spirit</li> 
 </ul> 
 <p></p>
 <p>Base Compensation Range: (&#x24;70,000.00 to &#x24;90,000.00) annually</p> 
 <p> Base salary is one component of total compensation. Employees may also be eligible for an annual bonus or commission and equity. Some roles may also be eligible for overtime pay.</p> 
 <p> The above represents the expected base compensation range for this job requisition. Ultimately, in determining your pay, we&apos;ll consider many factors including, but not limited to, skills, experience, qualifications, geographic location, and other job-related factors.</p> 
 <p></p>
 <div>
  <p><b> Benefits</b></p> 
  <p> We offer a competitive benefits program including:</p> 
  <ul> 
   <li>Medical, dental, vision, life &amp; disability insurance</li> 
   <li>401(k) plan with company match &amp; employee stock purchase plan (ESPP)</li> 
   <li>Flexible Time Off (FTO), wellbeing days, paid holidays, and summer Fridays</li> 
   <li>Mental health resources</li> 
   <li>Paid parental leave &amp; Backup Care</li> 
   <li>Tuition reimbursement</li> 
   <li>Employee Resource Groups (ERGs)</li> 
  </ul> 
  <p><b>Invoice Cloud is an Equal Opportunity Employer.</b></p> 
  <p> Invoice Cloud provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.</p> 
  <p> This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.</p> 
  <p> If you have a disability under the Americans with Disabilities Act or similar law, or you require a religious accommodation, and you wish to discuss potential accommodations related to applying for employment at our company, please contact jobs@engagesmart.com.</p> 
  <p> Click here to review EngageSmart&apos;s Job Applicant Privacy Policy.</p> 
  <p><b> To all recruitment agencies:</b> Invoice Cloud does not accept agency resumes. Please do not forward resumes to our job&apos;s alias, employees, or any other organization location. Invoice Cloud is not responsible for any fees related to unsolicited resumes.</p>
 </div>
</div>",https://engagesmart.com/careers-invoicecloud/?gh_jid=5778848003&gh_src=791f54f23us,0a1b5ba4666e2bed,,,,,Remote,Data Conversion Engineer,2 days ago,2023-10-18T12:35:40.440Z,3.5,10.0,"$70,000 - $90,000 a year",2023-10-20T12:35:40.442Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=0a1b5ba4666e2bed&from=jasx&tk=1hd6hajt4irpg807&vjs=3
257,Vaco,"Data Engineer - 6 Month Contract-to-Hire 
  
 Location: Fully Remote (Candidates within the United States preferred) 
  
 Duration: 6 Month Contract-to-Hire 
  
 About Us: Vaco is a global talent solutions company providing consulting, contingent staffing, direct hire, and project-based solutions in the areas of accounting, finance, technology, and administration. We work with top-tier clients across various industries, including healthcare, finance, technology, and more. 
  
 Position Overview: We are seeking a Data Engineer with a minimum of 3 years of relevant experience to join our team for a 6-month contract-to-hire position. This role offers an exciting opportunity to work with one of the leading healthcare giants in Nashville, Tennessee. As a Data Engineer, you will play a key role in designing, developing, and maintaining data pipelines using Python, SQL, and various Azure tools. 
  
 Key Responsibilities:
 
   Collaborate with cross-functional teams to understand business requirements and translate them into data engineering solutions.
   Design, develop, and optimize data pipelines to support data ingestion, transformation, and storage.
   Utilize Python and SQL to extract, transform, and load (ETL) data from various sources into a centralized data warehouse.
   Leverage Azure tools and services for data processing, storage, and analytics.
   Perform data validation, quality checks, and troubleshooting to ensure accuracy and reliability.
   Work with large datasets and implement optimization techniques for improved performance.
   Participate in code reviews, provide technical guidance, and mentor junior team members.
 
  
  Qualifications:
 
   Bachelor's degree in Computer Science, Engineering, or a related field.
   Minimum of 3 years of relevant experience as a Data Engineer, with a strong focus on Python, SQL, and Azure technologies.
   Deep understanding of data modeling, ETL processes, and data warehousing concepts.
   Proficiency in designing and implementing data pipelines using Azure services (e.g., Azure Data Factory, Azure Databricks, Azure SQL Database, etc.).
   Strong problem-solving skills and attention to detail.
   Excellent communication and collaboration skills.
   Experience in the healthcare industry is a plus.
 
  
  Why Join Us:
 
   Opportunity to work with a prominent healthcare company in Nashville, Tennessee.
   Engage with cutting-edge technologies in the Azure ecosystem.
   Fully remote work environment, providing flexibility and work-life balance.
 
  
  How to Apply: Interested candidates are invited to submit their resume and cover letter to [email address]. 
  
 Note: This is a 6-month contract-to-hire position. Candidates must be eligible to work in the United States. 
  Vaco is an equal opportunity employer and values diversity in our workforce. We encourage candidates from all backgrounds to apply.","<div>
 <b>Data Engineer - 6 Month Contract-to-Hire</b> 
 <br> 
 <b>Location:</b> Fully Remote (Candidates within the United States preferred) 
 <br> 
 <b>Duration:</b> 6 Month Contract-to-Hire 
 <br> 
 <b>About Us:</b> Vaco is a global talent solutions company providing consulting, contingent staffing, direct hire, and project-based solutions in the areas of accounting, finance, technology, and administration. We work with top-tier clients across various industries, including healthcare, finance, technology, and more. 
 <br> 
 <b>Position Overview:</b> We are seeking a Data Engineer with a minimum of 3 years of relevant experience to join our team for a 6-month contract-to-hire position. This role offers an exciting opportunity to work with one of the leading healthcare giants in Nashville, Tennessee. As a Data Engineer, you will play a key role in designing, developing, and maintaining data pipelines using Python, SQL, and various Azure tools. 
 <br> 
 <b>Key Responsibilities:</b>
 <ul>
  <li> Collaborate with cross-functional teams to understand business requirements and translate them into data engineering solutions.</li>
  <li> Design, develop, and optimize data pipelines to support data ingestion, transformation, and storage.</li>
  <li> Utilize Python and SQL to extract, transform, and load (ETL) data from various sources into a centralized data warehouse.</li>
  <li> Leverage Azure tools and services for data processing, storage, and analytics.</li>
  <li> Perform data validation, quality checks, and troubleshooting to ensure accuracy and reliability.</li>
  <li> Work with large datasets and implement optimization techniques for improved performance.</li>
  <li> Participate in code reviews, provide technical guidance, and mentor junior team members.</li>
 </ul>
 <br> 
 <b> Qualifications:</b>
 <ul>
  <li> Bachelor&apos;s degree in Computer Science, Engineering, or a related field.</li>
  <li> Minimum of 3 years of relevant experience as a Data Engineer, with a strong focus on Python, SQL, and Azure technologies.</li>
  <li> Deep understanding of data modeling, ETL processes, and data warehousing concepts.</li>
  <li> Proficiency in designing and implementing data pipelines using Azure services (e.g., Azure Data Factory, Azure Databricks, Azure SQL Database, etc.).</li>
  <li> Strong problem-solving skills and attention to detail.</li>
  <li> Excellent communication and collaboration skills.</li>
  <li> Experience in the healthcare industry is a plus.</li>
 </ul>
 <br> 
 <b> Why Join Us:</b>
 <ul>
  <li> Opportunity to work with a prominent healthcare company in Nashville, Tennessee.</li>
  <li> Engage with cutting-edge technologies in the Azure ecosystem.</li>
  <li> Fully remote work environment, providing flexibility and work-life balance.</li>
 </ul>
 <br> 
 <b> How to Apply:</b> Interested candidates are invited to submit their resume and cover letter to [email address]. 
 <br> 
 <i>Note: This is a 6-month contract-to-hire position. Candidates must be eligible to work in the United States.</i> 
 <br> Vaco is an equal opportunity employer and values diversity in our workforce. We encourage candidates from all backgrounds to apply.
</div>",https://www.aplitrak.com/?adid=Y2xheWJhbHRpbW9yZS42MzcxMC40NzA3QHZhY28uYXBsaXRyYWsuY29t,892c0fe88652dd05,,Full-time,,,"Nashville, TN","Data Engineer (Python, SQL, Azure)",13 days ago,2023-10-07T12:36:33.604Z,3.8,396.0,$50 - $55 an hour,2023-10-20T12:36:33.606Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0D_sybMACCpf9B-677oK5j6rPldVB6BlrVvFjO_o-GJZbzuF-qh4PxErFUqfUsv_6sRAtmmSLc6DSJNHK7BtyJXJ1yHjCh92kz8dm5wgeoxHaFxEP0BpXXXmxV5OBQ2zuyFFEfLV7aaEUrdjFxxqmfjnf0asGdvXcAi2yBNMvD5XSYbiuuhAv7qWJsoKVnZVvz5gFylHApbSaBEFw7HH15rCW5tl4OtU2-LmHAmmUWPmE-5Xdmti-JOysZo7i6_P-srhV2L2oXWfDXSqVG6NSZAa2VMmxmaXn0Brb_UVedhyPiN1iPshtlSUhs1RyLC1ylOIvj7qsPmMPXbjl70l7Z30zRF6eS-dPClQuPdGHRbS8hQ_XXKaXNkEWbSGz3aF-Zh7wA0MnUB9soTA4RNyZgD_sqNATIhMvhIkQXVJN5gRVU7hDL-0HRkldBkT56vTkv6bzD9xSdKHFpOyVWPkgGVoDCs13TNRY7sM4RJ8VNql65f216AFiuQWyFXDAdU4U5HfLL5msO_pv0kDyJx_fWB77BtfDONlWlU4PRU20j9n5NwlgVEbPq0G_NNRMTOZCax0XIpZYQZBSbtMLn3wBKFUsNaqCxg8mAT9_y0jWefYfgedlK6KCVFw5E0gAgQBhF1j17HSHGOOddGMiQP0qlZ&xkcb=SoA6-_M3Jmqg1iQlSZ0KbzkdCdPP&p=1&fvj=0&vjs=3&jsa=1462&tk=1hd6hbo9gimir801&from=jasx&wvign=1
280,INADEV,"Description:
 
  Formed in 2011, INADEV is focused on its founding principle to build innovative customer-centric solutions incredibly fast, secure, and at scale. We deliver world-class digital experiences to some of the largest federal agencies and commercial companies. Our technical expertise and innovations are comprised of codeless automation, identity intelligence, immersive technology, artificial intelligence/machine learning (AI/ML), virtualization, and digital transformation.
  POSITION DESCRIPTION:
 
   Perform migration and testing of static data and transaction data from one core system to another, audit, reconciliation and exception reporting.
   Provide support for data migration, data engineering, and integration of existing systems.
   Developing and integrating multiple data types across a range of data sets and sources.
   Performing day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner.
   Evaluate current system designs and identify areas for improvement to create a system that is highly available and has low data latency.
   Plan and design the integration of various source systems and the migration of data between systems.
   Build and implement the source system integration and data migration plan.
   Developing, managing, manipulating, storing and parsing data across a data pipeline for variety of target sources and data consumers
   Writing code to ensure the performance and reliability of data extraction and processing
   Supporting continuous process automation for data ingestion
   Assisting with the maintenance of applications and tools that reside on the data driven systems (upgrades, patches, configuration changes, etc.)
   Working with program management and engineers to implement and document complex and evolving requirements
   Actively and collaboratively participating as a member of a cross-functional Agile/Scrum team while following all Agile/Scrum best practices
   Advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
   Helping cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
   Demonstrating significant technical competence and ownership to broad audiences while driving progress on company strategic objectives at multiple levels.
   Generating and articulating technical strategy to diverse audiences, both technical and non-technical.
 
  PHYSICAL DEMANDS:
 
   Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions
 
  INADEV Corporation does not discriminate against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibits discrimination against all individuals based on their race, color, religion, sex, sexual orientation/gender identity, or national origin. Requirements: 
  NON-TECHNICAL REQUIREMENTS:
 
   Ability to pass a 7 year background check and have the ability to obtain a U.S. Government clearance.
   Must have been a resident of the continental U.S. for at least the last 3 years.
   Must be willing to work in Eastern Standard Business hours.
   Must possess good communication (written/verbal) skills.
 
  MANDATORY REQUIREMENTS:
 
   Must have a Bachelor's Degree in a technical discipline and 5+ years pertinent experience as a Data Engineer.
   Must have proven experience with data migration project from on-prem to cloud.
   Must have working knowledge of ETL tools like AWS Glue
   Experience with Data Reconciliation post migration
   Must have working experience with a commercial database like SQL Server, Oracle, MySQL
 
  DESIRED SKILLS:
 
   Experience with Microsoft SSIS or equivalent, AWS DMS/MGN preferred","<div>
 Description:
 <p></p>
 <p><br> Formed in 2011, INADEV is focused on its founding principle to build innovative customer-centric solutions incredibly fast, secure, and at scale. We deliver world-class digital experiences to some of the largest federal agencies and commercial companies. Our technical expertise and innovations are comprised of codeless automation, identity intelligence, immersive technology, artificial intelligence/machine learning (AI/ML), virtualization, and digital transformation.</p>
 <p><b> POSITION DESCRIPTION:</b></p>
 <ul>
  <li> Perform migration and testing of static data and transaction data from one core system to another, audit, reconciliation and exception reporting.</li>
  <li> Provide support for data migration, data engineering, and integration of existing systems.</li>
  <li> Developing and integrating multiple data types across a range of data sets and sources.</li>
  <li> Performing day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner.</li>
  <li> Evaluate current system designs and identify areas for improvement to create a system that is highly available and has low data latency.</li>
  <li> Plan and design the integration of various source systems and the migration of data between systems.</li>
  <li> Build and implement the source system integration and data migration plan.</li>
  <li> Developing, managing, manipulating, storing and parsing data across a data pipeline for variety of target sources and data consumers</li>
  <li> Writing code to ensure the performance and reliability of data extraction and processing</li>
  <li> Supporting continuous process automation for data ingestion</li>
  <li> Assisting with the maintenance of applications and tools that reside on the data driven systems (upgrades, patches, configuration changes, etc.)</li>
  <li> Working with program management and engineers to implement and document complex and evolving requirements</li>
  <li> Actively and collaboratively participating as a member of a cross-functional Agile/Scrum team while following all Agile/Scrum best practices</li>
  <li> Advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing</li>
  <li> Helping cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork</li>
  <li> Demonstrating significant technical competence and ownership to broad audiences while driving progress on company strategic objectives at multiple levels.</li>
  <li> Generating and articulating technical strategy to diverse audiences, both technical and non-technical.</li>
 </ul>
 <p><b> PHYSICAL DEMANDS:</b></p>
 <ul>
  <li> Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions</li>
 </ul>
 <p> INADEV Corporation does not discriminate against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibits discrimination against all individuals based on their race, color, religion, sex, sexual orientation/gender identity, or national origin.</p> Requirements: 
 <p><b> NON-TECHNICAL REQUIREMENTS:</b></p>
 <ul>
  <li> Ability to pass a 7 year background check and have the ability to obtain a U.S. Government clearance.</li>
  <li> Must have been a resident of the continental U.S. for at least the last 3 years.</li>
  <li> Must be willing to work in Eastern Standard Business hours.</li>
  <li> Must possess good communication (written/verbal) skills.</li>
 </ul>
 <p><b> MANDATORY REQUIREMENTS:</b></p>
 <ul>
  <li> Must have a Bachelor&apos;s Degree in a technical discipline and 5+ years pertinent experience as a Data Engineer.</li>
  <li> Must have proven experience with data migration project from on-prem to cloud.</li>
  <li> Must have working knowledge of ETL tools like AWS Glue</li>
  <li> Experience with Data Reconciliation post migration</li>
  <li> Must have working experience with a commercial database like SQL Server, Oracle, MySQL</li>
 </ul>
 <p><b> DESIRED SKILLS:</b></p>
 <ul>
  <li> Experience with Microsoft SSIS or equivalent, AWS DMS/MGN preferred</li>
 </ul>
</div>",https://recruiting.paylocity.com/recruiting/jobs/Details/2011376/INADEV/Data-Engineer?source=Indeed_Feed,cab4f56240621034,,Full-time,,,Remote,Data Engineer,6 days ago,2023-10-14T12:38:28.645Z,3.0,6.0,"$90,000 - $110,000 a year",2023-10-20T12:38:28.649Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=cab4f56240621034&from=jasx&tk=1hd6hf7p7k2l9800&vjs=3
292,Venturi Group,"We are currently seeking a highly skilled and experienced senior data engineer for an exciting opportunity with a cutting-edge company in the field of AI-powered business intelligence. We are looking for individuals who are passionate about data engineering and possess the technical expertise to design, build, scale, and enhance data engineering platforms, services, and tools.
This position offers a unique chance to join a pioneering team at the seed stage, working closely with accomplished founders to shape both the product and the company culture. The company, backed by renowned venture firms and notable angel investors, is focused on providing front-office users with instant access to business data insights, eliminating the need to rely on data teams.
Role and Responsibilities:
In this role, you will take on the following key responsibilities:

 Designing the workflows and data pipelines that power the company's answer engine.
 Developing methods to ensure the integrity of complex and evolving data sets.
 Making architectural decisions and overseeing the technical build-out as the company scales.
 Assisting in leading sprint processes, conducting code reviews, and participating in product meetings.
 Playing a vital role in the recruitment and expansion of the team.

Ideal Candidate Profile:
To be successful in this position, the ideal candidate should possess the following qualifications:

 Demonstrated experience (5+ years) in building and maintaining production data pipelines, databases, or web applications.
 Proficiency in Python and SQL.
 Familiarity with Cloud Languages (AWS or GCP).
 Knowledge of data visualization and business intelligence tools.
 Experience in creating data pipelines from scratch.
 Ability to process data in large volumes and manage data stores.
 Bachelor's degree in Computer Science or a relevant STEM field.
 Prior experience working at an early-stage startup backed by reputable venture capitalists, or having built data pipelines/warehouses from scratch in previous professional experience.

Bonus Points:
The following qualifications are not required but would be considered advantageous:

 Experience with machine learning models.
 Public GitHub project portfolio.

If you are a highly motivated data engineer seeking an opportunity to contribute to a groundbreaking AI-powered business intelligence tool, this position offers an exciting chance to join a dynamic team and make a significant impact.
Venturi is an equal opportunity employer, committed to supporting and creating a diverse and inclusive workforce that fosters mutual respect for our employees and the communities we serve. All qualified applicants will receive consideration for employment without regard to sex, race, color, national origin, sexual orientation, gender, gender identity, genetic information, religion, disability, age, veteran status, or any other legally protected status under national, federal, state, or local law.
Job Type: Full-time
Pay: $160,000.00 - $200,000.00 per year
Benefits:

 Dental insurance
 Health insurance
 Paid time off
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 5 years

Schedule:

 8 hour shift
 Monday to Friday

Work Location: Remote","<p>We are currently seeking a highly skilled and experienced senior data engineer for an exciting opportunity with a cutting-edge company in the field of AI-powered business intelligence. We are looking for individuals who are passionate about data engineering and possess the technical expertise to design, build, scale, and enhance data engineering platforms, services, and tools.</p>
<p>This position offers a unique chance to join a pioneering team at the seed stage, working closely with accomplished founders to shape both the product and the company culture. The company, backed by renowned venture firms and notable angel investors, is focused on providing front-office users with instant access to business data insights, eliminating the need to rely on data teams.</p>
<p><b>Role and Responsibilities:</b></p>
<p>In this role, you will take on the following key responsibilities:</p>
<ul>
 <li>Designing the workflows and data pipelines that power the company&apos;s answer engine.</li>
 <li>Developing methods to ensure the integrity of complex and evolving data sets.</li>
 <li>Making architectural decisions and overseeing the technical build-out as the company scales.</li>
 <li>Assisting in leading sprint processes, conducting code reviews, and participating in product meetings.</li>
 <li>Playing a vital role in the recruitment and expansion of the team.</li>
</ul>
<p><b>Ideal Candidate Profile:</b></p>
<p>To be successful in this position, the ideal candidate should possess the following qualifications:</p>
<ul>
 <li>Demonstrated experience (5+ years) in building and maintaining production data pipelines, databases, or web applications.</li>
 <li>Proficiency in Python and SQL.</li>
 <li>Familiarity with Cloud Languages (AWS or GCP).</li>
 <li>Knowledge of data visualization and business intelligence tools.</li>
 <li>Experience in creating data pipelines from scratch.</li>
 <li>Ability to process data in large volumes and manage data stores.</li>
 <li>Bachelor&apos;s degree in Computer Science or a relevant STEM field.</li>
 <li>Prior experience working at an early-stage startup backed by reputable venture capitalists, or having built data pipelines/warehouses from scratch in previous professional experience.</li>
</ul>
<p><b>Bonus Points:</b></p>
<p>The following qualifications are not required but would be considered advantageous:</p>
<ul>
 <li>Experience with machine learning models.</li>
 <li>Public GitHub project portfolio.</li>
</ul>
<p>If you are a highly motivated data engineer seeking an opportunity to contribute to a groundbreaking AI-powered business intelligence tool, this position offers an exciting chance to join a dynamic team and make a significant impact.</p>
<p><i>Venturi is an equal opportunity employer, committed to supporting and creating a diverse and inclusive workforce that fosters mutual respect for our employees and the communities we serve. All qualified applicants will receive consideration for employment without regard to sex, race, color, national origin, sexual orientation, gender, gender identity, genetic information, religion, disability, age, veteran status, or any other legally protected status under national, federal, state, or local law.</i></p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;160,000.00 - &#x24;200,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,8e47ce2fb2302cd9,,Full-time,,,Remote,Data Engineer,30+ days ago,2023-09-20T12:39:35.613Z,,,"$160,000 - $200,000 a year",2023-10-20T12:39:35.614Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DiMBqcaSMT8lrn_viPgFID_2aewekq0duxyJS2DdWDl6I0UnuoC7mcAdBs-ATn3cQJhn9CDpXovvb6u4GJHRkUdRUIl9WrtWgcaow6jx8ZQ5SWEu_BoYpajof4D9BgI_-yNKjetw9dzBzmkelwLh0MHZlaNz217eTFWNDYV0qnNe01DysLUiwRut5zEeZXmZ1SqpZoykhWwRChAftiz-Unw7e5YdqDk07OQ-6aP5jOEU0qcd7q17d_UI6hlBJe0HsWrP_0SJpi_ry8UddCuYuDnMGZFeG5RRjbxEiCiFg8StOx4NsAbIozMotUTX6VTTmW5eFepaevr46ZLbo3yj6uUhDIR55G3OHQEFglQJ_Kn2_WgHaecOE9gNdITTqAgCFwpWpBuIsmFIsFsDoby-JRrkLb0R8aFxhGVfP_591os19rPAT1xQ_2Fj9CUtgpPKRRDV40EGUB_1Jgoy7UWWeq6M6EEoLha3H-JpIHKMbhNZu6VyDGwae0xEXe-nNkRB42LakC32AxIpXWgvD_6VK1o_AtzQ3dE94yU_YZRn0v9lfvaCNEUyxrP8WZTojmP7l7FYc8WdomRhRFibUix8xKtsJUXTepR-Q2tRL0JEM0QA%3D%3D&xkcb=SoBw-_M3JmrH7SxcpR0GbzkdCdPP&p=13&fvj=1&vjs=3&jsa=7574&tk=1hd6hi1iggaj8801&from=jasx&wvign=1
295,TikTok,"Responsibilities 
TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo. 

 Why Join Us 
At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok. 

 About USDS 
At TikTok, we're committed to a process of continuous innovation and improvement in our user experience and safety controls. We're proud to be able to serve a global community of more than a billion people who use TikTok to creatively express themselves and be entertained, and we're dedicated to giving them a platform that builds opportunity and fosters connection. We also take our responsibility to safeguard our community seriously, both in how we address potentially harmful content and how we protect against unauthorized access to user data. 

 U.S. Data Security (“USDS”) is a standalone department of TikTok in the U.S. This new security-first division was created to bring heightened focus and governance to our data protection policies and content assurance protocols to keep U.S. users safe. Our focus is on providing oversight and protection of the TikTok platform and user data in the U.S., so millions of Americans can continue turning to TikTok to learn something new, earn a living, express themselves creatively, or be entertained. The teams within USDS that deliver on this commitment daily span Trust & Safety, Security & Privacy, Engineering, User & Product Ops, Corporate Functions and more. 

 
About the Role:
 As an Data Engineer, you will have significant responsibility and influence in shaping the Data Cycling Center's strategic direction. This role is inherently multi-functional, and the ideal candidate will work across disciplines. We are looking for someone with a love for data and the ability to iterate quickly. Successful candidates will have strong engineering skills and communication and a belief that data-driven processes lead to phenomenal products. 

 
What You'll Need:
 
 
 The perfect candidate will have strong data infrastructure and data architecture skills, strong operational skills to drive efficiency and speed, strong project management leadership, and a strong vision for how data engineering can proactively improve companies. 
 Experience working cross-functionally with business stakeholders, engineering, product, and SRE teams to understand business requirements and convert that into technical requirements, including developing prototypes to demonstrate the feasibility of data and analytics solutions. 
 
What You'll Do:
 
 
 Work with business stakeholders, engineering, product and SRE teams to understand business requirements and convert that into technical requirements, including developing prototypes to demonstrate the feasibility of data and analytics solutions. 
 Extract data from various sources such as APIs, HIVE tables and other structured and unstructured data sources to process and store large volumes of data ensuring data accuracy, consistency, and security. 
 Design, build, and maintain data pipelines utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources to ensure data is easily accessible and can be used effectively by other members of the organization. 
 Implement and monitor quality control measures to ensure data accuracy, completeness, and consistency. 
 Create and maintain technical documentation, such as data dictionaries, data flow diagrams, and system documentation, to ensure efficient and effective data management and analysis. 
 Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts 
 Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way 
 Ability to analyze and visualize data to provide business stakeholders with impactful, actionable insights 
 Qualifications 

 
 Bachelors degree in Statistics, Economics, Computer Science or another quantitative field 
 5+ years of experience working with data analytics and data engineering, including experience with data cleaning and preprocessing, data analysis and dashboard development. 
 2+ years experience building dashboards in Tableau, Power BI or any similar visualization tool. 
 Proficiency in distributed data processing using Big Data technologies like Spark/Scala, Java, Hadoop/HDFS/AWS/S3, Cassandra and Kafka 
 Proficiency in data modeling, data design, SQL, and NoSQL databases 
 
Preferred:
 
 
 Experience in a consumer web or mobile company 
 Strong background in algorithms and data structures 
 Experience working with PII and GDPR data 
 Ability to communicate effectively, both written and verbal, with technical and non-technical partners 
 Ability to deliver consistent high quality results while working in a dynamic & fast environment 
 Passionate, curious, and seeking to tackle every day problems with innovation 
 TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. 

 TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at usds.accommodations@tiktok.com 
Job Information 
The base salary range for this position in the selected city is $136800 - $259200 annually. 

 ​ 

 Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units. 

 ​ 

 At ByteDance/TikTok our benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support ByteDancers to give their best in both work and life. We offer the following benefits to eligible employees: 

 ​ 

 We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. 

 ​ 

 
Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off(PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. 

 ​ 

 We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice.","Responsibilities 
<br>TikTok is the leading destination for short-form mobile video. Our mission is to inspire creativity and bring joy. TikTok has global offices including Los Angeles, New York, London, Paris, Berlin, Dubai, Mumbai, Singapore, Jakarta, Seoul and Tokyo. 
<br>
<br> Why Join Us 
<br>At TikTok, our people are humble, intelligent, compassionate and creative. We create to inspire - for you, for us, and for more than 1 billion users on our platform. We lead with curiosity and aim for the highest, never shying away from taking calculated risks and embracing ambiguity as it comes. Here, the opportunities are limitless for those who dare to pursue bold ideas that exist just beyond the boundary of possibility. Join us and make impact happen with a career at TikTok. 
<br>
<br> About USDS 
<br>At TikTok, we&apos;re committed to a process of continuous innovation and improvement in our user experience and safety controls. We&apos;re proud to be able to serve a global community of more than a billion people who use TikTok to creatively express themselves and be entertained, and we&apos;re dedicated to giving them a platform that builds opportunity and fosters connection. We also take our responsibility to safeguard our community seriously, both in how we address potentially harmful content and how we protect against unauthorized access to user data. 
<br>
<br> U.S. Data Security (&#x201c;USDS&#x201d;) is a standalone department of TikTok in the U.S. This new security-first division was created to bring heightened focus and governance to our data protection policies and content assurance protocols to keep U.S. users safe. Our focus is on providing oversight and protection of the TikTok platform and user data in the U.S., so millions of Americans can continue turning to TikTok to learn something new, earn a living, express themselves creatively, or be entertained. The teams within USDS that deliver on this commitment daily span Trust &amp; Safety, Security &amp; Privacy, Engineering, User &amp; Product Ops, Corporate Functions and more. 
<br>
<br> 
<b>About the Role:</b>
<br> As an Data Engineer, you will have significant responsibility and influence in shaping the Data Cycling Center&apos;s strategic direction. This role is inherently multi-functional, and the ideal candidate will work across disciplines. We are looking for someone with a love for data and the ability to iterate quickly. Successful candidates will have strong engineering skills and communication and a belief that data-driven processes lead to phenomenal products. 
<br>
<br> 
<b>What You&apos;ll Need:</b>
<br> 
<ul> 
 <li>The perfect candidate will have strong data infrastructure and data architecture skills, strong operational skills to drive efficiency and speed, strong project management leadership, and a strong vision for how data engineering can proactively improve companies.</li> 
 <li>Experience working cross-functionally with business stakeholders, engineering, product, and SRE teams to understand business requirements and convert that into technical requirements, including developing prototypes to demonstrate the feasibility of data and analytics solutions.</li> 
</ul> 
<b>What You&apos;ll Do:</b>
<br> 
<ul> 
 <li>Work with business stakeholders, engineering, product and SRE teams to understand business requirements and convert that into technical requirements, including developing prototypes to demonstrate the feasibility of data and analytics solutions.</li> 
 <li>Extract data from various sources such as APIs, HIVE tables and other structured and unstructured data sources to process and store large volumes of data ensuring data accuracy, consistency, and security.</li> 
 <li>Design, build, and maintain data pipelines utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources to ensure data is easily accessible and can be used effectively by other members of the organization.</li> 
 <li>Implement and monitor quality control measures to ensure data accuracy, completeness, and consistency.</li> 
 <li>Create and maintain technical documentation, such as data dictionaries, data flow diagrams, and system documentation, to ensure efficient and effective data management and analysis.</li> 
 <li>Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts</li> 
 <li>Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way</li> 
 <li>Ability to analyze and visualize data to provide business stakeholders with impactful, actionable insights</li> 
</ul> Qualifications 
<br>
<ul> 
 <li>Bachelors degree in Statistics, Economics, Computer Science or another quantitative field</li> 
 <li>5+ years of experience working with data analytics and data engineering, including experience with data cleaning and preprocessing, data analysis and dashboard development.</li> 
 <li>2+ years experience building dashboards in Tableau, Power BI or any similar visualization tool.</li> 
 <li>Proficiency in distributed data processing using Big Data technologies like Spark/Scala, Java, Hadoop/HDFS/AWS/S3, Cassandra and Kafka</li> 
 <li>Proficiency in data modeling, data design, SQL, and NoSQL databases</li> 
</ul> 
<b>Preferred:</b>
<br> 
<ul> 
 <li>Experience in a consumer web or mobile company</li> 
 <li>Strong background in algorithms and data structures</li> 
 <li>Experience working with PII and GDPR data</li> 
 <li>Ability to communicate effectively, both written and verbal, with technical and non-technical partners</li> 
 <li>Ability to deliver consistent high quality results while working in a dynamic &amp; fast environment</li> 
 <li>Passionate, curious, and seeking to tackle every day problems with innovation</li> 
</ul> TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too. 
<br>
<br> TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at usds.accommodations@tiktok.com 
<br>Job Information 
<br>The base salary range for this position in the selected city is &#x24;136800 - &#x24;259200 annually. 
<br>
<br> &#x200b; 
<br>
<br> Compensation may vary outside of this range depending on a number of factors, including a candidate&#x2019;s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units. 
<br>
<br> &#x200b; 
<br>
<br> At ByteDance/TikTok our benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support ByteDancers to give their best in both work and life. We offer the following benefits to eligible employees: 
<br>
<br> &#x200b; 
<br>
<br> We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&amp;D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. 
<br>
<br> &#x200b; 
<br>
<br> 
<b>Our time off and leave plans are:</b> 10 paid holidays per year plus 17 days of Paid Personal Time Off(PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. 
<br>
<br> &#x200b; 
<br>
<br> We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice.",https://careers.tiktok.com/position/7247608478271834424/detail?spread=XKM9ZXE,0e319a6994ef9d47,,,,,"Mountain View, CA 94041",Data Engineer - USDS,30+ days ago,2023-09-20T12:39:36.679Z,3.4,163.0,"$136,800 - $259,200 a year",2023-10-20T12:39:36.681Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=0e319a6994ef9d47&from=jasx&tk=1hd6hi1iggaj8801&vjs=3
299,HTLF,"HTLF is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors.
 
 
 
   HTLF is a diversified financial services company headquartered in Denver, Colorado. We deliver community banking at scale by powering our geographically diverse group of banks with technology, efficiency and strength — giving local decision-making the opportunity and insights to focus on customers and growth. Relationships have been the core of our company since its founding in 1981. We're deeply invested in the communities we serve, and that's why our clients choose us as their banking partners.
 
 
 
   What's different about a career at HTLF? We believe our employees and their diverse backgrounds, perspectives and skills are our greatest assets. We wouldn't be HTLF without the people with whom we surround ourselves and empower to enrich the lives of our customers, employees and communities. We're dedicated to making HTLF the best place to work – where your opinions are valued, your feedback and ideas are heard, and your opportunities for personal growth and professional development are endless.
 
  Under limited direction, the Cloud Data Engineer 2 is responsible for the design, development, testing, implementation, maintenance, performance, and up-time of the Cloud Data Warehouse and application integrations at HTLF. The main goal is to ensure the accuracy, availability and consistent performance of the data that supports corporate reporting, analytics and integration needs
 
 
   PRIMARY & ESSENTIAL RESPONSIBILITIES:
 
 
   1. Design, develop, and maintain Cloud Data Warehouse and analytics architecture to meet the enterprise’s business analysis and reporting needs
 
 
   2. Gather and document business requirements
 
 
   3. Create and document technical needs and designs for ETL processes and databases, and ensure optimal technical infrastructure is utilized
 
 
   4. Design, develop, and support new and current ETL processes employing industry standards and best practices to enhance loading of data from and into different source/ target systems
 
 
   5. Consolidate and optimize available data warehouse infrastructure
 
 
   6. Design and implement ETL procedures for intake of data from both internal and external sources
 
 
   7. Design and implement ETL processes and architecture to ensure accurate functioning of analytics
 
 
   8. Ensure that all design and code is compliant with data governance rules and standards by performing data validation and quality checks
 
 
   9. Provide support for Data Governance processes by populating metadata and data lineage into Data Catalog tool
 
 
   10. Collaborate with business and technology stakeholders in ensuring data warehouse architecture development and utilization
 
 
   11. Carry out monitoring, tuning, and database performance analysis
 
 
   12. Perform the design and extension of data marts, meta data, and data models
 
 
   13. Ensure all data warehouse architecture code are maintained in a version control system
 
 
   14. Manage aspects of the warehouse such as data sourcing migration, quality, design and implementation
 
 
   15. Identify, research, and resolve technical problems as well as answer user questions
 
 
   16. Serves as a point of escalation and resolution for data integrity or performance issues and outages
 
 
   17. Serves as a technical resource to other teams and projects for developing database integrations with other corporate systems
 
 
   18. Implement integrations via APIs and/or no-code/low code middleware tools as appropriate to fulfill defined requirements
 
 
   19. Evaluate current system capabilities to identify changes that need to be made and determines the impact to the business while ensuring that programming and software requirements fulfill business objectives
 
 
   20. Assists in establishing and maintaining Service Level Agreements
 
 
   21. Provide on-call support based on the database recovery timeframe
 
 
   22. Remains up-to-date on the latest industry trends; articulates trends and potential clearly and confidently
 
 
   23. Works with project and business leaders to identify analytical requirements 24. Follow established methodologies during system development and ensures systems adhere to the standards and procedures established by the IT department
 
 
   25. Develops and maintains all system related documentation
 
 
   26. Performs other duties as assigned
 
 
   27. Completes annual E-Learning Plan and Bank Secrecy Act (BSA) training as assigned and keeps up-to-date knowledge of BSA as it relates to the job function
 
 
 
   REQUIRED SKILLS & EXPERIENCE:
 
 
   1. Bachelor’s Degree in an analytical related field, including information technology, science, and engineering or equivalent years of experience
 
 
   2. 7 - 10 years of experience with Informatica Cloud IDMC platform – Batch and Real Time Integration
 
 
   3. 5-7 years of experience with Snowflake and SQL Server
 
 
   4. Experience using source control tools such as GIT
 
 
   5. Advanced experience with SQL
 
 
   6. Experience working with Informatica Cloud Data Governance/Data Catalog tool
 
 
   7. 5 - 7 years of experience with Performance Tuning and Optimization methodologies
 
 
   8. Demonstrated experience with Data Warehouse/Data Lakehouse design, development and testing
 
 
   9. Experience with Kimball & Inmon Data Warehouse methodologies
 
 
   10. Experience working with dimensional models
 
 
   11. Experience understanding change data capture concepts and related implementation options
 
 
   12. Ability to evaluate data flows to provide suggestions to optimize delivery output, adhere to security protocols, and/or remain compliant with quality control standards
 
 
   13. Proficiency in data formats such as XML, CSV and JSON
 
 
   14. Proficiency in typical integration technologies such as HTTP, JMS, JDBC, REST, SOAP, WebServices and APIs
 
 
   15. Proficiency with Java
 
 
   16. Experience with gathering requirements and preparing specifications
 
 
   17. Ability to work in an agile matrix managed environment required
 
 
   18. Ability to multitask, prioritize, and manage time efficiently
 
 
   19. Accurate and precise attention to detail
 
 
   20. Excellent analytical, quantitative, and organizational skills
 
 
   21. Ability to document procedures and build work instructions
 
 
   22. Knowledge of Microsoft Office and Office 365
 
 
 
   OCCUPATIONAL CERTIFICATION:
 
 
   1. Informatica Certified Professional (ICP) preferred
 
 
   2. Snowflake SnowPro Certification preferred
 
 
 
   Scheduled Weekly Hours:
  40
 
 
   Time Type:
  Full time
 
 
   The targeted salary for this role is:
  $116,151.00 - $150,996.00
 
 
   You may also be offered incentive compensation, and benefits. Benefits may include Medical, Dental, Vision, 401(k), Health Savings Account, Paid Time Off. Actual compensation may vary based on geographic location of the specific role as well as work experience, education, and skills of the selected candidate.","<div>
 <div>
  HTLF is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors.
 </div>
 <div></div>
 <div>
   HTLF is a diversified financial services company headquartered in Denver, Colorado. We deliver community banking at scale by powering our geographically diverse group of banks with technology, efficiency and strength &#x2014; giving local decision-making the opportunity and insights to focus on customers and growth. Relationships have been the core of our company since its founding in 1981. We&apos;re deeply invested in the communities we serve, and that&apos;s why our clients choose us as their banking partners.
 </div>
 <div></div>
 <div>
   What&apos;s different about a career at HTLF? We believe our employees and their diverse backgrounds, perspectives and skills are our greatest assets. We wouldn&apos;t be HTLF without the people with whom we surround ourselves and empower to enrich the lives of our customers, employees and communities. We&apos;re dedicated to making HTLF the best place to work &#x2013; where your opinions are valued, your feedback and ideas are heard, and your opportunities for personal growth and professional development are endless.
 </div>
 <div></div> Under limited direction, the Cloud Data Engineer 2 is responsible for the design, development, testing, implementation, maintenance, performance, and up-time of the Cloud Data Warehouse and application integrations at HTLF. The main goal is to ensure the accuracy, availability and consistent performance of the data that supports corporate reporting, analytics and integration needs
 <div></div>
 <div>
   PRIMARY &amp; ESSENTIAL RESPONSIBILITIES:
 </div>
 <div>
   1. Design, develop, and maintain Cloud Data Warehouse and analytics architecture to meet the enterprise&#x2019;s business analysis and reporting needs
 </div>
 <div>
   2. Gather and document business requirements
 </div>
 <div>
   3. Create and document technical needs and designs for ETL processes and databases, and ensure optimal technical infrastructure is utilized
 </div>
 <div>
   4. Design, develop, and support new and current ETL processes employing industry standards and best practices to enhance loading of data from and into different source/ target systems
 </div>
 <div>
   5. Consolidate and optimize available data warehouse infrastructure
 </div>
 <div>
   6. Design and implement ETL procedures for intake of data from both internal and external sources
 </div>
 <div>
   7. Design and implement ETL processes and architecture to ensure accurate functioning of analytics
 </div>
 <div>
   8. Ensure that all design and code is compliant with data governance rules and standards by performing data validation and quality checks
 </div>
 <div>
   9. Provide support for Data Governance processes by populating metadata and data lineage into Data Catalog tool
 </div>
 <div>
   10. Collaborate with business and technology stakeholders in ensuring data warehouse architecture development and utilization
 </div>
 <div>
   11. Carry out monitoring, tuning, and database performance analysis
 </div>
 <div>
   12. Perform the design and extension of data marts, meta data, and data models
 </div>
 <div>
   13. Ensure all data warehouse architecture code are maintained in a version control system
 </div>
 <div>
   14. Manage aspects of the warehouse such as data sourcing migration, quality, design and implementation
 </div>
 <div>
   15. Identify, research, and resolve technical problems as well as answer user questions
 </div>
 <div>
   16. Serves as a point of escalation and resolution for data integrity or performance issues and outages
 </div>
 <div>
   17. Serves as a technical resource to other teams and projects for developing database integrations with other corporate systems
 </div>
 <div>
   18. Implement integrations via APIs and/or no-code/low code middleware tools as appropriate to fulfill defined requirements
 </div>
 <div>
   19. Evaluate current system capabilities to identify changes that need to be made and determines the impact to the business while ensuring that programming and software requirements fulfill business objectives
 </div>
 <div>
   20. Assists in establishing and maintaining Service Level Agreements
 </div>
 <div>
   21. Provide on-call support based on the database recovery timeframe
 </div>
 <div>
   22. Remains up-to-date on the latest industry trends; articulates trends and potential clearly and confidently
 </div>
 <div>
   23. Works with project and business leaders to identify analytical requirements 24. Follow established methodologies during system development and ensures systems adhere to the standards and procedures established by the IT department
 </div>
 <div>
   25. Develops and maintains all system related documentation
 </div>
 <div>
   26. Performs other duties as assigned
 </div>
 <div>
   27. Completes annual E-Learning Plan and Bank Secrecy Act (BSA) training as assigned and keeps up-to-date knowledge of BSA as it relates to the job function
 </div>
 <div></div>
 <div>
   REQUIRED SKILLS &amp; EXPERIENCE:
 </div>
 <div>
   1. Bachelor&#x2019;s Degree in an analytical related field, including information technology, science, and engineering or equivalent years of experience
 </div>
 <div>
   2. 7 - 10 years of experience with Informatica Cloud IDMC platform &#x2013; Batch and Real Time Integration
 </div>
 <div>
   3. 5-7 years of experience with Snowflake and SQL Server
 </div>
 <div>
   4. Experience using source control tools such as GIT
 </div>
 <div>
   5. Advanced experience with SQL
 </div>
 <div>
   6. Experience working with Informatica Cloud Data Governance/Data Catalog tool
 </div>
 <div>
   7. 5 - 7 years of experience with Performance Tuning and Optimization methodologies
 </div>
 <div>
   8. Demonstrated experience with Data Warehouse/Data Lakehouse design, development and testing
 </div>
 <div>
   9. Experience with Kimball &amp; Inmon Data Warehouse methodologies
 </div>
 <div>
   10. Experience working with dimensional models
 </div>
 <div>
   11. Experience understanding change data capture concepts and related implementation options
 </div>
 <div>
   12. Ability to evaluate data flows to provide suggestions to optimize delivery output, adhere to security protocols, and/or remain compliant with quality control standards
 </div>
 <div>
   13. Proficiency in data formats such as XML, CSV and JSON
 </div>
 <div>
   14. Proficiency in typical integration technologies such as HTTP, JMS, JDBC, REST, SOAP, WebServices and APIs
 </div>
 <div>
   15. Proficiency with Java
 </div>
 <div>
   16. Experience with gathering requirements and preparing specifications
 </div>
 <div>
   17. Ability to work in an agile matrix managed environment required
 </div>
 <div>
   18. Ability to multitask, prioritize, and manage time efficiently
 </div>
 <div>
   19. Accurate and precise attention to detail
 </div>
 <div>
   20. Excellent analytical, quantitative, and organizational skills
 </div>
 <div>
   21. Ability to document procedures and build work instructions
 </div>
 <div>
   22. Knowledge of Microsoft Office and Office 365
 </div>
 <div></div>
 <div>
   OCCUPATIONAL CERTIFICATION:
 </div>
 <div>
   1. Informatica Certified Professional (ICP) preferred
 </div>
 <div>
   2. Snowflake SnowPro Certification preferred
 </div>
 <div></div>
 <div>
   Scheduled Weekly Hours:
 </div> 40
 <div></div>
 <div>
   Time Type:
 </div> Full time
 <div></div>
 <div>
   The targeted salary for this role is:
 </div> &#x24;116,151.00 - &#x24;150,996.00
 <div></div>
 <div>
  <i> You may also be offered incentive compensation, and benefits. Benefits may include Medical, Dental, Vision, 401(k), Health Savings Account, Paid Time Off. Actual compensation may vary based on geographic location of the specific role as well as work experience, education, and skills of the selected candidate.</i>
 </div>
</div>",https://htlf.wd1.myworkdayjobs.com/en-US/Heartland-Careers/job/United-States-of-America-Remote/Cloud-Data-Engineer-II_23-0645?source=Indeed,94d26c8b78c2188d,,Full-time,,,Remote,Cloud Data Engineer II,30+ days ago,2023-09-20T12:40:15.265Z,3.6,26.0,"$116,151 - $150,996 a year",2023-10-20T12:40:15.268Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=94d26c8b78c2188d&from=jasx&tk=1hd6hi1iggaj8801&vjs=3
300,Blackbaud,"What we are looking for:
  Blackbaud is looking for a Software Architect to join our team! Software Architects at Blackbaud are influential to our business and local development community and carry over 12+ years of experience in design and development in a SaaS (or similar) model.
 
  About the role:
  As a Software Architect, you will drive continuous improvement of our systems and ensure that every application is implemented according to acceptable design, quality, performance, and security standards. You will platform best practices, assist in design problems and formulate high-level estimates for analyses, project planning, and impact assessments. As a Software Architect , you will also evaluate and recommend technology strategies where gaps exist, including performing proofs-of-concept as needed.
 
  What you’ll be doing:
 
   Collaborate with product owners and other business partners to understand and refine business requirements that drive the architecture of our Commerce Platform teams
   Collaborate and drive vision of our Identity & Access and Customer Enablement strategy at Blackbaud.
   Evaluate legacy and current application's architecture and database design to make recommendations for improvements on design, performance, and quality
   Work closely with other engineering teams to document the detailed application design, implement appropriate design patterns and best practices, and create integration strategies between systems
   Keep up with new technologies and trends, and be able to apply them when appropriate to our architecture or processes.
   Collaborate with IT Security to ensure solutions comply with corporate and regulatory policies
   Perform code and design reviews as needed
 
 
  What we’ll want you to have:
 
   Bachelor's degree in Computer Science/Engineering or Technology related field, or possess equivalent work experience
   12+ years of experience in design and development of complex web-based, high transaction, high volume, distributed systems offered in a SaaS model or similar
   Formal skills with the following: C#, .Net, web services, Microservice Architecture, ORM frameworks, RDBMS, NoSQL, data stores, build/continuous integration, data modeling, database design, messaging middleware systems and protocols
   Experience with cloud technologies: Azure, GCP, AWS
   Knowledge of distributed architectures and design patterns, and best practices
   Ability to mentor team members with varying degrees of technical depth and breadth of knowledge
 
 
  #LI-REMOTE
 
  Join our Software Engineering team and apply today!
 
  Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTube 
 
 Blackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!
 
  Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.
 
  To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.
 
  A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com.
  The starting base pay is $150,400.00 to $204,100.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.
 
  Benefits Include:
 
   Medical, dental, and vision insurance
   Remote-first workforce
   401(k) program with employer match
   Flexible paid time off
   Generous Parental Leave
   Volunteer for vacation
   Opportunities to connect to build community and belonging
   Pet insurance, legal and identity protection
   Tuition reimbursement program","<div>
 <p><b>What we are looking for:</b></p>
 <p> Blackbaud is looking for a Software Architect to join our team! Software Architects at Blackbaud are influential to our business and local development community and carry over 12+ years of experience in design and development in a SaaS (or similar) model.</p>
 <p></p>
 <p><b> About the role:</b></p>
 <p> As a Software Architect, you will drive continuous improvement of our systems and ensure that every application is implemented according to acceptable design, quality, performance, and security standards. You will platform best practices, assist in design problems and formulate high-level estimates for analyses, project planning, and impact assessments. As a Software Architect , you will also evaluate and recommend technology strategies where gaps exist, including performing proofs-of-concept as needed.</p>
 <p></p>
 <p><b> What you&#x2019;ll be doing:</b></p>
 <ul>
  <li><p> Collaborate with product owners and other business partners to understand and refine business requirements that drive the architecture of our Commerce Platform teams</p></li>
  <li><p> Collaborate and drive vision of our Identity &amp; Access and Customer Enablement strategy at Blackbaud.</p></li>
  <li><p> Evaluate legacy and current application&apos;s architecture and database design to make recommendations for improvements on design, performance, and quality</p></li>
  <li><p> Work closely with other engineering teams to document the detailed application design, implement appropriate design patterns and best practices, and create integration strategies between systems</p></li>
  <li><p> Keep up with new technologies and trends, and be able to apply them when appropriate to our architecture or processes.</p></li>
  <li><p> Collaborate with IT Security to ensure solutions comply with corporate and regulatory policies</p></li>
  <li><p> Perform code and design reviews as needed</p></li>
 </ul>
 <p></p>
 <p><b> What we&#x2019;ll want you to have:</b></p>
 <ul>
  <li><p> Bachelor&apos;s degree in Computer Science/Engineering or Technology related field, or possess equivalent work experience</p></li>
  <li><p> 12+ years of experience in design and development of complex web-based, high transaction, high volume, distributed systems offered in a SaaS model or similar</p></li>
  <li><p> Formal skills with the following: C#, .Net, web services, Microservice Architecture, ORM frameworks, RDBMS, NoSQL, data stores, build/continuous integration, data modeling, database design, messaging middleware systems and protocols</p></li>
  <li><p> Experience with cloud technologies: Azure, GCP, AWS</p></li>
  <li><p> Knowledge of distributed architectures and design patterns, and best practices</p></li>
  <li><p> Ability to mentor team members with varying degrees of technical depth and breadth of knowledge</p></li>
 </ul>
 <p></p>
 <p> #LI-REMOTE</p>
 <p></p>
 <p><b> Join our Software Engineering team and apply today!</b></p>
 <p></p>
 <p> Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTube </p>
 <p></p>
 <p>Blackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!</p>
 <p></p>
 <p> Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.</p>
 <p></p>
 <p><b><i> To all recruitment agencies:</i></b><i> We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.</i></p>
 <p></p>
 <p><b><i> A notice to candidates: Recruitment Fraudulent Alert:</i></b><b><i> </i></b><i>Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at</i><i> </i><i>https://careers.blackbaud.com/us/en</i><i> </i><i>or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email</i><i> </i><i>blackbaudrecruiting@blackbaud.com</i><i>.</i></p>
 <p></p> The starting base pay is &#x24;150,400.00 to &#x24;204,100.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.
 <p></p>
 <p> Benefits Include:</p>
 <ul>
  <li><p> Medical, dental, and vision insurance</p></li>
  <li><p> Remote-first workforce</p></li>
  <li><p> 401(k) program with employer match</p></li>
  <li><p> Flexible paid time off</p></li>
  <li><p> Generous Parental Leave</p></li>
  <li><p> Volunteer for vacation</p></li>
  <li><p> Opportunities to connect to build community and belonging</p></li>
  <li><p> Pet insurance, legal and identity protection</p></li>
  <li><p> Tuition reimbursement program</p></li>
 </ul>
</div>
<p></p>",https://careers.blackbaud.com/us/en/job/R0010858/?source=indeed,3417ee3c65b91eed,,,,,"Columbia, SC 29201","Principal Cyber Security Engineer, Data Protection",30+ days ago,2023-09-20T12:40:18.953Z,3.5,156.0,"$150,400 - $204,100 a year",2023-10-20T12:40:18.981Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=3417ee3c65b91eed&from=jasx&tk=1hd6hi1iggaj8801&vjs=3
301,Gainwell Technologies LLC,"Be part of a team that unleashes the power of leading-edge technologies to help improve the health and well-being of those most vulnerable in our country and communities. Working at Gainwell carries its rewards. You’ll have an incredible opportunity to grow your career in a company that values work flexibility, learning, and career development. You’ll add to your technical credentials and certifications while enjoying a generous, flexible vacation policy and educational assistance. We also have comprehensive leadership and technical development academies to help build your skills and capabilities.
 
 
  
   
     Summary
   
   
    
      As a App Data Capture/Scanning/Indexing Engineer at Gainwell, you can contribute your skills as we harness the power of technology to help our clients improve the health and well-being of the members they serve — a community’s most vulnerable. Connect your passion with purpose, teaming with people who thrive on finding innovative solutions to some of healthcare’s biggest challenges. Here are the details on this position.
      Guide on scanning and document-capturing the staff on how documents can be best captured. Analyzes, models, designs, develops and implements integration solutions for EDMS and HealthPAS Application suite for Medicaid sites. The position supports required Electronic Document Management Systems components providing automated workflow processing and document management processes crucial to the implementation and function of the Gainwell Medicaid Fiscal Agent services product. Serves as an Application Designer performing design, configuration, implementation, documentation, research, analysis, support, and other technical responsibilities supporting the role.
    
   
  
  
   
     Your role in our mission
   
   
     Essential Job Functions
    
      Guide letter authoring staff on how letter requirements can be best implemented.
      Support across all legacy sites for the letter manager solution as a whole
      New site implementations, Solution upgrades, and enhancements
      Extensive knowledge of MS Visual Studio, MS.NET C#, ASP.NET, Visual Basic .NET, TSQL, Web Services, Entity Framework, jQuery, Bootstrap, RestAPI, SOAP, SSIS, MS SQL Server, SSIS, Java Script, XML, XSD, HTML, CSS
      Support ongoing reconciliation of the image and data transfer between the data entry vendor and Health PAS.
      Support ingesting images, Index Data, and document data into Health PAS.
      Knowledge of 
     Experience with the following imaging software, include Microsoft Windows Server 2008 and 2012ing Installing and configuring software, testing, and troubleshooting.
      
        Kodak Scanner 4200, 1860, 5850
        Kodak Capture Pro 4.1, 4.5, 5.4.1, 5.6
      
    
   
  
  
   
     What we're looking for
   
   
    
      Experience and/or knowledge of document capture and storage systems required.
      Extensive knowledge of MS Visual Studio, MS.NET C#, ASP.NET, Visual Basic .NET, TSQL, Web Services, Entity Framework, jQuery, Bootstrap, RestAPI, SOAP, SSIS, MS SQL Server, SSIS, Java Script, XML, XSD, HTML, CSS
      Configure, test, and deploy our scanning and indexing system to allow for manual scanning and entry and automated receipt (via fax) of various documents to be committed into a document repository with the required index values for later retrieval by other systems. User security, third-party vendor submissions, and barcodes must also be addressed.
      Solid written and oral communication skills required for working with the district client, third-party vendors, and various Medicaid system project teams.
      Work with site and data entry vendor to define the protocol in a format for the transfer of images, and metadata to Health PAS Solution.
    
   
  
  
   
     What you should expect in this role
   
   
    
      All USA locations will be considered including remote.
      Good experience in healthcare systems preferred
    
     #LI-DN1 #LI- Remote
   
  
 
 
  The pay range for this position is $99,000.00 - $123,800.00 per year, however, the base pay offered may vary depending on geographic region, internal equity, job-related knowledge, skills, and experience among other factors. Put your passion to work at Gainwell. You’ll have the opportunity to grow your career in a company that values work flexibility, learning, and career development. All salaried, full-time candidates are eligible for our generous, flexible vacation policy, a 401(k) employer match, comprehensive health benefits, and educational assistance. We also have a variety of leadership and technical development academies to help build your skills and capabilities.
 
  We believe nothing is impossible when you bring together people who care deeply about making healthcare work better for everyone. Build your career with Gainwell, an industry leader. You’ll be joining a company where collaboration, innovation, and inclusion fuel our growth. Learn more about Gainwell at our company website and visit our Careers site for all available job role openings.
 
  Gainwell Technologies is committed to a diverse, equitable, and inclusive workplace. We are proud to be an Equal Opportunity Employer, where all qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We celebrate diversity and are dedicated to creating an inclusive environment for all employees.","<div>
 <p>Be part of a team that unleashes the power of leading-edge technologies to help improve the health and well-being of those most vulnerable in our country and communities. Working at Gainwell carries its rewards. You&#x2019;ll have an incredible opportunity to grow your career in a company that values work flexibility, learning, and career development. You&#x2019;ll add to your technical credentials and certifications while enjoying a generous, flexible vacation policy and educational assistance. We also have comprehensive leadership and technical development academies to help build your skills and capabilities.</p>
 <p></p>
 <div>
  <div>
   <div>
    <h2 class=""jobSectionHeader""><b><br> Summary</b></h2>
   </div>
   <div>
    <ul>
     <li> As a <b>App Data Capture/Scanning/Indexing Engineer</b> at Gainwell, you can contribute your skills as we harness the power of technology to help our clients improve the health and well-being of the members they serve &#x2014; a community&#x2019;s most vulnerable. Connect your passion with purpose, teaming with people who thrive on finding innovative solutions to some of healthcare&#x2019;s biggest challenges. Here are the details on this position.</li>
     <li> Guide on scanning and document-capturing the staff on how documents can be best captured. Analyzes, models, designs, develops and implements integration solutions for EDMS and HealthPAS Application suite for Medicaid sites. The position supports required Electronic Document Management Systems components providing automated workflow processing and document management processes crucial to the implementation and function of the Gainwell Medicaid Fiscal Agent services product. Serves as an Application Designer performing design, configuration, implementation, documentation, research, analysis, support, and other technical responsibilities supporting the role.</li>
    </ul>
   </div>
  </div>
  <div>
   <div>
    <h2 class=""jobSectionHeader""><b> Your role in our mission</b></h2>
   </div>
   <div>
    <p><b> Essential Job Functions</b></p>
    <ul>
     <li> Guide letter authoring staff on how letter requirements can be best implemented.</li>
     <li> Support across all legacy sites for the letter manager solution as a whole</li>
     <li> New site implementations, Solution upgrades, and enhancements</li>
     <li> Extensive knowledge of MS Visual Studio, MS.NET C#, ASP.NET, Visual Basic .NET, TSQL, Web Services, Entity Framework, jQuery, Bootstrap, RestAPI, SOAP, SSIS, MS SQL Server, SSIS, Java Script, XML, XSD, HTML, CSS</li>
     <li> Support ongoing reconciliation of the image and data transfer between the data entry vendor and Health PAS.</li>
     <li> Support ingesting images, Index Data, and document data into Health PAS.</li>
     <li> Knowledge of </li>
     <li>Experience with the following imaging software, include Microsoft Windows Server 2008 and 2012ing Installing and configuring software, testing, and troubleshooting.
      <ul>
       <li> Kodak Scanner 4200, 1860, 5850</li>
       <li> Kodak Capture Pro 4.1, 4.5, 5.4.1, 5.6</li>
      </ul></li>
    </ul>
   </div>
  </div>
  <div>
   <div>
    <h2 class=""jobSectionHeader""><b> What we&apos;re looking for</b></h2>
   </div>
   <div>
    <ul>
     <li> Experience and/or knowledge of document capture and storage systems required.</li>
     <li> Extensive knowledge of MS Visual Studio, MS.NET C#, ASP.NET, Visual Basic .NET, TSQL, Web Services, Entity Framework, jQuery, Bootstrap, RestAPI, SOAP, SSIS, MS SQL Server, SSIS, Java Script, XML, XSD, HTML, CSS</li>
     <li> Configure, test, and deploy our scanning and indexing system to allow for manual scanning and entry and automated receipt (via fax) of various documents to be committed into a document repository with the required index values for later retrieval by other systems. User security, third-party vendor submissions, and barcodes must also be addressed.</li>
     <li> Solid written and oral communication skills required for working with the district client, third-party vendors, and various Medicaid system project teams.</li>
     <li> Work with site and data entry vendor to define the protocol in a format for the transfer of images, and metadata to Health PAS Solution.</li>
    </ul>
   </div>
  </div>
  <div>
   <div>
    <h2 class=""jobSectionHeader""><b> What you should expect in this role</b></h2>
   </div>
   <div>
    <ul>
     <li> All USA locations will be considered including remote.</li>
     <li> Good experience in healthcare systems preferred</li>
    </ul>
    <p> #LI-DN1 #LI- Remote</p>
   </div>
  </div>
 </div>
 <p></p>
 <p><br> The pay range for this position is &#x24;99,000.00 - &#x24;123,800.00 per year, however, the base pay offered may vary depending on geographic region, internal equity, job-related knowledge, skills, and experience among other factors. Put your passion to work at Gainwell. You&#x2019;ll have the opportunity to grow your career in a company that values work flexibility, learning, and career development. All salaried, full-time candidates are eligible for our generous, flexible vacation policy, a 401(k) employer match, comprehensive health benefits, and educational assistance. We also have a variety of leadership and technical development academies to help build your skills and capabilities.</p>
 <p></p>
 <p><br> We believe nothing is impossible when you bring together people who care deeply about making healthcare work better for everyone. Build your career with Gainwell, an industry leader. You&#x2019;ll be joining a company where collaboration, innovation, and inclusion fuel our growth. Learn more about Gainwell at our company website and visit our Careers site for all available job role openings.</p>
 <p></p>
 <p><i><br> Gainwell Technologies is committed to a diverse, equitable, and inclusive workplace. We are proud to be an Equal Opportunity Employer, where all qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We celebrate diversity and are dedicated to creating an inclusive environment for all employees.</i></p>
</div>",https://jobs.gainwelltechnologies.com/job/Any-city-Data-Extraction-and-Indexing-Engineer-DC-99999/1054185600/?feedId=351800&utm_source=Indeed&utm_campaign=Gainwell_Indeed,08240a66dccbc643,,Full-time,,,"Washington, DC",Data Extraction and Indexing Engineer,30+ days ago,2023-09-20T12:40:25.229Z,2.7,178.0,"$99,000 - $123,800 a year",2023-10-20T12:40:25.231Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=08240a66dccbc643&from=jasx&tk=1hd6hi1iggaj8801&vjs=3
303,Cambia Health Solutions,"Senior Data Platform Engineer
 
 
   Location: WA, OR, UT, ID or remote within these 4 states
 
 
 
   Primary Job Purpose
 
 
   The Senior Data Platform Engineer V participates in a team environment for the delivery and maintenance of Cambia’s data platform (CDP). Responsible for programming, debugging, testing, and optimizing performance of data management processes (ETL/ELT) and database objects artifacts in a development environment. Generally responsible for ETL/ELT process design; unit, functional, and performance testing; and serving as a leader in technical and functional matters for other team members. This position provides technical, interpersonal, and administrative skills in performing day to day work.
 
 
 
   General Functions and Outcomes
 
 
   Develops efficient and maintainable program code.
   Develops efficient, effective, and maintainable program and system solutions in solving highly complex business problems.
   Directs creation of efficient, effective, and maintainable software by others.
   Provides consulting support for IT and Business partners by researching, identifying and resolving complex technical problems.
   Continuously seeks to improve job skills. Actively participates in the company's learning environment.
   Works on compensating for personal limits in both technical and professional soft skills.
   Meets established deadlines while maintaining a high level of quality. Negotiates changes to delivery dates or scope.
   Develops highly complex programs and systems in support of business processes and objectives.
   Prepares and/or directs the creation of system test plans, test criteria, and test data.
   Develops innovative solutions to problems.
   Understands, directs and participates in deliverables required by approved Development Lifecycles.
   Determines and/or directs system design and prepares work estimates for development or changes for multiple work efforts.
   Takes initiative and seeks out work.
   Directs the creation of program, system, operational and user documentation.
   Performs and/or directs testing and documents the results.
   Adheres to policies, procedures, and standards in place within IT/Engineering as well as all corporate policies, procedures and standards created by Cambia. Those include, but are not limited to, technical and architecture standards, production implementation standards, regular status reporting, regular participation in team, regular one on one meetings with Lead or Manager, and providing work estimates and regular time tracking.
   Expected deliverables include but are not limited to requirement analysis, system analysis, system design, data models, program design, source code development, test case development, testing, and documentation.
   May be responsible for on-call duties as defined by management.
   Drives continuous improvement.
   Mentors others to help build stronger team practices and stronger engineering skills within those teams.
   Ensures department meets commitments.
   Performs highly complex engineering duties including designing complex components and systems.
   Significantly contributes to the review of designs and code inspections.
   Lead or participate in architectural design for the team.
   Stays abreast of modern programming technologies and practices and incorporates them into our project work.
   Apply modern agile principles to continuously deliver value.
   Organizes and motivates teams to deliver high-quality, high-availability services.
   Creates training materials and classes.
 
 
   Minimum Requirements
 
 
   Communicate clearly in a timely manner in both verbal and written communication with other Software Development Engineers and Executives.
   Communicate effectively with peers.
   Able to work well with business customers and engineering and technical partners.
   Able to take direction from others in analyzing and solving program and system issues and problems.
   Highly technically proficient in core technologies and methodologies used by the team.
   Follow corporate and department coding standards and guidelines.
   Able to drive and champion rapidly changing technologies and methodologies and apply them to technological and/or business needs of significant scope.
   Able to provide technical direction and solutions to other team members. Recognizes differences of how senior managers think and work. Relays status to management and peers in a timely manner.
   Able to analyze, diagnose and resolve complex programming problems and system coordination issues.
   Able to work with minimum direction.
   Able to plan and manage his/her own work.
   Able to lead large multi-discipline teams (5+) for complex work efforts which includes estimating, coordinating, tracking progress, inspiring others to complete tasks on time, assisting with resolving issues and creating status reports. Creates a climate where people want to do their best. Can motivate and empower others on work teams.
   Able to learn continuous integration practices.
   Demonstrated expertise and ability to lead others in Test Driven Development including unit test creation.
   Demonstrated expertise and ability to lead others in defensive programming.
   Depending on business area, demonstrated experience of building Data pipeline: Data Exchange, Cleansing, Validation, Standardization, Search and Ranking based on Data Science.
   Ability to collaborate openly while solving technical problems.
   Ability to lead groups of business customers and technology partners.
   Ability to establish clarity from uncertainty and break down complex, open-ended problems.
   Excellent communication skills and the ability to collaborate effectively with a variety of technical and non-technical stakeholders.
   Anticipates problems and defines creative alternatives.
   Deep experience with multiple technologies
 
 
 
   Normally to be proficient in the competencies listed above
 
 
   Data Platform Engineer V would have a Bachelor's degree in Computer Science or Computer Engineering or related field and 10 years of experience in engineering or equivalent combination of education and experience. Experience will typically include significant involvement with one or more of the following key technologies: SQL, Snowflake, Oracle, Databricks, ETL/data integration tools and techniques, and data modeling (including dimensional, 3NF, and hierarchical), or equivalent combination of education and experience.
 
 
 
   Work Environment
 
 
   Work primarily performed in office environment.
   Travel may be required, locally or out of state.
   May be required to work outside normal hours and/or provide on call service.
 
 
 
   The expected hiring range for a Senior Data Platform Engineer is $129,500 - $175,500 depending on skills, experience, education, and training; relevant licensure / certifications; performance history; and work location. The bonus target for this position is 20%. The current full salary range for this role is $122,000 - $198,500.
 
 
 
   Base pay is just part of the compensation package at Cambia that is supplemented with an exceptional 401(k) match, bonus opportunity and other benefits. In keeping with our Cause and vision, we offer comprehensive well-being programs and benefits, which we periodically update to stay current. Some highlights:
 
 
 
  
   
     medical, dental, and vision coverage for employees and their eligible family members
   
  
   
     annual employer contribution to a health savings account ($1,200 or $2,500 depending on medical coverage, prorated based on hire date)
   
  
   
     paid time off varying by role and tenure in addition to 10 company holidays
   
  
   
     up to a 6% company match on employee 401k contributions, with a potential discretionary contribution based on company performance (no vesting period)
   
  
   
     up to 12 weeks of paid parental time off (eligible day one of employment if within first 12 months following birth or adoption)
   
  
   
     one-time furniture and equipment allowance for employees working from home
   
  
   
     up to $225 in Amazon gift cards for participating in various well-being activities. for a complete list see our 
    
     External Total Rewards
     page.
   
 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                       
                        
                         
                          
                           
                            
                             
                              
                               
                                
                                 
                                  
                                   
                                    
                                      We are an Equal Opportunity and Affirmative Action employer dedicated to workforce diversity and a drug and tobacco-free workplace. All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, age, sex, sexual orientation, gender identity, disability, protected veteran status or any other status protected by law. A background check is required.
                                    
                                    
                                    
                                      If you need accommodation for any part of the application process because of a medical condition or disability, please email 
                                     
                                      CambiaCareers@cambiahealth.com
                                     . Information about how Cambia Health Solutions collects, uses, and discloses information is available in our 
                                     
                                      Privacy Policy
                                     . As a health care company, we are committed to the health of our communities and employees during the COVID-19 pandemic. Please review the policy on our 
                                     
                                      Careers
                                      site.","<div>
 <div>
  Senior Data Platform Engineer
 </div>
 <div>
   Location: WA, OR, UT, ID or remote within these 4 states
 </div>
 <div></div>
 <div>
   Primary Job Purpose
 </div>
 <div>
   The Senior Data Platform Engineer V participates in a team environment for the delivery and maintenance of Cambia&#x2019;s data platform (CDP). Responsible for programming, debugging, testing, and optimizing performance of data management processes (ETL/ELT) and database objects artifacts in a development environment. Generally responsible for ETL/ELT process design; unit, functional, and performance testing; and serving as a leader in technical and functional matters for other team members. This position provides technical, interpersonal, and administrative skills in performing day to day work.
 </div>
 <div></div>
 <div>
   General Functions and Outcomes
 </div>
 <ul>
  <li> Develops efficient and maintainable program code.</li>
  <li> Develops efficient, effective, and maintainable program and system solutions in solving highly complex business problems.</li>
  <li> Directs creation of efficient, effective, and maintainable software by others.</li>
  <li> Provides consulting support for IT and Business partners by researching, identifying and resolving complex technical problems.</li>
  <li> Continuously seeks to improve job skills. Actively participates in the company&apos;s learning environment.</li>
  <li> Works on compensating for personal limits in both technical and professional soft skills.</li>
  <li> Meets established deadlines while maintaining a high level of quality. Negotiates changes to delivery dates or scope.</li>
  <li> Develops highly complex programs and systems in support of business processes and objectives.</li>
  <li> Prepares and/or directs the creation of system test plans, test criteria, and test data.</li>
  <li> Develops innovative solutions to problems.</li>
  <li> Understands, directs and participates in deliverables required by approved Development Lifecycles.</li>
  <li> Determines and/or directs system design and prepares work estimates for development or changes for multiple work efforts.</li>
  <li> Takes initiative and seeks out work.</li>
  <li> Directs the creation of program, system, operational and user documentation.</li>
  <li> Performs and/or directs testing and documents the results.</li>
  <li> Adheres to policies, procedures, and standards in place within IT/Engineering as well as all corporate policies, procedures and standards created by Cambia. Those include, but are not limited to, technical and architecture standards, production implementation standards, regular status reporting, regular participation in team, regular one on one meetings with Lead or Manager, and providing work estimates and regular time tracking.</li>
  <li> Expected deliverables include but are not limited to requirement analysis, system analysis, system design, data models, program design, source code development, test case development, testing, and documentation.</li>
  <li> May be responsible for on-call duties as defined by management.</li>
  <li> Drives continuous improvement.</li>
  <li> Mentors others to help build stronger team practices and stronger engineering skills within those teams.</li>
  <li> Ensures department meets commitments.</li>
  <li> Performs highly complex engineering duties including designing complex components and systems.</li>
  <li> Significantly contributes to the review of designs and code inspections.</li>
  <li> Lead or participate in architectural design for the team.</li>
  <li> Stays abreast of modern programming technologies and practices and incorporates them into our project work.</li>
  <li> Apply modern agile principles to continuously deliver value.</li>
  <li> Organizes and motivates teams to deliver high-quality, high-availability services.</li>
  <li> Creates training materials and classes.</li>
 </ul>
 <div>
   Minimum Requirements
 </div>
 <ul>
  <li> Communicate clearly in a timely manner in both verbal and written communication with other Software Development Engineers and Executives.</li>
  <li> Communicate effectively with peers.</li>
  <li> Able to work well with business customers and engineering and technical partners.</li>
  <li> Able to take direction from others in analyzing and solving program and system issues and problems.</li>
  <li> Highly technically proficient in core technologies and methodologies used by the team.</li>
  <li> Follow corporate and department coding standards and guidelines.</li>
  <li> Able to drive and champion rapidly changing technologies and methodologies and apply them to technological and/or business needs of significant scope.</li>
  <li> Able to provide technical direction and solutions to other team members. Recognizes differences of how senior managers think and work. Relays status to management and peers in a timely manner.</li>
  <li> Able to analyze, diagnose and resolve complex programming problems and system coordination issues.</li>
  <li> Able to work with minimum direction.</li>
  <li> Able to plan and manage his/her own work.</li>
  <li> Able to lead large multi-discipline teams (5+) for complex work efforts which includes estimating, coordinating, tracking progress, inspiring others to complete tasks on time, assisting with resolving issues and creating status reports. Creates a climate where people want to do their best. Can motivate and empower others on work teams.</li>
  <li> Able to learn continuous integration practices.</li>
  <li> Demonstrated expertise and ability to lead others in Test Driven Development including unit test creation.</li>
  <li> Demonstrated expertise and ability to lead others in defensive programming.</li>
  <li> Depending on business area, demonstrated experience of building Data pipeline: Data Exchange, Cleansing, Validation, Standardization, Search and Ranking based on Data Science.</li>
  <li> Ability to collaborate openly while solving technical problems.</li>
  <li> Ability to lead groups of business customers and technology partners.</li>
  <li> Ability to establish clarity from uncertainty and break down complex, open-ended problems.</li>
  <li> Excellent communication skills and the ability to collaborate effectively with a variety of technical and non-technical stakeholders.</li>
  <li> Anticipates problems and defines creative alternatives.</li>
  <li> Deep experience with multiple technologies</li>
 </ul>
 <div></div>
 <div>
  <i> Normally to be proficient in the competencies listed above</i>
 </div>
 <div>
   Data Platform Engineer V would have a Bachelor&apos;s degree in Computer Science or Computer Engineering or related field and 10 years of experience in engineering or equivalent combination of education and experience. Experience will typically include significant involvement with one or more of the following key technologies: SQL, Snowflake, Oracle, Databricks, ETL/data integration tools and techniques, and data modeling (including dimensional, 3NF, and hierarchical), or equivalent combination of education and experience.
 </div>
 <div></div>
 <div>
   Work Environment
 </div>
 <ul>
  <li> Work primarily performed in office environment.</li>
  <li> Travel may be required, locally or out of state.</li>
  <li> May be required to work outside normal hours and/or provide on call service.</li>
 </ul>
 <div></div>
 <div>
   The expected hiring range for a Senior Data Platform Engineer is &#x24;129,500 - &#x24;175,500 depending on skills, experience, education, and training; relevant licensure / certifications; performance history; and work location. The bonus target for this position is 20%. The current full salary range for this role is &#x24;122,000 - &#x24;198,500.
 </div>
 <div></div>
 <div>
   Base pay is just part of the compensation package at Cambia that is supplemented with an exceptional 401(k) match, bonus opportunity and other benefits. In keeping with our Cause and vision, we offer comprehensive well-being programs and benefits, which we periodically update to stay current. Some highlights:
 </div>
 <div></div>
 <ul>
  <li>
   <div>
     medical, dental, and vision coverage for employees and their eligible family members
   </div></li>
  <li>
   <div>
     annual employer contribution to a health savings account (&#x24;1,200 or &#x24;2,500 depending on medical coverage, prorated based on hire date)
   </div></li>
  <li>
   <div>
     paid time off varying by role and tenure in addition to 10 company holidays
   </div></li>
  <li>
   <div>
     up to a 6% company match on employee 401k contributions, with a potential discretionary contribution based on company performance (no vesting period)
   </div></li>
  <li>
   <div>
     up to 12 weeks of paid parental time off (eligible day one of employment if within first 12 months following birth or adoption)
   </div></li>
  <li>
   <div>
     one-time furniture and equipment allowance for employees working from home
   </div></li>
  <li>
   <div>
     up to &#x24;225 in Amazon gift cards for participating in various well-being activities. for a complete list see our 
    <div>
     External Total Rewards
    </div> page.
   </div></li>
 </ul>
 <div></div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                <div>
                 <div>
                  <div>
                   <div>
                    <div>
                     <div>
                      <div>
                       <div>
                        <div>
                         <div>
                          <div>
                           <div>
                            <div>
                             <div>
                              <div>
                               <div>
                                <div>
                                 <div>
                                  <div>
                                   <div>
                                    <div>
                                      We are an Equal Opportunity and Affirmative Action employer dedicated to workforce diversity and a drug and tobacco-free workplace. All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, age, sex, sexual orientation, gender identity, disability, protected veteran status or any other status protected by law. A background check is required.
                                    </div>
                                    <div></div>
                                    <div>
                                      If you need accommodation for any part of the application process because of a medical condition or disability, please email 
                                     <div>
                                      CambiaCareers@cambiahealth.com
                                     </div>. Information about how Cambia Health Solutions collects, uses, and discloses information is available in our 
                                     <div>
                                      Privacy Policy
                                     </div>. As a health care company, we are committed to the health of our communities and employees during the COVID-19 pandemic. Please review the policy on our 
                                     <div>
                                      Careers
                                     </div> site.
                                    </div>
                                   </div>
                                  </div>
                                 </div>
                                </div>
                               </div>
                              </div>
                             </div>
                            </div>
                           </div>
                          </div>
                         </div>
                        </div>
                       </div>
                      </div>
                     </div>
                    </div>
                   </div>
                  </div>
                 </div>
                </div>
               </div>
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>",https://cambiahealth.wd1.myworkdayjobs.com/en-US/External/job/Portland-OR/Senior-Data-Platform-Engineer_R-4162,cd82b2f3c6950df7,,Full-time,,,"Portland, OR",Senior Data Platform Engineer,30+ days ago,2023-09-20T12:40:26.704Z,3.5,202.0,"$122,000 - $198,500 a year",2023-10-20T12:40:26.706Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=cd82b2f3c6950df7&from=jasx&tk=1hd6hi1iggaj8801&vjs=3
304,LookFar Labs,"About the Role: We have an existing commercial SaaS platform that consists of 3 components: a web application, several 3rd party databases integrated into our backend, and a Natural Language Processing ML model based on a custom taxonomy.
We are looking to build 2.0 of our platform, with a brand new front end based on new algorithms, and scalable data science models that use a confluence of data from various data sources (e.g., patent, financial, and people). It’s a challenge and a fun opportunity for someone looking to make the next big platform that the world is going to use.
Our Data Engineer would need to create a new data pipeline, ETL process, and architecture for 2.0 of our platform. This could include multi-modal databases (including PostgreSQL and graph databases), and should consider the delineation between production, development, and staging/testing data pipelines and environments. The data pipeline should easily integrate new data sources, with both structured and unstructured data, and should enable associations between data as well. It should also enable and further enhance the strong entity resolution that we have already started building for our disparate, large data sets to be cleanly integrated.
You should also not rely solely on off the shelf tools or default pipelines. This role will require creativity and customization.
Your solutions should keep in mind scalability, to enable optimized usage of distributed computing frameworks like Spark. You should also have strong familiarity and experience with how to leverage the AWS ecosystem to bring in relevant AWS tools, services, and resources to enable substantial processing of very large datasets before runtime, entity resolution between very large datasets, and real-time processing in a scalable, distributed computing environment.
Role Responsibilities:

 Create and maintain a scalable ETL data pipeline that ingests multiple large datasets of both structured data (in the form of financial and patent data) and unstructured data (in the form of white papers, scraped websites, etc.), andenables entity resolution and other transformations for clean data integration andusage
 Create and maintain a multi-modal data storage system (including at least PostgreSQL, AWS architecture like S3, and graph databases) that enables scalable,real-time processing for production-level data
 Work with the data science team to enable ML Ops
 Have curiosity and passion for data, and the ability to efficiently query and obtain data via SQL
 Demonstrate a strong sense of ownership, of both technical and business

outcomes

 Assist dev and data science teams with processing and integrating data analysis
 Clearly document processes, methodologies, and tools usedExperience Required:
 B.S. in relevant technical degree
 Significant use and experience (at least 3-5 years) as a data engineer in the AWS ecosystem, including strong familiarity with structured and unstructured large datasets, enabling scalable and distributed compute, and ensuring real-time processingat scale
 Significant use and experience (at least 3-5 years) with writing complex SQL queries and analysis of data correlations
 At least a couple of years graph DB experience is required.
 Significant experience (at least 3-5 years) with the AWS ecosystem, including RedShift, Glue, and other tools, services, and resources that enable scalable,distributed compute
 Significant experience (at least 3-5 years) with multi-modal DB and ETL pipelines, including with graph databases
 Project management skills, ability to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform
 Excellent communication and story-telling skills (written and verbal)

Our Current Tech Stack: Please note this is not our future tech stack. AWS to host the infrastructure, including the CICD, SpringBoot, Angular, Python, PySpark, Kubernetes, EMR, Spark, Elasticsearch, RedShift, AWS (S3, Code Commit, Code Build, Code Deploy, EC2, EMR, etc.), Docker, Spacy, Scikit learn, Openpyxl, Streamlit, Watchdog, sklearn, seaborn, nltk, matplotlib, pandas, SQLAlchemy, and additional ML and python libraries.
This stack is subject to change as we build v2.0. We want to modernize and streamline our models, MLOps, code, deployment, front-end, and distributed processing capabilities.
Logistics:Geography, Work Status, Etc. The position is remote. The candidate must have the legal right to work in the United States on a W2.
Interview Process: We will conduct 3 rounds of interviews.

 First Round: Culture, fit, and background interview with the company Founders
 Second Round: Technical Interview
 Technical Project: Execute a small data engineering project, if selected for the third round of interview
 Third Round: In-Person Day in Washington D.C. (We will have the candidate fly out to D.C. to meet the founders and team.) Present the results of the data engineering project during the In-Person Day.

How to Apply: Please provide the following:

 Resume
 Cover Letter
 Any links to Git repositories or data engineering projects that we can review

About the Company: We are the source of truth for patent intelligence. Patents protect revenue and investment in the market. Given that, patent intelligence is not complete UNLESS it integrates financial and market data. We provide SaaS platforms that correlate multiple data sets (patent, financial, and people data) using scalable data science models, in order to answer fundamental questions related to patent and innovation strategy.
We provide patent intelligence to corporate IP departments and the defense sector. We are expanding to a larger commercial market, including technology transfer, venture capital, and financial institutions.
We are committed to creating a diverse environment and is proud to be an equal opportunity employer.
All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics.
Job Types: Permanent, Full-time
Pay: $115,000.00 - $145,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Paid time off

Compensation package:

 Yearly pay

Experience level:

 5 years
 6 years
 7 years
 8 years
 9 years

Schedule:

 Monday to Friday

Application Question(s):

 Are you able to work full-time in the US on a W2 without a need for work sponsorship?

Experience:

 multi-modal DB and ETL pipelines: 4 years (Required)
 AWS: 4 years (Required)
 SQL: 4 years (Required)
 working with structured & unstructured data: 4 years (Required)
 Redshift: 4 years (Required)
 ML Ops: 4 years (Required)
 Graph Databases: 3 years (Required)

Work Location: Remote","<p><b>About the Role: </b><br>We have an existing commercial SaaS platform that consists of 3 components: a web application, several 3rd party databases integrated into our backend, and a Natural Language Processing ML model based on a custom taxonomy.</p>
<p>We are looking to build 2.0 of our platform, with a brand new front end based on new algorithms, and scalable data science models that use a confluence of data from various data sources (e.g., patent, financial, and people). It&#x2019;s a challenge and a fun opportunity for someone looking to make the next big platform that the world is going to use.</p>
<p>Our Data Engineer would need to create a new data pipeline, ETL process, and architecture for 2.0 of our platform. This could include multi-modal databases (including PostgreSQL and graph databases), and should consider the delineation between production, development, and staging/testing data pipelines and environments. The data pipeline should easily integrate new data sources, with both structured and unstructured data, and should enable associations between data as well. It should also enable and further enhance the strong entity resolution that we have already started building for our disparate, large data sets to be cleanly integrated.</p>
<p>You should also not rely solely on off the shelf tools or default pipelines. This role will require creativity and customization.</p>
<p>Your solutions should keep in mind scalability, to enable optimized usage of distributed computing frameworks like Spark. You should also have strong familiarity and experience with how to leverage the AWS ecosystem to bring in relevant AWS tools, services, and resources to enable substantial processing of very large datasets before runtime, entity resolution between very large datasets, and real-time processing in a scalable, distributed computing environment.</p>
<p><b>Role Responsibilities:</b></p>
<ul>
 <li>Create and maintain a scalable ETL data pipeline that ingests multiple large datasets of both structured data (in the form of financial and patent data) and unstructured data (in the form of white papers, scraped websites, etc.), andenables entity resolution and other transformations for clean data integration andusage</li>
 <li>Create and maintain a multi-modal data storage system (including at least PostgreSQL, AWS architecture like S3, and graph databases) that enables scalable,real-time processing for production-level data</li>
 <li>Work with the data science team to enable ML Ops</li>
 <li><b>Have curiosity and passion for data</b>, and the ability to efficiently query and obtain data via SQL</li>
 <li>Demonstrate a strong sense of ownership, of both technical and business</li>
</ul>
<p>outcomes</p>
<ul>
 <li>Assist dev and data science teams with processing and integrating data analysis</li>
 <li>Clearly document processes, methodologies, and tools used<b>Experience Required:</b></li>
 <li>B.S. in relevant technical degree</li>
 <li>Significant use and experience (at least 3-5 years) as a data engineer in the AWS ecosystem, including strong familiarity with structured and unstructured large datasets, enabling scalable and distributed compute, and ensuring real-time processingat scale</li>
 <li>Significant use and experience (at least 3-5 years) with writing complex SQL queries and analysis of data correlations</li>
 <li>At least a couple of years graph DB experience is required.</li>
 <li>Significant experience (at least 3-5 years) with the AWS ecosystem, including RedShift, Glue, and other tools, services, and resources that enable scalable,distributed compute</li>
 <li>Significant experience (at least 3-5 years) with multi-modal DB and ETL pipelines, including with graph databases</li>
 <li>Project management skills, ability to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform</li>
 <li>Excellent communication and story-telling skills (written and verbal)</li>
</ul>
<p>Our Current Tech Stack: <b>Please note this is not our future tech stack.</b> AWS to host the infrastructure, including the CICD, SpringBoot, Angular, Python, PySpark, Kubernetes, EMR, Spark, Elasticsearch, RedShift, AWS (S3, Code Commit, Code Build, Code Deploy, EC2, EMR, etc.), Docker, Spacy, Scikit learn, Openpyxl, Streamlit, Watchdog, sklearn, seaborn, nltk, matplotlib, pandas, SQLAlchemy, and additional ML and python libraries.</p>
<p>This stack is subject to change as we build v2.0. We want to modernize and streamline our models, MLOps, code, deployment, front-end, and distributed processing capabilities.</p>
<p><b>Logistics:</b><br>Geography, Work Status, Etc. The position is remote. The candidate must have the legal right to work in the United States on a W2.</p>
<p><b>Interview Process:</b> We will conduct 3 rounds of interviews.</p>
<ul>
 <li>First Round: Culture, fit, and background interview with the company Founders</li>
 <li>Second Round: Technical Interview</li>
 <li>Technical Project: Execute a small data engineering project, if selected for the third round of interview</li>
 <li>Third Round: In-Person Day in Washington D.C. (We will have the candidate fly out to D.C. to meet the founders and team.) Present the results of the data engineering project during the In-Person Day.</li>
</ul>
<p><b>How to Apply: </b>Please provide the following:</p>
<ul>
 <li>Resume</li>
 <li>Cover Letter</li>
 <li>Any links to Git repositories or data engineering projects that we can review</li>
</ul>
<p><b>About the Company:</b> We are the source of truth for patent intelligence. Patents protect revenue and investment in the market. Given that, patent intelligence is not complete UNLESS it integrates financial and market data. We provide SaaS platforms that correlate multiple data sets (patent, financial, and people data) using scalable data science models, in order to answer fundamental questions related to patent and innovation strategy.</p>
<p>We provide patent intelligence to corporate IP departments and the defense sector. We are expanding to a larger commercial market, including technology transfer, venture capital, and financial institutions.</p>
<p>We are committed to creating a diverse environment and is proud to be an equal opportunity employer.</p>
<p>All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics.</p>
<p>Job Types: Permanent, Full-time</p>
<p>Pay: &#x24;115,000.00 - &#x24;145,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Are you able to work full-time in the US on a W2 without a need for work sponsorship?</li>
</ul>
<p>Experience:</p>
<ul>
 <li>multi-modal DB and ETL pipelines: 4 years (Required)</li>
 <li>AWS: 4 years (Required)</li>
 <li>SQL: 4 years (Required)</li>
 <li>working with structured &amp; unstructured data: 4 years (Required)</li>
 <li>Redshift: 4 years (Required)</li>
 <li>ML Ops: 4 years (Required)</li>
 <li>Graph Databases: 3 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,c0d7732f8a02f93e,,Permanent,Full-time,,Remote,Remote Sr. Data Engineer,30+ days ago,2023-09-20T12:40:37.956Z,,,"$115,000 - $145,000 a year",2023-10-20T12:40:37.959Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0D6GBkCh8qDOKp8Q8Je90DE9vUrmQqPUpTEHGF5kpqBlXdYvOV26Rfr3QQlsqhJNC2ByknejdY85tMHZqy12fKI9ayhdlRi_QvXMWCU0S-2wD8aryJK1JQi64fHaArw_PYN8ZT9U1TFXyL5GYHNm8xzyJuXSxjj328Ku3Hp4jtKKPp1B_T69CJsLLorPkNxEdqhO8lp64dxjBkc-W5R-qrpgUOyo1pUthWww_0DjuxxF2qtA-Ba0hzu-flPJ4Ti2SGnGXGDXcCpD64nmCtLRvk7YRw8YlIDd8WcWkjHkzUQWi7n0sjP6a-1-KG5LeKTkqlaS8SjRSsOlnb3SukYFIlnW_aLr6wwVsKmis96mQvOEiOgNVDOu3JgvN3CnQS7mZxWx8n2uClGC6W2qgu600TKAjLWih5f6RhVWdNMnTPrec4Ma8ZSaBt4PMDzFWjwFsKiS_e5F6be8zuNmResm7D7X_uvNvT8NzzBiJHnPM3xngqloNsIBltB98JJTjvc1uCC7JUAU7wqRmniLqUNkuon9kr0h1N4YAOLgfdXyxbLO-oIz7e2Ew13vNwlwF6U6eATLCRmUX4Ma7g9NEP1fsD3&xkcb=SoCj-_M3JmrH7SxcpR0PbzkdCdPP&p=4&fvj=1&vjs=3&jsa=7574&tk=1hd6hi1iggaj8801&from=jasx&wvign=1
305,Sentara Healthcare,"Sentara Healthcare is seeking to hire a qualified individual to join our team as a Cyber Security Data Protection Engineer.
        
         Position Status: Full-time, Day Shift
        
         Position Location: This position is 100% remote.
        
         Standard Working Hours: 8:00AM to 5:00PM (ET).
        
         Minimum Requirements:
        
          Practical working knowledge of engineering, implementing, operating, and supporting a Data Loss Prevention Platform. 
         Understanding of end-to-end implications of DLP implementation from business requirements to implementation challenges to constituent education to continual service improvement and support. 
         Experience with data protection and cloud tooling (DLP, Data Security, Encryption and CASB).
          A working knowledge of Cloud Security and supporting Technologies (e.g., AWS, Azure, GCP, SaaS, PaaS, DBaaS), particularly Data Loss Prevention within Microsoft O365, Azure Information Protection & AWS. 
         Proven experience of information protection and data classification technologies, concepts, and techniques to classify unstructured and structured data both on-premises and in the cloud.
          Understanding of vulnerability assessment tools, methodologies, and frameworks. 
         Ability to analyze and interpret vulnerability scan results and data protection requirements, identify risks, and recommend appropriate actions.
        
        
         Diversity and Inclusion at Sentara 
        Our vision is that everyone brings the strengths that come with diversity to work with them every day. When we are achieving our vision, we have team members that feel they belong and can be their authentic selves, and our workforce is reflective of the communities we serve.
         We are realizing this vision through our Diversity and Inclusion strategy, which has three pillars: A diverse and talented workforce, an inclusive and supportive workplace, and outreach and engagement with our community. We have made remarkable strides in these areas over the past several years and, as our world continues to evolve, we know our work is never done.
         Our strategies focus on both structural inclusion, which looks at our organizational structures, processes, and practices; as well as behavioral inclusion, which evaluates our mindsets, skillsets, and relationships. Together, these strategies are moving our organization forward in an environment that fosters a culture of mutual respect and belonging for all.
         Please visit the link below to learn more about Sentara’s commitment to diversity and inclusion:
        
         https://www.sentara.com/aboutus/mission-vision-and-values/diversity.aspx
        
         Sentara Overview For more than a decade, Modern Healthcare magazine has ranked Sentara Healthcare as one of the nation's top integrated healthcare systems. That's because we are dedicated to growth, innovation, and patient safety at more than 300 sites of care in Virginia and northeastern North Carolina, including 12 acute care hospitals.  Sentara Benefits As the third-largest employer in Virginia, Sentara Healthcare was named by Forbes Magazine as one of America's best large employers. We offer a variety of amenities to our employees, including, but not limited to:
        
        
          Medical, Dental, and Vision Insurance
          Paid Annual Leave, Sick Leave
          Flexible Spending Accounts
          Retirement funds with matching contribution
          Supplemental insurance policies, including legal, Life Insurance and AD&D among others
          Work Perks program including discounted movie and theme park tickets among other great deals
          Opportunities for further advancement within our organization
        
        
         Sentara employees strive to make our communities healthier places to live. We're setting the standard for medical excellence within a vibrant, creative, and highly productive workplace. For information about our employee benefits, please visit: Benefits - Sentara (sentaracareers.com)  Join our team! We are committed to quality healthcare, improving health every day, and provide the opportunity for training, development, and growth!
        
         Please Note: The Covid Vaccination(s) and yearly Flu Vaccination are required for employment.
        
         Note: Sentara Healthcare offers employees comprehensive health care and retirement benefits designed with you and your family's well-being in mind. Our benefits packages are designed to change with you by meeting your needs now and anticipating what comes next. You have a variety of options for medical, dental and vision insurance, life insurance, disability, and voluntary benefits as well as Paid Time Off in the form of sick time, vacation time and paid parental leave. Team Members have the opportunity to earn an annual flat amount Bonus payment if established system and employee eligibility criteria is met.
        
         For applicants within Washington State, the following hiring range will be applied: $70,215.60 to $117,026.00 annually.
       
      
     
    
  
 
 
   As the Cyber Security Data Protection Engineer, you will be responsible for designing, deploying, and maintaining technology to support Sentara Health’s data protection and vulnerability management strategies. As a Cyber Security Engineer focusing on vulnerability management and data protection, you will play a critical role in ensuring the security and integrity of our organization's sensitive data and information assets. Your responsibilities will include conducting vulnerability assessments, implementing data protection controls, and collaborating with cross-functional teams to mitigate risks. An Experienced Professional applies practical knowledge of job areas typically obtained through advanced education and work experience. Responsibilities typically include: • Works independently with general supervision. • Problems faced are difficult but typically not complex. • May influence others within the job area through explanation of facts, policies, and practices. Experience in lieu of Bachelor’s Degree 3 years of relevant experience with a degree 5+ years of relevant experience without a degree
 
 
 
  
    Experience:
  
  
    Practical working knowledge of engineering, implementing, operating, and supporting a Data Loss Prevention Platform. Understanding of end-to-end implications of DLP implementation from business requirements to implementation challenges to constituent education to continual service improvement and support. Experience with data protection and cloud tooling (DLP, Data Security, Encryption and CASB). A working knowledge of Cloud Security and supporting Technologies (e.g., AWS, Azure, GCP, SaaS, PaaS, DBaaS), particularly Data Loss Prevention within Microsoft O365, Azure Information Protection & AWS. Proven experience of information protection and data classification technologies, concepts, and techniques to classify unstructured and structured data both on-premises and in the cloud. Understanding of vulnerability assessment tools, methodologies, and frameworks. Ability to analyze and interpret vulnerability scan results and data protection requirements, identify risks, and recommend appropriate actions.
  
  
    Other Minimum Qualifications:
  
  
    Experience in monitoring and supporting IDS/IPS, Firewall, SIEM, DLP, vulnerability management tools, and log aggregation hardware/software required. Understanding of networking and infrastructure and understanding of core technologies such as IP Networking, L2/L3 network protocols (OSPF, BGP), LAN/WAN, TCP/IP, OSI Model, route, switch, DNS, DHCP, Domain Controllers, LDAP, SSO, QOS, VLAN, and ACL Basic working knowledge of IAM technologies such as AD, AAD, SAML, OAUTH, LDAP, Kerberos, and OpenID Basic working knowledge of Access Control and supporting concepts such as the principle of least privilege, RBAC, and MFA Solid working knowledge of ITIL (ITIL Certification preferred) A good understanding of Industry Security standards Exceptional interpersonal skills; must build strong relationships with partners (internally and externally) Strong problem solving and troubleshooting skills with the ability to exercise mature judgment Proven execution capabilities. Willingness to creatively ensure mission success
  
 
 
 
  
    Bachelor's Level Degree
  
 
 
  
    Information Technology 3 years
  
 
 
  
    Complex Problem Solving
    Coordination
    Critical Thinking
    Installation
    Judgment and Decision Making
    Microsoft Office
    Monitoring
    Service Orientation
    Speaking
    Systems Analysis
    Systems Evaluation
    Time Management
    Troubleshooting
    Writing
    Equipment Selection
    Instructing
    Leadership
    Project Management
    Technology Design
    Active Listening
    Communication","<div>
 <div>
  <ul>
   <li>
    <div>
     <div>
      <div>
       <div>
        <p><b>Sentara Healthcare</b> is seeking to hire a qualified individual to join our team as a <b>Cyber </b><b>Security Data Protection Engineer.</b></p>
        <p></p>
        <p><b> Position Status</b>: Full-time, Day Shift</p>
        <p></p>
        <p><b> Position Location:</b> This position is 100% remote.</p>
        <p></p>
        <p><b> Standard Working Hours</b>: 8:00AM to 5:00PM (ET).</p>
        <p></p>
        <p><b> Minimum Requirements:</b></p>
        <ul>
         <li> Practical working knowledge of engineering, implementing, operating, and supporting a Data Loss Prevention Platform. </li>
         <li>Understanding of end-to-end implications of DLP implementation from business requirements to implementation challenges to constituent education to continual service improvement and support. </li>
         <li>Experience with data protection and cloud tooling (DLP, Data Security, Encryption and CASB).</li>
         <li> A working knowledge of Cloud Security and supporting Technologies (e.g., AWS, Azure, GCP, SaaS, PaaS, DBaaS), particularly Data Loss Prevention within Microsoft O365, Azure Information Protection &amp; AWS. </li>
         <li>Proven experience of information protection and data classification technologies, concepts, and techniques to classify unstructured and structured data both on-premises and in the cloud.</li>
         <li> Understanding of vulnerability assessment tools, methodologies, and frameworks. </li>
         <li>Ability to analyze and interpret vulnerability scan results and data protection requirements, identify risks, and recommend appropriate actions.</li>
        </ul>
        <p></p>
        <p><b> Diversity and Inclusion at Sentara </b></p>
        <p>Our vision is that everyone brings the strengths that come with diversity to work with them every day. When we are achieving our vision, we have team members that feel they belong and can be their authentic selves, and our workforce is reflective of the communities we serve.</p>
        <p> We are realizing this vision through our Diversity and Inclusion strategy, which has three pillars: A diverse and talented workforce, an inclusive and supportive workplace, and outreach and engagement with our community. We have made remarkable strides in these areas over the past several years and, as our world continues to evolve, we know our work is never done.</p>
        <p> Our strategies focus on both <i>structural inclusion</i>, which looks at our organizational structures, processes, and practices; as well as <i>behavioral inclusion</i>, which evaluates our mindsets, skillsets, and relationships. Together, these strategies are moving our organization forward in an environment that fosters a culture of mutual respect and belonging for all.</p>
        <p><b> Please visit the link below to learn more about Sentara&#x2019;s commitment to diversity and inclusion:</b></p>
        <p></p>
        <p> https://www.sentara.com/aboutus/mission-vision-and-values/diversity.aspx</p>
        <p></p>
        <p><b> Sentara Overview</b><br> For more than a decade, Modern Healthcare magazine has ranked Sentara Healthcare as one of the nation&apos;s top integrated healthcare systems. That&apos;s because we are dedicated to growth, innovation, and patient safety at more than 300 sites of care in Virginia and northeastern North Carolina, including 12 acute care hospitals.<br> <br> <b>Sentara Benefits</b><br> As the third-largest employer in Virginia, Sentara Healthcare was named by Forbes Magazine as one of America&apos;s best large employers. We offer a variety of amenities to our employees, including, but not limited to:</p>
        <p></p>
        <ul>
         <li> Medical, Dental, and Vision Insurance</li>
         <li> Paid Annual Leave, Sick Leave</li>
         <li> Flexible Spending Accounts</li>
         <li> Retirement funds with matching contribution</li>
         <li> Supplemental insurance policies, including legal, Life Insurance and AD&amp;D among others</li>
         <li> Work Perks program including discounted movie and theme park tickets among other great deals</li>
         <li> Opportunities for further advancement within our organization</li>
        </ul>
        <p></p>
        <p> Sentara employees strive to make our communities healthier places to live. We&apos;re setting the standard for medical excellence within a vibrant, creative, and highly productive workplace. For information about our employee benefits, please visit: Benefits - Sentara (sentaracareers.com)<br> <br> Join our team! We are committed to quality healthcare, improving health every day, and provide the opportunity for training, development, and growth!</p>
        <p></p>
        <p><b> Please Note</b><b>:</b> The Covid Vaccination(s) and yearly Flu Vaccination are required for employment.</p>
        <p></p>
        <p><b><i> Note:</i></b><i> Sentara Healthcare offers employees comprehensive health care and retirement benefits designed with you and your family&apos;s well-being in mind. Our benefits packages are designed to change with you by meeting your needs now and anticipating what comes next. You have a variety of options for medical, dental and vision insurance, life insurance, disability, and voluntary benefits as well as Paid Time Off in the form of sick time, vacation time and paid parental leave. Team Members have the opportunity to earn an annual flat amount Bonus payment if established system and employee eligibility criteria is met.</i></p>
        <p></p>
        <p> For applicants within Washington State, the following hiring range will be applied: &#x24;70,215.60 to &#x24;117,026.00 annually.</p>
       </div>
      </div>
     </div>
    </div></li>
  </ul>
 </div>
 <div>
   As the Cyber Security Data Protection Engineer, you will be responsible for designing, deploying, and maintaining technology to support Sentara Health&#x2019;s data protection and vulnerability management strategies. As a Cyber Security Engineer focusing on vulnerability management and data protection, you will play a critical role in ensuring the security and integrity of our organization&apos;s sensitive data and information assets. Your responsibilities will include conducting vulnerability assessments, implementing data protection controls, and collaborating with cross-functional teams to mitigate risks. An Experienced Professional applies practical knowledge of job areas typically obtained through advanced education and work experience. Responsibilities typically include: &#x2022; Works independently with general supervision. &#x2022; Problems faced are difficult but typically not complex. &#x2022; May influence others within the job area through explanation of facts, policies, and practices. Experience in lieu of Bachelor&#x2019;s Degree 3 years of relevant experience with a degree 5+ years of relevant experience without a degree
 </div>
 <div></div>
 <div>
  <div>
   <p><b> Experience:</b></p>
  </div>
  <div>
   <p> Practical working knowledge of engineering, implementing, operating, and supporting a Data Loss Prevention Platform. Understanding of end-to-end implications of DLP implementation from business requirements to implementation challenges to constituent education to continual service improvement and support. Experience with data protection and cloud tooling (DLP, Data Security, Encryption and CASB). A working knowledge of Cloud Security and supporting Technologies (e.g., AWS, Azure, GCP, SaaS, PaaS, DBaaS), particularly Data Loss Prevention within Microsoft O365, Azure Information Protection &amp; AWS. Proven experience of information protection and data classification technologies, concepts, and techniques to classify unstructured and structured data both on-premises and in the cloud. Understanding of vulnerability assessment tools, methodologies, and frameworks. Ability to analyze and interpret vulnerability scan results and data protection requirements, identify risks, and recommend appropriate actions.</p>
  </div>
  <div>
   <p><b> Other </b><b>Minimum Qualifications</b><b>:</b></p>
  </div>
  <div>
   <p> Experience in monitoring and supporting IDS/IPS, Firewall, SIEM, DLP, vulnerability management tools, and log aggregation hardware/software required. Understanding of networking and infrastructure and understanding of core technologies such as IP Networking, L2/L3 network protocols (OSPF, BGP), LAN/WAN, TCP/IP, OSI Model, route, switch, DNS, DHCP, Domain Controllers, LDAP, SSO, QOS, VLAN, and ACL Basic working knowledge of IAM technologies such as AD, AAD, SAML, OAUTH, LDAP, Kerberos, and OpenID Basic working knowledge of Access Control and supporting concepts such as the principle of least privilege, RBAC, and MFA Solid working knowledge of ITIL (ITIL Certification preferred) A good understanding of Industry Security standards Exceptional interpersonal skills; must build strong relationships with partners (internally and externally) Strong problem solving and troubleshooting skills with the ability to exercise mature judgment Proven execution capabilities. Willingness to creatively ensure mission success</p>
  </div>
 </div>
 <div></div>
 <div>
  <ul>
   <li> Bachelor&apos;s Level Degree</li>
  </ul>
 </div>
 <div>
  <ul>
   <li> Information Technology 3 years</li>
  </ul>
 </div>
 <div>
  <ul>
   <li> Complex Problem Solving</li>
   <li> Coordination</li>
   <li> Critical Thinking</li>
   <li> Installation</li>
   <li> Judgment and Decision Making</li>
   <li> Microsoft Office</li>
   <li> Monitoring</li>
   <li> Service Orientation</li>
   <li> Speaking</li>
   <li> Systems Analysis</li>
   <li> Systems Evaluation</li>
   <li> Time Management</li>
   <li> Troubleshooting</li>
   <li> Writing</li>
   <li> Equipment Selection</li>
   <li> Instructing</li>
   <li> Leadership</li>
   <li> Project Management</li>
   <li> Technology Design</li>
   <li> Active Listening</li>
   <li> Communication</li>
  </ul>
 </div>
</div>",https://www.sentaracareers.com/job/18839114/cyber-security-data-protection-engineer-virginia-beach-va/?utm_medium=symphonytalent-jobads&utm_campaign=Default%20Campaign&utm_content=Cyber%20Security%20Data%20Protection%20Engineer&utm_term=JR-36011&utm_source=Indeed,3d3d61d35b4b3598,,Full-time,,,"5460 Wesleyan Dr, Virginia Beach, VA 23455",Cyber Security Data Protection Engineer,30+ days ago,2023-09-20T12:40:29.665Z,3.8,2751.0,"$70,216 - $117,026 a year",2023-10-20T12:40:29.667Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=3d3d61d35b4b3598&from=jasx&tk=1hd6hi1iggaj8801&vjs=3
306,"ESRI, Inc.","Overview: 
 
   In this position, you will bring locations to life with ArcGIS Velocity, Workflow Manager, and Geotrigger. You will work to identify and define customer requirements and build and release high quality software. In addition, you'll have the opportunity to run automation tests, monitor tests, and perform scripting using Python.
 
 
   The Professional Services division is the consulting and implementation arm of Esri. We break ground in new markets, push the technology envelope, and ultimately deliver transformational solutions to high-profile clients worldwide. The Professional Services organization is comprised of nearly 1,000 talented business and technical professionals who strive every day to help our users be successful.
  Responsibilities: 
 
  Work with software developers to design, build, test, and release high quality software
   Communicate with product users to identify product requirements and advocate for their needs throughout the software development lifecycle
   Research throughout industry standards and specifications to translate requirements to software design
   Develop support systems to ensure quality software through robust automated functional and performance testing
   Provide best practices, user documentation, demonstrations, and technical assistance for the product
   Collaborate in focused team efforts the design lifecycle
   Leverage the knowledge of the target audience to better understand business trends, customer communities, and go-to-market strategies
  Requirements: 
 
  5+ years of professional experience in a similar position supporting similar responsibilities
   Experience with bringing teams together to perform a task (for example: project coordination or project management)
   Enthusiasm for improving software capabilities for end users in real-world application
   Bachelor`s in Geographic Information Systems (GIS), Geography, Computer Science, or STEM related field
  Recommended Qualifications: 
 
  Experience using ArcGIS GeoEvent Server and/or ArcGIS Velocity, ArcGIS Enterprise, ArcGIS Online, and ArcGIS Pro
   Experience with complex enterprise systems
   Master`s in Geographic Information Systems (GIS), Geography, Computer Science or STEM related field
  The Company: 
 
   Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.
   
   Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.
   
   If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.
   
   Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
 
 
 
   A reasonable estimate of the base salary range is $89,440.00 - $147,680.00.
 
 
 
   #LI-MN1
 
 
   #LI-REMOTE","<div>
 Overview: 
 <div>
   In this position, you will bring locations to life with ArcGIS Velocity, Workflow Manager, and Geotrigger. You will work to identify and define customer requirements and build and release high quality software. In addition, you&apos;ll have the opportunity to run automation tests, monitor tests, and perform scripting using Python.
 </div>
 <div>
   The Professional Services division is the consulting and implementation arm of Esri. We break ground in new markets, push the technology envelope, and ultimately deliver transformational solutions to high-profile clients worldwide. The Professional Services organization is comprised of nearly 1,000 talented business and technical professionals who strive every day to help our users be successful.
 </div> Responsibilities: 
 <ul>
  <li>Work with software developers to design, build, test, and release high quality software</li>
  <li> Communicate with product users to identify product requirements and advocate for their needs throughout the software development lifecycle</li>
  <li> Research throughout industry standards and specifications to translate requirements to software design</li>
  <li> Develop support systems to ensure quality software through robust automated functional and performance testing</li>
  <li> Provide best practices, user documentation, demonstrations, and technical assistance for the product</li>
  <li> Collaborate in focused team efforts the design lifecycle</li>
  <li> Leverage the knowledge of the target audience to better understand business trends, customer communities, and go-to-market strategies</li>
 </ul> Requirements: 
 <ul>
  <li>5+ years of professional experience in a similar position supporting similar responsibilities</li>
  <li> Experience with bringing teams together to perform a task (for example: project coordination or project management)</li>
  <li> Enthusiasm for improving software capabilities for end users in real-world application</li>
  <li> Bachelor`s in Geographic Information Systems (GIS), Geography, Computer Science, or STEM related field</li>
 </ul> Recommended Qualifications: 
 <ul>
  <li>Experience using ArcGIS GeoEvent Server and/or ArcGIS Velocity, ArcGIS Enterprise, ArcGIS Online, and ArcGIS Pro</li>
  <li> Experience with complex enterprise systems</li>
  <li> Master`s in Geographic Information Systems (GIS), Geography, Computer Science or STEM related field</li>
 </ul> The Company: 
 <div>
   Our passion for improving quality of life through geography is at the heart of everything we do. Esri&#x2019;s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.
  <br> 
  <br> Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.
  <br> 
  <br> If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.
  <br> 
  <br> Esri&#x2019;s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
 </div>
 <div></div>
 <div>
  <br> A reasonable estimate of the base salary range is &#x24;89,440.00 - &#x24;147,680.00.
 </div>
 <div></div>
 <div>
  <br> #LI-MN1
 </div>
 <div>
   #LI-REMOTE
 </div>
</div>",https://external-esri.icims.com/jobs/18648/job?utm_source=indeed_integration&iis=Job%20Board&iisn=Indeed&indeed-apply-token=73a2d2b2a8d6d5c0a62696875eaebd669103652d3f0c2cd5445d3e66b1592b0f,ad5d4ac0639f9242,,Full-time,,,Remote,Sr. Product Engineer - Real-Time & Big Data,30+ days ago,2023-09-20T12:40:38.697Z,3.8,183.0,"$89,440 - $147,680 a year",2023-10-20T12:40:38.699Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=ad5d4ac0639f9242&from=jasx&tk=1hd6hi1iggaj8801&vjs=3
310,The Aerospace Corporation,"The Aerospace Corporation is the trusted partner to the nation’s space programs, solving the hardest problems and providing unmatched technical expertise. As the operator of a federally funded research and development center (FFRDC), we are broadly engaged across all aspects of space— delivering innovative solutions that span satellite, launch, ground, and cyber systems for defense, civil and commercial customers. When you join our team, you’ll be part of a special collection of problem solvers, thought leaders, and innovators. Join us and take your place in space.
 
 
 
   At Aerospace, we are committed to providing an inclusive and diverse workplace for all employees to share in our common passion and aspiration – to carry out a mission much bigger than ourselves.
 
 
 
   The Aerospace Corporation is hiring a Computer Science/Data Engineer for a Full-Time Remote or Remote-Hybrid (El Segundo, CA) opportunity in the Data Platforms and Architectures Department.
 
 
 
   Job Summary
 
 
   Data is at the heart of the space enterprise, and its analysis is crucial to scientific advancement and mission success. In this role you will work closely with subject matter experts across the organization to address real-world mission challenges related to the strategic analysis, processing, and visualization of data. You will design and implement user experiences that effectively communicate data analysis insights through interpretive narratives and interactive visualizations.
 
 
 
   What You’ll Be Doing
 
 
   Supporting the national, security, and commercial space enterprise through the identification, prototyping, assessment, and implementation of cutting-edge data engineering technologies in a cloud-native environment.
   Collaborating with technical subject matter experts and interdisciplinary teams to conceive, design, and develop enterprise data analysis pipelines and applications.
   Learning by doing, acquiring expertise in emerging technologies to further personal and organizational breadth of knowledge in an academic environment where self-enrichment, training, and mentorship is fundamental.
   Proposing innovative research and development projects, leading research and proof-of-concept development as the principal investigator.
 
 
 
   What Skills You’ll Bring
 
 
   Strong written and oral communication and interpersonal skills for coordinating efforts and working with members of various internal and external organizations.
   Strong reading comprehension, research, and analytical skills for understanding software documentation and source code, interpreting project requirements, and learning new technologies according to project demand.
   Integrity, self-motivation, initiative, and a collaborative approach to working in a team environment and proactively communicating progress and concerns to teammates and project stakeholders.
   Organizational, time management, and project management skills, including knowledge of software engineering lifecycles and familiarity with project management methodologies.
   Adaptability to working in a dynamic environment where duties, responsibilities, and activities vary in response to fluctuating organizational needs.
   Flexibility in completing projects with concurrent timelines, accurately estimating task duration, and working independently on execution.
   Demonstration of behavior consistent with the company’s values: Dedication to Mission Success, Technical Excellence, Commitment to Our People, Objectivity and Integrity, and Innovation.
 
 
 
   What You Need to be Successful
 
 
   Minimum Requirements:
 
 
  
   
     2+ years of software development industry experience.
   
  
   
     Bachelors from an accredited program in Computer Science or Computer Engineering.
   
   Intermediate to advanced knowledge of one or more programming languages, including Python and/or Java.
   Backend development project experience or course work in data structures and algorithms.
   Experience developing in Unix-like environments and using source/version control (Git).
   This position requires the ability to obtain and maintain a security clearance issued by the US government. U.S. citizenship is required to obtain a security clearance.
 
 
 
   How You Can Stand Out
 
 
   It would be impressive if you have one or more of these qualifications:
 
 
   Advanced degree (MS or PhD) from an accredited program in Computer Science or Computer Engineering.
   Intermediate to advanced knowledge of one or more scripting languages, including JavaScript or Typescript.
   Frontend library/framework development experience (e.g. React).
   Familiarity with conventional UI/UX design and/or effective data visualization design patterns.
   Experience developing REST APIs or application-database integration programming.
   Knowledge of data architectures and platforms, data mesh, data fabric, data pipelines, data engineering, data management, or data governance.
   Familiarity with data semantics, knowledge engineering, or virtual knowledge graphing (VSG).
   Experience building cloud-native applications, virtual environments, microservice architectures, containerized platforms, infrastructure as code, or automated deployments (e.g. Kubernetes, Docker, Argo).
   Experience implementing unit, integration, or functional software testing.
   Active US Government security clearance.
 
 
 
   We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. The grade-based pay range for this job is listed below. Individual salaries within that range are determined through a wide variety of factors including but not limited to education, experience, knowledge and skills.
 
 
   (Min - Mid - Max)
  $71,000 - $97,350 - $123,700
  Pay Basis: Annual
 
 
   Ways We Reward Our Employees
 
 
   During your interview process, our team will provide details of our industry-leading benefits.
 
 
   Benefits vary and are applicable based on Job Type. 
  A few highlights include:
 
 
  
   
     Comprehensive health care and wellness plans
   
  
   
     Paid holidays, sick time, and vacation
   
  
   
     Standard and alternate work schedules, including telework options
   
  
   
     401(k) Plan — Employees receive a total company-paid benefit of 8%, 10%, or 12% of eligible compensation based on years of service and matching contributions; employees are immediately eligible and vested in the plan upon hire
   
  
   
     Flexible spending accounts
   
  
   
     Variable pay program for exceptional contributions
   
  
   
     Relocation assistance
   
  
   
     Professional growth and development programs to help advance your career
   
  
   
     Education assistance programs
   
  
   
     An inclusive work environment built on teamwork, flexibility, and respect
   
 
 
 
   We are all unique, from diverse backgrounds and all walks of life, yet one thing bonds all of us to each other—the belief that we can make a difference. This core belief empowers us to do our best work at The Aerospace Corporation.
 
 
 
   Equal Opportunity Commitment
 
 
   The Aerospace Corporation is an Equal Opportunity/Affirmative Action employer. We believe that a diverse workforce creates an environment in which unique ideas are developed and differing perspectives are valued, producing superior customer solutions. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, age, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender, gender identity or expression, color, religion, genetic information, marital status, ancestry, national origin, protected veteran status, physical disability, medical condition, mental disability, or disability status and any other characteristic protected by state or federal law. If you’re an individual with a disability or a disabled veteran who needs assistance using our online job search and application tools or need reasonable accommodation to complete the job application process, please contact us by phone at 310.336.5432 or by email at 
  
   ieo.mailbox@aero.org
  . You can also review 
  
   Know Your Rights
  
  
   : Workplace Discrimination is Illegal
  , as well as the 
  
   Pay Transparency Policy Statement
  .","<div>
 <div>
  The Aerospace Corporation is the trusted partner to the nation&#x2019;s space programs, solving the hardest problems and providing unmatched technical expertise. As the operator of a federally funded research and development center (FFRDC), we are broadly engaged across all aspects of space&#x2014; delivering innovative solutions that span satellite, launch, ground, and cyber systems for defense, civil and commercial customers. When you join our team, you&#x2019;ll be part of a special collection of problem solvers, thought leaders, and innovators. Join us and take your place in space.
 </div>
 <div></div>
 <div>
   At Aerospace, we are committed to providing an inclusive and diverse workplace for all employees to share in our common passion and aspiration &#x2013; to carry out a mission much bigger than ourselves.
 </div>
 <div></div>
 <div>
   The Aerospace Corporation is hiring a Computer Science/Data Engineer for a Full-Time Remote or Remote-Hybrid (El Segundo, CA) opportunity in the Data Platforms and Architectures Department.
 </div>
 <div></div>
 <div>
   Job Summary
 </div>
 <div>
   Data is at the heart of the space enterprise, and its analysis is crucial to scientific advancement and mission success. In this role you will work closely with subject matter experts across the organization to address real-world mission challenges related to the strategic analysis, processing, and visualization of data. You will design and implement user experiences that effectively communicate data analysis insights through interpretive narratives and interactive visualizations.
 </div>
 <div></div>
 <div>
   What You&#x2019;ll Be Doing
 </div>
 <ul>
  <li> Supporting the national, security, and commercial space enterprise through the identification, prototyping, assessment, and implementation of cutting-edge data engineering technologies in a cloud-native environment.</li>
  <li> Collaborating with technical subject matter experts and interdisciplinary teams to conceive, design, and develop enterprise data analysis pipelines and applications.</li>
  <li> Learning by doing, acquiring expertise in emerging technologies to further personal and organizational breadth of knowledge in an academic environment where self-enrichment, training, and mentorship is fundamental.</li>
  <li> Proposing innovative research and development projects, leading research and proof-of-concept development as the principal investigator.</li>
 </ul>
 <div></div>
 <div>
   What Skills You&#x2019;ll Bring
 </div>
 <ul>
  <li> Strong written and oral communication and interpersonal skills for coordinating efforts and working with members of various internal and external organizations.</li>
  <li> Strong reading comprehension, research, and analytical skills for understanding software documentation and source code, interpreting project requirements, and learning new technologies according to project demand.</li>
  <li> Integrity, self-motivation, initiative, and a collaborative approach to working in a team environment and proactively communicating progress and concerns to teammates and project stakeholders.</li>
  <li> Organizational, time management, and project management skills, including knowledge of software engineering lifecycles and familiarity with project management methodologies.</li>
  <li> Adaptability to working in a dynamic environment where duties, responsibilities, and activities vary in response to fluctuating organizational needs.</li>
  <li> Flexibility in completing projects with concurrent timelines, accurately estimating task duration, and working independently on execution.</li>
  <li> Demonstration of behavior consistent with the company&#x2019;s values: Dedication to Mission Success, Technical Excellence, Commitment to Our People, Objectivity and Integrity, and Innovation.</li>
 </ul>
 <div></div>
 <div>
   What You Need to be Successful
 </div>
 <div>
   Minimum Requirements:
 </div>
 <ul>
  <li>
   <div>
     2+ years of software development industry experience.
   </div></li>
  <li>
   <div>
     Bachelors from an accredited program in Computer Science or Computer Engineering.
   </div></li>
  <li> Intermediate to advanced knowledge of one or more programming languages, including Python and/or Java.</li>
  <li> Backend development project experience or course work in data structures and algorithms.</li>
  <li> Experience developing in Unix-like environments and using source/version control (Git).</li>
  <li> This position requires the ability to obtain and maintain a security clearance issued by the US government. U.S. citizenship is required to obtain a security clearance.</li>
 </ul>
 <div></div>
 <div>
   How You Can Stand Out
 </div>
 <div>
   It would be impressive if you have one or more of these qualifications:
 </div>
 <ul>
  <li> Advanced degree (MS or PhD) from an accredited program in Computer Science or Computer Engineering.</li>
  <li> Intermediate to advanced knowledge of one or more scripting languages, including JavaScript or Typescript.</li>
  <li> Frontend library/framework development experience (e.g. React).</li>
  <li> Familiarity with conventional UI/UX design and/or effective data visualization design patterns.</li>
  <li> Experience developing REST APIs or application-database integration programming.</li>
  <li> Knowledge of data architectures and platforms, data mesh, data fabric, data pipelines, data engineering, data management, or data governance.</li>
  <li> Familiarity with data semantics, knowledge engineering, or virtual knowledge graphing (VSG).</li>
  <li> Experience building cloud-native applications, virtual environments, microservice architectures, containerized platforms, infrastructure as code, or automated deployments (e.g. Kubernetes, Docker, Argo).</li>
  <li> Experience implementing unit, integration, or functional software testing.</li>
  <li> Active US Government security clearance.</li>
 </ul>
 <div></div>
 <div>
   We offer a competitive compensation package where you&#x2019;ll be rewarded based on your performance and recognized for the value you bring to our business. The grade-based pay range for this job is listed below. Individual salaries within that range are determined through a wide variety of factors including but not limited to education, experience, knowledge and skills.
 </div>
 <div>
   (Min - Mid - Max)
 </div> &#x24;71,000 - &#x24;97,350 - &#x24;123,700
 <div></div> Pay Basis: Annual
 <div></div>
 <div>
   Ways We Reward Our Employees
 </div>
 <div>
   During your interview process, our team will provide details of our industry-leading benefits.
 </div>
 <div>
   Benefits vary and are applicable based on Job Type. 
  <i>A few highlights include:</i>
 </div>
 <ul>
  <li>
   <div>
     Comprehensive health care and wellness plans
   </div></li>
  <li>
   <div>
     Paid holidays, sick time, and vacation
   </div></li>
  <li>
   <div>
     Standard and alternate work schedules, including telework options
   </div></li>
  <li>
   <div>
     401(k) Plan &#x2014; Employees receive a total company-paid benefit of 8%, 10%, or 12% of eligible compensation based on years of service and matching contributions; employees are immediately eligible and vested in the plan upon hire
   </div></li>
  <li>
   <div>
     Flexible spending accounts
   </div></li>
  <li>
   <div>
     Variable pay program for exceptional contributions
   </div></li>
  <li>
   <div>
     Relocation assistance
   </div></li>
  <li>
   <div>
     Professional growth and development programs to help advance your career
   </div></li>
  <li>
   <div>
     Education assistance programs
   </div></li>
  <li>
   <div>
     An inclusive work environment built on teamwork, flexibility, and respect
   </div></li>
 </ul>
 <div></div>
 <div>
   We are all unique, from diverse backgrounds and all walks of life, yet one thing bonds all of us to each other&#x2014;the belief that we can make a difference. This core belief empowers us to do our best work at The Aerospace Corporation.
 </div>
 <div></div>
 <div>
   Equal Opportunity Commitment
 </div>
 <div>
   The Aerospace Corporation is an Equal Opportunity/Affirmative Action employer. We believe that a diverse workforce creates an environment in which unique ideas are developed and differing perspectives are valued, producing superior customer solutions. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, age, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender, gender identity or expression, color, religion, genetic information, marital status, ancestry, national origin, protected veteran status, physical disability, medical condition, mental disability, or disability status and any other characteristic protected by state or federal law. If you&#x2019;re an individual with a disability or a disabled veteran who needs assistance using our online job search and application tools or need reasonable accommodation to complete the job application process, please contact us by phone at 310.336.5432 or by email at 
  <div>
   ieo.mailbox@aero.org
  </div>. You can also review 
  <div>
   Know Your Rights
  </div>
  <div>
   : Workplace Discrimination is Illegal
  </div>, as well as the 
  <div>
   Pay Transparency Policy Statement
  </div>.
 </div>
</div>",https://aero.wd5.myworkdayjobs.com/en-US/External/job/El-Segundo-CA/Software-Engineer-Data-Engineer_R008007?sid=135,cbd644ff80fc0c98,,Full-time,,,"2310 East El Segundo Boulevard, El Segundo, CA 90245",Software Engineer/Data Engineer,30+ days ago,2023-09-20T12:41:00.721Z,4.0,142.0,"$123,700 a year",2023-10-20T12:41:00.723Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=cbd644ff80fc0c98&from=jasx&tk=1hd6hkmagh0mj800&vjs=3
311,CareFirst BlueCross BlueShield,"Resp & Qualifications 
 PURPOSE:
  This is a Big Data Administrator Lead position and not a developer position.
  The Lead Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise
  infrastructure targeting big data and platform data management (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent.) with emphasis on high availability, reliability, automation and performance. This role will focus on leading the migration and set up of the Enterprise Data Platform on Cloud using a combination of Cloudera CDP public cloud and other AWS services.
 
   Advanced (expert preferred) level experience in administrating and engineering relational databases (ex. MySQL, PostgreSQL), Big Data systems (ex. Cloudera Data Platform Private Cloud and Public Cloud), Apache Solr as SME, ETL (ex. Ab Initio), BI (ex. MicroStrategy), automation tools (ex. Ansible, Terraform, Bit Bucket) and experience working cloud solutions (specifically data products on AWS) are necessary.
   At least 8 years of Experienced with all the tasks involved in administration of big data and Meta Data Hub such as Cloudera.
   Experience with Ab Initio, EMR, S3, Dynamo DB, Mongo DB, ProgreSQL, RDS, DB2 is a Plus.
   DevOps (CI/CD Pipeline) is a Plus.
   Experience with Advance knowledge of UNIX and SQL
   Experience with manage metadata hub-MDH, Operational Console and troubleshoot environmental issues which affect these components
   Require prior experience with migration from on-premise to AWS Cloud.
   Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas. Require tracking and monitoring projects and tasks as the lead.
 
  ESSENTIAL FUNCTIONS:
 
   Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas.
   Require project tracking and task monitoring. the lead position ensures an overall successful implementation especially where team members all are working on multiple efforts at the same time. Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability. Plan, develop and lead administrators with project and efforts, achieve milestones and objectives. Oversees the delivery of engineering data initiatives and projects including hands on with install, configure, automation script, and deploy.
   Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform.
   Develop and implement techniques to prevent system problems, troubleshoots incidents to recover services, and support the root cause analysis. Develops and follows standard operating procedures (SOPs) for common tasks to ensure quality of service.
   Manages customer and stakeholder needs, generates and develops requirements, and performs functional analysis. Fulfills business objectives by collaborating with network staff to ensure reliable software and systems. Enforces the implementation of best practices for data auditing, scalability, reliability, high availability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Acts as a mentor for junior and senior team members.
   Installs, tunes, upgrades, troubleshoots, and maintains all computer systems relevant to the supported applications including all necessary tasks to perform operating system administration, user account management, disaster recovery strategy and networking configuration.
   Expands engineering job knowledge and leading technologies by reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; educational opportunities and participating in professional societies.
 
  SUPERVISORY RESPONSIBILITY: Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.  QUALIFICATIONS:  Education Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.  Experience: 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.  Knowledge, Skills and Abilities (KSAs)
 
   Experience with Cloudera CDP on-prem and public cloud; Solr SME.
   Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
   Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
   Able to effectively provide direction to and lead technical teams, tracking and monitoring projects and tasks.
   Knowledge of programming languages and web based technologies.
   Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python). 
 
 Salary Range: $105,408 - $209,352
  Resp & Qualifications 
 PURPOSE:
  This is a Big Data Administrator Lead position and not a developer position. 
 The Lead Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise
  infrastructure targeting big data and platform data management (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent.) with emphasis on high availability, reliability, automation and performance. This role will focus on leading the migration and set up of the Enterprise Data Platform on Cloud using a combination of Cloudera CDP public cloud and other AWS services. 
 
  Advanced (expert preferred) level experience in administrating and engineering relational databases (ex. MySQL, PostgreSQL), Big Data systems (ex. Cloudera Data Platform Private Cloud and Public Cloud), Apache Solr as SME, ETL (ex. Ab Initio), BI (ex. MicroStrategy), automation tools (ex. Ansible, Terraform, Bit Bucket) and experience working cloud solutions (specifically data products on AWS) are necessary. 
  At least 8 years of Experienced with all the tasks involved in administration of big data and Meta Data Hub such as Cloudera.
   Experience with Ab Initio, EMR, S3, Dynamo DB, Mongo DB, ProgreSQL, RDS, DB2 is a Plus.
   DevOps (CI/CD Pipeline) is a Plus.
   Experience with Advance knowledge of UNIX and SQL
   Experience with manage metadata hub-MDH, Operational Console and troubleshoot environmental issues which affect these components
   Require prior experience with migration from on-premise to AWS Cloud.
   Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas. Require tracking and monitoring projects and tasks as the lead.
 
  ESSENTIAL FUNCTIONS:
 
   Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas.
   Require project tracking and task monitoring. the lead position ensures an overall successful implementation especially where team members all are working on multiple efforts at the same time. Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability. Plan, develop and lead administrators with project and efforts, achieve milestones and objectives. Oversees the delivery of engineering data initiatives and projects including hands on with install, configure, automation script, and deploy.
   Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform.
   Develop and implement techniques to prevent system problems, troubleshoots incidents to recover services, and support the root cause analysis. Develops and follows standard operating procedures (SOPs) for common tasks to ensure quality of service.
   Manages customer and stakeholder needs, generates and develops requirements, and performs functional analysis. Fulfills business objectives by collaborating with network staff to ensure reliable software and systems. Enforces the implementation of best practices for data auditing, scalability, reliability, high availability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Acts as a mentor for junior and senior team members.
   Installs, tunes, upgrades, troubleshoots, and maintains all computer systems relevant to the supported applications including all necessary tasks to perform operating system administration, user account management, disaster recovery strategy and networking configuration.
   Expands engineering job knowledge and leading technologies by reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; educational opportunities and participating in professional societies.
 
  SUPERVISORY RESPONSIBILITY: Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.  QUALIFICATIONS:  Education Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.  Experience: 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.  Knowledge, Skills and Abilities (KSAs)
 
   Experience with Cloudera CDP on-prem and public cloud; Solr SME.
   Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
   Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
   Able to effectively provide direction to and lead technical teams, tracking and monitoring projects and tasks.
   Knowledge of programming languages and web based technologies.
   Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python). 
 
 Salary Range: $105,408 - $209,352
  Salary Range Disclaimer 
 Salary will be based on education, location, experience, certifications, etc. In addition to your salary, CareFirst offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).
  Equal Employment Opportunity 
 CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
  Where To Apply 
 Please visit our website to apply: www.carefirst.com/careers
  Federal Disc/Physical Demand 
 Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
  PHYSICAL DEMANDS:
  The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
  Sponsorship in US 
 Must be eligible to work in the U.S. without Sponsorship.","<div>
 <p><b>Resp &amp; Qualifications</b> </p>
 <p><b>PURPOSE:</b></p>
 <p> This is a Big Data Administrator Lead position and not a developer position.</p>
 <p> The Lead Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise</p>
 <p> infrastructure targeting big data and platform data management (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent.) with emphasis on high availability, reliability, automation and performance. This role will focus on leading the migration and set up of the Enterprise Data Platform on Cloud using a combination of Cloudera CDP public cloud and other AWS services.</p>
 <ul>
  <li> Advanced (expert preferred) level experience in administrating and engineering relational databases (ex. MySQL, PostgreSQL), Big Data systems (ex. Cloudera Data Platform Private Cloud and Public Cloud), Apache Solr as SME, ETL (ex. Ab Initio), BI (ex. MicroStrategy), automation tools (ex. Ansible, Terraform, Bit Bucket) and experience working cloud solutions (specifically data products on AWS) are necessary.</li>
  <li> At least 8 years of Experienced with all the tasks involved in administration of big data and Meta Data Hub such as Cloudera.</li>
  <li> Experience with Ab Initio, EMR, S3, Dynamo DB, Mongo DB, ProgreSQL, RDS, DB2 is a Plus.</li>
  <li> DevOps (CI/CD Pipeline) is a Plus.</li>
  <li> Experience with Advance knowledge of UNIX and SQL</li>
  <li> Experience with manage metadata hub-MDH, Operational Console and troubleshoot environmental issues which affect these components</li>
  <li> Require prior experience with migration from on-premise to AWS Cloud.</li>
  <li> Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas. Require tracking and monitoring projects and tasks as the lead.</li>
 </ul>
 <p><b><br> ESSENTIAL FUNCTIONS:</b></p>
 <ul>
  <li><p> Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas.</p></li>
  <li><p> Require project tracking and task monitoring. the lead position ensures an overall successful implementation especially where team members all are working on multiple efforts at the same time. Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability. Plan, develop and lead administrators with project and efforts, achieve milestones and objectives. Oversees the delivery of engineering data initiatives and projects including hands on with install, configure, automation script, and deploy.</p></li>
  <li><p> Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform.</p></li>
  <li><p> Develop and implement techniques to prevent system problems, troubleshoots incidents to recover services, and support the root cause analysis. Develops and follows standard operating procedures (SOPs) for common tasks to ensure quality of service.</p></li>
  <li><p> Manages customer and stakeholder needs, generates and develops requirements, and performs functional analysis. Fulfills business objectives by collaborating with network staff to ensure reliable software and systems. Enforces the implementation of best practices for data auditing, scalability, reliability, high availability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.</p></li>
  <li><p> Acts as a mentor for junior and senior team members.</p></li>
  <li><p> Installs, tunes, upgrades, troubleshoots, and maintains all computer systems relevant to the supported applications including all necessary tasks to perform operating system administration, user account management, disaster recovery strategy and networking configuration.</p></li>
  <li><p> Expands engineering job knowledge and leading technologies by reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; educational opportunities and participating in professional societies.</p></li>
 </ul>
 <p><b><br> SUPERVISORY RESPONSIBILITY:</b><br> Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.<br> <br> <b>QUALIFICATIONS:</b><br> <br> <b>Education Level:</b> Bachelor&apos;s Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor&apos;s degree, an additional 4 years of relevant work experience is required in addition to the required work experience.<br> <br> <b>Experience:</b> 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.<br> <br> <b>Knowledge, Skills and Abilities (KSAs)</b></p>
 <ul>
  <li> Experience with Cloudera CDP on-prem and public cloud; Solr SME.</li>
  <li> Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.</li>
  <li> Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.</li>
  <li> Able to effectively provide direction to and lead technical teams, tracking and monitoring projects and tasks.</li>
  <li> Knowledge of programming languages and web based technologies.</li>
  <li> Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python). </li>
 </ul>
 <p>Salary Range: &#x24;105,408 - &#x24;209,352</p>
 <p><b> Resp &amp; Qualifications</b> </p>
 <p><b>PURPOSE:</b></p>
 <p> This is a Big Data Administrator Lead position and not a developer position. </p>
 <p>The Lead Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise</p>
 <p> infrastructure targeting big data and platform data management (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent.) with emphasis on high availability, reliability, automation and performance. This role will focus on leading the migration and set up of the Enterprise Data Platform on Cloud using a combination of Cloudera CDP public cloud and other AWS services. </p>
 <ul>
  <li>Advanced (expert preferred) level experience in administrating and engineering relational databases (ex. MySQL, PostgreSQL), Big Data systems (ex. Cloudera Data Platform Private Cloud and Public Cloud), Apache Solr as SME, ETL (ex. Ab Initio), BI (ex. MicroStrategy), automation tools (ex. Ansible, Terraform, Bit Bucket) and experience working cloud solutions (specifically data products on AWS) are necessary. </li>
  <li>At least 8 years of Experienced with all the tasks involved in administration of big data and Meta Data Hub such as Cloudera.</li>
  <li> Experience with Ab Initio, EMR, S3, Dynamo DB, Mongo DB, ProgreSQL, RDS, DB2 is a Plus.</li>
  <li> DevOps (CI/CD Pipeline) is a Plus.</li>
  <li> Experience with Advance knowledge of UNIX and SQL</li>
  <li> Experience with manage metadata hub-MDH, Operational Console and troubleshoot environmental issues which affect these components</li>
  <li> Require prior experience with migration from on-premise to AWS Cloud.</li>
  <li> Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas. Require tracking and monitoring projects and tasks as the lead.</li>
 </ul>
 <p><b><br> ESSENTIAL FUNCTIONS:</b></p>
 <ul>
  <li><p> Represents team in all architectural and design discussions. Knowledgeable in the end-to-end process and able to act as an SME providing credible feedback and input in all impacted areas.</p></li>
  <li><p> Require project tracking and task monitoring. the lead position ensures an overall successful implementation especially where team members all are working on multiple efforts at the same time. Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability. Plan, develop and lead administrators with project and efforts, achieve milestones and objectives. Oversees the delivery of engineering data initiatives and projects including hands on with install, configure, automation script, and deploy.</p></li>
  <li><p> Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform.</p></li>
  <li><p> Develop and implement techniques to prevent system problems, troubleshoots incidents to recover services, and support the root cause analysis. Develops and follows standard operating procedures (SOPs) for common tasks to ensure quality of service.</p></li>
  <li><p> Manages customer and stakeholder needs, generates and develops requirements, and performs functional analysis. Fulfills business objectives by collaborating with network staff to ensure reliable software and systems. Enforces the implementation of best practices for data auditing, scalability, reliability, high availability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.</p></li>
  <li><p> Acts as a mentor for junior and senior team members.</p></li>
  <li><p> Installs, tunes, upgrades, troubleshoots, and maintains all computer systems relevant to the supported applications including all necessary tasks to perform operating system administration, user account management, disaster recovery strategy and networking configuration.</p></li>
  <li><p> Expands engineering job knowledge and leading technologies by reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; educational opportunities and participating in professional societies.</p></li>
 </ul>
 <p><b><br> SUPERVISORY RESPONSIBILITY:</b><br> Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.<br> <br> <b>QUALIFICATIONS:</b><br> <br> <b>Education Level:</b> Bachelor&apos;s Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor&apos;s degree, an additional 4 years of relevant work experience is required in addition to the required work experience.<br> <br> <b>Experience:</b> 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.<br> <br> <b>Knowledge, Skills and Abilities (KSAs)</b></p>
 <ul>
  <li> Experience with Cloudera CDP on-prem and public cloud; Solr SME.</li>
  <li> Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.</li>
  <li> Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.</li>
  <li> Able to effectively provide direction to and lead technical teams, tracking and monitoring projects and tasks.</li>
  <li> Knowledge of programming languages and web based technologies.</li>
  <li> Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python). </li>
 </ul>
 <p>Salary Range: &#x24;105,408 - &#x24;209,352</p>
 <p><b> Salary Range Disclaimer</b> </p>
 <p>Salary will be based on education, location, experience, certifications, etc. In addition to your salary, CareFirst offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).</p>
 <p><b> Equal Employment Opportunity</b> </p>
 <p>CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.</p>
 <p><b> Where To Apply</b> </p>
 <p>Please visit our website to apply: www.carefirst.com/careers</p>
 <p><b> Federal Disc/Physical Demand</b> </p>
 <p>Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.</p>
 <p><b> PHYSICAL DEMANDS:</b></p>
 <p> The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.</p>
 <p><b> Sponsorship in US</b> </p>
 <p>Must be eligible to work in the U.S. without Sponsorship.</p>
</div>",https://carefirstcareers.ttcportals.com/jobs/12970780-lead-data-engineer-remote?tm_job=18468-1A&tm_event=view&tm_company=2380,26e51b9e060db1da,,Full-time,,,"10780 Parkridge Blvd, Reston, VA 20191",Lead Data Engineer (Remote),30+ days ago,2023-09-20T12:41:12.066Z,3.8,732.0,"$105,408 - $209,352 a year",2023-10-20T12:41:12.068Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=26e51b9e060db1da&from=jasx&tk=1hd6hkmagh0mj800&vjs=3
313,NTT Ltd.,"NTT is a leading global IT solutions and services organisation that brings together people, data and things to create a better and more sustainable future.
  In today’s ‘iNTTerconnected’ world, connections matter more now than ever. By bringing together talented people, world-class technology partners and emerging innovators, we help our clients solve some of the world’s most significant technological, business and societal challenges.
  With people at the heart of our success, NTT is committed to attracting and growing the best talent and providing an environment where everyone feels they can belong and their contribution matters.
 
  Want to be a part of our team? POSITION SUMMARY: BASIC FUNCTION
  
  The IIOT & SCADA Engineer is tasked with developing, implementing, maintaining, and testing of SCADA, Controls, IIOT systems using proven industry standard designs to ensure scalability and maintainability. This role will also aid in the creation of design standards for NTT GDCA SCADA, controls & data systems. This is a senior level position that interfaces with multiple stakeholders in the company. You will develop small and large projects at all levels of complexity with external and internal clients. You will develop specific solutions, applications, or processes, which ultimately culminate in customer acceptance of the results.
  
  Role Includes:
  
  
 
  A wide range of process activities beginning with the request for proposal through development, final delivery, and support.
  Develop and build systems, products, and services that contribute toward increased top line revenue & profits.
  Develop and build systems and services that protect the company, customers, employees, and assets.
  Develop enterprise level IIOT & SCADA projects using best in class methodologies and techniques.
 
 
  Working at NTT
  ESSENTIAL DUTIES & RESPONSIBILITIES
 
   Provide detailed advice regarding applications and executing specialized tasks.
   Provide support and assistance on initiatives and ensure optimal delivery architecture in support of projects
   Design and develop scalable ETL packages
   Build test scenarios to enable thorough testing and validation to support accuracy of data transformation
   Define and implement data stores based on system and consumer requirements
   Assess, analyze, develop, document and implement changes based on requests for change
   Assess and analyze release components
   Modelling tests in coordination with testers maintaining and administering the tools and methods
   Ensuring information exchange with configuration management
   Ensure release processes and procedures are maintained and followed
   Investigates operational needs and problems, and opportunities, contributing to the recommendation of improvements in automated and non-automated components of new or changed processes and organization
   Investigates, manages and applies authorized requests for changes to base-lined requirements, in line with the change management policy
   Interprets and applies industry standards to meet project needs
   Evaluate potential solutions, demonstrating, installing and commissioning selected products
   Apply ethical and robust techniques in line with organizational policies and procedures
   Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the company
   Performs analysis required to troubleshoot related issues and assist in the resolution of issues
 
 
  KNOWLEDGE, SKILLS & ABILITIES
 
   Be an Innovator for the company
   Experience in controls environments (PLC programming not required)
   Experience in IIOT & Data systems
   Experience in Enterprise level SCADA systems
   Systems thinking - the ability to see how parts interact with the whole (big picture thinking)
   Problem solving, with the ability to mentor and work with other team members to resolve issues then focus on establishing long term solutions rather than quick fixes
   Knowledge of the business for which the systems architecture is being developed
   Interpersonal skills - servant leadership, collaboration, facilitation, and negotiation skills
   Excellent documentation skills
   Ability to explain complex technical issues in a way that non-technical people may understand
   Knowledge of IT governance and operations
   Comprehensive knowledge of hardware, software, application, and systems engineering
   Basic project and program management planning and organizational skills, as well as time management and prioritization capabilities
   Customer service orientation
   Ability to maintain confidentiality and show discretion
   Proficiency with computer systems on an administrative and user basis for standard Microsoft Office applications, as well as Visio.
   Collaborative with a focus on Team participation and consensus building
   Stress management and ability to handle a fast paced, rapidly changing environment
   Demonstrate the ability to plan and organize
   Ability to develop and nurture improved performance
   Analytical mind and business acumen
   Problem-solving aptitude Test the reliability and performance of each part of new and/or existing systems
   Develop in new technologies while observing the various constraints such as cost, time frame and security
   Inform and advise management on the technological and organizational impacts of the new systems and operational components
   Generating a set of acceptance test requirements, together with the designers, test engineers, and the user, which determine that all of the high-level requirements have been met, especially for the computer-human-interface
   Generating products such as sketches, models, an early user guide, and prototypes to keep the user and the engineers constantly up to date and in agreement on the system to be provided as it is evolving
   Build test environments, and pilot deployments
   Interface with the design and implementation engineers and architects, so that any problems that arise during design or implementation can be resolved in accordance with the fundamental design concepts, and still meet user needs and constraints
   Spearhead implementation and deployment of designs with innovative ideas
   Proficient in manual development and solution documentation
   Excellent problem solving and troubleshooting skills
   Ability to work under pressure and meet deadlines
   Solid understanding of database design principles
   Excellent oral and written communication skills with a keen sense of customer service
 
 
  #GlobalDataCentersCareers
 
  What will make you a good fit for the role?
  EDUCATION & EXPERIENCE 
 
  A bachelor's degree in engineering, computers, or information systems is recommended, however certifications in one or more information systems subject area may be acceptable with reasonable experience and exhibited skill or a reasonable amount of demonstrated industry experience
   5+ years of experience in Information Technology
   5+ Years in Diverse Controls Environments
   5+ Years in IIOT & Data systems
   5+ Years in Enterprise level SCADA systems
 
 
  PHYSICAL REQUIREMENTS
 
   Primarily sitting with some walking, standing, and bending.
   Able to hear and speak on a telephone.
   Close visual work on a computer terminal.
   Dexterity of hands and fingers to operate any required computer keyboard, mouse, and other technical instruments.
   Able to lift and carry up to 20 lbs.
 
 
  WORK CONDITIONS
 
   This role is expected to be remote with an occasional need to be on-site in a shared space.
   Extensive daily computer usage.
 
 
  SPECIAL REQUIREMENTS
 
   Some travel required.
   Must possess a current, valid state-issued driver’s license.
 
 
  This is a remote position that requires reliable internet connection and electricity. A monthly stipend is provided to cover expenses associated with working remotely and use of a personal mobile device, if applicable.
 
  NTT Global Data Centers Americas, Inc. offers competitive compensation based on experience, education, and location. Base salary for this position is
  $113,000 - $151,000.
 
  All regular full-time employees are eligible for an annual bonus; payout is dependent upon individual and company performance.
 
  Employees receive paid time-off, medical, dental, and vision benefits, life and supplemental insurance, short-term and long-term disability, flexible spending account, and 401k retirement plan to create a rich Total Rewards package.
  Join our growing global team and accelerate your career with us. Apply today.
 
  
   
    
     
      
       
        
          A career at NTT means:
         
           Being part of a global pioneer – where you gain exposure to our Fortune 500 clients and world-leading global technology partners and work with a network of over 40,000 smart and diverse colleagues across 57 countries, delivering services in over 200 countries.
           Being at the forefront of cutting-edge technology – backed with a 150-year heritage of using technology for good. With 40% of the world’s internet traffic running on our network and where Emoji were first invented, you can be proud of the group’s many new ‘firsts’.
           Making a difference – by doing meaningful work that helps to shape the future for our clients, and across industries and communities around the world. 
          Being your best self – in a progressive ‘Connected Working’ environment that promotes flexibility, connection and wellbeing. Where diversity and different perspectives are embraced to ensure equal opportunities for all.
           Having ongoing opportunities to own and develop your career – with a personal and professional development plan and access to the broadest learning offerings in the industry.","<p></p>
<div>
 <p>NTT is a leading global IT solutions and services organisation that brings together people, data and things to create a better and more sustainable future.</p>
 <p> In today&#x2019;s &#x2018;iNTTerconnected&#x2019; world, connections matter more now than ever. By bringing together talented people, world-class technology partners and emerging innovators, we help our clients solve some of the world&#x2019;s most significant technological, business and societal challenges.</p>
 <p> With people at the heart of our success, NTT is committed to attracting and growing the best talent and providing an environment where everyone feels they can belong and their contribution matters.</p>
 <p></p>
 <p><b><br> Want to be a part of our team?</b></p> POSITION SUMMARY: BASIC FUNCTION
 <br> 
 <br> The IIOT &amp; SCADA Engineer is tasked with developing, implementing, maintaining, and testing of SCADA, Controls, IIOT systems using proven industry standard designs to ensure scalability and maintainability. This role will also aid in the creation of design standards for NTT GDCA SCADA, controls &amp; data systems. This is a senior level position that interfaces with multiple stakeholders in the company. You will develop small and large projects at all levels of complexity with external and internal clients. You will develop specific solutions, applications, or processes, which ultimately culminate in customer acceptance of the results.
 <br> 
 <br> Role Includes:
 <br> 
 <br> 
 <ul>
  <li>A wide range of process activities beginning with the request for proposal through development, final delivery, and support.</li>
  <li>Develop and build systems, products, and services that contribute toward increased top line revenue &amp; profits.</li>
  <li>Develop and build systems and services that protect the company, customers, employees, and assets.</li>
  <li>Develop enterprise level IIOT &amp; SCADA projects using best in class methodologies and techniques.</li>
 </ul>
 <p></p>
 <p><b> Working at NTT</b></p>
 <p><b> ESSENTIAL DUTIES &amp; RESPONSIBILITIES</b></p>
 <ul>
  <li> Provide detailed advice regarding applications and executing specialized tasks.</li>
  <li> Provide support and assistance on initiatives and ensure optimal delivery architecture in support of projects</li>
  <li> Design and develop scalable ETL packages</li>
  <li> Build test scenarios to enable thorough testing and validation to support accuracy of data transformation</li>
  <li> Define and implement data stores based on system and consumer requirements</li>
  <li> Assess, analyze, develop, document and implement changes based on requests for change</li>
  <li> Assess and analyze release components</li>
  <li> Modelling tests in coordination with testers maintaining and administering the tools and methods</li>
  <li> Ensuring information exchange with configuration management</li>
  <li> Ensure release processes and procedures are maintained and followed</li>
  <li> Investigates operational needs and problems, and opportunities, contributing to the recommendation of improvements in automated and non-automated components of new or changed processes and organization</li>
  <li> Investigates, manages and applies authorized requests for changes to base-lined requirements, in line with the change management policy</li>
  <li> Interprets and applies industry standards to meet project needs</li>
  <li> Evaluate potential solutions, demonstrating, installing and commissioning selected products</li>
  <li> Apply ethical and robust techniques in line with organizational policies and procedures</li>
  <li> Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the company</li>
  <li> Performs analysis required to troubleshoot related issues and assist in the resolution of issues</li>
 </ul>
 <p></p>
 <p><b> KNOWLEDGE, SKILLS &amp; ABILITIES</b></p>
 <ul>
  <li> Be an Innovator for the company</li>
  <li> Experience in controls environments (PLC programming not required)</li>
  <li> Experience in IIOT &amp; Data systems</li>
  <li> Experience in Enterprise level SCADA systems</li>
  <li> Systems thinking - the ability to see how parts interact with the whole (big picture thinking)</li>
  <li> Problem solving, with the ability to mentor and work with other team members to resolve issues then focus on establishing long term solutions rather than quick fixes</li>
  <li> Knowledge of the business for which the systems architecture is being developed</li>
  <li> Interpersonal skills - servant leadership, collaboration, facilitation, and negotiation skills</li>
  <li> Excellent documentation skills</li>
  <li> Ability to explain complex technical issues in a way that non-technical people may understand</li>
  <li> Knowledge of IT governance and operations</li>
  <li> Comprehensive knowledge of hardware, software, application, and systems engineering</li>
  <li> Basic project and program management planning and organizational skills, as well as time management and prioritization capabilities</li>
  <li> Customer service orientation</li>
  <li> Ability to maintain confidentiality and show discretion</li>
  <li> Proficiency with computer systems on an administrative and user basis for standard Microsoft Office applications, as well as Visio.</li>
  <li> Collaborative with a focus on Team participation and consensus building</li>
  <li> Stress management and ability to handle a fast paced, rapidly changing environment</li>
  <li> Demonstrate the ability to plan and organize</li>
  <li> Ability to develop and nurture improved performance</li>
  <li> Analytical mind and business acumen</li>
  <li> Problem-solving aptitude Test the reliability and performance of each part of new and/or existing systems</li>
  <li> Develop in new technologies while observing the various constraints such as cost, time frame and security</li>
  <li> Inform and advise management on the technological and organizational impacts of the new systems and operational components</li>
  <li> Generating a set of acceptance test requirements, together with the designers, test engineers, and the user, which determine that all of the high-level requirements have been met, especially for the computer-human-interface</li>
  <li> Generating products such as sketches, models, an early user guide, and prototypes to keep the user and the engineers constantly up to date and in agreement on the system to be provided as it is evolving</li>
  <li> Build test environments, and pilot deployments</li>
  <li> Interface with the design and implementation engineers and architects, so that any problems that arise during design or implementation can be resolved in accordance with the fundamental design concepts, and still meet user needs and constraints</li>
  <li> Spearhead implementation and deployment of designs with innovative ideas</li>
  <li> Proficient in manual development and solution documentation</li>
  <li> Excellent problem solving and troubleshooting skills</li>
  <li> Ability to work under pressure and meet deadlines</li>
  <li> Solid understanding of database design principles</li>
  <li> Excellent oral and written communication skills with a keen sense of customer service</li>
 </ul>
 <p></p>
 <p> #GlobalDataCentersCareers</p>
 <p></p>
 <p><b> What will make you a good fit for the role?</b></p>
 <p><b> EDUCATION &amp; EXPERIENCE </b></p>
 <ul>
  <li>A bachelor&apos;s degree in engineering, computers, or information systems is recommended, however certifications in one or more information systems subject area may be acceptable with reasonable experience and exhibited skill or a reasonable amount of demonstrated industry experience</li>
  <li> 5+ years of experience in Information Technology</li>
  <li> 5+ Years in Diverse Controls Environments</li>
  <li> 5+ Years in IIOT &amp; Data systems</li>
  <li> 5+ Years in Enterprise level SCADA systems</li>
 </ul>
 <p></p>
 <p><b> PHYSICAL REQUIREMENTS</b></p>
 <ul>
  <li> Primarily sitting with some walking, standing, and bending.</li>
  <li> Able to hear and speak on a telephone.</li>
  <li> Close visual work on a computer terminal.</li>
  <li> Dexterity of hands and fingers to operate any required computer keyboard, mouse, and other technical instruments.</li>
  <li> Able to lift and carry up to 20 lbs.</li>
 </ul>
 <p></p>
 <p><b> WORK</b><b> CONDITIONS</b></p>
 <ul>
  <li> This role is expected to be remote with an occasional need to be on-site in a shared space<i>.</i></li>
  <li> Extensive daily computer usage.</li>
 </ul>
 <p></p>
 <p><b> SPECIAL REQUIREMENTS</b></p>
 <ul>
  <li> Some travel required.</li>
  <li> Must possess a current, valid state-issued driver&#x2019;s license.</li>
 </ul>
 <p></p>
 <p> This is a remote position that requires reliable internet connection and electricity. A monthly stipend is provided to cover expenses associated with working remotely and use of a personal mobile device, if applicable.</p>
 <p></p>
 <p> NTT Global Data Centers Americas, Inc. offers competitive compensation based on experience, education, and location. Base salary for this position is</p>
 <p> &#x24;113,000 - &#x24;151,000.</p>
 <p></p>
 <p> All regular full-time employees are eligible for an annual bonus; payout is dependent upon individual and company performance.</p>
 <p></p>
 <p> Employees receive paid time-off, medical, dental, and vision benefits, life and supplemental insurance, short-term and long-term disability, flexible spending account, and 401k retirement plan to create a rich Total Rewards package.</p>
 <p><b><br> Join our growing global team and accelerate your career with us. Apply today.</b></p>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <p><b><br> A career at NTT means:</b></p>
         <ul>
          <li><p> Being part of a <b>global pioneer &#x2013;</b> where you gain exposure to our Fortune 500 clients and world-leading global technology partners and work with a network of over 40,000 smart and diverse colleagues across 57 countries, delivering services in over 200 countries.</p></li>
          <li><p> Being at the forefront of <b>cutting-edge technology &#x2013;</b> backed with a 150-year heritage of using technology for good. With 40% of the world&#x2019;s internet traffic running on our network and where Emoji were first invented, you can be proud of the group&#x2019;s many new &#x2018;firsts&#x2019;.</p></li>
          <li><p><b> Making a difference &#x2013;</b> by doing meaningful work that helps to shape the future for our clients, and across industries and communities around the world. </p></li>
          <li><p>Being <b>your best self &#x2013;</b> in a progressive &#x2018;Connected Working&#x2019; environment that promotes flexibility, connection and wellbeing. Where diversity and different perspectives are embraced to ensure equal opportunities for all.</p></li>
          <li><p> Having ongoing opportunities to <b>own and develop your career &#x2013;</b> with a personal and professional development plan and access to the broadest learning offerings in the industry.</p></li>
         </ul>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<p></p>",https://careers.services.global.ntt/global/en/job/NTT1GLOBALR100954EXTERNALENGLOBAL/IIOT-SCADA-Data-Engineer?utm_source=glassdoor&utm_medium=phenom-feeds,107fdf5f44447f8a,,Full-time,,,"Sacramento, CA 94203",IIOT & SCADA Data Engineer,30+ days ago,2023-09-20T12:41:23.875Z,3.3,72.0,"$113,000 - $151,000 a year",2023-10-20T12:41:23.892Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=107fdf5f44447f8a&from=jasx&tk=1hd6hkmagh0mj800&vjs=3
317,Digital Plus Solutions,"You Are

 Driven to experience and learn more about design, and architecture, and take on progressive roles
 Collaborative and comfortable working across teams including data engineering, front end, product management, and DevOps
 Responsible and like to take ownership of challenging problems
 An effective communicator, including good documentation practices and articulating thought processes in a team setting
 Comfortable with working in an agile environment
 Curious about technology and the industry, and a constant learner

You Have

 MS/BS +3 years in Computer Science or a related field
 Expert programming experience with Python, Java, or Scala
 Good working knowledge of SQL databases such as Postgres and NoSQL databases such as MongoDB, Cassandra, Redis
 Experience with search engine database such as ElasticSearch is preferred
 Time-series databases such as InfluxDB, Druid, Prometheus
 Strong computer science fundamentals: data structures, algorithms, and distributed systems

Job Type: Contract
Salary: $83,000.00 - $120,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 5 years

Schedule:

 Monday to Friday

Application Question(s):

 How much experience do you have with MongoDB?
 How much experience do you have as a Data Engineer?
 Desired salary?
 Visa Status?

Work Location: Remote","<p><b>You Are</b></p>
<ul>
 <li>Driven to experience and learn more about design, and architecture, and take on progressive roles</li>
 <li>Collaborative and comfortable working across teams including data engineering, front end, product management, and DevOps</li>
 <li>Responsible and like to take ownership of challenging problems</li>
 <li>An effective communicator, including good documentation practices and articulating thought processes in a team setting</li>
 <li>Comfortable with working in an agile environment</li>
 <li>Curious about technology and the industry, and a constant learner</li>
</ul>
<p><b>You Have</b></p>
<ul>
 <li>MS/BS +3 years in Computer Science or a related field</li>
 <li>Expert programming experience with Python, Java, or Scala</li>
 <li>Good working knowledge of SQL databases such as Postgres and NoSQL databases such as MongoDB, Cassandra, Redis</li>
 <li>Experience with search engine database such as ElasticSearch is preferred</li>
 <li>Time-series databases such as InfluxDB, Druid, Prometheus</li>
 <li>Strong computer science fundamentals: data structures, algorithms, and distributed systems</li>
</ul>
<p>Job Type: Contract</p>
<p>Salary: &#x24;83,000.00 - &#x24;120,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>How much experience do you have with MongoDB?</li>
 <li>How much experience do you have as a Data Engineer?</li>
 <li>Desired salary?</li>
 <li>Visa Status?</li>
</ul>
<p>Work Location: Remote</p>",,6c954cb5fbc38370,,Contract,,,Remote,Data Engineer,11 days ago,2023-10-09T12:41:53.094Z,,,"$83,000 - $120,000 a year",2023-10-20T12:41:53.095Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=6c954cb5fbc38370&from=jasx&tk=1hd6hm9p3irpg800&vjs=3
318,Intellipro Group Inc,"Duties
The Strategy & Operations (SS&O) team is a key strategic partner for advertising sales leaders across sales strategy and analytics, operations and process and tools. Team members are experts in business strategy and operations and analytical and strategic thinking. We’re looking for an experienced Data Engineer who can support the SS&O team by streamlining and optimizing our data pipelines. You’ll know the ins and outs of Spark, providing support to help us fine tune and re-architect data pipelines as needed.
What You’ll Do:

 Build robust data pipelines that collect, process, and compute business metrics from sales data using Spark.
 Develop and iterate on existing workflows that will execute jobs consistently and at scale.
 Understand our database infrastructure to its core and optimize Spark SQL performance to improve landing times of our core tables.
 Work closely with the Sales Ops Business Intelligence team and other data engineering teams as necessary.
 Publish clear documentation and training materials to inform and educate SS&O team members, gather feedback to identify opportunities for improvement.

Skills

 Strong data analytics skills and experience working with and maintaining large data sets.
 Passion for building new solutions and enabling analysis at scale to drive decision-making.
 Proven ability to be successful in a complex, fast-paced environment.
 Organized, detail-oriented and strategically focused.
 Strong written and verbal communication skills.
 Understanding of sales and digital advertising platforms and working knowledge of sales systems and tools is a plus.

We are an Equal Opportunity Employer and take pride in a diverse environment. We do not discriminate in recruitment, hiring, training, promotion or other employment practices for reasons of race, color, religion, gender, sexual orientation, national origin, age, marital or veteran status, medical condition or disability.
Job Types: Full-time, Contract
Pay: $70.00 per hour
Schedule:

 8 hour shift

Application Question(s):

 Are you comfortable working initially for 6+ months contract position with possibility of extension?
 Do you have experience with Spark SQL?

Work Location: Remote","<p><b>Duties</b></p>
<p>The Strategy &amp; Operations (SS&amp;O) team is a key strategic partner for advertising sales leaders across sales strategy and analytics, operations and process and tools. Team members are experts in business strategy and operations and analytical and strategic thinking. We&#x2019;re looking for an experienced Data Engineer who can support the SS&amp;O team by streamlining and optimizing our data pipelines. You&#x2019;ll know the ins and outs of Spark, providing support to help us fine tune and re-architect data pipelines as needed.</p>
<p><b>What You&#x2019;ll Do:</b></p>
<ul>
 <li>Build robust data pipelines that collect, process, and compute business metrics from sales data using Spark.</li>
 <li>Develop and iterate on existing workflows that will execute jobs consistently and at scale.</li>
 <li>Understand our database infrastructure to its core and optimize Spark SQL performance to improve landing times of our core tables.</li>
 <li>Work closely with the Sales Ops Business Intelligence team and other data engineering teams as necessary.</li>
 <li>Publish clear documentation and training materials to inform and educate SS&amp;O team members, gather feedback to identify opportunities for improvement.</li>
</ul>
<p><b>Skills</b></p>
<ul>
 <li>Strong data analytics skills and experience working with and maintaining large data sets.</li>
 <li>Passion for building new solutions and enabling analysis at scale to drive decision-making.</li>
 <li>Proven ability to be successful in a complex, fast-paced environment.</li>
 <li>Organized, detail-oriented and strategically focused.</li>
 <li>Strong written and verbal communication skills.</li>
 <li>Understanding of sales and digital advertising platforms and working knowledge of sales systems and tools is a plus.</li>
</ul>
<p><b>We are an Equal Opportunity Employer and take pride in a diverse environment. We do not discriminate</b> <b>in recruitment, hiring, training, promotion or other employment practices for reasons of race, color, religion, gender, sexual orientation, national origin, age, marital or veteran status, medical condition or disability.</b></p>
<p>Job Types: Full-time, Contract</p>
<p>Pay: &#x24;70.00 per hour</p>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Are you comfortable working initially for 6+ months contract position with possibility of extension?</li>
 <li>Do you have experience with Spark SQL?</li>
</ul>
<p>Work Location: Remote</p>",,095f064bfba48308,,Full-time,Contract,,Remote,Data Engineer,Today,2023-10-21T13:02:47.257Z,,,$70 an hour,2023-10-21T13:02:47.258Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=095f064bfba48308&from=jasx&tk=1hd958u85jqvd802&vjs=3
319,TEKAPPS,"Mid to Sr Data Engineer
Backend Java development at least 3+ yrs
Python development at least 1+ yrs
Experience with microservice design and development using Spring at least 2+ years
Must have worked with automation testing frameworks like Karate, Cucumber
Must have worked with testing libraries JUNIT, TestNG, Spock
MongoDB development and NoSQL design at least 2+ years
MariaDB or MySQL relational design at least 2+ years
Good understanding of webservices, cloud computing with AWS, Kubernetes
Understanding of cyber security aspects including data protection, encryption, anonymization
Familiarity of ML tools and libraries in a Python environment such as Pandas, Scikit
Some exposure with streaming data warehouse engines like Databricks, Snowflake or equivalent
Some exposure to building big data tools experience Kafka, Spark
Nice to have knowledge of Flink, JSON schema, NewRelic, DataDog, gradle, GITHub
Job Types: Contract, Full-time
Pay: $50.00 - $60.00 per hour
Experience level:

 9 years

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Application Question(s):

 Work Authorization / Visa type

Experience:

 Java Backend development: 9 years (Preferred)
 Python: 3 years (Preferred)
 MongoDB and NoSQL: 3 years (Preferred)
 MySQL: 3 years (Preferred)

Work Location: Remote","<p>Mid to Sr Data Engineer</p>
<p>Backend Java development at least 3+ yrs</p>
<p>Python development at least 1+ yrs</p>
<p>Experience with microservice design and development using Spring at least 2+ years</p>
<p>Must have worked with automation testing frameworks like Karate, Cucumber</p>
<p>Must have worked with testing libraries JUNIT, TestNG, Spock</p>
<p>MongoDB development and NoSQL design at least 2+ years</p>
<p>MariaDB or MySQL relational design at least 2+ years</p>
<p>Good understanding of webservices, cloud computing with AWS, Kubernetes</p>
<p>Understanding of cyber security aspects including data protection, encryption, anonymization</p>
<p>Familiarity of ML tools and libraries in a Python environment such as Pandas, Scikit</p>
<p>Some exposure with streaming data warehouse engines like Databricks, Snowflake or equivalent</p>
<p>Some exposure to building big data tools experience Kafka, Spark</p>
<p>Nice to have knowledge of Flink, JSON schema, NewRelic, DataDog, gradle, GITHub</p>
<p>Job Types: Contract, Full-time</p>
<p>Pay: &#x24;50.00 - &#x24;60.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Work Authorization / Visa type</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Java Backend development: 9 years (Preferred)</li>
 <li>Python: 3 years (Preferred)</li>
 <li>MongoDB and NoSQL: 3 years (Preferred)</li>
 <li>MySQL: 3 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,4e37d548afe793c2,,Full-time,Contract,,Remote,"Data Engineer with Java, Python, Mongodb and NoSQL experience",Today,2023-10-21T13:02:51.788Z,,,$50 - $60 an hour,2023-10-21T13:02:51.841Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=4e37d548afe793c2&from=jasx&tk=1hd958u85jqvd802&vjs=3
320,CVS Health,"Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.  Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
 
  Position Summary
  Manages the creation and/or implementation of information security policies, programs, and procedures to cost-effectively and efficiently protect information and information systems assets from Intentional or inadvertent modification; disclosure or destruction; unauthorized access; reduced, interrupted or terminated, processing capability; malicious logic or virus activity; or loss, theft, damage or destruction of any IT resources.
 
  Designs, implements and deploys information security policies, procedures and guidelines. Responsible for developing, maintaining, publishing and/or enforcing information security standards and guidelines encompassing data, and intellectual security. Provides reports to management regarding the effectiveness of network and data security and making recommendations for the adoption of new procedures and technologies, as required. Monitors changes in legislation and accreditation standards that affect information security. Monitors internal control systems to ensure that appropriate information access levels and security clearances are maintained. Establishes meaningful metrics on key critical infrastructure components of information security and monitoring of these to ensure the confidentiality, integrity and availability of information and processes. Ensures awareness of organization's information security policies and procedures among employees, contractors, alliances and other third parties. Initiates, facilitates, and promotes activities to foster information security awareness within the organization. Provides direct information security training to all employees, contractors, alliances, and other third parties. Coordinates internal and external audits and follow up with implementation, based audit recommendations. Serves as an internal information security consultant to the organization. Monitors advancements in information security technologies. Communicates unresolved information security exposures, misuse, or non-compliance situations to senior management. Participates in the activities of the Information Security Committee, responsible for the organization's information security program.
  Extensive experience with data ingest process, Hadoop, Apache Nifi, Cloud platform (AWS, GCP, Azure)
 
 
  Support the design, development and implementation of Security Data Analytic (SDA) capabilities that leverage cybersecurity data and big data technologies to deliver efficiencies and new discovery capabilities
  Design and implement resilient data pipelines and architecture for ingesting unbounded data from multiple sources with different formats leveraging Hadoop, Spark, NiFi and Kafka, Cloud-based applications (AWS, GCP, Azure)
  Develop capabilities for ingesting and indexing large volumes of security-related data in Hadoop, AWS, GCP, Azure environments
  Stay apprised of the latest data science technologies and techniques in order to identify areas where new tools can be applied to support SDA use cases
  Design and implement batch pipelines using Hive, Spark, Pig and Python for global security analytics
  Design and implement discrete and behavioral analytic capabilities to support cybersecurity operations
  Develop visualization capabilities to data using Tableau
  Support the SDA infrastructure design and implementation and ensure integration with other GS systems and operational workflows
  Stay apprised of the latest industry security trends and threats and apply that knowledge to the design and implementation of SDA software solutions
 
  Required Qualifications
  4+ years experience designing and implementing capabilities in a Hadoop-based, Spark big data environment
  4+ years experience in design and implementing real-time data pipelines using NiFi, Kafka and Spark Streaming 
 4+ years experience processing large volumes of data with Hive, Pig, Spark, NiFi, Kafka and Python 
   Preferred Qualifications
 
   Familiarity with streaming big data applications like, Kafka, NiFi, Spark streaming/ Strom
   Strong background in Information System Security
   Excellent written and verbal communication skills
   Familiarity with statistical computing techniques, and statistics software (e.g. R and Tableau)
   Experience using source code management systems (e.g. Git)
   Interest and passion for learning about new technologies
   Experience in processing TB’s worth of data
 
  Education
  Bachelor in engineering, computer science, or related fields
 
  Pay Range
  The typical pay range for this role is:
  $94,500.00 - $196,000.00
 
  This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.  In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.  For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits
 
  CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
 
  You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
 
  CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.","<div>
 <p>Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand &#x2014; with heart at its center &#x2014; our purpose sends a personal message that how we deliver our services is just as important as what we deliver.<br> <br> Our Heart At Work Behaviors&#x2122; support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.</p>
 <p></p>
 <p><b> Position Summary</b></p>
 <p> Manages the creation and/or implementation of information security policies, programs, and procedures to cost-effectively and efficiently protect information and information systems assets from Intentional or inadvertent modification; disclosure or destruction; unauthorized access; reduced, interrupted or terminated, processing capability; malicious logic or virus activity; or loss, theft, damage or destruction of any IT resources.</p>
 <p></p>
 <p> Designs, implements and deploys information security policies, procedures and guidelines. Responsible for developing, maintaining, publishing and/or enforcing information security standards and guidelines encompassing data, and intellectual security. Provides reports to management regarding the effectiveness of network and data security and making recommendations for the adoption of new procedures and technologies, as required. Monitors changes in legislation and accreditation standards that affect information security. Monitors internal control systems to ensure that appropriate information access levels and security clearances are maintained. Establishes meaningful metrics on key critical infrastructure components of information security and monitoring of these to ensure the confidentiality, integrity and availability of information and processes. Ensures awareness of organization&apos;s information security policies and procedures among employees, contractors, alliances and other third parties. Initiates, facilitates, and promotes activities to foster information security awareness within the organization. Provides direct information security training to all employees, contractors, alliances, and other third parties. Coordinates internal and external audits and follow up with implementation, based audit recommendations. Serves as an internal information security consultant to the organization. Monitors advancements in information security technologies. Communicates unresolved information security exposures, misuse, or non-compliance situations to senior management. Participates in the activities of the Information Security Committee, responsible for the organization&apos;s information security program.</p>
 <p> Extensive experience with data ingest process, Hadoop, Apache Nifi, Cloud platform (AWS, GCP, Azure)</p>
 <p></p>
 <ul>
  <li>Support the design, development and implementation of Security Data Analytic (SDA) capabilities that leverage cybersecurity data and big data technologies to deliver efficiencies and new discovery capabilities</li>
  <li>Design and implement resilient data pipelines and architecture for ingesting unbounded data from multiple sources with different formats leveraging Hadoop, Spark, NiFi and Kafka, Cloud-based applications (AWS, GCP, Azure)</li>
  <li>Develop capabilities for ingesting and indexing large volumes of security-related data in Hadoop, AWS, GCP, Azure environments</li>
  <li>Stay apprised of the latest data science technologies and techniques in order to identify areas where new tools can be applied to support SDA use cases</li>
  <li>Design and implement batch pipelines using Hive, Spark, Pig and Python for global security analytics</li>
  <li>Design and implement discrete and behavioral analytic capabilities to support cybersecurity operations</li>
  <li>Develop visualization capabilities to data using Tableau</li>
  <li>Support the SDA infrastructure design and implementation and ensure integration with other GS systems and operational workflows</li>
  <li>Stay apprised of the latest industry security trends and threats and apply that knowledge to the design and implementation of SDA software solutions</li>
 </ul>
 <p><b><br> Required Qualifications</b></p>
 <p> 4+ years experience designing and implementing capabilities in a Hadoop-based, Spark big data environment</p>
 <p> 4+ years experience in design and implementing real-time data pipelines using NiFi, Kafka and Spark Streaming </p>
 <p>4+ years experience processing large volumes of data with Hive, Pig, Spark, NiFi, Kafka and Python<br> </p>
 <p><br> <br> <b>Preferred Qualifications</b></p>
 <ul>
  <li> Familiarity with streaming big data applications like, Kafka, NiFi, Spark streaming/ Strom</li>
  <li> Strong background in Information System Security</li>
  <li> Excellent written and verbal communication skills</li>
  <li> Familiarity with statistical computing techniques, and statistics software (e.g. R and Tableau)</li>
  <li> Experience using source code management systems (e.g. Git)</li>
  <li> Interest and passion for learning about new technologies</li>
  <li> Experience in processing TB&#x2019;s worth of data</li>
 </ul>
 <p><b><br> Education</b></p>
 <p> Bachelor in engineering, computer science, or related fields</p>
 <p></p>
 <p><b> Pay Range</b></p>
 <p> The typical pay range for this role is:</p>
 <p></p> &#x24;94,500.00 - &#x24;196,000.00
 <p></p>
 <p> This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.<br> <br> In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company&#x2019;s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (&#x201c;PTO&#x201d;) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.<br> <br> For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits</p>
 <p></p>
 <p> CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.</p>
 <p></p>
 <p> You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.</p>
 <p></p>
 <p> CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.</p>
</div>",https://www.indeed.com/rc/clk?jk=b1473aeba6e7b340&atk=&xpse=SoDX67I3JpCphDAApr0LbzkdCdPP,b1473aeba6e7b340,,Full-time,,,"1425 Union Meeting Rd, Blue Bell, PA","Mgr, Security Data Engineer",Today,2023-10-21T13:02:49.467Z,3.2,44841.0,"$90,000 - $196,000 a year",2023-10-21T13:02:49.469Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=b1473aeba6e7b340&from=jasx&tk=1hd958u85jqvd802&vjs=3
321,Nuna,"At Nuna, our mission is to make high-quality healthcare affordable and accessible for everyone. We are dedicated to tackling one of our nation's biggest problems with ingenuity, creativity, and a keen moral compass.  Nuna is committed to simple principles: a rigorous understanding of data, modern technology, and most importantly, compassion and care for our fellow human. We want to know what really works, what doesn't—and why. 
   Nuna partners with healthcare payers, including government agencies and health plans, to turn data into learnings and information into meaning.
 
  YOUR TEAM 
  We build technology to enable users (from data scientists to analysts to policy-makers) to understand healthcare data while ensuring its integrity, security and privacy. Our work runs the gamut from joining streams of messy real-world data to building queryable data warehouses to constructing visualizations and dashboards that provide actionable insight. We build systems that are auditable, automated, an accurate representation of the underlying data, and, most importantly, responsive to our end users' needs. We strive for a creative, collaborative engineering environment that implements best practices of peer review, readability, maintainability, and security of the code base and infrastructure. 
  The Nuna Integration team is responsible for developing the automation platform that ingests customer and third-party data into the Nuna value-based care cloud. This role primarily focuses on backend software development. We work closely with health data experts to combine engineering excellence with healthcare expertise, addressing the diverse data needs of our customers. Our tasks include integrating large datasets and orchestrating various processes. We measure the success of our products by their ability to improve healthcare and reduce costs. 
  YOUR OPPORTUNITIES 
  In this role, you will play a pivotal role in shaping and executing our overall architecture and strategy. You will have the opportunity to work with cutting-edge technologies such as generative AI and collaborate with a highly skilled team of engineers and health data experts. Your work will directly impact our ability to acquire, transform, and deliver high-quality data to drive critical business decisions. 
  Your responsibilities will include: 
  
  Architect and Develop Data Ingestion Solutions: Design, develop, and maintain robust and scalable data ingestion pipelines that efficiently collect data from various sources, ensuring data quality and reliability. 
  Data Curation and Transformation: Transform and curate raw data into structured and usable formats. Implement data validation, cleaning, and enrichment processes to maintain data integrity 
  Performance Optimization: Continuously optimize data ingestion and curation processes for speed, efficiency, and scalability 
  Collaboration: Collaborate with cross-functional teams including data scientists, data analysts, and domain experts to understand data requirements and deliver actionable insights 
  Quality Assurance: Implement best practices for data quality monitoring, validation, and error handling to ensure data accuracy and reliability 
  Documentation: Maintain comprehensive documentation for data ingestion and curation processes, making it easy for team members to understand and use the pipelines 
  Mentorship: Provide technical leadership and mentorship to junior engineers, fostering their growth and development 
  Stay Current: Keep abreast of emerging technologies and industry best practices in data engineering and data management 
 
 QUALIFICATIONS 
 Required Qualifications 
 
  Bachelor's or Master's degree in Computer Science, Engineering, or related field 
  8 years of experience in software engineering with a focus on backend software development 
  Proficiency in programming languages such as Python, Javascript, Typescript, Go or Java 
  Strong experience with data processing frameworks and tools, e.g. Apache Spark or similar 
  Deep understanding of data storage and retrieval technologies, both relational and NoSQL databases 
  Experience with data modeling, ETL processes, and data warehousing 
  Excellent problem-solving skills and a passion for delivering high-quality data solutions 
  Ability to diagram, articulate, and document data science and engineering concepts 
  Strong communication and collaboration skills 
 
 Preferred Qualifications 
 
  Experience with cloud technologies in AWS or GCP, as well as container systems, such as Docker or Kubernetes 
  Familiarity with orchestration tools such as Airflow or Prefect 
  Experience with Great Expectations library 
  
 We take into account an individual's qualifications, skillset, and experience in determining final salary. This role is eligible for health insurance, life insurance, retirement benefits, participation in the company's equity program, paid time off, including vacation and sick leave. The expected salary range for this position is $165,000 to $230,000. The actual offer will be at the company's sole discretion and determined by relevant business considerations, including the final candidate's qualifications, years of experience, and skillset.
 
   Nuna is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics and/or veteran status.","<div>
 <div>
  <p><i>At Nuna, our mission is to make high-quality healthcare affordable and accessible for everyone. We are dedicated to tackling one of our nation&apos;s biggest problems with ingenuity, creativity, and a keen moral compass.</i><br> <br> <i>Nuna is committed to simple principles: a rigorous understanding of data, modern technology, and most importantly, compassion and care for our fellow human. We want to know what really works, what doesn&apos;t&#x2014;and why.</i></p> 
  <p><i> Nuna partners with healthcare payers, including government agencies and health plans, to turn data into learnings and information into meaning.</i></p>
 </div>
 <h2 class=""jobSectionHeader""><b> YOUR TEAM</b></h2> 
 <p> We build technology to enable users (from data scientists to analysts to policy-makers) to understand healthcare data while ensuring its integrity, security and privacy. Our work runs the gamut from joining streams of messy real-world data to building queryable data warehouses to constructing visualizations and dashboards that provide actionable insight. We build systems that are auditable, automated, an accurate representation of the underlying data, and, most importantly, responsive to our end users&apos; needs. We strive for a creative, collaborative engineering environment that implements best practices of peer review, readability, maintainability, and security of the code base and infrastructure.</p> 
 <p> The Nuna Integration team is responsible for developing the automation platform that ingests customer and third-party data into the Nuna value-based care cloud. This role primarily focuses on backend software development. We work closely with health data experts to combine engineering excellence with healthcare expertise, addressing the diverse data needs of our customers. Our tasks include integrating large datasets and orchestrating various processes. We measure the success of our products by their ability to improve healthcare and reduce costs.</p> 
 <h2 class=""jobSectionHeader""><b> YOUR OPPORTUNITIES</b></h2> 
 <p> In this role, you will play a pivotal role in shaping and executing our overall architecture and strategy. You will have the opportunity to work with cutting-edge technologies such as generative AI and collaborate with a highly skilled team of engineers and health data experts. Your work will directly impact our ability to acquire, transform, and deliver high-quality data to drive critical business decisions.</p> 
 <p> Your responsibilities will include:</p> 
 <ul> 
  <li>Architect and Develop Data Ingestion Solutions: Design, develop, and maintain robust and scalable data ingestion pipelines that efficiently collect data from various sources, ensuring data quality and reliability.</li> 
  <li>Data Curation and Transformation: Transform and curate raw data into structured and usable formats. Implement data validation, cleaning, and enrichment processes to maintain data integrity</li> 
  <li>Performance Optimization: Continuously optimize data ingestion and curation processes for speed, efficiency, and scalability</li> 
  <li>Collaboration: Collaborate with cross-functional teams including data scientists, data analysts, and domain experts to understand data requirements and deliver actionable insights</li> 
  <li>Quality Assurance: Implement best practices for data quality monitoring, validation, and error handling to ensure data accuracy and reliability</li> 
  <li>Documentation: Maintain comprehensive documentation for data ingestion and curation processes, making it easy for team members to understand and use the pipelines</li> 
  <li>Mentorship: Provide technical leadership and mentorship to junior engineers, fostering their growth and development</li> 
  <li>Stay Current: Keep abreast of emerging technologies and industry best practices in data engineering and data management</li> 
 </ul>
 <h2 class=""jobSectionHeader""><b>QUALIFICATIONS</b></h2> 
 <h4 class=""jobSectionHeader""><b>Required Qualifications</b></h4> 
 <ul>
  <li>Bachelor&apos;s or Master&apos;s degree in Computer Science, Engineering, or related field</li> 
  <li>8 years of experience in software engineering with a focus on backend software development</li> 
  <li>Proficiency in programming languages such as Python, Javascript, Typescript, Go or Java</li> 
  <li>Strong experience with data processing frameworks and tools, e.g. Apache Spark or similar</li> 
  <li>Deep understanding of data storage and retrieval technologies, both relational and NoSQL databases</li> 
  <li>Experience with data modeling, ETL processes, and data warehousing</li> 
  <li>Excellent problem-solving skills and a passion for delivering high-quality data solutions</li> 
  <li>Ability to diagram, articulate, and document data science and engineering concepts</li> 
  <li>Strong communication and collaboration skills</li> 
 </ul>
 <h4 class=""jobSectionHeader""><b>Preferred Qualifications</b></h4> 
 <ul>
  <li>Experience with cloud technologies in AWS or GCP, as well as container systems, such as Docker or Kubernetes</li> 
  <li>Familiarity with orchestration tools such as Airflow or Prefect</li> 
  <li>Experience with Great Expectations library</li> 
 </ul> 
 <p>We take into account an individual&apos;s qualifications, skillset, and experience in determining final salary. This role is eligible for health insurance, life insurance, retirement benefits, participation in the company&apos;s equity program, paid time off, including vacation and sick leave. The expected salary range for this position is &#x24;165,000 to &#x24;230,000. The actual offer will be at the company&apos;s sole discretion and determined by relevant business considerations, including the final candidate&apos;s qualifications, years of experience, and skillset.</p>
 <div>
  <p><i> Nuna is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics and/or veteran status.</i></p>
 </div>
</div>",https://www.indeed.com/rc/clk?jk=7430eb68479730fd&atk=&xpse=SoB967I3JpCopISQKJ0KbzkdCdPP,7430eb68479730fd,,,,,"San Francisco, CA","Staff Software Engineer, Data Integration",Today,2023-10-21T13:02:57.107Z,,,"$165,000 - $230,000 a year",2023-10-21T13:02:57.109Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=7430eb68479730fd&from=jasx&tk=1hd958u85jqvd802&vjs=3
325,Peraton,"Peraton Overview
  Peraton drives missions of consequence spanning the globe and extending to the farthest reaches of the galaxy. As the world's leading mission capability integrator and transformative enterprise IT provider, we deliver trusted and highly differentiated national security solutions and technologies that keep people safe and secure. Peraton serves as a valued partner to essential government agencies across the intelligence, space, cyber, defense, civilian, health, and state and local markets. Every day, our employees do the can't be done, solving the most daunting challenges facing our customers.
  
 Responsibilities
  
  Peraton is looking for a 
 Big Data Cloud Senior Software Engineer. The qualified candidate will join our highly performing team supporting the Securities and Exchange Commission (SEC), Office of Data Science (ODS) Data Engineering Program. This is a telework position. 
  
  
 What you will do: 
  
 
  Provide support across data life cycle and including but not limited to the planning, design, development, implementation, and operational support of the data models, ETL, Data Sourcing processes and related software programs. 
  The SEC ODS will require support in the adoption of cloud-based technologies such as Spark/PySpark, Databricks, Scala, AWS Glue and S3. 
  The position will require cooperation with a wide range of stakeholders such as end users, SMEs, and technical staff from data vendors such as FINRA and other SEC divisions such as Office of Information Technology (OIT). 
  Design, development and tuning of ETL/ELT processes involving massive data (order of TB). 
  Migration of datasets from on-prem databases to AWS. 
 
  
 Qualifications
  
  
 Required Qualifications: 
  
 
  Bachelor's Degree and a minimum of 12 years of experience. 
  Development of complex Python scripts. 
  Experience in development and support activities in cloud-based environments using big data technologies and programming languages and tools such as Scala, Spark, PySpark, Presto, Ranger, Hadoop, and Hive etc. (at least 3 years of experience). 
  Experience working in Amazon Cloud environment (AWS) utilizing tools such as S3, EMR, Databricks, Data Lakes, AWS Glue, Amazon StageMaker, Amazon Redshift etc. 
  Design, development and tuning of complex ETL/ELT processes involving massive data (order of TB). 
  Design, development and support of Web Services (RESTful/SOAP) & Web Scraping (HTTP, CSS and HTML) ETL/ELT processes. 
  Experience working with complex file formats of structured, unstructured and semi-structured data including JSON, XML, CSV, Avro, Parquet etc. 
  Data modelling using RDBMS such as Amazon Relation Database Services (RDS), and PostgreSQL, etc. 
  Development of clear and concise technical documentation including data models, architecture and data workflow diagrams, design documents, and deployment documents. 
  Experience working with Git, and GitLab. 
  Ability to create technical guidance documents with data maps and conditions. 
  Ability to work independently with Stakeholders and Industry to approve technical guidance. 
  Experience/Knowledge in AWS cloud tools and Redshift data warehouse service 
  Industry XML data exchange message formats, required modifications, or extensions knowledge. 
  Must be a US Citizen without Dual Citizenship 
  Must be able to obtain and maintain the required agency clearance (SEC Public Trust) 
  AWS Solutions Architect or Developer Certification 
 
  
 Preferred Qualifications: 
  
 
  Current or previous knowledge and experience working with financial capital markets data is a plus. 
 
  
 Benefits: 
  
  At Peraton, our benefits are designed to help keep you at your best beyond the work you do with us daily. We're fully committed to the growth of our employees. From fully comprehensive medical plans to tuition reimbursement, tuition assistance, and fertility treatment, we are there to support you all the way. 
  
  
 Target Salary Range
  
  $112,000 - $179,000. This represents the typical salary range for this position based on experience and other factors.
  
  
 SCA / Union / Intern Rate or Range
  
  
 EEO
  An Equal Opportunity Employer including Disability/Veteran.
  
  
 Our Values
  
  
 Benefits
  At Peraton, our benefits are designed to help keep you at your best beyond the work you do with us daily. We're fully committed to the growth of our employees. From fully comprehensive medical plans to tuition reimbursement, tuition assistance, and fertility treatment, we are there to support you all the way.
  
 
   Paid Time-Off and Holidays
   Retirement
   Life & Disability Insurance
   Career Development
   Tuition Assistance and Student Loan Financing
   Paid Parental Leave
   Additional Benefits
   Medical, Dental, & Vision Care","<div>
 <b>Peraton Overview</b>
 <br> Peraton drives missions of consequence spanning the globe and extending to the farthest reaches of the galaxy. As the world&apos;s leading mission capability integrator and transformative enterprise IT provider, we deliver trusted and highly differentiated national security solutions and technologies that keep people safe and secure. Peraton serves as a valued partner to essential government agencies across the intelligence, space, cyber, defense, civilian, health, and state and local markets. Every day, our employees do the can&apos;t be done, solving the most daunting challenges facing our customers.
 <br> 
 <b>Responsibilities</b>
 <br> 
 <br> Peraton is looking for a 
 <b>Big Data Cloud Senior Software Engineer.</b> The qualified candidate will join our highly performing team supporting the Securities and Exchange Commission (SEC), Office of Data Science (ODS) Data Engineering Program. This is a telework position. 
 <br> 
 <br> 
 <b>What you will do: </b>
 <br> 
 <ul>
  <li>Provide support across data life cycle and including but not limited to the planning, design, development, implementation, and operational support of the data models, ETL, Data Sourcing processes and related software programs. </li>
  <li>The SEC ODS will require support in the adoption of cloud-based technologies such as Spark/PySpark, Databricks, Scala, AWS Glue and S3. </li>
  <li>The position will require cooperation with a wide range of stakeholders such as end users, SMEs, and technical staff from data vendors such as FINRA and other SEC divisions such as Office of Information Technology (OIT). </li>
  <li>Design, development and tuning of ETL/ELT processes involving massive data (order of TB). </li>
  <li>Migration of datasets from on-prem databases to AWS. </li>
 </ul>
 <br> 
 <b>Qualifications</b>
 <br> 
 <br> 
 <b>Required Qualifications: </b>
 <br> 
 <ul>
  <li>Bachelor&apos;s Degree and a minimum of 12 years of experience. </li>
  <li>Development of complex Python scripts. </li>
  <li>Experience in development and support activities in cloud-based environments using big data technologies and programming languages and tools such as Scala, Spark, PySpark, Presto, Ranger, Hadoop, and Hive etc. (at least 3 years of experience). </li>
  <li>Experience working in Amazon Cloud environment (AWS) utilizing tools such as S3, EMR, Databricks, Data Lakes, AWS Glue, Amazon StageMaker, Amazon Redshift etc. </li>
  <li>Design, development and tuning of complex ETL/ELT processes involving massive data (order of TB). </li>
  <li>Design, development and support of Web Services (RESTful/SOAP) &amp; Web Scraping (HTTP, CSS and HTML) ETL/ELT processes. </li>
  <li>Experience working with complex file formats of structured, unstructured and semi-structured data including JSON, XML, CSV, Avro, Parquet etc. </li>
  <li>Data modelling using RDBMS such as Amazon Relation Database Services (RDS), and PostgreSQL, etc. </li>
  <li>Development of clear and concise technical documentation including data models, architecture and data workflow diagrams, design documents, and deployment documents. </li>
  <li>Experience working with Git, and GitLab. </li>
  <li>Ability to create technical guidance documents with data maps and conditions. </li>
  <li>Ability to work independently with Stakeholders and Industry to approve technical guidance. </li>
  <li>Experience/Knowledge in AWS cloud tools and Redshift data warehouse service </li>
  <li>Industry XML data exchange message formats, required modifications, or extensions knowledge. </li>
  <li>Must be a US Citizen without Dual Citizenship </li>
  <li>Must be able to obtain and maintain the required agency clearance (SEC Public Trust) </li>
  <li>AWS Solutions Architect or Developer Certification </li>
 </ul>
 <br> 
 <b>Preferred Qualifications: </b>
 <br> 
 <ul>
  <li>Current or previous knowledge and experience working with financial capital markets data is a plus. </li>
 </ul>
 <br> 
 <b>Benefits: </b>
 <br> 
 <br> At Peraton, our benefits are designed to help keep you at your best beyond the work you do with us daily. We&apos;re fully committed to the growth of our employees. From fully comprehensive medical plans to tuition reimbursement, tuition assistance, and fertility treatment, we are there to support you all the way. 
 <br> 
 <br> 
 <b>Target Salary Range</b>
 <br> 
 <br> &#x24;112,000 - &#x24;179,000. This represents the typical salary range for this position based on experience and other factors.
 <br> 
 <br> 
 <b>SCA / Union / Intern Rate or Range</b>
 <br> 
 <br> 
 <b>EEO</b>
 <br> An Equal Opportunity Employer including Disability/Veteran.
 <br> 
 <br> 
 <b>Our Values</b>
 <br> 
 <br> 
 <b>Benefits</b>
 <br> At Peraton, our benefits are designed to help keep you at your best beyond the work you do with us daily. We&apos;re fully committed to the growth of our employees. From fully comprehensive medical plans to tuition reimbursement, tuition assistance, and fertility treatment, we are there to support you all the way.
 <br> 
 <ul>
  <li> Paid Time-Off and Holidays</li>
  <li> Retirement</li>
  <li> Life &amp; Disability Insurance</li>
  <li> Career Development</li>
  <li> Tuition Assistance and Student Loan Financing</li>
  <li> Paid Parental Leave</li>
  <li> Additional Benefits</li>
  <li> Medical, Dental, &amp; Vision Care</li>
 </ul>
</div>",https://careers.peraton.com/jobs/big-data-cloud-senior-software-engineer-undefined-undefined-jobs?iis=HireClix&iisn=Indeed,8b1b5f4e9d03f613,,Full-time,,,United States,Big Data Cloud Senior Software Engineer,1 day ago,2023-10-20T13:03:18.757Z,3.1,243.0,"$112,000 - $179,000 a year",2023-10-21T13:03:18.760Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BWrJOJIc9CpN6yMpv0V0AydpTkzwx7H4nhH8WAx3qz9DFPmyzxY6nkFs2NPrIYzV9U3hcwjKaVA33BgMnWMd4v82SJXW57QvVteSBjEghI4CKLr4s-c_H_ClqiqWF9tzO7sptnKtrlhohHAmwGl4AO6yiT79N_KfFBX4u0ZAjiFobWpkFW-oTgzb629thYEstYrkZDTnngfOVnBQyesf2tUgboUIxXmiYGfvE1YiAK3AevTI70hqJhj48jImR9GNRI2smRV9c5ycAZ_d1gb3cJA9HM9mxmqjJPzEZRAc7xtWt8f4dBrc4a3tdZzSY8rxXk7vRupvkfdDGQzqQiXTCkWeSnlLihKTc9EAsdfhTjCmDItr5oA2VhtABa3QLOKhLB1W2Vd1c1Z2hKabG8_f-fSMC2Fpt_qWwqvgmw3di0_nhvrXW1R09NcDNpLE0qE-Liw9uJZiEqk5RlJwZ1BWUCnRykA8zPHMgL2NJsjUds1SH2NOanepGVaaxdfu11tOM1Sf5hZuT_WSM-DzG0QX9Y4011kcsPs3IQ4pPjeTBvD3ktadjnCjOa2ivqegPEAm6_LQtvfsCb1hsqe31TIFQO3mQ_swd4fznIdGcI7U0BM43oL7jLHyqxakes_amvJ-MFk9Vaw4BO-x8If8NYFvXXkn5JGXthW5PYdgax9kZW_HfJ961retTYT5V4It9VbT3Q0yco86nZJHsWZZuCl6EdKn85IF5reniRU9mf3y0gQA%3D%3D&xkcb=SoCg-_M3JpCnpdRkAB0NbzkdCdPP&p=6&fvj=0&vjs=3&jsa=8129&tk=1hd95a3qqi9hs800&from=jasx&wvign=1
343,Care.com,"About Care.com 
  Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that. 
  Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you. 
  Office Locations: (This is a hybrid position) 
 
  NY, NY 10011 
  Austin, TX 78746 
  Shelton, CT 06484 
  
 What Your Days Will be Like: 
  The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com. 
  The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment. 
  What You'll Be Working On: 
  
  Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack 
  Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse 
  Identify data sources used for building out data architecture diagrams/models 
  Establish engineering practices and setup frameworks for ""Data as a Service"" 
  Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology. 
  Solve code level problems quickly and efficiently 
  Participate in demos and code reviews 
  Promote software best approach, standards, and processes 
  Shape development processes to promote a high-quality output while continuing to iterate quickly 
  Incorporate best practices for security, performance, and data privacy into data pipelines 
  
 What You'll Need to Succeed: 
  
  BS or MS in Computer Science or relevant engineering experience 
  5+ years work experience in Data Engineering/data pipelines 
  3+ years SQL experience is a must 
  1+ years Unix/batch scripting preferred 
  1+ years Python experience is a plus 
  1+ years Windows server admin experience is a plus 
  Experience interfacing with business teams and turning requirements and vision into a technical reality 
  MySQL & Vertica Experience a plus/preferred 
  AWS experience is a plus 
  Ability to drive efforts from start to finish as a self-motivator 
  Knowledge in Data Warehousing is a MUST 
  Proven ability to maintain performance level in a fast-paced agile environment 
  Pragmatic and realistic with solutions 
  
 For a list of our Perks + Benefits, click here! 
  Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com. 
  Company Overview: 
  Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC). 
  Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).","<div>
 <p><b>About Care.com</b></p> 
 <p> Care.com is a consumer tech company with heart. We&apos;re on a mission to solve a human challenge we all face: finding great care for the ones we love. We&apos;re moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.</p> 
 <p> Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we&apos;re applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you&apos;re all about using your talent for good, Care.com is the place for you.</p> 
 <h2 class=""jobSectionHeader""><b> Office Locations: (This is a hybrid position)</b></h2> 
 <ul>
  <li><b>NY, NY</b> 10011</li> 
  <li><b>Austin, TX</b> 78746</li> 
  <li><b>Shelton, CT</b> 06484</li> 
 </ul> 
 <p><b>What Your Days Will be Like:</b></p> 
 <p> The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.</p> 
 <p> The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.</p> 
 <p><b> What You&apos;ll Be Working On:</b></p> 
 <ul> 
  <li>Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack</li> 
  <li>Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse</li> 
  <li>Identify data sources used for building out data architecture diagrams/models</li> 
  <li>Establish engineering practices and setup frameworks for &quot;Data as a Service&quot;</li> 
  <li>Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.</li> 
  <li>Solve code level problems quickly and efficiently</li> 
  <li>Participate in demos and code reviews</li> 
  <li>Promote software best approach, standards, and processes</li> 
  <li>Shape development processes to promote a high-quality output while continuing to iterate quickly</li> 
  <li>Incorporate best practices for security, performance, and data privacy into data pipelines</li> 
 </ul> 
 <p><b>What You&apos;ll Need to Succeed:</b></p> 
 <ul> 
  <li>BS or MS in Computer Science or relevant engineering experience</li> 
  <li>5+ years work experience in Data Engineering/data pipelines</li> 
  <li>3+ years SQL experience is a must</li> 
  <li>1+ years Unix/batch scripting preferred</li> 
  <li>1+ years Python experience is a plus</li> 
  <li>1+ years Windows server admin experience is a plus</li> 
  <li>Experience interfacing with business teams and turning requirements and vision into a technical reality</li> 
  <li>MySQL &amp; Vertica Experience a plus/preferred</li> 
  <li>AWS experience is a plus</li> 
  <li>Ability to drive efforts from start to finish as a self-motivator</li> 
  <li>Knowledge in Data Warehousing is a MUST</li> 
  <li>Proven ability to maintain performance level in a fast-paced agile environment</li> 
  <li>Pragmatic and realistic with solutions</li> 
 </ul> 
 <p><b>For a list of our Perks + Benefits, click</b> <b>here!</b></p> 
 <p> Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.</p> 
 <p><b> Company Overview:</b></p> 
 <p> Available in more than 20 countries, Care.com is the world&apos;s leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today&apos;s families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com&apos;s industry-leading products&#x2014;from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).</p> 
 <p><i> Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. </i><b><i>Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance.</i></b><i> The range listed is just one component of Care.com&apos;s total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).</i></p>
</div>
<p></p>",https://www.care.com/vis/careers/job/5451121?gh_jid=5451121&gh_src=d04190671us&utm_medium=organic,87c74988107a90d3,,,,,Hybrid remote,Data Engineer,1 day ago,2023-10-20T13:04:24.819Z,4.2,1860.0,"$110,000 - $145,000 a year",2023-10-21T13:04:24.821Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=87c74988107a90d3&from=jasx&tk=1hd95bl7pje34802&vjs=3
364,eBay,"Looking for a company that inspires passion, courage and creativity, where you can be on the team shaping the future of global commerce? Want to shape how millions of people buy, sell, connect, and share around the world? If you’re interested in joining a purpose driven community that is dedicated to crafting an ambitious and inclusive work environment, join eBay – a company you can be proud to be with.
 
  Core Technology (CT) is a distributed team responsible for the end-to-end eBay technology platform. This platform runs our entire infrastructure and all the services that come together to form ebay.com.
 
  We are looking for a senior engineer who can design complex, scalable distributed systems within eBay's data infrastructure and media platform. You’ll be working with a world class team of engineers, designers, and product managers to create amazing new features on one of the world’s most visited e-commerce sites.
 
  Join our team of engineers who enjoy working on large-scale, challenging, and interesting problems. We work in small, focused, collaborative project teams following an agile methodology.
 
  About the team:
  We are the Data Infrastructure Services and Clients + Media Platform team. We build the client and service tiers for reliable, scalable, and distributed data products and media platform. The problems we solve range from finding ways to scale legacy databases beyond their scaling limits to building next-generation cloud-native distributed databases in-house to building cutting-edge media technologies managing images and videos on the eBay platform. We collaborate with other engineering teams, product managers, and architects to create new features on one of the world’s most visited e-commerce sites. Our team is relatively small, but we solve huge problems.
 
  You will apply your technical skills to develop solutions that meet feature requirements and align to implementation schedules, development goals and programming principles. Our core technical stack is Java/J2EE with a focus on database access clients and distributed services, relational document, KV, and graph databases.
 
  Role Responsibilities
 
   Launch critical projects & lead all phases of the software development lifecycle - requirement/design reviews, development, testing and code reviews
   Develop innovative solutions that meet market needs and align to implementation schedules, development goals and principles
   Create testing harness/infrastructure/test cases/automation geared towards high quality products
   Estimate engineering effort, mitigate project risks, plan implementation, and rollout system changes
   Lead product initiatives, recommend improvements, mentor junior team members
   Self-learner: has a constant interest in learning new things and independently seeks areas of improvement for personal growth
   Apply Java, Oracle DB, NoSQL and other technologies eBay is using such as NodeJS, Spring framework, etc.
 
 
  Job Requirements
 
   Excellent understanding of computer science fundamentals, data structures and algorithms
   Strong know-how in object-oriented software design and patterns
   Strong knowledge of Restful API, relational and NoSql databases
   Writing clean, simple, and tested code to support new product features
   Evangelize, mentor, and promote development standard methodologies throughout the company
   Provide scope on proposed features and bug fixes
   Excellent communication, interpersonal and analytical skills
   8+ years of software development experience
   Master's/PhD degree in computer science/engineering or equivalent professional experience
 
 
  Benefits are an essential part of your total compensation for the work you do every day. Whether you’re single, in a growing family, or nearing retirement, eBay offers a variety of comprehensive and competitive benefit programs to meet your needs. Including maternal & paternal leave, paid sabbatical, and plans to help ensure your financial security today and in the years ahead because we know feeling financially secure during your working years and through retirement is important.
 
  Here at eBay, we love creating opportunities for others by connecting people from widely diverse backgrounds, perspectives, and geographies. So, being diverse and inclusive isn’t just something we strive for, it is who we are, and part of what we do each and every single day. We want to ensure that as an employee, you feel eBay is a place where, no matter who you are, you feel safe, included, and that you have the opportunity to bring your unique self to work. To learn about eBay’s Diversity & Inclusion click here: https://www.ebayinc.com/company/diversity-inclusion/
 
  The pay range for this position at commencement of employment in California, Washington, or New York is expected in the range below. $149,200 - $234,850
  Base pay offered may vary depending on multiple individualized factors, including location, skills, and experience. The total compensation package for this position may also include other elements, including a target bonus and restricted stock units (as applicable) in addition to a full range of medical, financial, and/or other benefits (including 401(k) eligibility and various paid time off benefits, such as PTO and parental leave). Details of participation in these benefit plans will be provided if an employee receives an offer of employment.
  If hired, employees will be in an “at-will position” and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors.
 
  eBay Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talent@ebay.com. We will make every effort to respond to your request for disability assistance as soon as possible. View our accessibility info to learn more about eBay's commitment to ensuring digital accessibility for people with disabilities. For more information see: EEO is the Law Poster and EEO is the Law Poster Supplement.
 
  Jobs posted with location as ""Remote - United States (Excludes: HI, NM)"" excludes residents of Hawaii and New Mexico.","<div>
 <p>Looking for a company that inspires passion, courage and creativity, where you can be on the team shaping the future of global commerce? Want to shape how millions of people buy, sell, connect, and share around the world? If you&#x2019;re interested in joining a purpose driven community that is dedicated to crafting an ambitious and inclusive work environment, join eBay &#x2013; a company you can be proud to be with.</p>
 <p></p>
 <p> Core Technology (CT) is a distributed team responsible for the end-to-end eBay technology platform. This platform runs our entire infrastructure and all the services that come together to form ebay.com.</p>
 <p></p>
 <p> We are looking for a senior engineer who can design complex, scalable distributed systems within eBay&apos;s data infrastructure and media platform. You&#x2019;ll be working with a world class team of engineers, designers, and product managers to create amazing new features on one of the world&#x2019;s most visited e-commerce sites.</p>
 <p></p>
 <p> Join our team of engineers who enjoy working on large-scale, challenging, and interesting problems. We work in small, focused, collaborative project teams following an agile methodology.</p>
 <p></p>
 <h2 class=""jobSectionHeader""><b> About the team:</b></h2>
 <p> We are the Data Infrastructure Services and Clients + Media Platform team. We build the client and service tiers for reliable, scalable, and distributed data products and media platform. The problems we solve range from finding ways to scale legacy databases beyond their scaling limits to building next-generation cloud-native distributed databases in-house to building cutting-edge media technologies managing images and videos on the eBay platform. We collaborate with other engineering teams, product managers, and architects to create new features on one of the world&#x2019;s most visited e-commerce sites. Our team is relatively small, but we solve huge problems.</p>
 <p></p>
 <p> You will apply your technical skills to develop solutions that meet feature requirements and align to implementation schedules, development goals and programming principles. Our core technical stack is Java/J2EE with a focus on database access clients and distributed services, relational document, KV, and graph databases.</p>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Role Responsibilities</b></h2>
 <ul>
  <li><p> Launch critical projects &amp; lead all phases of the software development lifecycle - requirement/design reviews, development, testing and code reviews</p></li>
  <li><p> Develop innovative solutions that meet market needs and align to implementation schedules, development goals and principles</p></li>
  <li><p> Create testing harness/infrastructure/test cases/automation geared towards high quality products</p></li>
  <li><p> Estimate engineering effort, mitigate project risks, plan implementation, and rollout system changes</p></li>
  <li><p> Lead product initiatives, recommend improvements, mentor junior team members</p></li>
  <li><p> Self-learner: has a constant interest in learning new things and independently seeks areas of improvement for personal growth</p></li>
  <li><p> Apply Java, Oracle DB, NoSQL and other technologies eBay is using such as NodeJS, Spring framework, etc.</p></li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Job Requirements</b></h2>
 <ul>
  <li><p> Excellent understanding of computer science fundamentals, data structures and algorithms</p></li>
  <li><p> Strong know-how in object-oriented software design and patterns</p></li>
  <li><p> Strong knowledge of Restful API, relational and NoSql databases</p></li>
  <li><p> Writing clean, simple, and tested code to support new product features</p></li>
  <li><p> Evangelize, mentor, and promote development standard methodologies throughout the company</p></li>
  <li><p> Provide scope on proposed features and bug fixes</p></li>
  <li><p> Excellent communication, interpersonal and analytical skills</p></li>
  <li><p> 8+ years of software development experience</p></li>
  <li><p> Master&apos;s/PhD degree in computer science/engineering or equivalent professional experience</p></li>
 </ul>
 <p></p>
 <p> Benefits are an essential part of your total compensation for the work you do every day. Whether you&#x2019;re single, in a growing family, or nearing retirement, eBay offers a variety of comprehensive and competitive benefit programs to meet your needs. Including maternal &amp; paternal leave, paid sabbatical, and plans to help ensure your financial security today and in the years ahead because we know feeling financially secure during your working years and through retirement is important.</p>
 <p></p>
 <p> Here at eBay, we love creating opportunities for others by connecting people from widely diverse backgrounds, perspectives, and geographies. So, being diverse and inclusive isn&#x2019;t just something we strive for, it is who we are, and part of what we do each and every single day. We want to ensure that as an employee, you feel eBay is a place where, no matter who you are, you feel safe, included, and that you have the opportunity to bring your unique self to work. To learn about eBay&#x2019;s Diversity &amp; Inclusion click here: https://www.ebayinc.com/company/diversity-inclusion/</p>
 <p></p>
 <p> The pay range for this position at commencement of employment in California, Washington, or New York is expected in the range below.</p> &#x24;149,200 - &#x24;234,850
 <p> Base pay offered may vary depending on multiple individualized factors, including location, skills, and experience. The total compensation package for this position may also include other elements, including a target bonus and restricted stock units (as applicable) in addition to a full range of medical, financial, and/or other benefits (including 401(k) eligibility and various paid time off benefits, such as PTO and parental leave). Details of participation in these benefit plans will be provided if an employee receives an offer of employment.</p>
 <p> If hired, employees will be in an &#x201c;at-will position&#x201d; and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors.</p>
 <p></p>
 <p> eBay Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talent@ebay.com. We will make every effort to respond to your request for disability assistance as soon as possible. View our accessibility info to learn more about eBay&apos;s commitment to ensuring digital accessibility for people with disabilities. For more information see: EEO is the Law Poster and EEO is the Law Poster Supplement.</p>
 <p></p>
 <p> Jobs posted with location as &quot;Remote - United States (Excludes: HI, NM)&quot; excludes residents of Hawaii and New Mexico.</p>
</div>
<p></p>",https://www.indeed.com/rc/clk?jk=dbbbf7562df97384&atk=&xpse=SoDS67I3JpDPkxxcBx0LbzkdCdPP,dbbbf7562df97384,,,,,"San Jose, CA 95125",Staff Java Software Engineer - Data,23 days ago,2023-09-28T13:06:21.323Z,4.0,2419.0,"$149,200 - $234,850 a year",2023-10-21T13:06:21.324Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=dbbbf7562df97384&from=jasx&tk=1hd95ff4chbht800&vjs=3
369,Equitable,"Senior Data Engineer, Marketing Analytics (230000SU)
 
 
   Primary Location 
  : UNITED STATES-NC-Charlotte
 
 
   Organization 
  : Equitable
 
 
   Schedule 
  : Full-time
 
 
   Description
  
   At Equitable, our power is in our people.  We're individuals from different cultures and backgrounds. Those differences make us stronger as a team and a force for good in our communities. Here, you'll work with dynamic individuals, build your skills, and unleash new ways of working and thinking. Are you ready to join an organization that will help unlock your potential?
   
   
   We are seeking a highly motivated and collaborative Sr. Data Engineer to join our Marketing Analytics team. If you have a passion for building and optimizing data pipelines in the cloud, possess deep expertise in Databricks, SQL, Python, and PySpark, and excel at solving complex problems, we want to hear from you.
   
   
   As a Sr. Data Engineer, you will play a pivotal role in designing, implementing, and optimizing our data infrastructure, pipelines, and integration. Your work will be central to our ability to analyze and leverage data for marketing insights. You will collaborate closely with cross-functional teams, including marketing, IT, and business units, to ensure our data solutions align with business needs. The ideal candidate has a strong track record of managing data pipelines, extensive experience in working with large datasets, and the ability to bridge the gap between technical and business requirements.
   
   
   If you are a collaborative problem solver with a proven background in data engineering, a deep understanding of data pipelines, and expertise in Databricks, SQL, Python, and PySpark, we encourage you to apply for this exciting opportunity. Join our team at Equitable and play a pivotal role in shaping our data-driven marketing initiatives.
   
   
   Key Job Responsibilities
   
   
  
   Design, build, and optimize data pipelines, architectures, and datasets to support marketing analytics and decision-making. 
   Develop and maintain code using Databricks, SQL, Python, and PySpark to ensure efficient data processing. 
   Manage marketing and sales data workflows using tools such as Marketing Cloud Intelligence (Datorama) and Salesforce Marketing Cloud and others. 
   Collaborate with cross-functional teams in a dynamic environment to understand and address their data needs. 
   Support and partner with the IT organization to implement data solutions in an enterprise fashion. 
   Utilize Cloudera and Azure DevOps to enhance data infrastructure and manage data workflows. 
   Proactively identify opportunities to improve data quality, reliability, and accessibility. 
   Troubleshoot and resolve data-related issues to minimize disruptions to analytics processes. 
   Employ strong project management and organizational skills to deliver on time and within scope. 
   Mentor junior team members and share best practices in data engineering. 
   
  The base salary range for this position is $67,000-$100,000. Actual base salaries vary based on skills, experience, and geographical location. In addition to base pay, Equitable provides compensation to reward performance with base salary increases, spot bonuses, and short-term incentive compensation opportunities. Eligibility for these programs depends on level and functional area of responsibility. 
   For eligible employees, Equitable provides a full range of benefits. This includes medical, dental, vision, a 401(k) plan, and paid time off. For detailed descriptions of these benefits, please reference the link below. 
   Equitable Pay and Benefits: Equitable Total Rewards Program 
   
  
 
 
  Qualifications
   
   
   
  Required Qualifications
   
   
   
   Bachelor's degree in Computer Science, Data Engineering, or a STEM related field; advanced degree is a plus. 
   5+ years of proven experience in a Data Engineer role, with a track record of building and optimizing data models, data pipelines, and data integration. 
   5+ years expertise in SQL and Python for data processing and analysis. 
   5+ years expertise in Databricks and PySpark for developing cloud data pipelines. 
   2+ years experience working with Azure Cloud and Azure DevOps. 
   2+ years experience with Datorama and Salesforce Marketing Cloud.
  
   
   
   Preferred Qualifications
   
   
   
   Experience working with Rest API’s and SFTP to access, ingest, transform first party data and third party data. 
   Strong project management and organizational skills to manage complex data projects. 
   Experience working with large datasets and a deep understanding of enterprise data management principles. 
   Ability to collaborate effectively with cross-functional teams in a fast-paced environment. 
   Experience working with Change Management and CI/CD 
   Exceptional problem-solving skills and attention to detail.
  
   
   
   Skills
   
   
   Business Data Analysis: Knowledge of business data analysis; ability to collect, identify, analyze and interpret business data using various kinds of techniques to meet business needs and requirements.
   
   
   Cross-functional Collaboration: Knowledge of collaborative techniques and approaches; ability to promote a culture of continuous improvement and working together across functions to solve business problems and meet business goals.
   
   
   Data Analysis Tools: Knowledge of key uses and benefits of data analysis tools; ability to utilize data analysis tools to identify factors influencing business performance and to gain greater insight into trends within a business, industry and customer base.
   
   
   Data Gathering and Analysis: Knowledge of data gathering and analysis tools, techniques and processes; ability to gather and analyze data on the learning needs of a target population.
   
   
   Industry Knowledge: Knowledge of the organization's industry group, trends, directions, major issues, regulatory considerations, and trendsetters; ability to apply industry knowledge appropriately to diverse situations.
   
   
   Market Research: Knowledge of market research; ability to collect, collate and analyze information about existing or potential markets and market needs.
   
   
   Storytelling: Knowledge of concepts and ability to plan, create and present business proposals, initiatives and ideas by storytelling actual business scenarios that are situation-specific, engaging, memorable and persuasive as compared to one-way, fact-based presentations.
   
   
   Diversity, Equity and Inclusion: Demonstrates a commitment to Diversity, Equity and Inclusion by treating everyone with respect and dignity, ensuring all voices are heard and advocating for change.
   
   
   ABOUT EQUITABLE  At Equitable, we’re a team of over ten thousand strong; committed to helping our clients secure their financial well-being so that they can pursue long and fulfilling lives.  We turn challenges into opportunities by thinking, working, and leading differently – where everyone is a leader. We encourage every employee to leverage their unique talents to become a force for good at Equitable and in their local communities.  We are continuously investing in our people by offering growth, internal mobility, comprehensive compensation and benefits to support overall well-being, flexibility, and a culture of collaboration and teamwork.  We are looking for talented, dedicated, purposeful people who want to make an impact. Join Equitable and pursue a career with purpose.  **********  Equitable is committed to providing equal employment opportunities to our employees, applicants and candidates based on individual qualifications, without regard to race, color, religion, gender, gender identity and expression, age, national origin, mental or physical disabilities, sexual orientation, veteran status, genetic information or any other class protected by federal, state and local laws.  NOTE: Equitable participates in the E-Verify program.  If reasonable accommodation is needed to participate in the job application or interview process or to perform the essential job functions of this position, please contact Human Resources at (212) 314-2211 or email us at TalentAcquisition@equitable.com. 
   #LI-Remote","<div>
 <div>
  <p><b>Senior Data Engineer, Marketing Analytics</b></p> (230000SU)
 </div>
 <div>
  <b> Primary Location</b> 
  <b>:</b> UNITED STATES-NC-Charlotte
 </div>
 <div>
  <b> Organization</b> 
  <b>:</b> Equitable
 </div>
 <div>
  <b> Schedule</b> 
  <b>:</b> Full-time
 </div>
 <div>
  <b> Description</b>
  <div></div>
  <p><br> At Equitable, our power is in our people.<br> <br> We&apos;re individuals from different cultures and backgrounds. Those differences make us stronger as a team and a force for good in our communities. Here, you&apos;ll work with dynamic individuals, build your skills, and unleash new ways of working and thinking. Are you ready to join an organization that will help unlock your potential?</p>
  <br> 
  <p></p> 
  <p> We are seeking a highly motivated and collaborative Sr. Data Engineer to join our Marketing Analytics team. If you have a passion for building and optimizing data pipelines in the cloud, possess deep expertise in Databricks, SQL, Python, and PySpark, and excel at solving complex problems, we want to hear from you.</p>
  <br> 
  <p></p> 
  <p> As a Sr. Data Engineer, you will play a pivotal role in designing, implementing, and optimizing our data infrastructure, pipelines, and integration. Your work will be central to our ability to analyze and leverage data for marketing insights. You will collaborate closely with cross-functional teams, including marketing, IT, and business units, to ensure our data solutions align with business needs. The ideal candidate has a strong track record of managing data pipelines, extensive experience in working with large datasets, and the ability to bridge the gap between technical and business requirements.</p>
  <br> 
  <p></p> 
  <p> If you are a collaborative problem solver with a proven background in data engineering, a deep understanding of data pipelines, and expertise in Databricks, SQL, Python, and PySpark, we encourage you to apply for this exciting opportunity. Join our team at Equitable and play a pivotal role in shaping our data-driven marketing initiatives.</p>
  <br> 
  <p></p> 
  <p><b> Key Job Responsibilities</b></p>
  <br> 
  <p><b> </b></p>
  <ul>
   <li>Design, build, and optimize data pipelines, architectures, and datasets to support marketing analytics and decision-making.</li> 
   <li>Develop and maintain code using Databricks, SQL, Python, and PySpark to ensure efficient data processing.</li> 
   <li>Manage marketing and sales data workflows using tools such as Marketing Cloud Intelligence (Datorama) and Salesforce Marketing Cloud and others.</li> 
   <li>Collaborate with cross-functional teams in a dynamic environment to understand and address their data needs.</li> 
   <li>Support and partner with the IT organization to implement data solutions in an enterprise fashion.</li> 
   <li>Utilize Cloudera and Azure DevOps to enhance data infrastructure and manage data workflows.</li> 
   <li>Proactively identify opportunities to improve data quality, reliability, and accessibility.</li> 
   <li>Troubleshoot and resolve data-related issues to minimize disruptions to analytics processes.</li> 
   <li>Employ strong project management and organizational skills to deliver on time and within scope.</li> 
   <li>Mentor junior team members and share best practices in data engineering.</li> 
  </ul> 
  <p>The base salary range for this position is &#x24;67,000-&#x24;100,000. Actual base salaries vary based on skills, experience, and geographical location. In addition to base pay, Equitable provides compensation to reward performance with base salary increases, spot bonuses, and short-term incentive compensation opportunities. Eligibility for these programs depends on level and functional area of responsibility.</p> 
  <p> For eligible employees, Equitable provides a full range of benefits. This includes medical, dental, vision, a 401(k) plan, and paid time off. For detailed descriptions of these benefits, please reference the link below.</p> 
  <p><b> Equitable Pay and Benefits</b>: Equitable Total Rewards Program<br> </p>
  <div> 
  </div>
 </div>
 <div>
  <b>Qualifications</b>
  <br> 
  <p></p> 
  <p></p> 
  <p><b>Required Qualifications</b></p>
  <br> 
  <p></p> 
  <ul> 
   <li>Bachelor&apos;s degree in Computer Science, Data Engineering, or a STEM related field; advanced degree is a plus.</li> 
   <li>5+ years of proven experience in a Data Engineer role, with a track record of building and optimizing data models, data pipelines, and data integration.</li> 
   <li>5+ years expertise in SQL and Python for data processing and analysis.</li> 
   <li>5+ years expertise in Databricks and PySpark for developing cloud data pipelines.</li> 
   <li>2+ years experience working with Azure Cloud and Azure DevOps.</li> 
   <li>2+ years experience with Datorama and Salesforce Marketing Cloud.</li>
  </ul>
  <br> 
  <p></p> 
  <p><b> Preferred Qualifications</b></p>
  <br> 
  <p></p> 
  <ul> 
   <li>Experience working with Rest API&#x2019;s and SFTP to access, ingest, transform first party data and third party data.</li> 
   <li>Strong project management and organizational skills to manage complex data projects.</li> 
   <li>Experience working with large datasets and a deep understanding of enterprise data management principles.</li> 
   <li>Ability to collaborate effectively with cross-functional teams in a fast-paced environment.</li> 
   <li>Experience working with Change Management and CI/CD</li> 
   <li>Exceptional problem-solving skills and attention to detail.</li>
  </ul>
  <br> 
  <p></p> 
  <p><b> Skills</b></p>
  <br> 
  <p></p> 
  <p><b> Business Data Analysis:</b> Knowledge of business data analysis; ability to collect, identify, analyze and interpret business data using various kinds of techniques to meet business needs and requirements.</p>
  <br> 
  <p></p> 
  <p><b> Cross-functional Collaboration:</b> Knowledge of collaborative techniques and approaches; ability to promote a culture of continuous improvement and working together across functions to solve business problems and meet business goals.</p>
  <br> 
  <p></p> 
  <p><b> Data Analysis Tools:</b> Knowledge of key uses and benefits of data analysis tools; ability to utilize data analysis tools to identify factors influencing business performance and to gain greater insight into trends within a business, industry and customer base.</p>
  <br> 
  <p></p> 
  <p><b> Data Gathering and Analysis:</b> Knowledge of data gathering and analysis tools, techniques and processes; ability to gather and analyze data on the learning needs of a target population.</p>
  <br> 
  <p></p> 
  <p><b> Industry Knowledge:</b> Knowledge of the organization&apos;s industry group, trends, directions, major issues, regulatory considerations, and trendsetters; ability to apply industry knowledge appropriately to diverse situations.</p>
  <br> 
  <p></p> 
  <p><b> Market Research:</b> Knowledge of market research; ability to collect, collate and analyze information about existing or potential markets and market needs.</p>
  <br> 
  <p></p> 
  <p><b> Storytelling:</b> Knowledge of concepts and ability to plan, create and present business proposals, initiatives and ideas by storytelling actual business scenarios that are situation-specific, engaging, memorable and persuasive as compared to one-way, fact-based presentations.</p>
  <br> 
  <p></p> 
  <p><b> Diversity, Equity and Inclusion:</b> Demonstrates a commitment to Diversity, Equity and Inclusion by treating everyone with respect and dignity, ensuring all voices are heard and advocating for change.</p>
  <br> 
  <p></p> 
  <p><b> ABOUT EQUITABLE</b><br> <br> At Equitable, we&#x2019;re a team of over ten thousand strong; committed to helping our clients secure their financial well-being so that they can pursue long and fulfilling lives.<br> <br> We turn challenges into opportunities by thinking, working, and leading differently &#x2013; where everyone is a leader. We encourage every employee to leverage their unique talents to become a force for good at Equitable and in their local communities.<br> <br> We are continuously investing in our people by offering growth, internal mobility, comprehensive compensation and benefits to support overall well-being, flexibility, and a culture of collaboration and teamwork.<br> <br> We are looking for talented, dedicated, purposeful people who want to make an impact. Join Equitable and pursue a career with purpose.<br> <br> **********<br> <br> Equitable is committed to providing equal employment opportunities to our employees, applicants and candidates based on individual qualifications, without regard to race, color, religion, gender, gender identity and expression, age, national origin, mental or physical disabilities, sexual orientation, veteran status, genetic information or any other class protected by federal, state and local laws.<br> <br> NOTE: Equitable participates in the E-Verify program.<br> <br> If reasonable accommodation is needed to participate in the job application or interview process or to perform the essential job functions of this position, please contact Human Resources at (212) 314-2211 or email us at TalentAcquisition@equitable.com.</p> 
  <p> #LI-Remote</p>
 </div>
</div>
<div></div>",https://www.indeed.com/rc/clk?jk=2a6492ae7ded8c78&atk=&xpse=SoBm67I3JpDMhTyQDR0LbzkdCdPP,2a6492ae7ded8c78,,Full-time,,,"Charlotte, NC","Senior Data Engineer, Marketing Analytics",17 days ago,2023-10-04T13:06:46.897Z,3.4,562.0,"$67,000 - $100,000 a year",2023-10-21T13:06:46.899Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2a6492ae7ded8c78&from=jasx&tk=1hd95ff4chbht800&vjs=3
370,ManTech International Corporation,"Secure our Nation, Ignite your Future 
 
 Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International Corporation, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement. 
 
 Currently, ManTech is seeking a motivated, career and customer-oriented Data Analyst/Engineer to join our team. 
 
 Each day U.S. Customs and Border Protection (CBP) oversees the massive flow of people, capital, and products that enter and depart the United States via air, land, sea, and cyberspace. The volume and complexity of both physical and virtual border crossings require the application of solutions to promote efficient trade and travel. Further, effective solutions help CBP ensure the movement of people, capital, and products is legal, safe, and secure. In response to this challenge, ManTech, as a trusted mission partner of CBP, seeks capable, qualified, and versatile Senior Data Engineers to facilitate data-driven decision making in response to national security threats. 
 
 Responsibilities include, but are not limited to: 
 
  Responsible for data analysis of large database tables to understand the data structures, definitions and patterns which will be used to support various predictive models 
  Working closely with client to assist in managing data needs, and supporting collection of data needed for various operational needs. 
  Data analysis, problem solving, investigation and creative thinking with massive amount of data supporting variety of operational scenarios 
  Assist with implementing cloud techniques and workflows (on premise to Cloud Platforms) 
  Respond to data queries/analysis requests from various groups within an organization. Create and publish regularly scheduled and/or ad hoc reports as needed. 
  Researching and documenting data definitions for all subject areas and primary data sets supporting the core business applications. 
  Responsible for source code control using GitLab 
  Demonstrate a strong practical understanding of application-relevant cargo and passenger data and databases used to support analytic application development, functionality and targeting end user (officer) operation. 
 
 
 Basic Qualifications: 
 
  Minimum 4-5 years’ experience in application development/full life cycle on Data Warehouse engagements 
  At least 3 years’ experience in large (80TB+) and complex data warehousing architecture, design and implementation/migration 
  Experience with one or more relational database systems such as Oracle, MySQL, Postgres, SQL server, etc. 
  Experience in Extract-Transform-Load (ETL) development 
  Knowledge of ETL concepts, tools, and data structures 
  Experience with cloud platforms like Amazon Web Services (AWS), Microsoft Azure, etc. 
  Experience with migrating customers/projects to the cloud 
  Knowledge of Continuous Integration & Continuous Development tools (CI/CD) 
  Must be able to multitask efficiently and progressively and work comfortably in an ever-changing data environment. 
  Must work well in a team environment as well as independently. 
  Excellent verbal/written communication and problem solving skills; ability to communicate information to a variety of groups at different technical skill levels. 
 
 
 Preferred Qualifications: 
 
  Experience with relational databases and knowledge of query tools and/or BI tools like Power BI or OBIEE and data analysis tools 
  Experience with the Hadoop eco system, including HDFS, YARN, Hive, Pig, and batch-oriented and streaming distributed processing methods such as Spark, Kafka, or Storm 
  Experience with Atlassian suite of tools such as Jira and Confluence 
 
 
 Education: 
 
  HS Diploma & 15+ years 
  AS/AA & 13-18+ years 
  BS/BA & 7-12+ years 
  MS/MA/MBA & 5-9+ years 
  PhD/Doctorate & 3-7+ years 
 
 
 Security Clearance Requirements: 
 
  Must be a U.S Citizenship with the ability to obtain DHS CBP Suitability is required. 
  Active TS Clearance preferred. 
 
 
 Certifications: 
 
  N/A 
 
 
 Physical Requirements: 
 
  The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, or to communicate with co-workers, management, and customers, which may involve delivering presentations. 
 
 The projected compensation range for this position is $108,200-$180,300. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it’s employees beyond just compensation. ManTech’s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections. 
 
 For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone. 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                       ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law. 
                       
                       If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services. 
                       
                       If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access","<div>
 <p><b>Secure our Nation, Ignite your Future </b></p>
 <p></p>
 <p>Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International Corporation, you&#x2019;ll help protect our national security while working on innovative projects that offer opportunities for advancement. </p>
 <p></p>
 <p>Currently, ManTech is seeking a motivated, career and customer-oriented Data Analyst/Engineer to join our team. </p>
 <p></p>
 <p>Each day U.S. Customs and Border Protection (CBP) oversees the massive flow of people, capital, and products that enter and depart the United States via air, land, sea, and cyberspace. The volume and complexity of both physical and virtual border crossings require the application of solutions to promote efficient trade and travel. Further, effective solutions help CBP ensure the movement of people, capital, and products is legal, safe, and secure. In response to this challenge, ManTech, as a trusted mission partner of CBP, seeks capable, qualified, and versatile Senior Data Engineers to facilitate data-driven decision making in response to national security threats. </p>
 <p></p>
 <p>Responsibilities include, but are not limited to: </p>
 <ul>
  <li>Responsible for data analysis of large database tables to understand the data structures, definitions and patterns which will be used to support various predictive models </li>
  <li>Working closely with client to assist in managing data needs, and supporting collection of data needed for various operational needs. </li>
  <li>Data analysis, problem solving, investigation and creative thinking with massive amount of data supporting variety of operational scenarios </li>
  <li>Assist with implementing cloud techniques and workflows (on premise to Cloud Platforms) </li>
  <li>Respond to data queries/analysis requests from various groups within an organization. Create and publish regularly scheduled and/or ad hoc reports as needed. </li>
  <li>Researching and documenting data definitions for all subject areas and primary data sets supporting the core business applications. </li>
  <li>Responsible for source code control using GitLab </li>
  <li>Demonstrate a strong practical understanding of application-relevant cargo and passenger data and databases used to support analytic application development, functionality and targeting end user (officer) operation. </li>
 </ul>
 <p></p>
 <p>Basic Qualifications: </p>
 <ul>
  <li>Minimum 4-5 years&#x2019; experience in application development/full life cycle on Data Warehouse engagements </li>
  <li>At least 3 years&#x2019; experience in large (80TB+) and complex data warehousing architecture, design and implementation/migration </li>
  <li>Experience with one or more relational database systems such as Oracle, MySQL, Postgres, SQL server, etc. </li>
  <li>Experience in Extract-Transform-Load (ETL) development </li>
  <li>Knowledge of ETL concepts, tools, and data structures </li>
  <li>Experience with cloud platforms like Amazon Web Services (AWS), Microsoft Azure, etc. </li>
  <li>Experience with migrating customers/projects to the cloud </li>
  <li>Knowledge of Continuous Integration &amp; Continuous Development tools (CI/CD) </li>
  <li>Must be able to multitask efficiently and progressively and work comfortably in an ever-changing data environment. </li>
  <li>Must work well in a team environment as well as independently. </li>
  <li>Excellent verbal/written communication and problem solving skills; ability to communicate information to a variety of groups at different technical skill levels. </li>
 </ul>
 <p></p>
 <p>Preferred Qualifications: </p>
 <ul>
  <li>Experience with relational databases and knowledge of query tools and/or BI tools like Power BI or OBIEE and data analysis tools </li>
  <li>Experience with the Hadoop eco system, including HDFS, YARN, Hive, Pig, and batch-oriented and streaming distributed processing methods such as Spark, Kafka, or Storm </li>
  <li>Experience with Atlassian suite of tools such as Jira and Confluence </li>
 </ul>
 <p></p>
 <p>Education: </p>
 <ul>
  <li>HS Diploma &amp; 15+ years </li>
  <li>AS/AA &amp; 13-18+ years </li>
  <li>BS/BA &amp; 7-12+ years </li>
  <li>MS/MA/MBA &amp; 5-9+ years </li>
  <li>PhD/Doctorate &amp; 3-7+ years </li>
 </ul>
 <p></p>
 <p>Security Clearance Requirements: </p>
 <ul>
  <li>Must be a U.S Citizenship with the ability to obtain DHS CBP Suitability is required. </li>
  <li>Active TS Clearance preferred. </li>
 </ul>
 <p></p>
 <p>Certifications: </p>
 <ul>
  <li>N/A </li>
 </ul>
 <p></p>
 <p>Physical Requirements: </p>
 <ul>
  <li>The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, or to communicate with co-workers, management, and customers, which may involve delivering presentations. </li>
 </ul>
 <p></p>The projected compensation range for this position is &#x24;108,200-&#x24;180,300. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it&#x2019;s employees beyond just compensation. ManTech&#x2019;s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections. 
 <p></p>
 <p>For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone. </p>
 <p></p>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                <div>
                 <div>
                  <div>
                   <div>
                    <div>
                     <div>
                      <div>
                       <p>ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law. </p>
                       <p></p>
                       <p>If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech&apos;s Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech&apos;s utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services. </p>
                       <p></p>
                       <p>If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access</p>
                      </div>
                     </div>
                    </div>
                   </div>
                  </div>
                 </div>
                </div>
               </div>
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>",https://mantech.wd1.myworkdayjobs.com/en-US/External/job/USA-Remote-Work/Cloud-Migration-Data-Engineer_R43515?source=Indeed,2e42f3bb52125beb,,Full-time,,,Remote,Cloud Migration/Data Engineer,22 days ago,2023-09-29T13:06:27.072Z,3.9,1702.0,"$108,200 - $180,300 a year",2023-10-21T13:06:27.075Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2e42f3bb52125beb&from=jasx&tk=1hd95ff4chbht800&vjs=3
382,Alight,"Our story
 
  At Alight, we believe a company’s success starts with its people. At our core, we Champion People, help our colleagues Grow with Purpose and true to our name we encourage colleagues to “Be Alight.”
 
  It’s why we’re so driven to connect passion with purpose. Our team’s expertise in human insights and cloud technology, gives companies and employees around the world the ability to power confident decisions, for life. 
 
 With a comprehensive total rewards package, continuing education and training, and tremendous potential with a growing global organization, Alight is the perfect place to put your passion to work.
  Join our team if you Champion People, want to Grow with Purpose through acting with integrity and if you embody the meaning of Be Alight.
 
  Learn more at careers.alight.com 
 
 Position responsible for delivery of data solutions supporting analytic and reporting efforts. Specifically, the position will focus on the creation of data pipelines to optimize the architecture, design, governance, and movement to support self-service BI as well as performing statistical analysis, correlations, etc.
 
  Responsibilities include leading, defining, designing, building, testing and maintenance of the AWS data architecture, including ingestion of data from S3 across various source types – Parquet, csv, text, etc. Responsible for the architecture of the transformation and integration layers utilizing various ETL tools and Redshift to create optimal structures to support centralized usage across BU and analytics. Responsible for helping propagate knowledge across this space to other team members and help to promote standards and best practice definition.
 
  What You’ll Do:
 
 
   Focuses on data architecture, design, and delivery of data solutions across the AWS environment.
   Designs, implements, and ensures data solutions are built, maintained, and updated based on established business requirements.
   Collaborates with leadership to provide meaningful and credible feedback on data architecture, design, and delivery.
   Works autonomously, in team settings and in partnership with management to review data design and solutions.
   Identifies information needed, sources, and uses tools to deliver optimal solutions per the use case.
   Partners with IT, Database Administrators, and business owners to ensure all data/data sources needed for reporting and analytics are defined and incorporated into the appropriate data solution.
   Develops, implements, communicates, and maintains automated processes adhering to the deployment and support standards.
   Develops data quality metrics that identify gaps and ensures compliance with standards across the enterprise.
   Leads analysis, estimation, planning and implementation of data solutions.
   Serves as a liaison with functional groups around data and BI. Leads planning and execution of multiple, simultaneous initiatives.
   Conducts business data analysis and design to support effective report development and business decisions.
   Leverages external best in class reporting solutions to support data needs.
   Understand the data ecosystem to support placing data in the correct infrastructure.
   Create the processes to show where and how data should be moved/ aggregated once it is landed from source systems into the data lake environment (S3)
   Create the design and models for combing data across sources for efficient query patterns from BI and analytics.
   Ensure governance standards are followed.
   Review ongoing performance of existing assets and modify if needed.
   Fast-track the use of Redshift and other AWS tools & services across the data lake environment.
 
 
  What You'll Need:
 
 
   bachelor’s degree in Business, Technology or equivalent
   Demonstrated successful business analysis, design and development of data solutions, or equivalent combination of education and experience in data solution architecture, design, and solution delivery.
   Multiple years of interaction with business and technology partners to collect and translate business needs into service deliverables.
   Experience with Data Visualization and Business Intelligence tools
   Extensive development of data solutions across disparate sources - Hadoop, Hive, Microsoft SQL, Redshift, Spark, NoSQL, etc.
   Deep understanding of AWS data architecture, design, data modeling, and optimization of data solutions to support analytics and BI solutions requiring very fast query resolution.
   Experience designing solutions across disparate structures utilizing Redshift and other AWS tools and services.
   Ability to collaborate with multiple stakeholders, develop business requirements and design data solutions across the AWS environment.
   Problem anticipation, problem solving and issue resolution.
   Ability to influence organizational change.
   Strong interpersonal and communication skills
   Proactively builds and leverages relationships internally and externally.
   Exhibits an outstanding ability to communicate and share ideas across the organization.
   Manages multiple tasks to deliver according to schedule and priority.
 
 
  **US Citizenship requirements- as this role supports services provided to the federal government and/or a federal government contractor, proof will be required to verify US citizenship status at time of hire.**
  Background Check Required 
 
 By applying for a position with Alight, you understand that, should you be made an offer, it will be contingent on your undergoing and successfully completing a background check consistent with Alight’s employment policies. Background checks may include some or all of the following based on the nature of the position: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. You will be notified during the hiring process which checks are required by the position.
 
  Equal Employment Opportunity
 
  Alight is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, ancestry, national origin, physical or mental disability, veteran or military status, or any other legally protected characteristics or conduct covered by federal, state or local law. In addition, Alight takes affirmative action to ensure that applicants are employed, and that employees are treated during employment, without regard to their race, color, religion, sex, sexual orientation, gender identity, protected veteran status, or national origin.
 
  Reasonable Accommodations
 
  Alight provides reasonable accommodations to the known limitations of otherwise qualified employees and applicants for employment with disabilities, sincerely held religious beliefs, practices and observances, unless doing so would result in an undue hardship. Applicants for employment may request a reasonable accommodation/modification by contacting his/her recruiter.
 
  Diversity Statement 
 
 At Alight, we believe that diversity should be visible, valued, and sustained throughout the organization. Alight provides equal treatment and employment opportunities to all employees and applicants for employment without regard to any protected status or other protected characteristic.
 
  Authorization to Work in the United States
 
  Applicants for employment in the United States must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Alight.
 
  #Li-remote
  P&T
  We offer you a competitive total rewards package, continuing education & training, and tremendous potential with a growing worldwide organization.
  Pay Transparency Statement: Alight takes into consideration a candidate’s experience, education, certification/credentials, market data, internal equity, and geography when determining an offer for a successful employment candidate, and Alight does so on an individualized, non-discriminatory basis. Therefore, an offer may fall anywhere between the estimated minimum base salary for this role of $117,100.00/year (for full time employees) and the estimated maximum base salary for this role of $143,100.00/year (for full time employees). In addition to a base salary, this position may be eligible for a bonus and/or other incentive plans. Alight also offers a comprehensive benefits package; for specific details on our benefits package, please visit: https://careers.alight.com/us/en/alight-us-benefits-2023
  DISCLAIMER:
  Nothing in this job description restricts management's right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units. Alight Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, age, disability, genetic information, pregnancy, childbirth or related medical condition, veteran, marital, parental, citizenship, or domestic partner status, or any other status protected by applicable national, federal, state or local law. Alight Solutions is committed to a diverse workforce and is an affirmative action employer.","<div>
 <p><b>Our story</b></p>
 <p></p>
 <p> At Alight, we believe a company&#x2019;s success starts with its people. At our core, we Champion People, help our colleagues Grow with Purpose and true to our name we encourage colleagues to &#x201c;Be Alight.&#x201d;</p>
 <p></p>
 <p> It&#x2019;s why we&#x2019;re so driven to connect passion with purpose. Our team&#x2019;s expertise in human insights and cloud technology, gives companies and employees around the world the ability to power confident decisions, for life. </p>
 <p></p>
 <p>With a comprehensive total rewards package, continuing education and training, and tremendous potential with a growing global organization, Alight is the perfect place to put your passion to work.</p>
 <p> Join our team if you Champion People, want to Grow with Purpose through acting with integrity and if you embody the meaning of Be Alight.</p>
 <p></p>
 <p> Learn more at careers.alight.com </p>
 <p></p>
 <p>Position responsible for delivery of data solutions supporting analytic and reporting efforts. Specifically, the position will focus on the creation of data pipelines to optimize the architecture, design, governance, and movement to support self-service BI as well as performing statistical analysis, correlations, etc.</p>
 <p></p>
 <p> Responsibilities include leading, defining, designing, building, testing and maintenance of the AWS data architecture, including ingestion of data from S3 across various source types &#x2013; Parquet, csv, text, etc. Responsible for the architecture of the transformation and integration layers utilizing various ETL tools and Redshift to create optimal structures to support centralized usage across BU and analytics. Responsible for helping propagate knowledge across this space to other team members and help to promote standards and best practice definition.</p>
 <p></p>
 <p><b> What You&#x2019;ll Do:</b></p>
 <p></p>
 <ul>
  <li> Focuses on data architecture, design, and delivery of data solutions across the AWS environment.</li>
  <li> Designs, implements, and ensures data solutions are built, maintained, and updated based on established business requirements.</li>
  <li> Collaborates with leadership to provide meaningful and credible feedback on data architecture, design, and delivery.</li>
  <li> Works autonomously, in team settings and in partnership with management to review data design and solutions.</li>
  <li> Identifies information needed, sources, and uses tools to deliver optimal solutions per the use case.</li>
  <li> Partners with IT, Database Administrators, and business owners to ensure all data/data sources needed for reporting and analytics are defined and incorporated into the appropriate data solution.</li>
  <li> Develops, implements, communicates, and maintains automated processes adhering to the deployment and support standards.</li>
  <li> Develops data quality metrics that identify gaps and ensures compliance with standards across the enterprise.</li>
  <li> Leads analysis, estimation, planning and implementation of data solutions.</li>
  <li> Serves as a liaison with functional groups around data and BI. Leads planning and execution of multiple, simultaneous initiatives.</li>
  <li> Conducts business data analysis and design to support effective report development and business decisions.</li>
  <li> Leverages external best in class reporting solutions to support data needs.</li>
  <li> Understand the data ecosystem to support placing data in the correct infrastructure.</li>
  <li> Create the processes to show where and how data should be moved/ aggregated once it is landed from source systems into the data lake environment (S3)</li>
  <li> Create the design and models for combing data across sources for efficient query patterns from BI and analytics.</li>
  <li> Ensure governance standards are followed.</li>
  <li> Review ongoing performance of existing assets and modify if needed.</li>
  <li> Fast-track the use of Redshift and other AWS tools &amp; services across the data lake environment.</li>
 </ul>
 <p></p>
 <p><b> What You&apos;ll Need:</b></p>
 <p></p>
 <ul>
  <li> bachelor&#x2019;s degree in Business, Technology or equivalent</li>
  <li> Demonstrated successful business analysis, design and development of data solutions, or equivalent combination of education and experience in data solution architecture, design, and solution delivery.</li>
  <li> Multiple years of interaction with business and technology partners to collect and translate business needs into service deliverables.</li>
  <li> Experience with Data Visualization and Business Intelligence tools</li>
  <li> Extensive development of data solutions across disparate sources - Hadoop, Hive, Microsoft SQL, Redshift, Spark, NoSQL, etc.</li>
  <li> Deep understanding of AWS data architecture, design, data modeling, and optimization of data solutions to support analytics and BI solutions requiring very fast query resolution.</li>
  <li> Experience designing solutions across disparate structures utilizing Redshift and other AWS tools and services.</li>
  <li> Ability to collaborate with multiple stakeholders, develop business requirements and design data solutions across the AWS environment.</li>
  <li> Problem anticipation, problem solving and issue resolution.</li>
  <li> Ability to influence organizational change.</li>
  <li> Strong interpersonal and communication skills</li>
  <li> Proactively builds and leverages relationships internally and externally.</li>
  <li> Exhibits an outstanding ability to communicate and share ideas across the organization.</li>
  <li> Manages multiple tasks to deliver according to schedule and priority.</li>
 </ul>
 <p></p>
 <p><b> **US Citizenship requirements- as this role supports services provided to the federal government and/or a federal government contractor, proof will be required to verify US citizenship status at time of hire.**</b></p>
 <p><b> Background Check Required </b></p>
 <p></p>
 <p>By applying for a position with Alight, you understand that, should you be made an offer, it will be contingent on your undergoing and successfully completing a background check consistent with Alight&#x2019;s employment policies. Background checks may include some or all of the following based on the nature of the position: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. You will be notified during the hiring process which checks are required by the position.</p>
 <p></p>
 <p><b> Equal Employment Opportunity</b></p>
 <p></p>
 <p> Alight is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, ancestry, national origin, physical or mental disability, veteran or military status, or any other legally protected characteristics or conduct covered by federal, state or local law. In addition, Alight takes affirmative action to ensure that applicants are employed, and that employees are treated during employment, without regard to their race, color, religion, sex, sexual orientation, gender identity, protected veteran status, or national origin.</p>
 <p></p>
 <p><b> Reasonable Accommodations</b></p>
 <p></p>
 <p> Alight provides reasonable accommodations to the known limitations of otherwise qualified employees and applicants for employment with disabilities, sincerely held religious beliefs, practices and observances, unless doing so would result in an undue hardship. Applicants for employment may request a reasonable accommodation/modification by contacting his/her recruiter.</p>
 <p></p>
 <p><b> Diversity Statement </b></p>
 <p></p>
 <p>At Alight, we believe that diversity should be visible, valued, and sustained throughout the organization. Alight provides equal treatment and employment opportunities to all employees and applicants for employment without regard to any protected status or other protected characteristic.</p>
 <p></p>
 <p><b> Authorization to Work in the United States</b></p>
 <p></p>
 <p> Applicants for employment in the United States must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Alight.</p>
 <p></p>
 <p> #Li-remote</p>
 <p> P&amp;T</p>
 <p> We offer you a competitive total rewards package, continuing education &amp; training, and tremendous potential with a growing worldwide organization.</p>
 <br> Pay Transparency Statement: Alight takes into consideration a candidate&#x2019;s experience, education, certification/credentials, market data, internal equity, and geography when determining an offer for a successful employment candidate, and Alight does so on an individualized, non-discriminatory basis. Therefore, an offer may fall anywhere between the estimated minimum base salary for this role of &#x24;117,100.00/year (for full time employees) and the estimated maximum base salary for this role of &#x24;143,100.00/year (for full time employees). In addition to a base salary, this position may be eligible for a bonus and/or other incentive plans. Alight also offers a comprehensive benefits package; for specific details on our benefits package, please visit: https://careers.alight.com/us/en/alight-us-benefits-2023
 <p><b><br> DISCLAIMER:</b></p>
 <p><br> Nothing in this job description restricts management&apos;s right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.</p> Alight Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, age, disability, genetic information, pregnancy, childbirth or related medical condition, veteran, marital, parental, citizenship, or domestic partner status, or any other status protected by applicable national, federal, state or local law. Alight Solutions is committed to a diverse workforce and is an affirmative action employer.
</div>",https://www.indeed.com/rc/clk?jk=51e0161aa5f1d9c6&atk=&xpse=SoCN67I3JpDE8lw8M50LbzkdCdPP,51e0161aa5f1d9c6,,,,,"Lincolnshire, IL 60069",Data Engineer II,28 days ago,2023-09-23T13:07:51.353Z,3.1,1023.0,"$117,100 a year",2023-10-21T13:07:51.354Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=51e0161aa5f1d9c6&from=jasx&tk=1hd95ff4chbht800&vjs=3
409,Procentrix,"Position Description 

 The Data Engineer will be part of an Agile development team implementing a data and analytics solution built on Snowflake and Power BI. The primary source of the data is Microsoft Dataverse via Azure Data Lake Storage Gen2. They will work as part of a small team that will be designing and implementing the end-to-end functionality needed, primarily focused on the data requirements. They will work in a collaborative environment and must be prepared to pick up whatever needs to be done and approach it with a sense of urgency while delivering quality production-ready solutions. Specific responsibilities include: 

 Design, implementation, and testing of data transformation code using SQL. 
Develop process and scripts using a language such as Python or PowerShell to support data and file operations. 
Create and maintain a security model to secure access to a database. 
Implement build and release pipelines to automate instantiation of cloud services, run schema/DDL, etc. 

 The projected compensation range for this position is $130,000- $155,000 annualized (USD). The final salary offered will generally fall within this range and is determined by various factors, including but not limited to the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as internal pay equity, location, contract-specific affordability, and other organizational requirements. 

 Required Skills 
At least 8 years of experience performing ETL and data engineering work in a data warehouse environment. 
Implementing software development solutions utilizing Agile Methodology 

 
Advanced level skills and experience in the following:
 SQL Development 
Python or PowerShell Development 
Creation and execution of unit tests for SQL code 
Data model and data warehousing concepts 
Snowflake 
Git source control (clone, branch, commit, push, etc.) 
Build and release pipeline development using Azure DevOps or GitHub Enterprise 

 Desirable Skills 
Federal government experience 
Active Public Trust 
Power BI experience 

 Job ID","Position Description 
<br>
<br> The Data Engineer will be part of an Agile development team implementing a data and analytics solution built on Snowflake and Power BI. The primary source of the data is Microsoft Dataverse via Azure Data Lake Storage Gen2. They will work as part of a small team that will be designing and implementing the end-to-end functionality needed, primarily focused on the data requirements. They will work in a collaborative environment and must be prepared to pick up whatever needs to be done and approach it with a sense of urgency while delivering quality production-ready solutions. Specific responsibilities include: 
<br>
<br> Design, implementation, and testing of data transformation code using SQL. 
<br>Develop process and scripts using a language such as Python or PowerShell to support data and file operations. 
<br>Create and maintain a security model to secure access to a database. 
<br>Implement build and release pipelines to automate instantiation of cloud services, run schema/DDL, etc. 
<br>
<br> The projected compensation range for this position is &#x24;130,000- &#x24;155,000 annualized (USD). The final salary offered will generally fall within this range and is determined by various factors, including but not limited to the individual&#x2019;s particular combination of education, knowledge, skills, competencies, and experience, as well as internal pay equity, location, contract-specific affordability, and other organizational requirements. 
<br>
<br> Required Skills 
<br>At least 8 years of experience performing ETL and data engineering work in a data warehouse environment. 
<br>Implementing software development solutions utilizing Agile Methodology 
<br>
<br> 
<b>Advanced level skills and experience in the following:</b>
<br> SQL Development 
<br>Python or PowerShell Development 
<br>Creation and execution of unit tests for SQL code 
<br>Data model and data warehousing concepts 
<br>Snowflake 
<br>Git source control (clone, branch, commit, push, etc.) 
<br>Build and release pipeline development using Azure DevOps or GitHub Enterprise 
<br>
<br> Desirable Skills 
<br>Federal government experience 
<br>Active Public Trust 
<br>Power BI experience 
<br>
<br> Job ID",https://procentrix.powerappsportals.com/VacancyDetails/?id=ef608393-5859-ee11-be6f-000d3a590bb7,c7bc15ce61ba752a,,,,,Remote,SENIOR DATA ENGINEER (SNOWFLAKE / MICROSOFT),21 days ago,2023-09-30T13:10:01.276Z,4.6,13.0,"$130,000 - $155,000 a year",2023-10-21T13:10:01.277Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=c7bc15ce61ba752a&from=jasx&tk=1hd95mfqo28gs002&vjs=3
412,Fable,"Fable is a mission-driven start-up based in Silicon Valley, founded in 2019 by global tech industry veteran Padmasree Warrior.
 
 
 
   ️ PURPOSE
 
 
   We are building Fable because stress, anxiety, depression, and social isolation are on the rise and affecting people globally, across all age and income levels — and we can help. Stories promote empathy, emotional intelligence, and other cognitive abilities that can lead to better mental health. Just 30 minutes of reading every day can improve our mental well-being. Fable helps our community make reading a daily healthy habit. Community members enjoy the many benefits of reading while making deeper human connections with other readers. We are backed by top investors, including Redpoint Ventures, Tiger Global, M13, Gaingels, and notable angel investors who believe in our mission and team.
 
 
 
   \uD83C\uDFD7 WHAT WE ARE BUILDING
 
 
   Fable is a community-powered platform for discovering, reading, and discussing books, articles, and podcasts, for deeper connections, upskilling, and mental wellness.
   
 
 
  
 
  Social Reading
 
 
   Fable makes it easy for people to discover, join and build communities to read together based on their interest graph. Fable members read together, sharing highlights, comments and insights. We make reading interactive and fun in the Fable eReader.
 
 
 
   Organized Reading
 
 
   Fable helps people organize all of their reading in one central place. Members can create reading “Lists,” share their lists, follow other members, import their reading lists from other platforms, and rate and review books. We make personalized recommendations based on your reading preferences.
 
 
 
   Healthy Habits
 
 
   Every day, Fable helps members set and reach their reading goals to reap the wellness benefits of reading. Our communities set their own milestones to pace themselves and fill the micro-moments in their lives with stories.
 
 
 
   \uD83E\uDD39 
  ROLE
 
 
   We’re looking for an experienced data engineer with a love of working with data at scale to join our distributed team in building the world’s best platform for social reading.
 
 
 
   This is an independent role where you’ll be working with various stakeholders across Fable, both in and out of engineering, to design and launch new systems for extracting, transforming and storing data. You’ll be called upon to improve Fable’s data system’s reliability, efficiency and legibility and will be expected to scale your solutions to the business environment of a small startup, iterate quickly, and make pragmatic choices around what tools and technologies to adopt.
 
 
 
   \uD83D\uDCAA\uD83C\uDFFD WHAT YOU WILL DO
 
 
 
   Develop data models and pipelines to enable reporting, modeling and machine learning
  Develop a content knowledge graph, combining multiple data sources
  Ensure data quality, perform data audits
  Improve performance of data/analytics infrastructure
  Leverage Google Cloud tools and services to bring data workloads to production
  Collaborate with backend engineering and data teams
  Be an advocate of data-driven thinking and communicate data and metrics to the entire company!
 
 
 
   ✔️ SKILLS YOU WILL NEED
 
 
 
   Thought leadership to ideate with business leaders, identify areas of opportunity, and influence decisions
  Experience working in a cloud environment (GCP, AWS, Azure)
  Experience with DBT or similar data frameworks a plus
  Experience with helping organize disparate data needs and workflows into a consistent, reliable system
  Curiosity to understand business needs and translate them to data solutions
  Experience in data modeling and creating data pipelines
 
 
 
   \uD83D\uDE0A IS THIS YOU?
 
 
 
   You're self-motivated, and take ownership and responsibility
  You love working with smart, fun, sincere and dedicated peers
  You want to be the one to make it happen
  You are resilient and can cope with ambiguity
  Comfortable in a fast-paced and at times unpredictable start-up environment
  Big plus if you have a love for stories and reading!
 
 
 
   \uD83D\uDC50\uD83C\uDFFD YOUR TEAM AT FABLE
 
 
   At Fable, you'll join a passionate, high-performing and empathetic team of people who love stories. We are proud to work on a purpose-driven product with a mission of improving mental wellness. We are a tech company with the soul of an artist. We are an early-stage startup and as such a constant work in progress. We have no time for bureaucracy and are looking for leaders, not spectators. We listen, understand, and consider before we judge. We are committed to diversity and inclusion, and have a set of values that are an integral part of our company culture. Please check out our diversity and inclusion manifesto and company values here.
 
 
 
   \uD83D\uDD11 
  WHY WORK AT FABLE
 
 
   This is a unique opportunity if you are looking to join a small team making a big impact, and work on a fast-growing product while having fun along the way.
 
 
 
   - Annual Base Pay for this role: $120k - $160k (dependent on location and commensurate with experience)
 
 
  Competitive stock options
  Comprehensive health and dental plans
  Flexible vacation days
  Self contributing 401k
  Open and transparent culture
  Parental leave (we believe in life integration not just work-life balance)
  Work from anywhere, any time
 
 
 
   WANT TO LEARN MORE ABOUT FABLE?
 
 
  Check out our Founder’s message, meet our team and read our principles to make sure we are right for you
  Read more about Fable in Fortune and Marie Claire
  Listen to our Founder talk about mental wellness and stories
  Top 50 seed companies to work for 2021
  
 
  We are an equal opportunity employer and embrace diversity at our company. We do not discriminate by race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We are a gender-balanced team committed to diversity and an inclusive environment.","<div>
 <div>
  Fable is a mission-driven start-up based in Silicon Valley, founded in 2019 by global tech industry veteran Padmasree Warrior.
 </div>
 <div></div>
 <ul>
  <li><br> &#xfe0f;<b> PURPOSE</b></li>
 </ul>
 <div>
   We are building Fable because stress, anxiety, depression, and social isolation are on the rise and affecting people globally, across all age and income levels &#x2014; and we can help. Stories promote empathy, emotional intelligence, and other cognitive abilities that can lead to better mental health. Just 30 minutes of reading every day can improve our mental well-being. Fable helps our community make reading a daily healthy habit. Community members enjoy the many benefits of reading while making deeper human connections with other readers. We are backed by top investors, including Redpoint Ventures, Tiger Global, M13, Gaingels, and notable angel investors who believe in our mission and team.
 </div>
 <div></div>
 <div>
  <b><br> \uD83C\uDFD7 WHAT WE ARE BUILDING</b>
 </div>
 <div>
   Fable is a community-powered platform for discovering, reading, and discussing books, articles, and podcasts, for deeper connections, upskilling, and mental wellness.
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  <b>Social Reading</b>
 </div>
 <div>
   Fable makes it easy for people to discover, join and build communities to read together based on their interest graph. Fable members read together, sharing highlights, comments and insights. We make reading interactive and fun in the Fable eReader.
 </div>
 <div></div>
 <div>
  <b><br> Organized Reading</b>
 </div>
 <div>
   Fable helps people organize all of their reading in one central place. Members can create reading &#x201c;Lists,&#x201d; share their lists, follow other members, import their reading lists from other platforms, and rate and review books. We make personalized recommendations based on your reading preferences.
 </div>
 <div></div>
 <div>
  <b><br> Healthy Habits</b>
 </div>
 <div>
   Every day, Fable helps members set and reach their reading goals to reap the wellness benefits of reading. Our communities set their own milestones to pace themselves and fill the micro-moments in their lives with stories.
 </div>
 <div></div>
 <div>
  <br> \uD83E\uDD39 
  <b>ROLE</b>
 </div>
 <div>
   We&#x2019;re looking for an experienced data engineer with a love of working with data at scale to join our distributed team in building the world&#x2019;s best platform for social reading.
 </div>
 <div></div>
 <div>
  <br> This is an independent role where you&#x2019;ll be working with various stakeholders across Fable, both in and out of engineering, to design and launch new systems for extracting, transforming and storing data. You&#x2019;ll be called upon to improve Fable&#x2019;s data system&#x2019;s reliability, efficiency and legibility and will be expected to scale your solutions to the business environment of a small startup, iterate quickly, and make pragmatic choices around what tools and technologies to adopt.
 </div>
 <div></div>
 <div>
  <b><br> \uD83D\uDCAA\uD83C\uDFFD WHAT YOU WILL DO</b>
 </div>
 <div></div>
 <ul>
  <li><br> Develop data models and pipelines to enable reporting, modeling and machine learning</li>
  <li>Develop a content knowledge graph, combining multiple data sources</li>
  <li>Ensure data quality, perform data audits</li>
  <li>Improve performance of data/analytics infrastructure</li>
  <li>Leverage Google Cloud tools and services to bring data workloads to production</li>
  <li>Collaborate with backend engineering and data teams</li>
  <li>Be an advocate of data-driven thinking and communicate data and metrics to the entire company!</li>
 </ul>
 <div></div>
 <div>
  <b><br> &#x2714;&#xfe0f; SKILLS YOU WILL NEED</b>
 </div>
 <div></div>
 <ul>
  <li><br> Thought leadership to ideate with business leaders, identify areas of opportunity, and influence decisions</li>
  <li>Experience working in a cloud environment (GCP, AWS, Azure)</li>
  <li>Experience with DBT or similar data frameworks a plus</li>
  <li>Experience with helping organize disparate data needs and workflows into a consistent, reliable system</li>
  <li>Curiosity to understand business needs and translate them to data solutions</li>
  <li>Experience in data modeling and creating data pipelines</li>
 </ul>
 <div></div>
 <div>
  <b><br> \uD83D\uDE0A IS THIS YOU?</b>
 </div>
 <div></div>
 <ul>
  <li><br> You&apos;re self-motivated, and take ownership and responsibility</li>
  <li>You love working with smart, fun, sincere and dedicated peers</li>
  <li>You want to be the one to make it happen</li>
  <li>You are resilient and can cope with ambiguity</li>
  <li>Comfortable in a fast-paced and at times unpredictable start-up environment</li>
  <li>Big plus if you have a love for stories and reading!</li>
 </ul>
 <div></div>
 <div>
  <b><br> \uD83D\uDC50\uD83C\uDFFD YOUR TEAM AT FABLE</b>
 </div>
 <div>
   At Fable, you&apos;ll join a passionate, high-performing and empathetic team of people who love stories. We are proud to work on a purpose-driven product with a mission of improving mental wellness. We are a tech company with the soul of an artist. We are an early-stage startup and as such a constant work in progress. We have no time for bureaucracy and are looking for leaders, not spectators. We listen, understand, and consider before we judge. We are committed to diversity and inclusion, and have a set of values that are an integral part of our company culture. Please check out our diversity and inclusion manifesto and company values here.
 </div>
 <div></div>
 <div>
  <br> \uD83D\uDD11 
  <b>WHY WORK AT FABLE</b>
 </div>
 <div>
   This is a unique opportunity if you are looking to join a small team making a big impact, and work on a fast-growing product while having fun along the way.
 </div>
 <div></div>
 <div>
  <br> - Annual Base Pay for this role: &#x24;120k - &#x24;160k (dependent on location and commensurate with experience)
 </div>
 <ul>
  <li>Competitive stock options</li>
  <li>Comprehensive health and dental plans</li>
  <li>Flexible vacation days</li>
  <li>Self contributing 401k</li>
  <li>Open and transparent culture</li>
  <li>Parental leave (we believe in life integration not just work-life balance)</li>
  <li>Work from anywhere, any time</li>
 </ul>
 <div></div>
 <div>
  <b><br> WANT TO LEARN MORE ABOUT FABLE?</b>
 </div>
 <ul>
  <li>Check out our Founder&#x2019;s message, meet our team and read our principles to make sure we are right for you</li>
  <li>Read more about Fable in Fortune and Marie Claire</li>
  <li>Listen to our Founder talk about mental wellness and stories</li>
  <li>Top 50 seed companies to work for 2021</li>
 </ul> 
 <div>
  We are an equal opportunity employer and embrace diversity at our company. We do not discriminate by race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We are a gender-balanced team committed to diversity and an inclusive environment.
 </div>
</div>",https://www.indeed.com/rc/clk?jk=2ea2f8e64c5398d4&atk=&xpse=SoD967I3JpDTqWyQKx0LbzkdCdPP,2ea2f8e64c5398d4,,Full-time,,,Remote,Senior Data Engineer,21 days ago,2023-09-30T13:10:12.217Z,5.0,4.0,"$120,000 - $160,000 a year",2023-10-21T13:10:12.218Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2ea2f8e64c5398d4&from=jasx&tk=1hd95mfqo28gs002&vjs=3
413,Accrete.AI,"The U.S. Government agencies we work with have contracts that require all personnel working on their corresponding contracts to have U.S. citizenship – do you meet this requirement?
  
  
 Accrete is looking for a Data Engineer that will be responsible for supporting production data pipelines, developing the foundation for the Accrete data lake, and implementing best practices from data engineering at Accrete. This will support new and existing applications running on Linux and Windows operating systems in private and public cloud infrastructures. The Data Engineering team at Accrete designs, develops, and maintains data pipelines, batch data analytics, and data stores of various kinds, including analytics and stores in support of artificial intelligence workloads for Accrete AI systems and applications. 
  
 Accrete is an AI prime defense contractor with the U.S. government that creates AI software, enabling its customers to make better decisions, faster. Accrete is on a mission to create AI so powerful it amplifies human reasoning and enables enterprises to grow in previously unimaginable ways. Prior to launching Accrete in 2017, Prashant Bhuyan, Accrete’s Founder and CEO, spent over a decade in high-frequency trading where he and a core team experimented with and developed AI technology that ultimately became the early underpinnings of Accrete.
  Accrete’s solutions enable the Department of Defense to predict covert behavior from foreign adversaries seeking to influence the supply chain; the U.S. Air Force to identify vulnerabilities in microprocessor firmware; major music labels to identify superstars before competitors; auto dealers to automatically generate marketing content from vehicle feature lists; employee benefits brokers to identify the shortest path to the hottest leads; and more. 
  To learn more about Accrete, please visit our website: Accrete.ai 
  
 Responsibilities: 
 
  Design, develop, and support specific scalable pipelines for the movement of data between systems.
   Provide technical guidance in software design and development activities.
   Supervising and overseeing aspects of data engineering on multiple work streams.
   Recommend new technologies to ensure quality and productivity.
   Work closely with other teams, tech leads, architects, and Product Management to bring new data-backed products to market.
  
  
 Requirements: 
 
   2-3+ years of expertise with a data-oriented language, such as Python, C, Rust, Java, Go.
   2-3+ years experience in cloud computing, with AWS, Azure or GCP
   2-3+ years experience designing and building scalable, reliable data pipelines using the following technologies: Hadoop, Hive, Spark, Kafka, Airflow, or similar services
   Experiencing designing for and implementing in cloud-native systems
   Expertise in matching APIs, data access patterns, and data storage formats
   Functional knowledge of event processing systems such as Apache Kafka or similar
   Functional knowledge of the following databases or equivalent: Postgres, MongoDB, Neo4J, Redis
   High collaborative, values mentoring, teaching, pair-programming, and teaching software development best practices to other engineers
   Excellent problem-solving and critical-thinking skills
   Ability to build and maintain effective, trusting partnerships with product managers, architects, and technical leads for individual products and applications
   Excellent communication and leadership skills
  
  
 The base salary range for this role is $135,000 to 140,000. 
  
 Benefits: 
 We offer a competitive salary, benefits package, and opportunities for growth and advancement within the company. If you are an innovative and results-driven leader, we encourage you to apply for this exciting opportunity. 
  Accrete is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.","<div>
 <ul>
  <li><i>The U.S. Government agencies we work with have contracts that require all personnel working on their corresponding contracts to have U.S. citizenship &#x2013; do you meet this requirement?</i></li>
 </ul> 
 <p></p> 
 <p>Accrete is looking for a Data Engineer that will be responsible for supporting production data pipelines, developing the foundation for the Accrete data lake, and implementing best practices from data engineering at Accrete. This will support new and existing applications running on Linux and Windows operating systems in private and public cloud infrastructures. The Data Engineering team at Accrete designs, develops, and maintains data pipelines, batch data analytics, and data stores of various kinds, including analytics and stores in support of artificial intelligence workloads for Accrete AI systems and applications.</p> 
 <p></p> 
 <p>Accrete is an AI prime defense contractor with the U.S. government that creates AI software, enabling its customers to make better decisions, faster. Accrete is on a mission to create AI so powerful it amplifies human reasoning and enables enterprises to grow in previously unimaginable ways. Prior to launching Accrete in 2017, Prashant Bhuyan, Accrete&#x2019;s Founder and CEO, spent over a decade in high-frequency trading where he and a core team experimented with and developed AI technology that ultimately became the early underpinnings of Accrete.</p>
 <p><br> Accrete&#x2019;s solutions enable the Department of Defense to predict covert behavior from foreign adversaries seeking to influence the supply chain; the U.S. Air Force to identify vulnerabilities in microprocessor firmware; major music labels to identify superstars before competitors; auto dealers to automatically generate marketing content from vehicle feature lists; employee benefits brokers to identify the shortest path to the hottest leads; and more.</p> 
 <p> To learn more about Accrete, please visit our website: Accrete.ai</p> 
 <p></p> 
 <p><b>Responsibilities: </b></p>
 <ul>
  <li>Design, develop, and support specific scalable pipelines for the movement of data between systems.</li>
  <li> Provide technical guidance in software design and development activities.</li>
  <li> Supervising and overseeing aspects of data engineering on multiple work streams.</li>
  <li> Recommend new technologies to ensure quality and productivity.</li>
  <li> Work closely with other teams, tech leads, architects, and Product Management to bring new data-backed products to market.</li>
 </ul> 
 <p></p> 
 <p><b>Requirements:</b></p> 
 <ul>
  <li> 2-3+ years of expertise with a data-oriented language, such as Python, C, Rust, Java, Go.</li>
  <li> 2-3+ years experience in cloud computing, with AWS, Azure or GCP</li>
  <li> 2-3+ years experience designing and building scalable, reliable data pipelines using the following technologies: Hadoop, Hive, Spark, Kafka, Airflow, or similar services</li>
  <li> Experiencing designing for and implementing in cloud-native systems</li>
  <li> Expertise in matching APIs, data access patterns, and data storage formats</li>
  <li> Functional knowledge of event processing systems such as Apache Kafka or similar</li>
  <li> Functional knowledge of the following databases or equivalent: Postgres, MongoDB, Neo4J, Redis</li>
  <li> High collaborative, values mentoring, teaching, pair-programming, and teaching software development best practices to other engineers</li>
  <li> Excellent problem-solving and critical-thinking skills</li>
  <li> Ability to build and maintain effective, trusting partnerships with product managers, architects, and technical leads for individual products and applications</li>
  <li> Excellent communication and leadership skills</li>
 </ul> 
 <p></p> 
 <p>The base salary range for this role is &#x24;135,000 to 140,000.</p> 
 <p></p> 
 <p><b>Benefits: </b></p>
 <p><i>We offer a competitive salary, benefits package, and opportunities for growth and advancement within the company. If you are an innovative and results-driven leader, we encourage you to apply for this exciting opportunity.</i></p> 
 <p><i> Accrete is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.</i></p>
</div>",https://www.indeed.com/rc/clk?jk=23a44fa83da2854b&atk=&xpse=SoDi67I3JpDTLCwldJ0LbzkdCdPP,23a44fa83da2854b,,Full-time,,,Remote,Data Engineer,23 days ago,2023-09-28T13:10:16.580Z,,,"$130,000 - $140,000 a year",2023-10-21T13:10:16.581Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=23a44fa83da2854b&from=jasx&tk=1hd95mfqo28gs002&vjs=3
414,General Mills,"Job Description:
 
Employer: General Mills, Inc. 

 
Job Title: Senior Data Engineer (multiple positions) 

Job Requisition: #25002 | 20330.305.6 

Job Location: 1 General Mills Blvd. Minneapolis, MN 55426 | Telecommuting 100% of time is permitted. 

Job Type: Full Time 

Rate of Pay: $133,385 - $174,600 per year 

 
Duties: Work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions that will generate insights from our connected data, enabling General Mills to advance the data-driven decision-making capabilities of our enterprise. Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals. Solve complex data problems to deliver insights that helps our business to achieve their goals. Create data products for analytics and data scientist team members to improve their productivity. Advise, consult, mentor and coach other data and analytic professionals on data standards and practices. Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions. Lead evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve our productivity as a team. Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes. Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives. 

 Telecommuting 100% of time is permitted. 

 
Requirements: Employer will accept a Bachelor's degree in Computer Science, Management Information Systems, Engineering, or related field and 5 years of post-baccalaureate, progressively responsible experience in job offered or 5 years of post-baccalaureate, progressively responsible experience in data engineering or architecture. 

 
Must have experience in each of the following:
 1. 4 years of experience working with data analysis 
2. 4 years of experience with SQL or Hive QL 
3. 4 years of experience developing and maintaining data warehouses in big data solutions 
4. 4 years of experience with Big Data development using Hadoop 
5. 4 years of experience with Hive, BigQuery , Impala OR Spark 
6. 4 years of experience automating the data pipelines/processes 
7. 4 years of experience with Hadoop ecosystems (HDFS, YARN, Hive, HBase, Sqoop, Spark, and or Hue, ) 
8. 3 years of experience utilizing Agile Development Methodology 
9. 3 years of experience with Git Repositories 

 Telecommuting 100% of time is permitted. Background check and drug testing required. 

Contact: Apply online at https://careers.generalmills.com/careers/ Please refer to job requisition number- #25002 

 The salary range for this position $133,385-$174,600 per year. At General Mills we strive for each employee’s pay at any point in their career to reflect their experiences, performance and skills for their current role. The salary range for this role represents the numerous factors considered in the hiring decision including, but not limited to, education, skills, work experience, certifications, etc. As such, pay for the successful candidate(s) could fall anywhere within the stated range. Beyond base salary, General Mills offers a competitive Total Rewards package focusing on your overall well-being. We are proud to offer a foundation of health benefits, retirement and financial wellbeing, time off programs, wellbeing support and perks. Benefits may vary by role, country, region, union status, and other employment status factors. You may also be eligible to participate in an annual incentive program. An incentive award, if any, depends on various factors, including, individual and organizational performance. 
. 

 
Company Overview:
 We exist to make food the world loves. But we do more than that. Our company is a place that prioritizes being a force for good, a place to expand learning, explore new perspectives and reimagine new possibilities, every day. We look for people who want to bring their best — bold thinkers with big hearts who challenge one other and grow together. Because becoming the undisputed leader in food means surrounding ourselves with people who are hungry for what’s next.","<b>Job Description:</b>
<br> 
<b>Employer:</b> General Mills, Inc. 
<br>
<br> 
<b>Job Title:</b> Senior Data Engineer (multiple positions) 
<br>
<b>Job Requisition:</b> #25002 | 20330.305.6 
<br>
<b>Job Location:</b> 1 General Mills Blvd. Minneapolis, MN 55426 | Telecommuting 100% of time is permitted. 
<br>
<b>Job Type:</b> Full Time 
<br>
<b>Rate of Pay:</b> &#x24;133,385 - &#x24;174,600 per year 
<br>
<br> 
<b>Duties:</b> Work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions that will generate insights from our connected data, enabling General Mills to advance the data-driven decision-making capabilities of our enterprise. Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals. Solve complex data problems to deliver insights that helps our business to achieve their goals. Create data products for analytics and data scientist team members to improve their productivity. Advise, consult, mentor and coach other data and analytic professionals on data standards and practices. Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions. Lead evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve our productivity as a team. Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes. Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives. 
<br>
<br> Telecommuting 100% of time is permitted. 
<br>
<br> 
<b>Requirements:</b> Employer will accept a Bachelor&apos;s degree in Computer Science, Management Information Systems, Engineering, or related field and 5 years of post-baccalaureate, progressively responsible experience in job offered or 5 years of post-baccalaureate, progressively responsible experience in data engineering or architecture. 
<br>
<br> 
<b>Must have experience in each of the following:</b>
<br> 1. 4 years of experience working with data analysis 
<br>2. 4 years of experience with SQL or Hive QL 
<br>3. 4 years of experience developing and maintaining data warehouses in big data solutions 
<br>4. 4 years of experience with Big Data development using Hadoop 
<br>5. 4 years of experience with Hive, BigQuery , Impala OR Spark 
<br>6. 4 years of experience automating the data pipelines/processes 
<br>7. 4 years of experience with Hadoop ecosystems (HDFS, YARN, Hive, HBase, Sqoop, Spark, and or Hue, ) 
<br>8. 3 years of experience utilizing Agile Development Methodology 
<br>9. 3 years of experience with Git Repositories 
<br>
<br> Telecommuting 100% of time is permitted. Background check and drug testing required. 
<br>
<b>Contact:</b> Apply online at https://careers.generalmills.com/careers/ Please refer to job requisition number- #25002 
<br>
<br> The salary range for this position &#x24;133,385-&#x24;174,600 per year. At General Mills we strive for each employee&#x2019;s pay at any point in their career to reflect their experiences, performance and skills for their current role. The salary range for this role represents the numerous factors considered in the hiring decision including, but not limited to, education, skills, work experience, certifications, etc. As such, pay for the successful candidate(s) could fall anywhere within the stated range. Beyond base salary, General Mills offers a competitive Total Rewards package focusing on your overall well-being. We are proud to offer a foundation of health benefits, retirement and financial wellbeing, time off programs, wellbeing support and perks. Benefits may vary by role, country, region, union status, and other employment status factors. You may also be eligible to participate in an annual incentive program. An incentive award, if any, depends on various factors, including, individual and organizational performance. 
<br>. 
<br>
<br> 
<b>Company Overview:</b>
<br> We exist to make food the world loves. But we do more than that. Our company is a place that prioritizes being a force for good, a place to expand learning, explore new perspectives and reimagine new possibilities, every day. We look for people who want to bring their best &#x2014; bold thinkers with big hearts who challenge one other and grow together. Because becoming the undisputed leader in food means surrounding ourselves with people who are hungry for what&#x2019;s next.",https://www.indeed.com/rc/clk?jk=1cc34471192a5294&atk=&xpse=SoA367I3JpDSiOzag50LbzkdCdPP,1cc34471192a5294,,Full-time,,,"1 General Mills Boulevard, Minneapolis, MN 55426",Senior Data Engineer,23 days ago,2023-09-28T13:10:19.508Z,3.9,2800.0,"$133,385 - $174,600 a year",2023-10-21T13:10:19.510Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=1cc34471192a5294&from=jasx&tk=1hd95mfqo28gs002&vjs=3
415,Johnson & Johnson,"Biosense Webster Inc., part of Johnson & Johnson MedTech, is currently recruiting for a Clinical Data Engineer . This role can be remote from anywhere within the United States. At Biosense Webster, Inc. we have one goal — to ensure those with cardiac arrhythmias can live the lives they want. This means transforming the latest advancements in electrophysiology into a suite of tools that empowers physicians with a range of treatments for the best outcomes. 
     
     Quality products and approaches are achievable only through collaboration with the smartest minds in electrophysiology. For more than 30 years, we’ve been the global market leader in the science and technology of cardiac arrhythmia treatment, working with thousands of electrophysiologists to identify and develop diagnostic and treatment tools. And through onsite training, online courses and our global education centers, we work together to set new standards every day. 
     Learn more about Biosense Webster at www.biosensewebster.com 
     The Data Engineer is a key player in a cross functional team providing subject matter expertise for enhancing and troubleshooting complex Data platform business needs. The ideal candidate is an experienced data professional that exhibits strong skillset within redarning of the data via Tableau or other visualization packages. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing data architecture to support the next generation of products and data initiatives. 
     
     Key Responsibilities: 
     
      Maintain support and be the primary contact for managing, triaging, and resolving issues related to Clinical data managed outside of the Electronic Data Capture System (EDC). 
     
     
      Experienced in the utilization of AWS services and infrastructure to support large and complex data platforms. 
     
     
      Hands-on database administrator skills to manage or identify root cause and resolve issues stemming from the management of structured and/or unstructured data sources. 
     
     
     The base pay range for this position is $90,000 to $140,000 based on experience . The Company maintains highly competitive, performance-based compensation programs. Under current guidelines, this position is eligible for an annual performance bonus. The annual performance bonus is a cash bonus intended to provide an incentive to achieve annual targeted results by rewarding for individual and the corporation’s performance over a calendar/performance year. Bonuses are awarded at the Company’s discretion on an individual basis.
      
     
     Employees may be eligible to participate in Company employee benefit programs such as health insurance, savings plan, pension plan, disability plan, vacation pay, sick time, holiday pay, and work, personal and family time off in accordance with the terms of the applicable plans. Additional information can be found through the link below.
      
     
     https://www.careers.jnj.com/employee-benefits 
     QUALIFICATIONS 
     Required Qualifications: 
     
     Education: 
     
      Minimum of a Bachelors’ Degree required ; Advanced Degree preferred . Desired fields of study include Computer Science, Statistics, Informatics, Information Systems or related quantitative field. 
     
     
     Experience and Skills: 
     
      Minimum 2-4+ years of experience in a Data role, with exposure to the following software/tools: 
     
     o Big data tools: AWS Services (S3, GLUE, Postgres, Lambda and Sagemaker etc.) 
     o Relational SQL and NoSQL databases 
     o Analysis and visualization software such as R, RStudio, Python, and Tableau 
     o Data pipeline and workflow management tools 
     
      Advanced SQL knowledge and experience with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. 
     
     
      Experience with contributing to the building and optimizing data pipelines, architectures, and data sets. 
     
     
      A working knowledge of manipulating, processing, and extracting value from large datasets. 
     
     
      Experience supporting and working with cross-functional teams in a dynamic environment. 
     
     
      Experience in medical device or pharmaceutical environment preferred. 
     
     
      Excellent written, oral and presentation skills. 
     
     
     Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.","<div>
 <div>
  <div>
   <div>
    <div>
     <p>Biosense Webster Inc., part of Johnson &amp; Johnson MedTech, is currently recruiting for a <b>Clinical Data Engineer </b>. This role can be remote from anywhere within the United States. At Biosense Webster, Inc. we have one goal &#x2014; to ensure those with cardiac arrhythmias can live the lives they want. This means transforming the latest advancements in electrophysiology into a suite of tools that empowers physicians with a range of treatments for the best outcomes. </p>
     <p></p>
     <p>Quality products and approaches are achievable only through collaboration with the smartest minds in electrophysiology. For more than 30 years, we&#x2019;ve been the global market leader in the science and technology of cardiac arrhythmia treatment, working with thousands of electrophysiologists to identify and develop diagnostic and treatment tools. And through onsite training, online courses and our global education centers, we work together to set new standards every day. </p>
     <p>Learn more about Biosense Webster at www.biosensewebster.com </p>
     <p>The Data Engineer is a key player in a cross functional team providing subject matter expertise for enhancing and troubleshooting complex Data platform business needs. The ideal candidate is an experienced data professional that exhibits strong skillset within redarning of the data via Tableau or other visualization packages. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing data architecture to support the next generation of products and data initiatives. </p>
     <p></p>
     <p><b>Key Responsibilities: </b></p>
     <ul>
      <li>Maintain support and be the primary contact for managing, triaging, and resolving issues related to Clinical data managed outside of the Electronic Data Capture System (EDC). </li>
     </ul>
     <ul>
      <li>Experienced in the utilization of AWS services and infrastructure to support large and complex data platforms. </li>
     </ul>
     <ul>
      <li>Hands-on database administrator skills to manage or identify root cause and resolve issues stemming from the management of structured and/or unstructured data sources. </li>
     </ul>
     <p></p>
     <p>The base pay range for this position is &#x24;90,000 to &#x24;140,000 <b>based on experience </b>. The Company maintains highly competitive, performance-based compensation programs. Under current guidelines, this position is eligible for an annual performance bonus. The annual performance bonus is a cash bonus intended to provide an incentive to achieve annual targeted results by rewarding for individual and the corporation&#x2019;s performance over a calendar/performance year. Bonuses are awarded at the Company&#x2019;s discretion on an individual basis.</p>
     <br> 
     <p></p>
     <p>Employees may be eligible to participate in Company employee benefit programs such as health insurance, savings plan, pension plan, disability plan, vacation pay, sick time, holiday pay, and work, personal and family time off in accordance with the terms of the applicable plans. Additional information can be found through the link below.</p>
     <br> 
     <p></p>
     <p>https://www.careers.jnj.com/employee-benefits </p>
     <h2 class=""jobSectionHeader""><b>QUALIFICATIONS</b></h2> 
     <p><b>Required Qualifications: </b></p>
     <p></p>
     <p><b>Education: </b></p>
     <ul>
      <li>Minimum of a Bachelors&#x2019; Degree <b>required </b>; Advanced Degree <i>preferred </i>. Desired fields of study include Computer Science, Statistics, Informatics, Information Systems or related quantitative field. </li>
     </ul>
     <p></p>
     <p><b>Experience and Skills: </b></p>
     <ul>
      <li>Minimum 2-4+ years of experience in a Data role, with exposure to the following software/tools: </li>
     </ul>
     <p>o Big data tools: AWS Services (S3, GLUE, Postgres, Lambda and Sagemaker etc.) </p>
     <p>o Relational SQL and NoSQL databases </p>
     <p>o Analysis and visualization software such as R, RStudio, Python, and <b>Tableau </b></p>
     <p>o Data pipeline and workflow management tools </p>
     <ul>
      <li>Advanced SQL knowledge and experience with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. </li>
     </ul>
     <ul>
      <li>Experience with contributing to the building and optimizing data pipelines, architectures, and data sets. </li>
     </ul>
     <ul>
      <li>A working knowledge of manipulating, processing, and extracting value from large datasets. </li>
     </ul>
     <ul>
      <li>Experience supporting and working with cross-functional teams in a dynamic environment. </li>
     </ul>
     <ul>
      <li>Experience in medical device or pharmaceutical environment <i>preferred. </i></li>
     </ul>
     <ul>
      <li>Excellent written, oral and presentation skills. </li>
     </ul>
     <p></p>
     <p>Johnson &amp; Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.</p>
    </div>
   </div>
  </div>
 </div>
</div>",https://jobs.jnj.com/en/jobs/2306144036w/clinical-data-engineer-biosense-webster-inc/?src=indeed,05cfb82e458953ee,,,,,"33 Technology Drive, Irvine, CA 92618","Clinical Data Engineer - Biosense Webster, Inc.",23 days ago,2023-09-28T13:10:19.836Z,4.2,7874.0,"$90,000 - $140,000 a year",2023-10-21T13:10:19.837Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=05cfb82e458953ee&from=jasx&tk=1hd95mfqo28gs002&vjs=3
419,Skillquotient,"Data Analyst OR Data Engineer
Data Engineer Or Data Scientist
Location: Remote
Duration: W2 Project
Rate: open
Experience: Mid/junior level is fine, but, needs to be good with SQL, Python, Spark and ETL
Data Engineer:

 Able to build data pipelines and move data from source to target.
 Able to work with large data volume.
 Able to build the data schema.
 Able to perform end to end testing of data products.

Skill Set :
Python
Spark
SQL
ETL
Tableau (Optional)
Data Analyst/Business Analyst candidates with these skillsets:

 Should interact with business users to understand business requirements and convert the same into technical specifications
 Able to perform data analysis to understand the data and write SQLs for analyzing, reporting and testing
 Able to write source to target mapping documents
 Able to query Databases/Data Lake to understand the data;
 Able to communicate business needs to developers
 Able to create test cases and perform unit testing and integration testing
 SQL knowledge
 Tableau knowledge is a plus

Job Types: Permanent, Full-time
Salary: $40.38 - $86.72 per hour
Benefits:

 Health insurance
 Paid time off

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p>Data Analyst OR Data Engineer</p>
<p>Data Engineer Or Data Scientist</p>
<p>Location: Remote</p>
<p>Duration: W2 Project</p>
<p>Rate: open</p>
<p>Experience: Mid/junior level is fine, but, needs to be good with SQL, Python, Spark and ETL</p>
<p>Data Engineer:</p>
<ul>
 <li>Able to build data pipelines and move data from source to target.</li>
 <li>Able to work with large data volume.</li>
 <li>Able to build the data schema.</li>
 <li>Able to perform end to end testing of data products.</li>
</ul>
<p>Skill Set :</p>
<p>Python</p>
<p>Spark</p>
<p>SQL</p>
<p>ETL</p>
<p>Tableau (Optional)</p>
<p>Data Analyst/Business Analyst candidates with these skillsets:</p>
<ul>
 <li>Should interact with business users to understand business requirements and convert the same into technical specifications</li>
 <li>Able to perform data analysis to understand the data and write SQLs for analyzing, reporting and testing</li>
 <li>Able to write source to target mapping documents</li>
 <li>Able to query Databases/Data Lake to understand the data;</li>
 <li>Able to communicate business needs to developers</li>
 <li>Able to create test cases and perform unit testing and integration testing</li>
 <li>SQL knowledge</li>
 <li>Tableau knowledge is a plus</li>
</ul>
<p>Job Types: Permanent, Full-time</p>
<p>Salary: &#x24;40.38 - &#x24;86.72 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
 <li>Paid time off</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,2fd16bfd74b3658d,,Full-time,Contract,Permanent,Remote,Senior Data Engineer,17 days ago,2023-10-04T13:11:04.636Z,,,$40.38 - $86.72 an hour,2023-10-21T13:11:04.638Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2fd16bfd74b3658d&from=jasx&tk=1hd95ofc82ea2001&vjs=3
421,Brillient Corporation,"1. Architecture Design: Working with a system or solution architect, designing the architecture of a data end-to-end data pipeline.
Development of data-related instruments:
2. Build, customize, and manage Software components, integration tools, databases, QA, and analytical systems.
3. Data pipeline maintenance/testing: Direct the Testing team and assist with testing the reliability and performance of each part of a system, suggest and test improvements, and test any changes to the overall system.
4. Manage data and metadata. Manage the flow, management, and processing of structured and unstructured data and its related metadata.
5. Track pipeline stability. Monitor the overall performance and stability of the system including standard processing, maintenance routines and activities, and ongoing innovation and improvements.
Education Requirements
Bachelor's Degree: Computer Science, Computer Engineering, Information Technology, Software Engineering, Electrical Engineering, Mathematics, Statistics
Successful candidates will have.
1. A software engineering background. Proficient in the management of teams using one or several programming languages specifically Java (Hadoop, Apache Hive), Scala (Kafka, Apache Spark), and Python
2. Familiar with various SQL and NoSQL database management systems like MySQL, Oracle, PostgreSQL, MongoDB, and others.
3. Knowledge of on-prem virtualized and containerized systems and modern cloud-based technologies such as BigQuery, Snowflake, Firebolt, and Amazon Redshift.
Knowledge of how to secure each step of the data pipeline to ensure data protection and chain of custody.
4. A strong understanding of data modeling, algorithms, and data transformation techniques
Experience with the existing ETL and BI solutions to facilitate building ETL/ELT pipelines, data storage, and analytical tools.
5. Creativity and problem-solving skills, looking for optimal (and often innovative) solutions to complicated issues.
6. Highly attentive to detail, be able to carry out monotonous tasks, and find ways to decompose big problems into smaller, achievable steps.
7. Communication skills and successful collaboration skills with team members and other stakeholders.
Job Type: Full-time
Pay: $130,000.00 - $145,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 5 years

Schedule:

 8 hour shift

Work Location: Remote","<p>1. Architecture Design: Working with a system or solution architect, designing the architecture of a data end-to-end data pipeline.</p>
<p>Development of data-related instruments:</p>
<p>2. Build, customize, and manage Software components, integration tools, databases, QA, and analytical systems.</p>
<p>3. Data pipeline maintenance/testing: Direct the Testing team and assist with testing the reliability and performance of each part of a system, suggest and test improvements, and test any changes to the overall system.</p>
<p>4. Manage data and metadata. Manage the flow, management, and processing of structured and unstructured data and its related metadata.</p>
<p>5. Track pipeline stability. Monitor the overall performance and stability of the system including standard processing, maintenance routines and activities, and ongoing innovation and improvements.</p>
<p>Education Requirements</p>
<p>Bachelor&apos;s Degree: Computer Science, Computer Engineering, Information Technology, Software Engineering, Electrical Engineering, Mathematics, Statistics</p>
<p>Successful candidates will have.</p>
<p>1. A software engineering background. Proficient in the management of teams using one or several programming languages specifically Java (Hadoop, Apache Hive), Scala (Kafka, Apache Spark), and Python</p>
<p>2. Familiar with various SQL and NoSQL database management systems like MySQL, Oracle, PostgreSQL, MongoDB, and others.</p>
<p>3. Knowledge of on-prem virtualized and containerized systems and modern cloud-based technologies such as BigQuery, Snowflake, Firebolt, and Amazon Redshift.</p>
<p>Knowledge of how to secure each step of the data pipeline to ensure data protection and chain of custody.</p>
<p>4. A strong understanding of data modeling, algorithms, and data transformation techniques</p>
<p>Experience with the existing ETL and BI solutions to facilitate building ETL/ELT pipelines, data storage, and analytical tools.</p>
<p>5. Creativity and problem-solving skills, looking for optimal (and often innovative) solutions to complicated issues.</p>
<p>6. Highly attentive to detail, be able to carry out monotonous tasks, and find ways to decompose big problems into smaller, achievable steps.</p>
<p>7. Communication skills and successful collaboration skills with team members and other stakeholders.</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;130,000.00 - &#x24;145,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,f90735e4bc814f60,,Full-time,,,"Reston, VA",Data Engineer,17 days ago,2023-10-04T13:11:05.965Z,2.7,139.0,"$130,000 - $145,000 a year",2023-10-21T13:11:05.966Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=f90735e4bc814f60&from=jasx&tk=1hd95ofc82ea2001&vjs=3
