 Here are the key skills and responsibilities extracted from the job description:

Skills:
- 10+ years of IT experience focusing on enterprise data architecture and management  
- Experience with Databricks required
- 8+ years experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
- Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services
- Advanced level SQL experience 
- Experience with AWS environment, CI/CD pipelines, and Python (Python 3)

Responsibilities:
- Plan, create, and maintain data architectures
- Obtain data, formulate dataset processes, and store optimized data
- Identify problems and inefficiencies and apply solutions  
- Create and manage data lifecycle policies
- Create, maintain, and manage ETL/ELT pipelines
- Create, maintain, and manage data transformations
- Maintain documentation
- Create, maintain, and manage data pipeline schedules
- Monitor data pipelines
- Create, maintain, and manage data quality gates
- Support AI/ML teams with optimizing feature engineering code
- Create, maintain, and manage Spark jobs
- Research existing data in the data lake
- Create, manage, and maintain ksqlDB and Kafka queries/code
- Maintain Python-based data processing scripts
- Unit tests for all data processing codes
- Maintain data lake with optimizations

https://www.indeed.com/rc/clk?jk=c1dc76acaaac3b9b&from=jasx&tk=1hd1ft1i8joou800&vjs=3