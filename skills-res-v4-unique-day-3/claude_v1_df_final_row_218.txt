 Here are the key skills and responsibilities extracted from the job description:

Skills:
- 10+ years of IT experience focusing on enterprise data architecture and management
- Experience with Databricks required 
- 8+ years experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
- Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services
- Advanced level SQL experience 
- Experience with AWS environment, CI/CD pipelines, and Python (Python 3)

Responsibilities:
- Plan, create, and maintain data architectures
- Obtain data, formulate dataset processes, and store optimized data
- Identify problems and inefficiencies and apply solutions
- Determine tasks where manual participation can be eliminated with automation
- Create and manage data lifecycle policies  
- Create, maintain, and manage ETL/ELT pipelines
- Create, maintain, and maintain Spark Structured Steaming jobs
- Research existing data in the data lake to determine best sources
- Create, manage, and maintain ksqlDB and Kafka Streams queries/code
- Maintain and update Python-based data processing scripts
- Unit tests for all the data processing and Lambda codes
- Maintain PCIS Reporting Database data lake with optimizations
- Debug, troubleshoot, and implement solutions to complex issues

https://www.indeed.com/rc/clk?jk=5581f8a1bea3816b&from=jasx&tk=1hd6h6bheiolg800&vjs=3