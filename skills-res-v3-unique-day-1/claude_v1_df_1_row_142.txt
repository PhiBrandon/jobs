 Here are the key skills and responsibilities extracted from the job description:

Skills:
- 3+ years of real-world data engineering development experience in Snowflake and AWS 
- Proficient in snowflake services such as snowpipe, stages, stored procedures, views, materialized views, tasks and streams
- Strong programming skills in SQL, Python, Scala
- Knowledge of data security measures in Snowflake, including role-based access control (RBAC) and data encryption
- Experience with SDLC tools like Jira, GitHub, AWS CodeBuild, AWS CodeArtifact
- Experience with data integration from different sources  
- Understanding of data modeling and database design principles
- Experience with ELT/ETL tools and custom integration solutions
- Experience with distributed data technologies like Spark, DBT, Kafka
- Experience designing and implementing data warehousing solutions in AWS with Redshift
- Experience with orchestration using Apache Airflow
- Expert knowledge of AWS services like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift
- Understanding of data quality and governance

Responsibilities:
- Follow established data architectures and develop/maintain data pipelines
- Ensure reliability, scalability and efficiency of data systems
- Configure and manage Snowflake data warehousing and data lake solutions
- Collaborate with cross-functional teams to understand data requirements 
- Contribute to data quality assurance efforts
- Evaluate and implement new technologies
- Develop data governance strategies
- Document data engineering processes and flows
- Address complex data engineering issues and resolve bottlenecks
- Assess best practices and design schemas for analytics solutions
- Participate in Agile ceremonies and continuous improvement activities