,company,description,descriptionHTML,externalApplyLink,id,jobType,jobType/0,jobType/1,jobType/2,location,positionName,postedAt,postingDateParsed,rating,reviewsCount,salary,scrapedAt,searchInput/country,searchInput/location,searchInput/position,url
0,OrangePeople,"We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.
Responsibilities:

 Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.
 Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.
 Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.
 Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.
 Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.
 Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.
 Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.
 Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.
 Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.
 Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.
 Is familiar with SOC 2 compliance and its impact on company policies and processes.
 Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.
 Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.

Requirements:

 Bachelor's or Master’s degree in Computer Science, Engineering, or a related field.
 8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.
 Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.
 Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.
 Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.
 Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.
 Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.
 Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.
 Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.
 A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.

Job Type: Contract
Pay: From $60.00 per hour
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Vision insurance

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Python: 5 years (Preferred)
 data storage, processing, and analytics: 6 years (Preferred)
 SQL and NoSQL: 6 years (Preferred)
 healthcare: 5 years (Preferred)
 Airflow: 5 years (Preferred)
 Kubernetes: 6 years (Preferred)
 Docker: 6 years (Preferred)
 ETL: 5 years (Preferred)
 data engineering: 8 years (Preferred)
 Apache Spark & Snowflake: 6 years (Preferred)
 Big data: 6 years (Preferred)
 Databricks: 5 years (Preferred)
 AWS Services: 5 years (Preferred)
 PySpark: 5 years (Preferred)

Work Location: Remote","<p>We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.</li>
 <li>Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.</li>
 <li>Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.</li>
 <li>Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.</li>
 <li>Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.</li>
 <li>Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.</li>
 <li>Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.</li>
 <li>Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.</li>
 <li>Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.</li>
 <li>Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.</li>
 <li>Is familiar with SOC 2 compliance and its impact on company policies and processes.</li>
 <li>Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.</li>
 <li>Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.</li>
</ul>
<p><b>Requirements:</b></p>
<ul>
 <li>Bachelor&apos;s or Master&#x2019;s degree in Computer Science, Engineering, or a related field.</li>
 <li>8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.</li>
 <li>Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.</li>
 <li>Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.</li>
 <li>Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.</li>
 <li>Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.</li>
 <li>Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.</li>
 <li>Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.</li>
 <li>Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.</li>
 <li>A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: From &#x24;60.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Python: 5 years (Preferred)</li>
 <li>data storage, processing, and analytics: 6 years (Preferred)</li>
 <li>SQL and NoSQL: 6 years (Preferred)</li>
 <li>healthcare: 5 years (Preferred)</li>
 <li>Airflow: 5 years (Preferred)</li>
 <li>Kubernetes: 6 years (Preferred)</li>
 <li>Docker: 6 years (Preferred)</li>
 <li>ETL: 5 years (Preferred)</li>
 <li>data engineering: 8 years (Preferred)</li>
 <li>Apache Spark &amp; Snowflake: 6 years (Preferred)</li>
 <li>Big data: 6 years (Preferred)</li>
 <li>Databricks: 5 years (Preferred)</li>
 <li>AWS Services: 5 years (Preferred)</li>
 <li>PySpark: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,2865c3b2b6437bd7,,Contract,,,Remote,Cloud Data Engineer,Today,2023-10-18T13:30:38.652Z,,,From $60 an hour,2023-10-18T13:30:38.749Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2865c3b2b6437bd7&from=jasx&tk=1hd1fm1o6iman800&vjs=3
4,UnitedHealthcare,"At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.
  
  
 Positions in this function are responsible for the management and manipulation of mostly structured data, with a focus on building business intelligence tools, conducting analysis, performing normalization operations, and assuring data quality. Depending on the specific role and business line, example responsibilities in this function could include creating specifications to bring data into a common structure, creating product specifications and models, developing data solutions to support analyses, performing analysis, interpreting results, developing actionable insights and presenting recommendations for use across the company. Roles in this function could partner with stakeholders to understand data requirements and develop tools and models such as segmentation, dashboards, data visualizations, decision aids and business case analysis to support the organization. Other roles involved could include producing and managing the delivery of activity and value analytics to external stakeholders and clients. Team members will typically use business intelligence, data visualization, query, analytic and statistical software to build solutions, perform analysis and interpret data.
  
  
  You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.
  
  
 Primary Responsibilities: 
  General Job Profile 
  
  This position will work directly with stakeholders within our UHC Core Operations and more broadly within UHG as appropriate to deliver quality data solutions. This position provides visibility, analytical, ad hoc and development capabilities leveraging data from different functional areas across the UHG landscape 
  
  Job Scope and Guidelines 
  
  Works in an Agile framework within a matrix environment working in sprints and utilizing agile tools (e.g., RallyDev) 
  Instills an Agile framework within the team and across the matrix environment to operate as applicable and fully utilizing RallyDev tools 
  Build, maintain and/or adhere to a structured data governance process to be used across all datasets with a focus on quality and accuracy 
  Works closely within Quality Management & Insights (QMI) and across UnitedHealthcare 
  Identify and participate in the resolution of data integrity issues and organizational problems 
  
 
 Functional Competencies 
  
  Demonstrate and apply understanding of UnitedHealth Group's business (e.g., specific business capabilities, functions, processes, and business cycles) and knowledge of operations, goals, and policies and procedures of internal business partners (e.g., information contacts) to provide effective support to internal and/or external customers 
  Manage and protect data, adhering to applicable legal/regulatory requirements (e.g., HIPAA, PHI, PII, DOI, state and federal regulations) 
  Propose and/or define long-term strategies for implementing process and/or data and reporting improvements 
  Identify and/or provide opportunities for additional training and learning to support process and report improvements 
  Review and/or identify appropriate data infrastructure to use based on customers' needs in alignment with QMI priorities 
  Develop business context diagrams (e.g., business data flows, process flows) to analyze/confirm the definition of project requirements 
  Demonstrate understanding of the difference between business requirements and technical solutions and define approach for storing and updating business requirements 
  Collaborate with business and technical stakeholders (e.g., business owners, process owners, domain experts) to identify specific business requirements. Perform reviews with all stakeholders to obtain approval/signoff of project requirements documents 
  Update progress to project schedule to track/measure progress one’s progress fulfilling aligned tasks. In addition to supporting ongoing monitoring by keeping project documentation or applications updated (e.g., RallyDev)
 
  
  
  
 You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
  Required Qualifications: 
  
  3+ years of SQL/TSQL development experience 
  3+ years of SSIS package development experience 
  3+ years of experience with Data Modeling, ETL construction with advanced job scheduling 
  3+ years of experience performing data analysis and report development 
  3+ years of experience working in relational databases, database structures and design, systems design, data management, data warehouse
 
  
  
  
 Preferred Qualifications: 
  
  Bachelor’s Degree 
  Experience with MS Access
 
  
  
 
  All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy
 
  
  
 California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The salary range for California/Colorado/Connecticut/Nevada/New Jersey/New York/Rhode Island/Washington residents is $67,800 to $133,100 annually. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
  
  
  At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
  
  
  
 Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
  
  
  UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.","<div>
 <p>At UnitedHealthcare, we&#x2019;re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start <b>Caring. Connecting. Growing together.</b></p>
 <br> 
 <p> </p>
 <p>Positions in this function are responsible for the management and manipulation of mostly structured data, with a focus on building business intelligence tools, conducting analysis, performing normalization operations, and assuring data quality. Depending on the specific role and business line, example responsibilities in this function could include creating specifications to bring data into a common structure, creating product specifications and models, developing data solutions to support analyses, performing analysis, interpreting results, developing actionable insights and presenting recommendations for use across the company. Roles in this function could partner with stakeholders to understand data requirements and develop tools and models such as segmentation, dashboards, data visualizations, decision aids and business case analysis to support the organization. Other roles involved could include producing and managing the delivery of activity and value analytics to external stakeholders and clients. Team members will typically use business intelligence, data visualization, query, analytic and statistical software to build solutions, perform analysis and interpret data.</p>
 <br> 
 <p></p> 
 <p> You&#x2019;ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.</p>
 <br> 
 <p> </p>
 <p><b>Primary Responsibilities:</b></p> 
 <p><b> General Job Profile</b></p> 
 <ul> 
  <li>This position will work directly with stakeholders within our UHC Core Operations and more broadly within UHG as appropriate to deliver quality data solutions. This position provides visibility, analytical, ad hoc and development capabilities leveraging data from different functional areas across the UHG landscape</li> 
 </ul> 
 <p><b> </b><b>Job Scope and Guidelines</b></p> 
 <ul> 
  <li>Works in an Agile framework within a matrix environment working in sprints and utilizing agile tools (e.g., RallyDev)</li> 
  <li>Instills an Agile framework within the team and across the matrix environment to operate as applicable and fully utilizing RallyDev tools</li> 
  <li>Build, maintain and/or adhere to a structured data governance process to be used across all datasets with a focus on quality and accuracy</li> 
  <li>Works closely within Quality Management &amp; Insights (QMI) and across UnitedHealthcare</li> 
  <li>Identify and participate in the resolution of data integrity issues and organizational problems</li> 
 </ul> 
 <p></p>
 <p><b>Functional Competencies</b></p> 
 <ul> 
  <li>Demonstrate and apply understanding of UnitedHealth Group&apos;s business (e.g., specific business capabilities, functions, processes, and business cycles) and knowledge of operations, goals, and policies and procedures of internal business partners (e.g., information contacts) to provide effective support to internal and/or external customers</li> 
  <li>Manage and protect data, adhering to applicable legal/regulatory requirements (e.g., HIPAA, PHI, PII, DOI, state and federal regulations)</li> 
  <li>Propose and/or define long-term strategies for implementing process and/or data and reporting improvements</li> 
  <li>Identify and/or provide opportunities for additional training and learning to support process and report improvements</li> 
  <li>Review and/or identify appropriate data infrastructure to use based on customers&apos; needs in alignment with QMI priorities</li> 
  <li>Develop business context diagrams (e.g., business data flows, process flows) to analyze/confirm the definition of project requirements</li> 
  <li>Demonstrate understanding of the difference between business requirements and technical solutions and define approach for storing and updating business requirements</li> 
  <li>Collaborate with business and technical stakeholders (e.g., business owners, process owners, domain experts) to identify specific business requirements. Perform reviews with all stakeholders to obtain approval/signoff of project requirements documents</li> 
  <li>Update progress to project schedule to track/measure progress one&#x2019;s progress fulfilling aligned tasks. In addition to supporting ongoing monitoring by keeping project documentation or applications updated (e.g., RallyDev)</li>
 </ul>
 <br> 
 <p></p> 
 <p> </p>
 <p>You&#x2019;ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.</p>
 <p><b> Required Qualifications:</b></p> 
 <ul> 
  <li>3+ years of SQL/TSQL development experience</li> 
  <li>3+ years of SSIS package development experience</li> 
  <li>3+ years of experience with Data Modeling, ETL construction with advanced job scheduling</li> 
  <li>3+ years of experience performing data analysis and report development</li> 
  <li>3+ years of experience working in relational databases, database structures and design, systems design, data management, data warehouse</li>
 </ul>
 <br> 
 <p></p> 
 <p> </p>
 <p><b>Preferred Qualifications:</b></p> 
 <ul> 
  <li>Bachelor&#x2019;s Degree</li> 
  <li>Experience with MS Access</li>
 </ul>
 <br> 
 <p></p> 
 <ul>
  <li>All employees working remotely will be required to adhere to UnitedHealth Group&#x2019;s Telecommuter Policy</li>
 </ul>
 <br> 
 <p> </p>
 <p><b>California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: </b>The salary range for California/Colorado/Connecticut/Nevada/New Jersey/New York/Rhode Island/Washington residents is &#x24;67,800 to &#x24;133,100 annually. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you&#x2019;ll find a far-reaching choice of benefits and incentives.</p>
 <br> 
 <p></p> 
 <p><i> At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone&#x2013;of every race, gender, sexuality, age, location and income&#x2013;deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes &#x2014; an enterprise priority reflected in our mission.</i></p>
 <br> 
 <p></p> 
 <p><i> </i></p>
 <p><i>Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.</i></p>
 <br> 
 <p></p> 
 <p><i> UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.</i></p>
</div>",https://careers.unitedhealthgroup.com/job/19297158/data-engineer-business-insights-remote-remote/?src=JB-22473,445662e46d0fc401,,Full-time,,,"Chicago, IL 60601","Data Engineer, Business Insights - Remote",Today,2023-10-18T13:30:38.447Z,3.7,2350.0,"$67,800 - $133,100 a year",2023-10-18T13:30:38.450Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=445662e46d0fc401&from=jasx&tk=1hd1fm1o6iman800&vjs=3
6,SCIGON Solution,"Skills:

 Extensive experience as a Data Engineer or similar role, with a focus on Scala, Java, and Python programming languages.
 Proven expertise in Apache Spark, HBase, and Hive, with the ability to process and manipulate large datasets effectively.
 Familiarity with AWS services (e.g., EMR, S3, Redshift) is a plus.
 Experience with data migration from on-premises to cloud environments is desirable.
 A bachelor's degree in Computer Science, Data Science, or a related field (a master's degree is a plus).
 Strong problem-solving skills and attention to detail.
 Excellent communication and teamwork abilities.

Responsibilities:

 Develop Data Processing Solutions: Leverage your expertise in Scala (60%), Java, and Python to design, develop, and maintain data processing solutions that operate at scale, ensuring the efficiency, reliability, and performance of data workflows.
 Big Data Mastery: Utilize your in-depth knowledge of Big Data technologies, including Apache Spark, HBase, and Hive, to process and manipulate large datasets efficiently.
 Cloud Expertise: If you have experience with AWS, contribute to the migration of data processing systems from on-premises to the cloud, ensuring seamless and secure operations.
 Data Migration: Collaborate on data migration initiatives, playing a pivotal role in transitioning data processing and handling systems to modern cloud-based platforms.
 Data Handling: Employ best practices in data handling, ensuring the integrity, security, and accessibility of data assets while complying with relevant regulations.

Job Type: Contract
Pay: $58.00 - $68.00 per hour
Experience level:

 5 years
 6 years

Schedule:

 Monday to Friday

Application Question(s):

 Due to numerous fraudulent applications, we require candidates to show any valid IDs at the beginning of the video interview stage (We only need to see your name and photo, you can cover the rest of the details). Are you willing to provide it? (Yes/No):

Education:

 Bachelor's (Preferred)

Experience:

 Data Engineering: 5 years (Preferred)
 Scala: 3 years (Preferred)
 Java: 3 years (Preferred)
 Python: 3 years (Preferred)
 Apache Spark: 3 years (Preferred)
 HBase, and Hive: 3 years (Preferred)
 AWS Services: S3, Redshift: 3 years (Preferred)
 data migration from on-premises to cloud environment: 3 years (Preferred)
 AWS EMR: 3 years (Preferred)
 AWS Airflow: 3 years (Preferred)
 Industry: 5 years (Preferred)

Work Location: Remote","<p>Skills:</p>
<ul>
 <li>Extensive experience as a Data Engineer or similar role, with a focus on Scala, Java, and Python programming languages.</li>
 <li>Proven expertise in Apache Spark, HBase, and Hive, with the ability to process and manipulate large datasets effectively.</li>
 <li>Familiarity with AWS services (e.g., EMR, S3, Redshift) is a plus.</li>
 <li>Experience with data migration from on-premises to cloud environments is desirable.</li>
 <li>A bachelor&apos;s degree in Computer Science, Data Science, or a related field (a master&apos;s degree is a plus).</li>
 <li>Strong problem-solving skills and attention to detail.</li>
 <li>Excellent communication and teamwork abilities.</li>
</ul>
<p>Responsibilities:</p>
<ul>
 <li>Develop Data Processing Solutions: Leverage your expertise in Scala (60%), Java, and Python to design, develop, and maintain data processing solutions that operate at scale, ensuring the efficiency, reliability, and performance of data workflows.</li>
 <li>Big Data Mastery: Utilize your in-depth knowledge of Big Data technologies, including Apache Spark, HBase, and Hive, to process and manipulate large datasets efficiently.</li>
 <li>Cloud Expertise: If you have experience with AWS, contribute to the migration of data processing systems from on-premises to the cloud, ensuring seamless and secure operations.</li>
 <li>Data Migration: Collaborate on data migration initiatives, playing a pivotal role in transitioning data processing and handling systems to modern cloud-based platforms.</li>
 <li>Data Handling: Employ best practices in data handling, ensuring the integrity, security, and accessibility of data assets while complying with relevant regulations.</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;58.00 - &#x24;68.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
 <li>6 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Due to numerous fraudulent applications, we require candidates to show any valid IDs at the beginning of the video interview stage (We only need to see your name and photo, you can cover the rest of the details). Are you willing to provide it? (Yes/No):</li>
</ul>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Preferred)</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Engineering: 5 years (Preferred)</li>
 <li>Scala: 3 years (Preferred)</li>
 <li>Java: 3 years (Preferred)</li>
 <li>Python: 3 years (Preferred)</li>
 <li>Apache Spark: 3 years (Preferred)</li>
 <li>HBase, and Hive: 3 years (Preferred)</li>
 <li>AWS Services: S3, Redshift: 3 years (Preferred)</li>
 <li>data migration from on-premises to cloud environment: 3 years (Preferred)</li>
 <li>AWS EMR: 3 years (Preferred)</li>
 <li>AWS Airflow: 3 years (Preferred)</li>
 <li>Industry: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,334af16420b57c82,,Contract,,,Remote,Sr. Data Engineer (Contract/W2 Only/ No Vendor C2C),Today,2023-10-18T13:30:55.953Z,,,$58 - $68 an hour,2023-10-18T13:30:56.047Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=334af16420b57c82&from=jasx&tk=1hd1fm1o6iman800&vjs=3
7,Sleeper,"Position Summary
  The Senior/First Data Engineer will play a crucial role in building, maintaining, and enhancing ETL processes that drive our analytics and machine learning platforms. This individual will be responsible for developing actionable insights from complex data sets, and work closely with various business units to inform strategy and decision-making.
  You will be the first hire in this function.
 
  Location
 
   SF Bay Area, NYC, or Remote
 
 
  Key Responsibilities
 
   ETL & Backend Development:
 
 
   Design and optimize ETL pipelines.
   Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB.
 
 
   Data Architecture:
 
 
   Design scalable and efficient data models for Cassandra and ScyllaDB.
   Ensure data integrity, quality, and security.
 
 
   Data Science Support:
 
 
   Collaborate with data scientists, providing them with clean and reliable datasets.
   Assist in implementing and scaling data science models.
 
 
   Innovation & Research:
 
 
   Stay abreast of latest technologies.
   Recommend technical improvements for data processing and storage.
 
 
  Qualifications
  Required
 
   Bachelor’s or Master’s degree in Computer Science, Engineering, or a related technical field.
   5+ years of experience in backend development, with a strong focus on data engineering.
   
     Technical skills: Expertise in Python, Java, Scala, and Elixer for backend and ETL processes.
     Mastery of ETL tools/frameworks (e.g. Apache Kafka, Apache Airflow).
     Deep knowledge of SQL/NoSQL databases, including Cassandra and ScyllaDB, and data warehousing solutions (e.g., Redshift, BigQueary, Snowflake).
     Proficiency in cloud platforms (AWS, GCP, Azure) and distributed systems.
     Familiarity with data science concepts, tools, and libraries (e.g. Pandas, Scikit-learn).
     Soft Skills: Exceptional problem-solving skills.
     Strong communication for technical and non-technical discussions.
   
 
  Nice-to-have
 
   Experience with cloud platforms like AWS, GCP, or Azure.
   Exceptional communication skills, both verbal and written.
   Expertise in machine learning algorithms and frameworks (e.g., TensorFlow, PyTorch, scikit-learn).
 
 
  Benefits
 
   Competitive salary ($150,000-$225,000/year) and stock options
   Comprehensive health, dental, and vision plans
   401(k)
   Flexible working hours and remote work options
   Regular team building events and activities","<div>
 <h2 class=""jobSectionHeader""><b>Position Summary</b></h2>
 <p> The Senior/First Data Engineer will play a crucial role in building, maintaining, and enhancing ETL processes that drive our analytics and machine learning platforms. This individual will be responsible for developing actionable insights from complex data sets, and work closely with various business units to inform strategy and decision-making.</p>
 <p> You will be the first hire in this function.</p>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Location</b></h2>
 <ul>
  <li><p> SF Bay Area, NYC, or Remote</p></li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Key Responsibilities</b></h2>
 <ul>
  <li><p> ETL &amp; Backend Development:</p></li>
 </ul>
 <ul>
  <li><p> Design and optimize ETL pipelines.</p></li>
  <li><p> Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB.</p></li>
 </ul>
 <ul>
  <li><p> Data Architecture:</p></li>
 </ul>
 <ul>
  <li><p> Design scalable and efficient data models for Cassandra and ScyllaDB.</p></li>
  <li><p> Ensure data integrity, quality, and security.</p></li>
 </ul>
 <ul>
  <li><p> Data Science Support:</p></li>
 </ul>
 <ul>
  <li><p> Collaborate with data scientists, providing them with clean and reliable datasets.</p></li>
  <li><p> Assist in implementing and scaling data science models.</p></li>
 </ul>
 <ul>
  <li><p> Innovation &amp; Research:</p></li>
 </ul>
 <ul>
  <li><p> Stay abreast of latest technologies.</p></li>
  <li><p> Recommend technical improvements for data processing and storage.</p></li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Qualifications</b></h2>
 <h3 class=""jobSectionHeader""><b> Required</b></h3>
 <ul>
  <li><p> Bachelor&#x2019;s or Master&#x2019;s degree in Computer Science, Engineering, or a related technical field.</p></li>
  <li><p> 5+ years of experience in backend development, with a strong focus on data engineering.</p>
   <ul>
    <li><p> Technical skills: Expertise in Python, Java, Scala, and Elixer for backend and ETL processes.</p></li>
    <li><p> Mastery of ETL tools/frameworks (e.g. Apache Kafka, Apache Airflow).</p></li>
    <li><p> Deep knowledge of SQL/NoSQL databases, including Cassandra and ScyllaDB, and data warehousing solutions (e.g., Redshift, BigQueary, Snowflake).</p></li>
    <li><p> Proficiency in cloud platforms (AWS, GCP, Azure) and distributed systems.</p></li>
    <li><p> Familiarity with data science concepts, tools, and libraries (e.g. Pandas, Scikit-learn).</p></li>
    <li><p> Soft Skills: Exceptional problem-solving skills.</p></li>
    <li><p> Strong communication for technical and non-technical discussions.</p></li>
   </ul></li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Nice-to-have</b></h3>
 <ul>
  <li><p> Experience with cloud platforms like AWS, GCP, or Azure.</p></li>
  <li><p> Exceptional communication skills, both verbal and written.</p></li>
  <li><p> Expertise in machine learning algorithms and frameworks (e.g., TensorFlow, PyTorch, scikit-learn).</p></li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Benefits</b></h2>
 <ul>
  <li><p> Competitive salary (&#x24;150,000-&#x24;225,000/year) and stock options</p></li>
  <li><p> Comprehensive health, dental, and vision plans</p></li>
  <li><p> 401(k)</p></li>
  <li><p> Flexible working hours and remote work options</p></li>
  <li><p> Regular team building events and activities</p></li>
 </ul>
</div>
<p></p>",https://www.indeed.com/applystart?jk=8baee8cb684bbc8a&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXh3KzOnQ4B8j-IF7wQPwfQQ5Z_dA0xndw9ylAW4Hz2CAQ&vjfrom=serp&astse=b85fa03b646a3712&assa=8318,8baee8cb684bbc8a,,Full-time,,,"Washington, DC",Senior/First Data Engineer,Today,2023-10-18T13:30:58.745Z,,,"$150,000 - $225,000 a year",2023-10-18T13:30:58.748Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=8baee8cb684bbc8a&from=jasx&tk=1hd1fm1o6iman800&vjs=3
8,The Talent Trove,"We are hiring for one of our major Health Care client and we are looking for a Senior Data Engineer with Snowflake and Data Factory experience in Azure/ any cloud environment.
Roles and responsibilities:

 Data Pipeline Development: Develop and maintain data pipelines that extract, transform, and load (ETL) data from various sources into a centralized data storage system, such as a data warehouse or data lake. Ensure the smooth flow of data from source systems to destination systems while adhering to data quality and integrity standards.
 Data Integration: Integrate data from multiple sources and systems, including databases, APIs, log files, streaming platforms, and external data providers. Handle data ingestion, transformation, and consolidation to create a unified and reliable data foundation for analysis and reporting.
 Data Transformation and Processing: Develop data transformation routines to clean, normalize, and aggregate data. Apply data processing techniques to handle complex data structures, handle missing or inconsistent data, and prepare the data for analysis, reporting, or machine learning tasks.
 Contribute to common frameworks and best practices in code development, deployment, and automation/orchestration of data pipelines.
 Implement data governance in line with company standards
 Partner with Data Analytics and Product leaders to design best practices and standards for developing and productionalizing analytic pipelines.
 Partner with Infrastructure leaders on architecture approaches to advance the data and analytics platform, including exploring new tools and techniques that leverage the cloud environment (Azure, Snowflake, others)
 Monitoring and Support: Monitor data pipelines and data systems to detect and resolve issues promptly. Develop monitoring tools, alerts, and automated error handling mechanisms to ensure data integrity and system reliability.

Qualifications:

 Extensive experience designing data solutions including data modeling.
 Extensive hands-on experience developing data processing jobs (SQL) that demonstrates a strong understanding of software engineering principles.
 Experience orchestrating data pipelines using technology like ADF, Airflow etc
 Experience working with both real-time and batch data, knowing the strengths and weaknesses of both and when to apply one over another.
 Experience building data pipelines on either AWS, Azure or GCP, following best practices in Cloud deployments
 Experience working with Hive /HBase / Presto
 Fluent in SQL (any flavor), with experience using Window functions and more advanced features.
 Understanding of DevOps tools, Git workflow and building CI/CD pipelines
 Ability to work with business and technical audiences on business requirement meetings, technical white boarding exercises, and SQL coding/debugging sessions.
 Experience supporting big data pipelines.
 Experience applying data governance controls within a highly regulated environment.

Preferred Qualifications:

 Bachelor’s Degree or higher in Database Management, Information Technology, Computer Science or similar
 Experience working in projects with agile/scrum methodologies.
 Familiar with Azure Data Factory or Apache Airflow
 Familiar with Azure Databricks or Snowflake
 Experience with shell scripting languages
 Well versed in Python, in fulfilling multiple general-purpose use-cases, and not limited to developing data APIs and pipelines.
 Experience with Apache Spark and related Big Data stack and technologies
 Experience working with Apache Kafka, building appropriate producer/consumer apps.
 Familiarity with production quality ML and/or AI model development and deployment.
 Experience working with Kubernetes and Docker, and knowledgeable about cloud infrastructure automation and management (e.g., Terraform)

Job Type: Contract
Pay: $65.00 per hour
Benefits:

 401(k)
 Dental insurance
 Health insurance

Experience level:

 10 years

Schedule:

 8 hour shift

Work Location: Remote","<p>We are hiring for one of our major Health Care client and we are looking for a Senior Data Engineer with Snowflake and Data Factory experience in Azure/ any cloud environment.</p>
<p><b>Roles and responsibilities:</b></p>
<ul>
 <li>Data Pipeline Development: Develop and maintain data pipelines that extract, transform, and load (ETL) data from various sources into a centralized data storage system, such as a data warehouse or data lake. Ensure the smooth flow of data from source systems to destination systems while adhering to data quality and integrity standards.</li>
 <li>Data Integration: Integrate data from multiple sources and systems, including databases, APIs, log files, streaming platforms, and external data providers. Handle data ingestion, transformation, and consolidation to create a unified and reliable data foundation for analysis and reporting.</li>
 <li>Data Transformation and Processing: Develop data transformation routines to clean, normalize, and aggregate data. Apply data processing techniques to handle complex data structures, handle missing or inconsistent data, and prepare the data for analysis, reporting, or machine learning tasks.</li>
 <li>Contribute to common frameworks and best practices in code development, deployment, and automation/orchestration of data pipelines.</li>
 <li>Implement data governance in line with company standards</li>
 <li>Partner with Data Analytics and Product leaders to design best practices and standards for developing and productionalizing analytic pipelines.</li>
 <li>Partner with Infrastructure leaders on architecture approaches to advance the data and analytics platform, including exploring new tools and techniques that leverage the cloud environment (Azure, Snowflake, others)</li>
 <li>Monitoring and Support: Monitor data pipelines and data systems to detect and resolve issues promptly. Develop monitoring tools, alerts, and automated error handling mechanisms to ensure data integrity and system reliability.</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Extensive experience designing data solutions including data modeling.</li>
 <li>Extensive hands-on experience developing data processing jobs (SQL) that demonstrates a strong understanding of software engineering principles.</li>
 <li>Experience orchestrating data pipelines using technology like ADF, Airflow etc</li>
 <li>Experience working with both real-time and batch data, knowing the strengths and weaknesses of both and when to apply one over another.</li>
 <li>Experience building data pipelines on either AWS, Azure or GCP, following best practices in Cloud deployments</li>
 <li>Experience working with Hive /HBase / Presto</li>
 <li>Fluent in SQL (any flavor), with experience using Window functions and more advanced features.</li>
 <li>Understanding of DevOps tools, Git workflow and building CI/CD pipelines</li>
 <li>Ability to work with business and technical audiences on business requirement meetings, technical white boarding exercises, and SQL coding/debugging sessions.</li>
 <li>Experience supporting big data pipelines.</li>
 <li>Experience applying data governance controls within a highly regulated environment.</li>
</ul>
<p><b>Preferred Qualifications:</b></p>
<ul>
 <li>Bachelor&#x2019;s Degree or higher in Database Management, Information Technology, Computer Science or similar</li>
 <li>Experience working in projects with agile/scrum methodologies.</li>
 <li>Familiar with Azure Data Factory or Apache Airflow</li>
 <li>Familiar with Azure Databricks or Snowflake</li>
 <li>Experience with shell scripting languages</li>
 <li>Well versed in Python, in fulfilling multiple general-purpose use-cases, and not limited to developing data APIs and pipelines.</li>
 <li>Experience with Apache Spark and related Big Data stack and technologies</li>
 <li>Experience working with Apache Kafka, building appropriate producer/consumer apps.</li>
 <li>Familiarity with production quality ML and/or AI model development and deployment.</li>
 <li>Experience working with Kubernetes and Docker, and knowledgeable about cloud infrastructure automation and management (e.g., Terraform)</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;65.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,7f70c2bb5373b7a9,,Contract,,,Remote,Senior Data Engineer,Today,2023-10-18T13:31:01.959Z,,,$65 an hour,2023-10-18T13:31:01.960Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=7f70c2bb5373b7a9&from=jasx&tk=1hd1fm1o6iman800&vjs=3
9,VUI,"Role: Data Engineer
 Location: Remote

Contract-to-hire (C2H) Only
Job Description:
Looking for a data engineer to design, develop and maintain pipelines and workflows and create analytics to digitally transform the current NA Accident and Health reporting and business decision processes.
Responsibilities:
Development and support of data pipelines that produce data assets for various A&H workstreams including UW, UA, Actuarial, Claims
Handle data pipelines while testing for data curation, parsing, cleaning, transformation and enrichment of data
Work with fundamentals of data processing, data pipeline, data lineage and ETL (Extract-Transform-Load) methodologies
Implement the project according to the Software Development Life Cycle (SDLC) and programming by using fast paced agile methodology, involving task completion, user stories
Utilize knowledge of database management system software, object oriented programming development, system architecture and components and various programming languages
Review and analyze business workflows and user data needs
Design and implement business performance dashboards
Write customized queries/programs to generate automatic periodical reports highlighting all the Key Performance Indicators (KPIs)
Build applications using SQL and/or Python scripts to manipulate data, monitor and help to improve data quality
Design, build and maintain end-to-end data solutions supporting our processes with the right data architecture
Have working knowledge of Apache Spark, big data processing and building products on distributed cluster-computing framework
Construct workflow charts and diagrams and writing specifications.
Documentation of end-to-end data pipeline process.
Documentation of data assets for information management purposes.
Ad hoc team / business support as needed.
Exploration and evaluation of new technologies and platforms.
Requirements:
Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field
5+ years as a data engineer
Sound Python and SQL skills with ability to query and analyze data, understand complexity and data structuresExperience with data and analytics technology, including but not limited to Hadoop, Spark, Java, Python, R, ElasticSearch, and others
Experience with Palantir Foundry.
Familiarity with relational database concepts
Detail-oriented, analytical, and inquisitiveGood communication skills
Highly organized with strong time-management skills
Ability to work independently and collaborate well with others
Ability to affect smooth organizational transformations
Job Type: Contract
Salary: $40.00 - $60.00 per hour
Experience level:

 10 years
 11+ years
 6 years
 7 years
 8 years
 9 years

Schedule:

 Monday to Friday

Application Question(s):

 What is your work authorization?

Experience:

 Palantir Foundry: 2 years (Required)
 Data Engineer: 5 years (Required)
 Data analytics: 2 years (Required)

Work Location: Remote","<ul>
 <li><b>Role: Data Engineer</b></li>
 <li><b>Location: Remote</b></li>
</ul>
<p><b>Contract-to-hire (C2H) Only</b></p>
<p><b>Job Description:</b></p>
<p>Looking for a data engineer to design, develop and maintain pipelines and workflows and create analytics to digitally transform the current NA Accident and Health reporting and business decision processes.</p>
<p><b>Responsibilities:</b></p>
<p>Development and support of data pipelines that produce data assets for various A&amp;H workstreams including UW, UA, Actuarial, Claims</p>
<p>Handle data pipelines while testing for data curation, parsing, cleaning, transformation and enrichment of data</p>
<p>Work with fundamentals of data processing, data pipeline, data lineage and ETL (Extract-Transform-Load) methodologies</p>
<p>Implement the project according to the Software Development Life Cycle (SDLC) and programming by using fast paced agile methodology, involving task completion, user stories</p>
<p>Utilize knowledge of database management system software, object oriented programming development, system architecture and components and various programming languages</p>
<p>Review and analyze business workflows and user data needs</p>
<p>Design and implement business performance dashboards</p>
<p>Write customized queries/programs to generate automatic periodical reports highlighting all the Key Performance Indicators (KPIs)</p>
<p>Build applications using SQL and/or Python scripts to manipulate data, monitor and help to improve data quality</p>
<p>Design, build and maintain end-to-end data solutions supporting our processes with the right data architecture</p>
<p>Have working knowledge of Apache Spark, big data processing and building products on distributed cluster-computing framework</p>
<p>Construct workflow charts and diagrams and writing specifications.</p>
<p>Documentation of end-to-end data pipeline process.</p>
<p>Documentation of data assets for information management purposes.</p>
<p>Ad hoc team / business support as needed.</p>
<p>Exploration and evaluation of new technologies and platforms.</p>
<p><b>Requirements:</b></p>
<p>Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field</p>
<p>5+ years as a data engineer</p>
<p>Sound <b>Python and</b> SQL skills with ability to query and analyze data, understand complexity and data structures<br>Experience with <b>data and analytics technology</b>, including but not limited to <b>Hadoop, Spark, Java, Python, R, ElasticSearch</b>, and others</p>
<p>Experience with <b>Palantir Foundry</b>.</p>
<p>Familiarity with <b>relational database</b> concepts</p>
<p>Detail-oriented, analytical, and inquisitive<br>Good communication skills</p>
<p>Highly organized with strong time-management skills</p>
<p>Ability to work independently and collaborate well with others</p>
<p>Ability to affect smooth organizational transformations</p>
<p>Job Type: Contract</p>
<p>Salary: &#x24;40.00 - &#x24;60.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>What is your work authorization?</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Palantir Foundry: 2 years (Required)</li>
 <li>Data Engineer: 5 years (Required)</li>
 <li>Data analytics: 2 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,5bcbbacaaee9d357,,Contract,,,Remote,Data Engineer - Palantir Foundry,Today,2023-10-18T13:31:02.290Z,,,$40 - $60 an hour,2023-10-18T13:31:02.291Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=5bcbbacaaee9d357&from=jasx&tk=1hd1fm1o6iman800&vjs=3
12,Stratagen,"Stratagen is currently seeking a highly skilled Data Engineer 2 to join our team. In this role, you will play a key part in implementing essential changes to the legacy National Crime Information Center (NCIC) system in the FBI’s Criminal Justice Information Services (CJIS) Division . These changes are critical for enabling the agile development of the NCIC 3rd Generation (N3G) system. As a Data Engineer, you will have diverse responsibilities encompassing data profiling, data design, data management, and the generation of test data.
Required:

 B.A. or B.S. from an accredited institution
 A minimum of twelve (12) years of data management, database administration or equivalent experience
 A minimum of eight (8) years of data engineering, data analysis, or equivalent experience
 Active Top Secret Clearance

Preferred; however, not required:

 Prior work experience with the Federal Government
 Prior work experience with CJIS systems and associated persistent and/or non- persistent data
 Prior work experience with either Amazon web Services (AwS) or Azure
 Familiarity with Agile development
 Prior work experience with the Scaled Agile Framework (SAFe)
 Experience building data visualizations and reports
 Experience working with IT and business stakeholders in designing the data architecture for a system
 Experience synthesizing and analyzing extremely large data sets
 Familiarity with optimization models

Job Type: Full-time
Pay: $100,000.00 - $140,000.00 per year
Benefits:

 Dental insurance
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Retirement plan
 Vision insurance

Experience level:

 11+ years

Schedule:

 Monday to Friday

Security clearance:

 Top Secret (Required)

Work Location: Remote","<p>Stratagen is currently seeking a highly skilled Data Engineer 2 to join our team. In this role, you will play a key part in implementing essential changes to the legacy National Crime Information Center (NCIC) system in the FBI&#x2019;s Criminal Justice Information Services (CJIS) Division . These changes are critical for enabling the agile development of the NCIC 3rd Generation (N3G) system. As a Data Engineer, you will have diverse responsibilities encompassing data profiling, data design, data management, and the generation of test data.</p>
<p>Required:</p>
<ul>
 <li>B.A. or B.S. from an accredited institution</li>
 <li>A minimum of twelve (12) years of data management, database administration or equivalent experience</li>
 <li>A minimum of eight (8) years of data engineering, data analysis, or equivalent experience</li>
 <li><i>Active Top Secret Clearance</i></li>
</ul>
<p>Preferred; however, not required:</p>
<ul>
 <li>Prior work experience with the Federal Government</li>
 <li>Prior work experience with CJIS systems and associated persistent and/or non- persistent data</li>
 <li>Prior work experience with either Amazon web Services (AwS) or Azure</li>
 <li>Familiarity with Agile development</li>
 <li>Prior work experience with the Scaled Agile Framework (SAFe)</li>
 <li>Experience building data visualizations and reports</li>
 <li>Experience working with IT and business stakeholders in designing the data architecture for a system</li>
 <li>Experience synthesizing and analyzing extremely large data sets</li>
 <li>Familiarity with optimization models</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;100,000.00 - &#x24;140,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>11+ years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Security clearance:</p>
<ul>
 <li>Top Secret (Required)</li>
</ul>
<p>Work Location: Remote</p>",,e65490aaaff7f716,,Full-time,,,Remote,Data Engineer 2,1 day ago,2023-10-17T13:31:10.576Z,,,"$100,000 - $140,000 a year",2023-10-18T13:31:10.649Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e65490aaaff7f716&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
13,FACEBOOK APP,"AWS data engineers have a wide range of responsibilities, which can include:
Creating data models that can be used to extract information from various sources and store it in a usable formatMaintaining the integrity of data by designing backup and recovery proceduresIdentifying opportunities to improve performance by improving database structure or indexing methodsConducting research to identify new technologies that can be applied to current projectsAnalyzing data to find patterns or insights that can be used to develop strategies or make business decisionsDeveloping new applications using existing data sets to create new products or improve existing servicesMaintaining existing applications by updating existing code or adding new features to meet new requirementsDesigning and implementing security measures to protect data from unauthorized access or misuseRecommending infrastructure changes to improve storage capacity or performance
Job Types: Contract, Full-time
Salary: $40.34 - $86.70 per hour
Experience level:

 10 years
 11+ years
 7 years
 8 years
 9 years

Experience:

 AWS Data Engineer: 10 years (Preferred)

Work Location: Remote","<p>AWS data engineers have a wide range of responsibilities, which can include:</p>
<p>Creating data models that can be used to extract information from various sources and store it in a usable format<br>Maintaining the integrity of data by designing backup and recovery procedures<br>Identifying opportunities to improve performance by improving database structure or indexing methods<br>Conducting research to identify new technologies that can be applied to current projects<br>Analyzing data to find patterns or insights that can be used to develop strategies or make business decisions<br>Developing new applications using existing data sets to create new products or improve existing services<br>Maintaining existing applications by updating existing code or adding new features to meet new requirements<br>Designing and implementing security measures to protect data from unauthorized access or misuse<br>Recommending infrastructure changes to improve storage capacity or performance</p>
<p>Job Types: Contract, Full-time</p>
<p>Salary: &#x24;40.34 - &#x24;86.70 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Experience:</p>
<ul>
 <li>AWS Data Engineer: 10 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,77f1038f4ded6818,,Full-time,Contract,,Remote,senior AWS Data Engineer,1 day ago,2023-10-17T13:31:11.925Z,,,$40.34 - $86.70 an hour,2023-10-18T13:31:11.928Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=77f1038f4ded6818&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
14,Dama Technology Inc,"About Dama Financial
   At Dama Financial, we use technology to solve problems that critically impact the growth and reputation of the cannabis industry. We offer innovative, compliant, sustainable financial and traceability products, removing the barriers that exclude cannabis businesses from accessing the fundamental solutions required to support a rapidly growing industry. We have a diverse team of professionals with deep expertise in financial services, payments technology, cannabis regulations, and successfully building and growing companies. Throughout the organization, you’ll find people who solve problems, deliver solutions, and deal with uncertainties while building best in class products for the industry.
 
  
  
 
   The Role
   The Data & Analytics team within Dama is growing to help clients scale their retail operations with a world-class point-of-sale system and flexible payment options. The team is also responsible for helping internal teams become more data-driven by helping them to understand, utilize and extract value from the data generated by our systems and available in our warehouse.
 
  
  
  Although we’re a growth-stage company, our environment is typical of a start-up 
 
   We work in small, high-performing teams, are fast-paced, and we all get a lot done by everyone wearing many hats.
  
 
   We are serious about optimizing our time and staying focused on the most important goals and outcomes.
  
 
   We are a 100% remote team meaning we focus on communication to ensure we can stay in sync despite our physical distance.
 
  
  
 
   What you'll do
  
 
   Report to Director of Data & Analytics
  
 
   Build and maintain pipelines to move data from source systems into our cloud data warehouse
  
 
   Transform and model data using SQL & Python
  
 
   Monitor data pipelines for errors/data quality issues and work with Dev Ops Engineering to improve observability and alerting
  
 
   Improve data quality, reliability, efficiency, and performance while optimizing cost
  
 
   Document data models, schemas, business logic, pipelines, and other metadata
  
 
   Collaborate with engineers and product managers to understand data requirements
 
  
  
 
   What we’re looking for
  
 
   You’re a great creative problem solver with an analytical mind who loves to dig in and solve hard problems
  
 
   You see data flowing and you can’t stop yourself from classifying, categorizing, organizing and directing those flows to create efficient/performant, useful and usable datasets for both operations and decision support
  
 
   You’re not just a “data person”, you’re an Engineer who specializes in data and metadata.
  
 
   You love learning new things and have a passion for building, monitoring and improving a well-oiled “data machine”
  
 
   You have the ability to work both independently and collaboratively as part of a remote team
 
  
  
  Required Experience 
 
   1-3 years of experience in a Data Engineer role working with the “Modern Data Stack”
  
 
   SQL. You know SQL. SQL is a friend of yours. You two probably share a secret handshake
  
 
   Ideal candidate will have experience with both BigQuery & dbt (including Python)
 
  
  
  Nice-to-have Experience 
 
   Integration tools (Airbyte, Stitch, Fivetran)
  
 
   Orchestration tools (Dagster, Airflow, dbt Cloud)
  
 
   Metadata tools (DataHub, OpenMetadata)
  
 
   BI/Dashboard tools (Looker, Superset, PowerBI)
 
  
  
 
   Benefits
  
 
   Healthcare
  
 
   401K
  
 
   Generous PTO
  
 
   Collaborative Environment
 
  
  
 
   What we offer
  
 
   A low ego environment where you can give and receive direct feedback.
  
 
   Managers who care about your career development.
 
  
  
 
   Due to the nature of financial systems, you will be required to pass a background check.
 
  
  
 
   Send resumes to 
  jobs@damafinancial.com
 
  
  
  CHR: Jr./Mid-Level Data Engineer
  
  
 
   LI: Jr./Mid Level Data Engineer
 
  
  
  Salary commensurate upon experience
  
  
  Bonus goals based on company goals","<div>
 <div>
  <b>About Dama Financial</b>
  <br> At Dama Financial, we use technology to solve problems that critically impact the growth and reputation of the cannabis industry. We offer innovative, compliant, sustainable financial and traceability products, removing the barriers that exclude cannabis businesses from accessing the fundamental solutions required to support a rapidly growing industry. We have a diverse team of professionals with deep expertise in financial services, payments technology, cannabis regulations, and successfully building and growing companies. Throughout the organization, you&#x2019;ll find people who solve problems, deliver solutions, and deal with uncertainties while building best in class products for the industry.
 </div>
 <br> 
 <div></div> 
 <div>
  <b> The Role</b>
  <br> The Data &amp; Analytics team within Dama is growing to help clients scale their retail operations with a world-class point-of-sale system and flexible payment options. The team is also responsible for helping internal teams become more data-driven by helping them to understand, utilize and extract value from the data generated by our systems and available in our warehouse.
 </div>
 <br> 
 <div></div> 
 <p> Although we&#x2019;re a growth-stage company, our environment is typical of a start-up</p> 
 <ul>
  <li> We work in small, high-performing teams, are fast-paced, and we all get a lot done by everyone wearing many hats.</li>
 </ul> 
 <ul>
  <li> We are serious about optimizing our time and staying focused on the most important goals and outcomes.</li>
 </ul> 
 <ul>
  <li> We are a 100% remote team meaning we focus on communication to ensure we can stay in sync despite our physical distance.</li>
 </ul>
 <br> 
 <p></p> 
 <div>
  <b> What you&apos;ll do</b>
 </div> 
 <ul>
  <li> Report to Director of Data &amp; Analytics</li>
 </ul> 
 <ul>
  <li> Build and maintain pipelines to move data from source systems into our cloud data warehouse</li>
 </ul> 
 <ul>
  <li> Transform and model data using SQL &amp; Python</li>
 </ul> 
 <ul>
  <li> Monitor data pipelines for errors/data quality issues and work with Dev Ops Engineering to improve observability and alerting</li>
 </ul> 
 <ul>
  <li> Improve data quality, reliability, efficiency, and performance while optimizing cost</li>
 </ul> 
 <ul>
  <li> Document data models, schemas, business logic, pipelines, and other metadata</li>
 </ul> 
 <ul>
  <li> Collaborate with engineers and product managers to understand data requirements</li>
 </ul>
 <br> 
 <p></p> 
 <div>
  <b> What we&#x2019;re looking for</b>
 </div> 
 <ul>
  <li> You&#x2019;re a great creative problem solver with an analytical mind who loves to dig in and solve hard problems</li>
 </ul> 
 <ul>
  <li> You see data flowing and you can&#x2019;t stop yourself from classifying, categorizing, organizing and directing those flows to create efficient/performant, useful and usable datasets for both operations and decision support</li>
 </ul> 
 <ul>
  <li> You&#x2019;re not just a &#x201c;data person&#x201d;, you&#x2019;re an Engineer who specializes in data and metadata.</li>
 </ul> 
 <ul>
  <li> You love learning new things and have a passion for building, monitoring and improving a well-oiled &#x201c;data machine&#x201d;</li>
 </ul> 
 <ul>
  <li> You have the ability to work both independently and collaboratively as part of a remote team</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> Required Experience</b></p> 
 <ul>
  <li> 1-3 years of experience in a Data Engineer role working with the &#x201c;Modern Data Stack&#x201d;</li>
 </ul> 
 <ul>
  <li> SQL. You know SQL. SQL is a friend of yours. You two probably share a secret handshake</li>
 </ul> 
 <ul>
  <li> Ideal candidate will have experience with both BigQuery &amp; dbt (including Python)</li>
 </ul>
 <br> 
 <div></div> 
 <p><b> Nice-to-have Experience</b></p> 
 <ul>
  <li> Integration tools (Airbyte, Stitch, Fivetran)</li>
 </ul> 
 <ul>
  <li> Orchestration tools (Dagster, Airflow, dbt Cloud)</li>
 </ul> 
 <ul>
  <li> Metadata tools (DataHub, OpenMetadata)</li>
 </ul> 
 <ul>
  <li> BI/Dashboard tools (Looker, Superset, PowerBI)</li>
 </ul>
 <br> 
 <p></p> 
 <div>
  <b> Benefits</b>
 </div> 
 <ul>
  <li> Healthcare</li>
 </ul> 
 <ul>
  <li> 401K</li>
 </ul> 
 <ul>
  <li> Generous PTO</li>
 </ul> 
 <ul>
  <li> Collaborative Environment</li>
 </ul>
 <br> 
 <div></div> 
 <div>
  <b> What we offer</b>
 </div> 
 <ul>
  <li> A low ego environment where you can give and receive direct feedback.</li>
 </ul> 
 <ul>
  <li> Managers who care about your career development.</li>
 </ul>
 <br> 
 <div></div> 
 <div>
  <b> Due to the nature of financial systems, you will be required to pass a background check.</b>
 </div>
 <br> 
 <div></div> 
 <div>
  <b> Send resumes to </b>
  <b>jobs@damafinancial.com</b>
 </div>
 <br> 
 <div></div> 
 <p> <b>CHR: Jr./Mid-Level Data Engineer</b></p>
 <br> 
 <div></div> 
 <div>
  <b> LI: Jr./Mid Level Data Engineer</b>
 </div>
 <br> 
 <p></p> 
 <p> Salary commensurate upon experience</p>
 <br> 
 <p></p> 
 <p> Bonus goals based on company goals</p>
</div>",https://secure.entertimeonline.com/ta/CBIZ20462.careers?ShowJob=352679041,28162dd96d9b7a0d,,Full-time,,,"South San Francisco, CA 94083",Jr./Mid-Level Data Engineer,Today,2023-10-18T13:31:06.374Z,,,"$70,000 - $125,000 a year",2023-10-18T13:31:06.380Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=28162dd96d9b7a0d&from=jasx&tk=1hd1fm1o6iman800&vjs=3
15,Cognizant Technology Solutions,"We are Cognizant Artificial Intelligence 
  Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them. 
  With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks
  
  
  Role and Responsibilities: 
  
  5+ years of industry experience in software development data engineering or a related field with a solid track record of building services for manipulating processing datasets 
  Hands-on experience and advanced knowledge of AWS DataOps (i.e. IAM Lambda Step Functions EMR/Glue and DynamoDB) 
  Hands-on experience and advanced knowledge of SQL/Non-relational Data Modeling 
  Experience working with data streaming technologies (Kafka Spark Streaming etc.) 
 
 
  Designing and implementing complex ingestion and processing pipelines through orchestration 
  Design and implement API interfaces for engineering teams to interact with ingestion/processing pipelines 
  Design implement and support scalable multi-tenant service and data infrastructure solutions to integrate with multi heterogeneous data sources aggregate and retrieve data in a fast and secure mode curate data that can be used in reporting analysis machine learning models and ad-hoc data requests 
  Interface with other engineering and ML teams to extract transform and load data from a wide variety of data sources 
  Work with business product owners to understand gather and analyze their processing and extraction needs to solve problems
 
  
  
  Salary and Other Compensation 
  The annual salary for this position is between USD ($110kp/a – $120kp/a) depending on experience and other qualifications of the successful candidate. 
  This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans. 
  Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements: 
  
  Medical/Dental/Vision/Life Insurance 
  Paid holidays plus Paid Time Off 
  401(k) plan and contributions 
  Long-term/Short-term Disability 
  Paid Parental Leave 
  Employee Stock Purchase Plan 
  
 Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
  
  
  #LI-JL1 
  #CB 
  #IND123
 
  Employee Status : Full Time Employee
  Shift : Day Job
  Travel : No
  Job Posting : Oct 17 2023
 
 
   About Cognizant
  Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 
  Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
 
  Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
  If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.","<div>
 <p><b>We are Cognizant Artificial Intelligence</b></p> 
 <p> Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them.</p> 
 <p> With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks</p>
 <br> 
 <p></p> 
 <p><b> Role and Responsibilities:</b></p> 
 <ul> 
  <li>5+ years of industry experience in software development data engineering or a related field with a solid track record of building services for manipulating processing datasets</li> 
  <li>Hands-on experience and advanced knowledge of AWS DataOps (i.e. IAM Lambda Step Functions EMR/Glue and DynamoDB)</li> 
  <li>Hands-on experience and advanced knowledge of SQL/Non-relational Data Modeling</li> 
  <li>Experience working with data streaming technologies (Kafka Spark Streaming etc.)</li> 
 </ul>
 <ul>
  <li>Designing and implementing complex ingestion and processing pipelines through orchestration</li> 
  <li>Design and implement API interfaces for engineering teams to interact with ingestion/processing pipelines</li> 
  <li>Design implement and support scalable multi-tenant service and data infrastructure solutions to integrate with multi heterogeneous data sources aggregate and retrieve data in a fast and secure mode curate data that can be used in reporting analysis machine learning models and ad-hoc data requests</li> 
  <li>Interface with other engineering and ML teams to extract transform and load data from a wide variety of data sources</li> 
  <li>Work with business product owners to understand gather and analyze their processing and extraction needs to solve problems</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> Salary and Other Compensation</b></p> 
 <p> The annual salary for this position is between USD (&#x24;110kp/a &#x2013; &#x24;120kp/a) depending on experience and other qualifications of the successful candidate.</p> 
 <p> This position is also eligible for Cognizant&#x2019;s discretionary annual incentive program, based on performance and subject to the terms of Cognizant&#x2019;s applicable plans.</p> 
 <p> Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:</p> 
 <ul> 
  <li>Medical/Dental/Vision/Life Insurance</li> 
  <li>Paid holidays plus Paid Time Off</li> 
  <li>401(k) plan and contributions</li> 
  <li>Long-term/Short-term Disability</li> 
  <li>Paid Parental Leave</li> 
  <li>Employee Stock Purchase Plan</li> 
 </ul> 
 <p>Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.</p>
 <br> 
 <p></p> 
 <p> #LI-JL1</p> 
 <p> #CB</p> 
 <p> #IND123</p>
 <p></p>
 <p><b><br> Employee Status : </b>Full Time Employee</p>
 <p><b> Shift : </b>Day Job</p>
 <p><b> Travel : </b>No</p>
 <p><b> Job Posting : </b>Oct 17 2023</p>
 <p></p>
 <div>
  <b> About Cognizant</b>
 </div> Cognizant (Nasdaq-100: CTSH) is one of the world&apos;s leading professional services companies, transforming clients&apos; business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 <p></p>
 <p> Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.</p>
 <p></p>
 <p> Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.</p>
 <p> If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.</p>
</div>
<p></p>",https://click.appcast.io/track/hra6v3l-org?cs=hqw&jg=6mpd&ittk=LCLTYKNZ66,8c2d8bd464c9e1f8,,Full-time,,,"Chicago, IL 60290",Sr. AWS Data Engineer (Remote),Today,2023-10-18T13:31:09.464Z,3.9,15967.0,"$110,000 - $120,000 a year",2023-10-18T13:31:09.467Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=8c2d8bd464c9e1f8&from=jasx&tk=1hd1fm1o6iman800&vjs=3
19,Circle,"Circle is a financial technology company at the epicenter of the emerging internet of money, where value can finally travel like other digital data — globally, nearly instantly and less expensively than legacy settlement systems. This ground-breaking new internet layer opens up previously unimaginable possibilities for payments, commerce and markets that can help raise global economic prosperity and enhance inclusion. Our infrastructure – including USDC, a blockchain-based dollar – helps businesses, institutions and developers harness these breakthroughs and capitalize on this major turning point in the evolution of money and technology. 
   What you'll be part of: 
   Circle is committed to visibility and stability in everything we do. As we grow as an organization, we're expanding into some of the world's strongest jurisdictions. Speed and efficiency are motivators for our success and our employees live by our company values: Multistakeholder, Mindfulness, Driven by Excellence and High Integrity. Circlers are consistently evolving in a remote world where strength in numbers fuels team success. We have built a flexible and diverse work environment where new ideas are encouraged and everyone is a stakeholder.
 
  What you'll be responsible for: 
  As a member of the Data Engineering - Business ETL team, you own the ETL/ELT pipelines and data warehouse that is used for financial and regulatory reporting. Your work powers Circle business functions, including the Compliance, Finance, Accounting, and Analytics teams for experimentation, operational excellence, and actionable insights, so as to fuel and accelerate business growth. High integrity to data accuracy and quality is crucial. Your work will directly impact Circle's transparency, trust, and accountability needs. 
  What you'll work on: 
  
  Collaborating with business teams on the design, deployment and continuous improvement of the scalable data platform that ingests, stores, and aggregates various datasets, including data pipelines, platforms, and warehouses, and surfacing data to both internal and customer-facing applications. 
  Being a domain expert on data modeling, data pipelines, data quality and data warehousing. 
  Designing, building and maintaining data ETL/ELT pipelines to source and aggregate the required data for various data analyses and reporting needs, as well as to continually improve the operations, monitoring and performance of the data warehouse. 
  Developing integrations with third party systems to source, qualify and ingest various datasets. 
  Providing data analytics and visualization tools to extract valuable insights from the data to enable data-driven decisions. 
  Working closely with across groups, such as the product, engineering, data science, compliance, and security teams, for data modeling, general management of data life cycle, data governance and processes for meeting regulatory and legal requirements. 
  
 You will aspire to our four core values: 
  
  Multistakeholder - you have dedication and commitment to our customers, shareholders, employees and families and local communities. 
  Mindful - you seek to be respectful, an active listener and to pay attention to detail. 
  Driven by Excellence - you are driven by our mission and our passion for customer success which means you relentlessly pursue excellence, that you do not tolerate mediocrity and you work intensely to achieve your goals. 
  High Integrity - you seek open and honest communication, and you hold yourself to very high moral and ethical standards. You reject manipulation, dishonesty and intolerance. 
  
 What you'll bring to Circle: 
  For Senior Data Engineer (III) 
  
  4+ years of professional data engineering experience. 
  Proficient in one or more programming languages (Java, Scala, Python). 
  Advanced experience in SQL in big data warehouse systems such as Snowflake, BigQuery, Databricks, etc. 
  Experience in SQL and NoSQL, such as MySQL, PostgreSQL, Cassandra, HBase, Redis, DynamoDB, Neo4j, etc. 
  Experience with workflow orchestration management engines such as Airflow, Dagster, DBT, etc 
  Experience with Cloud Services (AWS, Google Cloud, Microsoft Azure, etc). 
  Experience in building scalable infrastructure to support batch, micro-batch or stream data processing for large volumes of data. 
  Experience with financial or compliance data, bonus if in similar business domains, such as payment systems, credit cards, bank transfers, blockchains, etc. 
  Experience in data provenance and governance. 
  Internal knowledge of open source or related big data technologies. 
  Ability to tackle complex and ambiguous problems. 
  Self-starter who takes ownership, gets results, and enjoys moving at a fast pace. 
  Excellent communication skills, able to collaborate with across remote teams, share ideas and present concepts effectively. 
  
 For Staff Data Engineer (IV) 
  All the requirements of above and: 
  
  7+ years of professional data engineering experience. 
  Led teams (>5) technically on architecture and system design. 
  Expert in one of the domains of ETL/ELT pipelines, feature engineering, data modeling and architecture, or data ingestion. 
  Experience working autonomously and able to identify large impactful projects to pursue with minimal guidance. 
  Deep understanding/experience with: 
  
   Data warehouse architecture and design 
   Integration of the data stack to other tools/services 
   Extensive knowledge with implementing data quality checks 
   
 
 Additional Information: 
  
  This position is eligible for day-one PERM sponsorship for qualified candidates. 
  
 Circle is on a mission to create an inclusive financial future, with transparency at our core. We consider a wide variety of elements when crafting our compensation ranges and total compensation packages. 
  Starting pay is determined by various factors, including but not limited to: relevant experience, skill set, qualifications, and other business and organizational needs. Please note that compensation ranges may differ for candidates in other locations. 
  Senior Data Engineer (III) 
  Base Pay Range: $147,500 - $195,000 
  Annual Bonus Target: 12.5% 
  Staff Data Engineer (IV) 
  Base Pay Range: $172,500 - $227,500 
  Annual Bonus Target: 15% 
  Also Included: Equity & Benefits (including medical, dental, vision and 401(k)). Circle has a discretionary vacation policy. We also provide 10 days of paid sick leave per year and 11 paid holidays per year in the U.S.
 
   We are an equal opportunity employer and value diversity at Circle. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Additionally, Circle participates in the E-Verify Program in certain locations, as required by law. 
   #LI-Remote","<div>
 <div>
  <p>Circle is a financial technology company at the epicenter of the emerging internet of money, where value can finally travel like other digital data &#x2014; globally, nearly instantly and less expensively than legacy settlement systems. This ground-breaking new internet layer opens up previously unimaginable possibilities for payments, commerce and markets that can help raise global economic prosperity and enhance inclusion. Our infrastructure &#x2013; including USDC, a blockchain-based dollar &#x2013; helps businesses, institutions and developers harness these breakthroughs and capitalize on this major turning point in the evolution of money and technology.</p> 
  <p><b> What you&apos;ll be part of:</b></p> 
  <p> Circle is committed to visibility and stability in everything we do. As we grow as an organization, we&apos;re expanding into some of the world&apos;s strongest jurisdictions. Speed and efficiency are motivators for our success and our employees live by our company values: Multistakeholder, Mindfulness, Driven by Excellence and High Integrity. Circlers are consistently evolving in a remote world where strength in numbers fuels team success. We have built a flexible and diverse work environment where new ideas are encouraged and everyone is a stakeholder.</p>
 </div>
 <p><b> What you&apos;ll be responsible for:</b></p> 
 <p> As a member of the Data Engineering - Business ETL team, you own the ETL/ELT pipelines and data warehouse that is used for financial and regulatory reporting. Your work powers Circle business functions, including the Compliance, Finance, Accounting, and Analytics teams for experimentation, operational excellence, and actionable insights, so as to fuel and accelerate business growth. High integrity to data accuracy and quality is crucial. Your work will directly impact Circle&apos;s transparency, trust, and accountability needs.</p> 
 <p><b> What you&apos;ll work on:</b></p> 
 <ul> 
  <li>Collaborating with business teams on the design, deployment and continuous improvement of the scalable data platform that ingests, stores, and aggregates various datasets, including data pipelines, platforms, and warehouses, and surfacing data to both internal and customer-facing applications.</li> 
  <li>Being a domain expert on data modeling, data pipelines, data quality and data warehousing.</li> 
  <li>Designing, building and maintaining data ETL/ELT pipelines to source and aggregate the required data for various data analyses and reporting needs, as well as to continually improve the operations, monitoring and performance of the data warehouse.</li> 
  <li>Developing integrations with third party systems to source, qualify and ingest various datasets.</li> 
  <li>Providing data analytics and visualization tools to extract valuable insights from the data to enable data-driven decisions.</li> 
  <li>Working closely with across groups, such as the product, engineering, data science, compliance, and security teams, for data modeling, general management of data life cycle, data governance and processes for meeting regulatory and legal requirements.</li> 
 </ul> 
 <p><b>You will aspire to our four core values:</b></p> 
 <ul> 
  <li>Multistakeholder - you have dedication and commitment to our customers, shareholders, employees and families and local communities.</li> 
  <li>Mindful - you seek to be respectful, an active listener and to pay attention to detail.</li> 
  <li>Driven by Excellence - you are driven by our mission and our passion for customer success which means you relentlessly pursue excellence, that you do not tolerate mediocrity and you work intensely to achieve your goals.</li> 
  <li>High Integrity - you seek open and honest communication, and you hold yourself to very high moral and ethical standards. You reject manipulation, dishonesty and intolerance.</li> 
 </ul> 
 <p><b>What you&apos;ll bring to Circle:</b></p> 
 <p> For Senior Data Engineer (III)</p> 
 <ul> 
  <li>4+ years of professional data engineering experience.</li> 
  <li>Proficient in one or more programming languages (Java, Scala, Python).</li> 
  <li>Advanced experience in SQL in big data warehouse systems such as Snowflake, BigQuery, Databricks, etc.</li> 
  <li>Experience in SQL and NoSQL, such as MySQL, PostgreSQL, Cassandra, HBase, Redis, DynamoDB, Neo4j, etc.</li> 
  <li>Experience with workflow orchestration management engines such as Airflow, Dagster, DBT, etc</li> 
  <li>Experience with Cloud Services (AWS, Google Cloud, Microsoft Azure, etc).</li> 
  <li>Experience in building scalable infrastructure to support batch, micro-batch or stream data processing for large volumes of data.</li> 
  <li>Experience with financial or compliance data, bonus if in similar business domains, such as payment systems, credit cards, bank transfers, blockchains, etc.</li> 
  <li>Experience in data provenance and governance.</li> 
  <li>Internal knowledge of open source or related big data technologies.</li> 
  <li>Ability to tackle complex and ambiguous problems.</li> 
  <li>Self-starter who takes ownership, gets results, and enjoys moving at a fast pace.</li> 
  <li>Excellent communication skills, able to collaborate with across remote teams, share ideas and present concepts effectively.</li> 
 </ul> 
 <p>For Staff Data Engineer (IV)</p> 
 <p> All the requirements of above and:</p> 
 <ul> 
  <li>7+ years of professional data engineering experience.</li> 
  <li>Led teams (&gt;5) technically on architecture and system design.</li> 
  <li>Expert in one of the domains of ETL/ELT pipelines, feature engineering, data modeling and architecture, or data ingestion.</li> 
  <li>Experience working autonomously and able to identify large impactful projects to pursue with minimal guidance.</li> 
  <li>Deep understanding/experience with:</li> 
  <ul>
   <li>Data warehouse architecture and design</li> 
   <li>Integration of the data stack to other tools/services</li> 
   <li>Extensive knowledge with implementing data quality checks</li> 
  </ul> 
 </ul>
 <p><b>Additional Information:</b></p> 
 <ul> 
  <li>This position is eligible for day-one PERM sponsorship for qualified candidates.</li> 
 </ul> 
 <p><i>Circle is on a mission to create an inclusive financial future, with transparency at our core. We consider a wide variety of elements when crafting our compensation ranges and total compensation packages.</i></p> 
 <p><i> Starting pay is determined by various factors, including but not limited to: relevant experience, skill set, qualifications, and other business and organizational needs. Please note that compensation ranges may differ for candidates in other locations.</i></p> 
 <p><b><i> Senior Data Engineer (III)</i></b></p> 
 <p><i> Base Pay Range: &#x24;147,500 - &#x24;195,000</i></p> 
 <p><i> Annual Bonus Target: 12.5%</i></p> 
 <p><b><i> Staff Data Engineer (IV)</i></b></p> 
 <p><i> Base Pay Range: &#x24;172,500 - &#x24;227,500</i></p> 
 <p><i> Annual Bonus Target: 15%</i></p> 
 <p><i> Also Included: Equity &amp; Benefits (including medical, dental, vision and 401(k)). Circle has a discretionary vacation policy. We also provide 10 days of paid sick leave per year and 11 paid holidays per year in the U.S.</i></p>
 <div>
  <p> We are an <b>equal opportunity employer</b> and value diversity at Circle. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Additionally, Circle participates in the <b>E-Verify Program</b> in certain locations, as required by law.</p> 
  <h6 class=""jobSectionHeader""> #LI-Remote</h6>
 </div>
</div>",https://boards.greenhouse.io/circle/jobs/6976295002?gh_src=cd1087ea2us,c41c14c5c2c43e9e,,,,,"San Francisco, CA",Senior Data Engineer,Today,2023-10-18T13:31:26.049Z,,,"$147,500 - $195,000 a year",2023-10-18T13:31:26.050Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=c41c14c5c2c43e9e&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
20,INADEV,"******CANDIDATES MUST ANSWER ALL SCREENING QUESTIONS FOR APPLICATIONS TO BE CONSIDERED******
Formed in 2011, INADEV is focused on its founding principle to build innovative customer-centric solutions incredibly fast, secure, and at scale. We deliver world-class digital experiences to some of the largest federal agencies and commercial companies. Our technical expertise and innovations are comprised of codeless automation, identity intelligence, immersive technology, artificial intelligence/machine learning (AI/ML), virtualization, and digital transformation.
POSITION DESCRIPTION:

 Prepare data migration strategy, Integration plans, DB Performance, business sign-off plan, Live Confidence Plan (LCT plan - all data is migrated successfully).
 Provide support for data migration, data engineering, and integration of existing systems.
 Developing and integrating multiple data types across a range of data sets and sources.
 Performing day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner.
 Evaluate current system designs and identify areas for improvement to create a system that is highly available and has low data latency.
 Plan and design the integration of various source systems and the migration of data between systems.
 Build and implement the source system integration and data migration plan.
 Developing, managing, manipulating, storing and parsing data across a data pipeline for variety of target sources and data consumers
 Writing code to ensure the performance and reliability of data extraction and processing
 Supporting continuous process automation for data ingestion
 Assisting with the maintenance of applications and tools that reside on the data driven systems (upgrades, patches, configuration changes, etc.)
 Working with program management and engineers to implement and document complex and evolving requirements
 Actively and collaboratively participating as a member of a cross-functional Agile/Scrum team while following all Agile/Scrum best practices
 Advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
 Helping cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
 Demonstrating significant technical competence and ownership to broad audiences while driving progress on company strategic objectives at multiple levels.
 Generating and articulating technical strategy to diverse audiences, both technical and non-technical.

NON-TECHNICAL REQUIREMENTS:

 Ability to pass a 7 year background check and have the ability to obtain a U.S. Government clearance.
 Must have been a resident of the continental U.S. for at least the last 3 years.
 Must be willing to work in Eastern Standard Business hours.
 Must possess good communication (written/verbal) skills.

MANDATORY REQUIREMENTS:

 Must have a Bachelor's Degree in a technical discipline and 10+ years pertinent experience with the design, management, and solutioning of large, complex data sets and models.
 Must have at least 2+ years of experience working with/in public cloud environments.
 Must have proven experience leading data migration project from on-prem to cloud.
 Must have extensive experience/knowledge of data integration.
 Must have experience with Cloud Native databases
 Experience with Enterprise Architecture & Distributed Systems

DESIRED SKILLS:

 Experience with AWS DMS/MGN
 Technical knowledge of Datalake/DeltaLake/Lakehouse

PHYSICAL DEMANDS:

 Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions

INADEV Corporation does not discriminate against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibits discrimination against all individuals based on their race, color, religion, sex, sexual orientation/gender identity, or national origin.
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:

 401(k) matching
 Dental insurance
 Flexible spending account
 Health insurance
 Referral program
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 10 years
 7 years
 8 years
 9 years

Schedule:

 8 hour shift
 Monday to Friday

Application Question(s):

 This position requires the ability to obtain/maintain a U.S. Government Clearance and pass a 7 year employment background check. Do you meet these mandatory requirements?
 Are you open to working on a Permanent/W2 basis?
 This position requires that all candidates being considered MUST have resided in the Continental United States for at least the last 3 years and be able/willing to work on East Standard Business hours. Do you meet these mandatory requirements?
 Do you have a Bachelor's Degree in a Technical Discipline (i.e. Computer Science, Computer Engineering, Information Technology, Information Systems)?
 Do you have at least 7 years of professional experience as a Data Engineer?
 Do have professional experience leading data migration projects from on-prem to cloud?
 Do you have at least 7 years of professional experience in Data Manipulation working specifically with Python?
 Do you have at least 7 years of professional ETL experience?

Work Location: Remote","<p><b>******CANDIDATES MUST ANSWER ALL SCREENING QUESTIONS FOR APPLICATIONS TO BE CONSIDERED******</b></p>
<p>Formed in 2011, INADEV is focused on its founding principle to build innovative customer-centric solutions incredibly fast, secure, and at scale. We deliver world-class digital experiences to some of the largest federal agencies and commercial companies. Our technical expertise and innovations are comprised of codeless automation, identity intelligence, immersive technology, artificial intelligence/machine learning (AI/ML), virtualization, and digital transformation.</p>
<p><b>POSITION DESCRIPTION:</b></p>
<ul>
 <li>Prepare data migration strategy, Integration plans, DB Performance, business sign-off plan, Live Confidence Plan (LCT plan - all data is migrated successfully).</li>
 <li>Provide support for data migration, data engineering, and integration of existing systems.</li>
 <li>Developing and integrating multiple data types across a range of data sets and sources.</li>
 <li>Performing day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner.</li>
 <li>Evaluate current system designs and identify areas for improvement to create a system that is highly available and has low data latency.</li>
 <li>Plan and design the integration of various source systems and the migration of data between systems.</li>
 <li>Build and implement the source system integration and data migration plan.</li>
 <li>Developing, managing, manipulating, storing and parsing data across a data pipeline for variety of target sources and data consumers</li>
 <li>Writing code to ensure the performance and reliability of data extraction and processing</li>
 <li>Supporting continuous process automation for data ingestion</li>
 <li>Assisting with the maintenance of applications and tools that reside on the data driven systems (upgrades, patches, configuration changes, etc.)</li>
 <li>Working with program management and engineers to implement and document complex and evolving requirements</li>
 <li>Actively and collaboratively participating as a member of a cross-functional Agile/Scrum team while following all Agile/Scrum best practices</li>
 <li>Advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing</li>
 <li>Helping cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork</li>
 <li>Demonstrating significant technical competence and ownership to broad audiences while driving progress on company strategic objectives at multiple levels.</li>
 <li>Generating and articulating technical strategy to diverse audiences, both technical and non-technical.</li>
</ul>
<p><b>NON-TECHNICAL REQUIREMENTS:</b></p>
<ul>
 <li>Ability to pass a 7 year background check and have the ability to obtain a U.S. Government clearance.</li>
 <li>Must have been a resident of the continental U.S. for at least the last 3 years.</li>
 <li>Must be willing to work in Eastern Standard Business hours.</li>
 <li>Must possess good communication (written/verbal) skills.</li>
</ul>
<p><b>MANDATORY REQUIREMENTS:</b></p>
<ul>
 <li>Must have a Bachelor&apos;s Degree in a technical discipline and 10+ years pertinent experience with the design, management, and solutioning of large, complex data sets and models.</li>
 <li>Must have at least 2+ years of experience working with/in public cloud environments.</li>
 <li>Must have proven experience leading data migration project from on-prem to cloud.</li>
 <li>Must have extensive experience/knowledge of data integration.</li>
 <li>Must have experience with Cloud Native databases</li>
 <li>Experience with Enterprise Architecture &amp; Distributed Systems</li>
</ul>
<p><b>DESIRED SKILLS:</b></p>
<ul>
 <li>Experience with AWS DMS/MGN</li>
 <li>Technical knowledge of Datalake/DeltaLake/Lakehouse</li>
</ul>
<p><b>PHYSICAL DEMANDS:</b></p>
<ul>
 <li>Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions</li>
</ul>
<p>INADEV Corporation does not discriminate against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibits discrimination against all individuals based on their race, color, religion, sex, sexual orientation/gender identity, or national origin.</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;125,000.00 - &#x24;150,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Referral program</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>This position requires the ability to obtain/maintain a U.S. Government Clearance and pass a 7 year employment background check. Do you meet these mandatory requirements?</li>
 <li>Are you open to working on a Permanent/W2 basis?</li>
 <li>This position requires that all candidates being considered MUST have resided in the Continental United States for at least the last 3 years and be able/willing to work on East Standard Business hours. Do you meet these mandatory requirements?</li>
 <li>Do you have a Bachelor&apos;s Degree in a Technical Discipline (i.e. Computer Science, Computer Engineering, Information Technology, Information Systems)?</li>
 <li>Do you have at least 7 years of professional experience as a Data Engineer?</li>
 <li>Do have professional experience leading data migration projects from on-prem to cloud?</li>
 <li>Do you have at least 7 years of professional experience in Data Manipulation working specifically with Python?</li>
 <li>Do you have at least 7 years of professional ETL experience?</li>
</ul>
<p>Work Location: Remote</p>",,fdd1d18770845207,,Full-time,,,Remote,Sr. Data Engineer,Today,2023-10-18T13:31:36.878Z,3.0,6.0,"$125,000 - $150,000 a year",2023-10-18T13:31:36.881Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=fdd1d18770845207&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
23,Nascent,"About Nascent… 
 Nascent is a team of builders who back early-stage web3 founders creating products and primitives for an open financial world. Founded in 2020, we've invested in 50+ early-stage teams (https://www.nascent.xyz/portfolio) that we believe have the potential to create substantive change, expand boundaries and find new horizons. Building from a base of permanent capital, we also deploy a sizable liquid portfolio utilizing a range of strategies that ensure we are among the most active users of the open financial system we are helping to build. The fluid structure that enables our team to build, use, and invest in the future of crypto makes Nascent both an ideal early-stage partner and long-term ally. 
 The Opportunity 
 As a Data Engineer at Nascent, you'll play a critical role in building and maintaining our data infrastructure and pipelines across our liquid and investing activities. This includes designing and building ETLs, optimizing research and transaction infrastructure, and implementing APIs for data querying and access. Reporting directly to the Data Engineering Lead you'll support the strategy and execution of data acquisition to enable internal research and externally facing data projects. You'll serve as a key partner across the organization, delivering internal data projects such as dashboarding and visualizations to improve investing performance. 
 The ideal candidate will have experience in designing and building data pipelines and APIs, with a strong foundation in data modeling, database design, and query optimization. Think data engineer with a software engineering lens, you can design and build software beyond data pipelines (you know the difference). You'll have a passion for working with large datasets and a desire to leverage data to drive business insights and support decision-making. As a fast-moving crypto-native firm, we have a lot of activities and both ingest and create a lot of data—working closely with our Data Engineering Lead you will execute high impact work across data-related activities for the firm. This is a full time fully remote position with preference for EST working hours. 
 About you 
 
  You thrive in less structured environments and are at your best when driving and delivering results with the freedom to build and execute your own plan.
   You are as excited by starting a project as completing, maintaining, and continually optimizing it.
   You find the plethora of opportunities to leverage data to drive value for the firm exciting versus overwhelming
  
 Required Experience 
 
  Building and managing data pipelines on bare metal outside of pure cloud infrastructure (AWS, Azure, GCP, etc.)
   Hands-on experience from beginning to end of a shipped working product (i.e., you're an experienced builder)
   Substantive experience building and maintaining enterprise data systems, ETL frameworks, & pipelines
   Excellent Python and SQL skills
   Familiarity with APIs (REST, Websockets, etc)
   Experience beyond testing (e.g. quality processes, verification & validation)
   Experience in configuring and managing cloud infrastructure (preferred AWS)
  
 Our Team & Culture 
 At Nascent, we are an interdisciplinary team of investors, builders & creators, capable of achieving more together than we can as individuals. We offer the opportunity to contribute to building the future global economic system with a world-class team and culture that pairs the freedom to explore, experiment & play with a competitive drive to win. We invest in our people by providing the autonomy to build, coupled with accountability & honest feedback to help learn, grow, perform & win. We're a fully distributed team that understands the value of in-person time—we host two team retreats per year and encourage team members to come together for more frequent in-person work. 
 Principles that drive our team & work 
 
  Build for the long term
   Align incentives
   Be nimble
   Compete to win
   Explore, experiment, play
   Always be building
   Give and embrace real feedback
  
 What We Offer 
 At Nascent, we offer a competitive total compensation package heavily weighted toward bonus, ensuring that when we perform at our best and the firm wins we all win. 
 
  The opportunity to learn, experiment and build in an entrepreneurial environment
   Remote and distributed working environment
   Comprehensive health benefits package including dental, vision, and life
   Generous paid parental leave & supported return to work
   Home Office, coworking space and wellness stipend
   Retirement plan matching contributions
   Open vacation policy as well as flexible work hours and location
   Access to our internal performance coaching, technical experts and support for continuing your skill development and growth
   Team activities and bi-annual in-person team retreats
  
 We are an equal opportunity employer and celebrate diversity and differences of perspectives. We do not discriminate on the basis of any status, inclusive of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","<div>
 <p><b><i>About Nascent&#x2026;</i></b></p> 
 <p>Nascent is a team of builders who back early-stage web3 founders creating products and primitives for an open financial world. Founded in 2020, we&apos;ve invested in 50+ early-stage teams (https://www.nascent.xyz/portfolio) that we believe have the potential to create substantive change, expand boundaries and find new horizons. Building from a base of permanent capital, we also deploy a sizable liquid portfolio utilizing a range of strategies that ensure we are among the most active users of the open financial system we are helping to build. The fluid structure that enables our team to build, use, and invest in the future of crypto makes Nascent both an ideal early-stage partner and long-term ally.</p> 
 <p><b><i>The</i></b> <b><i>Opportunity</i></b></p> 
 <p>As a Data Engineer at Nascent, you&apos;ll play a critical role in building and maintaining our data infrastructure and pipelines across our liquid and investing activities. This includes designing and building ETLs, optimizing research and transaction infrastructure, and implementing APIs for data querying and access. Reporting directly to the Data Engineering Lead you&apos;ll support the strategy and execution of data acquisition to enable internal research and externally facing data projects. You&apos;ll serve as a key partner across the organization, delivering internal data projects such as dashboarding and visualizations to improve investing performance. </p>
 <p>The ideal candidate will have experience in designing and building data pipelines and APIs, with a strong foundation in data modeling, database design, and query optimization. Think data engineer with a software engineering lens, you can design and build software beyond data pipelines (you know the difference). You&apos;ll have a passion for working with large datasets and a desire to leverage data to drive business insights and support decision-making. As a fast-moving crypto-native firm, we have a lot of activities and both ingest and create a lot of data&#x2014;working closely with our Data Engineering Lead you will execute high impact work across data-related activities for the firm. This is a full time fully remote position with preference for EST working hours. </p>
 <p><b><i>About you</i></b></p> 
 <ul>
  <li>You thrive in less structured environments and are at your best when driving and delivering results with the freedom to build and execute your own plan.</li>
  <li> You are as excited by starting a project as completing, maintaining, and continually optimizing it.</li>
  <li> You find the plethora of opportunities to leverage data to drive value for the firm exciting versus overwhelming</li>
 </ul> 
 <p><b><i>Required Experience</i></b></p> 
 <ul>
  <li>Building and managing data pipelines on bare metal outside of pure cloud infrastructure (AWS, Azure, GCP, etc.)</li>
  <li> Hands-on experience from beginning to end of a shipped working product (i.e., you&apos;re an experienced builder)</li>
  <li> Substantive experience building and maintaining enterprise data systems, ETL frameworks, &amp; pipelines</li>
  <li> Excellent Python and SQL skills</li>
  <li> Familiarity with APIs (REST, Websockets, etc)</li>
  <li> Experience beyond testing (e.g. quality processes, verification &amp; validation)</li>
  <li> Experience in configuring and managing cloud infrastructure (preferred AWS)</li>
 </ul> 
 <p><b><i>Our Team &amp; Culture</i></b></p> 
 <p>At Nascent, we are an interdisciplinary team of investors, builders &amp; creators, capable of achieving more together than we can as individuals. We offer the opportunity to contribute to building the future global economic system with a world-class team and culture that pairs the freedom to explore, experiment &amp; play with a competitive drive to win. We invest in our people by providing the autonomy to build, coupled with accountability &amp; honest feedback to help learn, grow, perform &amp; win. We&apos;re a fully distributed team that understands the value of in-person time&#x2014;we host two team retreats per year and encourage team members to come together for more frequent in-person work.</p> 
 <p><b><i>Principles that drive our team &amp; work</i></b></p> 
 <ul>
  <li>Build for the long term</li>
  <li> Align incentives</li>
  <li> Be nimble</li>
  <li> Compete to win</li>
  <li> Explore, experiment, play</li>
  <li> Always be building</li>
  <li> Give and embrace real feedback</li>
 </ul> 
 <p><b><i>What We Offer</i></b></p> 
 <p>At Nascent, we offer a competitive total compensation package heavily weighted toward bonus, ensuring that when we perform at our best and the firm wins we all win.</p> 
 <ul>
  <li>The opportunity to learn, experiment and build in an entrepreneurial environment</li>
  <li> Remote and distributed working environment</li>
  <li> Comprehensive health benefits package including dental, vision, and life</li>
  <li> Generous paid parental leave &amp; supported return to work</li>
  <li> Home Office, coworking space and wellness stipend</li>
  <li> Retirement plan matching contributions</li>
  <li> Open vacation policy as well as flexible work hours and location</li>
  <li> Access to our internal performance coaching, technical experts and support for continuing your skill development and growth</li>
  <li> Team activities and bi-annual in-person team retreats</li>
 </ul> 
 <p><i>We are an equal opportunity employer and celebrate diversity and differences of perspectives. We do not discriminate on the basis of any status, inclusive of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</i></p>
</div>
<p></p>",https://nascent-xyz.breezy.hr/p/cfd0b01e7274-data-engineer?source=indeed&ittk=5TY4HQRALW,713aaea093a693fd,,Full-time,,,Remote,Data Engineer,1 day ago,2023-10-17T13:31:24.498Z,4.3,4.0,"$130,000 - $160,000 a year",2023-10-18T13:31:24.501Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=713aaea093a693fd&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
24,OpenEarth Foundation,"Lead Data Engineer:
Building Climate Solutions for Cities
Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg
Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.
Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.
We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.
We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.
We have funding and a team of experts focused on Earth systems and digital innovation.
Your mission, should you choose to accept it:
As a lead data engineer, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.
As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.
You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.
The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We're working on the planet's problems and we need the planet's best people to fix them.
The following requirements describe our ideal candidate. If you don't meet some of the requirements, you're encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.
Essential Functions and Specific Duties:

 Design, architect, build and maintain data pipeline systems
 Write code for importing and updating large datasets to relational database and search indexes
 Define and maintain database schemas and data file formats
 Collaborate with web developers on optimizing database schemas for APIs and Web applications
 Collaborate with a team of software engineering peers
 Mentor and guide more junior data engineering staff
 Define and maintain data management processes for the organization
 Work with product managers to develop schedules, estimate tasks, and define success criteria
 Collaborate with team members from other disciplines such as web development, design, product management, and devops
 Coordinate with Open Source contributors
 Coordinate with open standards community to define interoperability standards
 Actively participate in team building and culture development activities at Open Earth Foundation
 Other duties as assigned

Required skills:

 Python programming focused on big data management
 PostgreSQL or other relational database
 Docker
 Kubernetes
 Git

Optional skills that will make a candidate stand out:

 Generative AI and large language model (LLM) APIs and data applications
 GIS tools such as ESRI
 Amazon Web Services
 ElasticSearch
 Data pipeline tools, e.g. Pachyderm
 Experience with 100Gb or larger data sets
 Climate action data such as emissions, targets, and action plans
 Physical (lat, lon, alt) and political (city, state, country) geographical data
 Remote-sensing and satellite data
 RESTful Web APIs
 Engineering leadership
 Open Source project maintainership
 Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis

Qualifications:

 Bachelor’s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience
 5 years of experience in software development for data systems
 3 shipped projects

Interpersonal skills:

 Clear communicator with good verbal and written skills in English (additional languages a plus)
 Creative, flexible and efficient with a focus on details
 Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.
 Ability to embrace new challenges, take ownership and initiative as a key team player.

Compensation and benefits

 This position is full-time with compensation of $60,000-$105,000 /year, dependent on experience and location
 Open Earth offers unlimited paid time off, paid holidays and paid sick leave
 You will work remotely within a dynamic and international environment
 We celebrate our achievements during our annual team retreat

OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.
Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.
We look forward to hearing from you!
Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don’t believe you meet all the requirements, please apply and note what’s missing in your cover letter.Lead Data Engineer:
Building Climate Solutions for Cities
Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg
Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.
Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.
We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.
We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.
We have funding and a team of experts focused on Earth systems and digital innovation.
Your mission, should you choose to accept it:
As a lead data engineer, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.
As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.
You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.
The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We're working on the planet's problems and we need the planet's best people to fix them.
The following requirements describe our ideal candidate. If you don't meet some of the requirements, you're encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.
Essential Functions and Specific Duties:

 Design, architect, build and maintain data pipeline systems
 Write code for importing and updating large datasets to relational database and search indexes
 Define and maintain database schemas and data file formats
 Collaborate with web developers on optimizing database schemas for APIs and Web applications
 Collaborate with a team of software engineering peers
 Mentor and guide more junior data engineering staff
 Define and maintain data management processes for the organization
 Work with product managers to develop schedules, estimate tasks, and define success criteria
 Collaborate with team members from other disciplines such as web development, design, product management, and devops
 Coordinate with Open Source contributors
 Coordinate with open standards community to define interoperability standards
 Actively participate in team building and culture development activities at Open Earth Foundation
 Other duties as assigned

Required skills:

 Python programming focused on big data management
 PostgreSQL or other relational database
 Docker
 Kubernetes
 Git

Optional skills that will make a candidate stand out:

 Generative AI and large language model (LLM) APIs and data applications
 GIS tools such as ESRI
 Amazon Web Services
 ElasticSearch
 Data pipeline tools, e.g. Pachyderm
 Experience with 100Gb or larger data sets
 Climate action data such as emissions, targets, and action plans
 Physical (lat, lon, alt) and political (city, state, country) geographical data
 Remote-sensing and satellite data
 RESTful Web APIs
 Engineering leadership
 Open Source project maintainership
 Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis

Qualifications:

 Bachelor’s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience
 5 years of experience in software development for data systems
 3 shipped projects

Interpersonal skills:

 Clear communicator with good verbal and written skills in English (additional languages a plus)
 Creative, flexible and efficient with a focus on details
 Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.
 Ability to embrace new challenges, take ownership and initiative as a key team player.

Compensation and benefits

 This position is full-time with compensation of $60,000-$105,000 /year, dependent on experience and location
 Open Earth offers unlimited paid time off, paid holidays and paid sick leave
 You will work remotely within a dynamic and international environment
 We celebrate our achievements during our annual team retreat

OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.
Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.
We look forward to hearing from you!
Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don’t believe you meet all the requirements, please apply and note what’s missing in your cover letter.
Job Type: Full-time
Pay: $60,000.00 - $105,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Flexible schedule
 Health insurance
 Paid time off
 Vision insurance

Compensation package:

 Bonus opportunities
 Yearly pay

Experience level:

 3 years
 4 years
 5 years
 6 years
 7 years
 8 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p><b>Lead Data Engineer:</b></p>
<p><b>Building Climate Solutions for Cities</b></p>
<p>Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg</p>
<p><i><b>Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.</b></i></p>
<p><i>Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.</i></p>
<p><i>We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.</i></p>
<p><i>We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.</i></p>
<p><i>We have funding and a team of experts focused on Earth systems and digital innovation.</i></p>
<p><i><b>Your mission, should you choose to accept it:</b></i></p>
<p>As a <b>lead data engineer</b>, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.</p>
<p>As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.</p>
<p>You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.</p>
<p>The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We&apos;re working on the planet&apos;s problems and we need the planet&apos;s best people to fix them.</p>
<p><i>The following requirements describe our ideal candidate. If you don&apos;t meet some of the requirements, you&apos;re encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.</i></p>
<p><b>Essential Functions and Specific Duties:</b></p>
<ul>
 <li>Design, architect, build and maintain data pipeline systems</li>
 <li>Write code for importing and updating large datasets to relational database and search indexes</li>
 <li>Define and maintain database schemas and data file formats</li>
 <li>Collaborate with web developers on optimizing database schemas for APIs and Web applications</li>
 <li>Collaborate with a team of software engineering peers</li>
 <li>Mentor and guide more junior data engineering staff</li>
 <li>Define and maintain data management processes for the organization</li>
 <li>Work with product managers to develop schedules, estimate tasks, and define success criteria</li>
 <li>Collaborate with team members from other disciplines such as web development, design, product management, and devops</li>
 <li>Coordinate with Open Source contributors</li>
 <li>Coordinate with open standards community to define interoperability standards</li>
 <li>Actively participate in team building and culture development activities at Open Earth Foundation</li>
 <li>Other duties as assigned</li>
</ul>
<p><b>Required skills:</b></p>
<ul>
 <li>Python programming focused on big data management</li>
 <li>PostgreSQL or other relational database</li>
 <li>Docker</li>
 <li>Kubernetes</li>
 <li>Git</li>
</ul>
<p><b>Optional skills that will make a candidate stand out:</b></p>
<ul>
 <li>Generative AI and large language model (LLM) APIs and data applications</li>
 <li>GIS tools such as ESRI</li>
 <li>Amazon Web Services</li>
 <li>ElasticSearch</li>
 <li>Data pipeline tools, e.g. Pachyderm</li>
 <li>Experience with 100Gb or larger data sets</li>
 <li>Climate action data such as emissions, targets, and action plans</li>
 <li>Physical (lat, lon, alt) and political (city, state, country) geographical data</li>
 <li>Remote-sensing and satellite data</li>
 <li>RESTful Web APIs</li>
 <li>Engineering leadership</li>
 <li>Open Source project maintainership</li>
 <li>Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience</li>
 <li>5 years of experience in software development for data systems</li>
 <li>3 shipped projects</li>
</ul>
<p><b>Interpersonal skills:</b></p>
<ul>
 <li>Clear communicator with good verbal and written skills in English (additional languages a plus)</li>
 <li>Creative, flexible and efficient with a focus on details</li>
 <li>Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.</li>
 <li>Ability to embrace new challenges, take ownership and initiative as a key team player.</li>
</ul>
<p><i><b>Compensation and benefits</b></i></p>
<ul>
 <li>This position is full-time with compensation of &#x24;60,000-&#x24;105,000 /year, dependent on experience and location</li>
 <li>Open Earth offers unlimited paid time off, paid holidays and paid sick leave</li>
 <li>You will work remotely within a dynamic and international environment</li>
 <li>We celebrate our achievements during our annual team retreat</li>
</ul>
<p><i>OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.</i></p>
<p><i>Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.</i></p>
<p><i>We look forward to hearing from you!</i></p>
<p><i>Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don&#x2019;t believe you meet all the requirements, please apply and note what&#x2019;s missing in your cover letter.</i><b>Lead Data Engineer:</b></p>
<p><b>Building Climate Solutions for Cities</b></p>
<p>Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg</p>
<p><i><b>Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.</b></i></p>
<p><i>Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.</i></p>
<p><i>We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.</i></p>
<p><i>We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.</i></p>
<p><i>We have funding and a team of experts focused on Earth systems and digital innovation.</i></p>
<p><i><b>Your mission, should you choose to accept it:</b></i></p>
<p>As a <b>lead data engineer</b>, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.</p>
<p>As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.</p>
<p>You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.</p>
<p>The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We&apos;re working on the planet&apos;s problems and we need the planet&apos;s best people to fix them.</p>
<p><i>The following requirements describe our ideal candidate. If you don&apos;t meet some of the requirements, you&apos;re encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.</i></p>
<p><b>Essential Functions and Specific Duties:</b></p>
<ul>
 <li>Design, architect, build and maintain data pipeline systems</li>
 <li>Write code for importing and updating large datasets to relational database and search indexes</li>
 <li>Define and maintain database schemas and data file formats</li>
 <li>Collaborate with web developers on optimizing database schemas for APIs and Web applications</li>
 <li>Collaborate with a team of software engineering peers</li>
 <li>Mentor and guide more junior data engineering staff</li>
 <li>Define and maintain data management processes for the organization</li>
 <li>Work with product managers to develop schedules, estimate tasks, and define success criteria</li>
 <li>Collaborate with team members from other disciplines such as web development, design, product management, and devops</li>
 <li>Coordinate with Open Source contributors</li>
 <li>Coordinate with open standards community to define interoperability standards</li>
 <li>Actively participate in team building and culture development activities at Open Earth Foundation</li>
 <li>Other duties as assigned</li>
</ul>
<p><b>Required skills:</b></p>
<ul>
 <li>Python programming focused on big data management</li>
 <li>PostgreSQL or other relational database</li>
 <li>Docker</li>
 <li>Kubernetes</li>
 <li>Git</li>
</ul>
<p><b>Optional skills that will make a candidate stand out:</b></p>
<ul>
 <li>Generative AI and large language model (LLM) APIs and data applications</li>
 <li>GIS tools such as ESRI</li>
 <li>Amazon Web Services</li>
 <li>ElasticSearch</li>
 <li>Data pipeline tools, e.g. Pachyderm</li>
 <li>Experience with 100Gb or larger data sets</li>
 <li>Climate action data such as emissions, targets, and action plans</li>
 <li>Physical (lat, lon, alt) and political (city, state, country) geographical data</li>
 <li>Remote-sensing and satellite data</li>
 <li>RESTful Web APIs</li>
 <li>Engineering leadership</li>
 <li>Open Source project maintainership</li>
 <li>Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience</li>
 <li>5 years of experience in software development for data systems</li>
 <li>3 shipped projects</li>
</ul>
<p><b>Interpersonal skills:</b></p>
<ul>
 <li>Clear communicator with good verbal and written skills in English (additional languages a plus)</li>
 <li>Creative, flexible and efficient with a focus on details</li>
 <li>Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.</li>
 <li>Ability to embrace new challenges, take ownership and initiative as a key team player.</li>
</ul>
<p><i><b>Compensation and benefits</b></i></p>
<ul>
 <li>This position is full-time with compensation of &#x24;60,000-&#x24;105,000 /year, dependent on experience and location</li>
 <li>Open Earth offers unlimited paid time off, paid holidays and paid sick leave</li>
 <li>You will work remotely within a dynamic and international environment</li>
 <li>We celebrate our achievements during our annual team retreat</li>
</ul>
<p><i>OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.</i></p>
<p><i>Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.</i></p>
<p><i>We look forward to hearing from you!</i></p>
<p><i>Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don&#x2019;t believe you meet all the requirements, please apply and note what&#x2019;s missing in your cover letter.</i></p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;60,000.00 - &#x24;105,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Bonus opportunities</li>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
 <li>4 years</li>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,37b5edb36e885430,,Full-time,,,Remote,Lead Data Engineer,Today,2023-10-18T13:31:48.180Z,,,"$60,000 - $105,000 a year",2023-10-18T13:31:48.183Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=37b5edb36e885430&from=jasx&tk=1hd1fm1o6iman800&vjs=3
25,Gen4 Dental,"Company Description
At Gen4, we pride ourselves on our commitment to providing an incredible patient experience. We are passionate about our craft and the impact we can have on our patients. We aim to foster a doctor-centric organization that allows our doctors to do more of what they love. Culture, high performance, growth, and development are deeply embedded in our business and we have our sights set on finding individuals who are excited to be a part of a growing, flourishing company like ours. We offer competitive pay and a comprehensive benefits package available on the 1st of the month after 30 days of employment for full-time employees.
To learn more about us, check out our website here: https://gen4dental.com/
Job Description
The Data Engineer is a detail-oriented and innovative engineer with a deep understanding of Microsoft data platforms and proficiency in Python. In this role, you will spearhead the development and maintenance of our data infrastructure to facilitate efficient data processing and analytics, enhancing our dental services through informed, data-driven strategies.
Duties & Responsibilities

 Develop, construct, test, and maintain robust ETL processes primarily using Microsoft SQL Server Integration Services (SSIS) and\or Python.
 Design and implement relational and non-relational database systems utilizing Microsoft SQL Server, ensuring data integrity, availability, and confidentiality.
 Leverage Microsoft Azure data services (Azure Data Factory, Azure SQL Data Warehouse, Azure Data Lake, etc.) for scalable and cost-effective solutions.
 Develop analytical tools and solutions using Microsoft Power BI to empower teams with actionable insights for strategic planning and operational efficiency.
 Optimize and automate data delivery processes and establish routines for database tasks to improve system performance and stability.
 Uphold high standards for data quality by devising and implementing effective data cleaning and validation procedures.
 Collaborate closely with cross-functional teams to align data management and analytics solutions with business objectives.

Qualifications
Required:

 Minimum of 5+ years’ experience as a Data Engineer, ETL Developer, or similar role with a focus on Microsoft data platforms.
 Proficient in SQL with a solid understanding of Microsoft SQL Server and SSIS.
 Experience with Azure data services and Microsoft Power BI is essential.
 Knowledge of data modeling principles, including experience with data warehousing and big data technologies.
 Python programming experience.
 Strong problem-solving skills with an analytical mindset.
 Excellent communication skills, capable of explaining complex technical concepts to non-technical stakeholders.
 Multi-site healthcare experience.

Preferred:

 Bachelor's degree in a technology or engineering field, or equivalent combination of experience
 Dental industry experience
 Google Bigquery experience
 Postgresql experience.

Physical Requirements:

 Prolonged periods of sitting at a desk and working on a computer.
 Ability to lift up to 15 pounds.
 Excellent written, speaking and listening skills, requiring the perception of speech.
 Must have high finger dexterity to perform duties involving work on the computer.
 Able to travel as needed.

Equipment Used:
General office equipment (e.g. computer).
Additional information
Working conditions include those typically seen in an office environment. Prolonged periods of sitting at a desk and working on a computer.
Equal Opportunity Employer
Gen4 Dental Partners provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Job Type: Full-time
Pay: $115,000.00 - $125,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Paid time off
 Vision insurance

Schedule:

 Monday to Friday

Application Question(s):

 Are you at least 18 years of age?

Work Location: Remote","<p><b>Company Description</b></p>
<p>At Gen4, we pride ourselves on our commitment to providing an incredible patient experience. We are passionate about our craft and the impact we can have on our patients. We aim to foster a doctor-centric organization that allows our doctors to do more of what they love. Culture, high performance, growth, and development are deeply embedded in our business and we have our sights set on finding individuals who are excited to be a part of a growing, flourishing company like ours. <b>We offer competitive pay and a comprehensive benefits package available on the 1st of the month after 30 days of employment for full-time employees</b>.</p>
<p>To learn more about us, check out our website here: https://gen4dental.com/</p>
<p><b>Job Description</b></p>
<p>The Data Engineer is a detail-oriented and innovative engineer with a deep understanding of Microsoft data platforms and proficiency in Python. In this role, you will spearhead the development and maintenance of our data infrastructure to facilitate efficient data processing and analytics, enhancing our dental services through informed, data-driven strategies.</p>
<p><b>Duties &amp; Responsibilities</b></p>
<ul>
 <li>Develop, construct, test, and maintain robust ETL processes primarily using Microsoft SQL Server Integration Services (SSIS) and\or Python.</li>
 <li>Design and implement relational and non-relational database systems utilizing Microsoft SQL Server, ensuring data integrity, availability, and confidentiality.</li>
 <li>Leverage Microsoft Azure data services (Azure Data Factory, Azure SQL Data Warehouse, Azure Data Lake, etc.) for scalable and cost-effective solutions.</li>
 <li>Develop analytical tools and solutions using Microsoft Power BI to empower teams with actionable insights for strategic planning and operational efficiency.</li>
 <li>Optimize and automate data delivery processes and establish routines for database tasks to improve system performance and stability.</li>
 <li>Uphold high standards for data quality by devising and implementing effective data cleaning and validation procedures.</li>
 <li>Collaborate closely with cross-functional teams to align data management and analytics solutions with business objectives.</li>
</ul>
<p><b>Qualifications</b></p>
<p><b>Required:</b></p>
<ul>
 <li>Minimum of 5+ years&#x2019; experience as a Data Engineer, ETL Developer, or similar role with a focus on Microsoft data platforms.</li>
 <li>Proficient in SQL with a solid understanding of Microsoft SQL Server and SSIS.</li>
 <li>Experience with Azure data services and Microsoft Power BI is essential.</li>
 <li>Knowledge of data modeling principles, including experience with data warehousing and big data technologies.</li>
 <li>Python programming experience.</li>
 <li>Strong problem-solving skills with an analytical mindset.</li>
 <li>Excellent communication skills, capable of explaining complex technical concepts to non-technical stakeholders.</li>
 <li>Multi-site healthcare experience.</li>
</ul>
<p><b>Preferred:</b></p>
<ul>
 <li>Bachelor&apos;s degree in a technology or engineering field, or equivalent combination of experience</li>
 <li>Dental industry experience</li>
 <li>Google Bigquery experience</li>
 <li>Postgresql experience.</li>
</ul>
<p><b>Physical Requirements:</b></p>
<ul>
 <li>Prolonged periods of sitting at a desk and working on a computer.</li>
 <li>Ability to lift up to 15 pounds.</li>
 <li>Excellent written, speaking and listening skills, requiring the perception of speech.</li>
 <li>Must have high finger dexterity to perform duties involving work on the computer.</li>
 <li>Able to travel as needed.</li>
</ul>
<p><b>Equipment Used:</b></p>
<p>General office equipment (e.g. computer).</p>
<p><b>Additional information</b></p>
<p>Working conditions include those typically seen in an office environment. Prolonged periods of sitting at a desk and working on a computer.</p>
<p><b>Equal Opportunity Employer</b></p>
<p>Gen4 Dental Partners provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;115,000.00 - &#x24;125,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Are you at least 18 years of age?</li>
</ul>
<p>Work Location: Remote</p>",,e1d633e960423d61,,Full-time,,,Remote,Data Engineer,Today,2023-10-18T13:31:48.858Z,2.0,4.0,"$115,000 - $125,000 a year",2023-10-18T13:31:48.861Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e1d633e960423d61&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
26,Agilon Health,"Agilon health is transforming healthcare by empowering community-based physicians with the resources and expertise they need to innovate the payment and delivery of care for seniors.
The agilon health Total Care Model is powered by our purpose-built platform and frees physicians from the constraints of the traditional fee-for-service reimbursement model, all enabled through a growing national network of like-minded physician partners. With agilon health, physicians are able to practice team-based, coordinated care to serve the individual needs of their senior patients and to transition to a sustainable and predictable, long-term business model.
As you might imagine, analytics and insights are the heart of how we support our physician partners and is our special sauce. We have a lot of analytics programs in place already, but we need more! From generating insights related to our risk adjustment programs to helping analyze health plan attribution, health plan and membership trends, building our next generation Cube or analytics surrounding our brand-new data lake, there's a lot of data to process and actionable insights to present, as well as analytics infrastructure to build. You will be executing some of this work individually, and others in tight partnership with other critical teams such as Finance, Clinical Analytics, and Medical Economics.
There’s much more data we can be leveraging and analyzing to generate and test our hypotheses to help improve patient outcomes and reduce medical waste. Come join the team and help make a direct impact on our senior members’ lives!
More about this role:
- Be part of an agile team working collaboratively with Agilon leadership and many different cross-functional teams, including UX, Product, Technology, Operations, and Clinician teams- Leverage your analytical thinking to explore our ever-growing datasets to test hypotheses and bring life to your own insights- Generate insights that help improve patient outcomes and/or reduce medical waste- Continuously learn and share your new-found knowledge with others
Desired Traits:
1. Experience: A minimum of 4 years of experience performing data analysis and generating intuitive dashboards.
2. Healthcare domain expertise:
a. Exposure to the healthcare industry & domain is preferred, with specific knowledge in either risk adjustment, clinical analytics, clinical quality, or health economics.b. Familiarity with healthcare data models.
3. Education: A bachelor's degree or equivalent education is required.
4. Technical skills:
a. should have strong proficiency in SQL, and expertise in Snowflake would be a plus.b. Data modeling skills are also important.c. Familiarity with data visualization tools such as Sigma, Tableau, Power BI, or Excel is necessary to create intuitive dashboards.d. Some experience with coding in Python or other programming languages is nice to have.
5. Analytical and problem-solving skills: Strong analytical and problem-solving abilities are essential for effectively analyzing data and providing meaningful insights.6. Business Acumen: should be able to translate business needs into data analysis requirements, understanding the goals and objectives of the organization.7. Curiosity and exploration: Should have an intellectual curiosity about data and the ability to go beyond immediate problems. Balancing deadlines and exploring new opportunities for analysis and insights is important.8. Ownership and user focus: Takes ownership over the insights the team generates, and the processes used to develop those analyses, keeping our users' needs in clear focus9. Startup environment readiness: Comfortable in a startup environment and ready to “roll up your sleeves!”10. Communication skills: Excellent communication skills are necessary to effectively convey complex findings to both technical and non-technical stakeholders.11. Nimble learner: Should be eager to learn and adapt quickly to new technologies, methodologies, and industry trends.12. Enthusiasm and drive: A proactive and enthusiastic approach to getting things done is valued in a candidate.
Job Type: Full-time
Pay: $100,000.00 - $115,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Parental leave
 Professional development assistance
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 4 years

Schedule:

 Monday to Friday

Work Location: Remote","<p>Agilon health is transforming healthcare by empowering community-based physicians with the resources and expertise they need to innovate the payment and delivery of care for seniors.</p>
<p>The agilon health Total Care Model is powered by our purpose-built platform and frees physicians from the constraints of the traditional fee-for-service reimbursement model, all enabled through a growing national network of like-minded physician partners. With agilon health, physicians are able to practice team-based, coordinated care to serve the individual needs of their senior patients and to transition to a sustainable and predictable, long-term business model.</p>
<p>As you might imagine, analytics and insights are the heart of how we support our physician partners and is our special sauce. We have a lot of analytics programs in place already, but we need more! From generating insights related to our risk adjustment programs to helping analyze health plan attribution, health plan and membership trends, building our next generation Cube or analytics surrounding our brand-new data lake, there&apos;s a lot of data to process and actionable insights to present, as well as analytics infrastructure to build. You will be executing some of this work individually, and others in tight partnership with other critical teams such as Finance, Clinical Analytics, and Medical Economics.</p>
<p>There&#x2019;s much more data we can be leveraging and analyzing to generate and test our hypotheses to help improve patient outcomes and reduce medical waste. Come join the team and help make a direct impact on our senior members&#x2019; lives!</p>
<p><b>More about this role:</b></p>
<p>- Be part of an agile team working collaboratively with Agilon leadership and many different cross-functional teams, including UX, Product, Technology, Operations, and Clinician teams<br>- Leverage your analytical thinking to explore our ever-growing datasets to test hypotheses and bring life to your own insights<br>- Generate insights that help improve patient outcomes and/or reduce medical waste<br>- Continuously learn and share your new-found knowledge with others</p>
<p><b>Desired Traits:</b></p>
<p>1. Experience: A minimum of 4 years of experience performing data analysis and generating intuitive dashboards.</p>
<p>2. Healthcare domain expertise:</p>
<p>a. Exposure to the healthcare industry &amp; domain is preferred, with specific knowledge in either risk adjustment, clinical analytics, clinical quality, or health economics.<br>b. Familiarity with healthcare data models.</p>
<p>3. Education: A bachelor&apos;s degree or equivalent education is required.</p>
<p>4. Technical skills:</p>
<p>a. should have strong proficiency in SQL, and expertise in Snowflake would be a plus.<br>b. Data modeling skills are also important.<br>c. Familiarity with data visualization tools such as Sigma, Tableau, Power BI, or Excel is necessary to create intuitive dashboards.<br>d. Some experience with coding in Python or other programming languages is nice to have.</p>
<p>5. Analytical and problem-solving skills: Strong analytical and problem-solving abilities are essential for effectively analyzing data and providing meaningful insights.<br>6. Business Acumen: should be able to translate business needs into data analysis requirements, understanding the goals and objectives of the organization.<br>7. Curiosity and exploration: Should have an intellectual curiosity about data and the ability to go beyond immediate problems. Balancing deadlines and exploring new opportunities for analysis and insights is important.<br>8. Ownership and user focus: Takes ownership over the insights the team generates, and the processes used to develop those analyses, keeping our users&apos; needs in clear focus<br>9. Startup environment readiness: Comfortable in a startup environment and ready to &#x201c;roll up your sleeves!&#x201d;<br>10. Communication skills: Excellent communication skills are necessary to effectively convey complex findings to both technical and non-technical stakeholders.<br>11. Nimble learner: Should be eager to learn and adapt quickly to new technologies, methodologies, and industry trends.<br>12. Enthusiasm and drive: A proactive and enthusiastic approach to getting things done is valued in a candidate.</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;100,000.00 - &#x24;115,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Parental leave</li>
 <li>Professional development assistance</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>4 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,f32a0eb4fc36bab8,,Full-time,,,Remote,Senior Engineer BOI Insights (Healthcare Data Analytics),1 day ago,2023-10-17T13:31:56.460Z,2.8,62.0,"$100,000 - $115,000 a year",2023-10-18T13:31:56.463Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=f32a0eb4fc36bab8&from=jasx&tk=1hd1foafmk78p800&vjs=3
27,Leidos,"Description 
 Leidos has an immediate opening for a Senior Systems Engineer supporting development of a data Lakehouse at HHS in Washington DC. This position is an excellent opportunity to interface directly with government personnel to help design and guide technical solutions and business processes for critical public health research and response mission spaces.
 
  Primary Responsibilities
 
   Work directly with the customer to capture and formalize ‘Baseline’ and ‘To Be’ business user stories, use cases, requirements, system architectures, and data schema.
   Track and communicate solution and system development, production and maintenance activities.
   Design and execute test plans for solution and system testing.
   Help support the identification and considerations for alternative application solutions to meet business customer needs.
   Work with the broader development, including SQL developers and Azure engineers.
   Consider security and infrastructure implications of requested changes.
 
 
  Basic Qualifications
 
   Bachelor’s Degree in Systems Engineering, Computer Science, Information Technology, Business Science, or related field required
   8+ years’ of combined experience in system engineering, software development or business analysis required.
   Candidate must demonstrate a willing initiative to solicit customer requirements and translate them into formalized structured technical products. Candidate must have strong verbal and written communication skills. Concise writing and ability to communicate technical content to broad audiences are critical candidate abilities.
   Candidate must be a US Citizen and be able to obtain and maintain a high-risk public trust clearance.
   Must have experience or familiarity with: executing Agile, Scrum, and Kanban methodologies. Soliciting and documenting use cases and designs for system requirements (User Stories, Use Cases, Requirements, Specifications, Data Schema, Business Process Workflows). Use of system management tools such as Azure DevOps, Jira, Redmine, or similar system. Developing and executing testing and acceptance plans. Tracking bugs, issues and resolutions. Performing data analysis and reporting. Knowledge of government system security policies (ATO process) Ability to flexibly pivot to varying needs of the project while maintaining situational awareness
 
 
  Preferred Qualifications
 
   The preferred candidate will possess broader Systems Engineering knowledge and can readily execute low-level engineering tasks as well as high-level technical project management tasks.
   The preferred candidate will have a MS in Systems Engineering or a related discipline.
   Preferred candidates will have experience or familiarity with:
   Knowledge of the Microsoft Azure ecosystem
   Experience with Data Lakes and preferably Data Lakehouses
   Experience working with federal IT systems
   Experience working with SQL developers (or knowledge of SQL)
   Prioritizing and communicating requirements and system development activities.
   Communicating impact of requirement changes on active development activities.
   Executing and presenting trade space of alternatives with cost benefit analysis.
   Supporting Authority to Operate (ATO) activities and other production system processes.
   Experience in developing advanced data visualizations.
 
 
  Pay Range: Pay Range $97,500.00 - $176,250.00
 
  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.
  #Remote","<div>
 <p><b>Description</b> </p>
 <p>Leidos has an immediate opening for a Senior Systems Engineer supporting development of a data Lakehouse at HHS in Washington DC. This position is an excellent opportunity to interface directly with government personnel to help design and guide technical solutions and business processes for critical public health research and response mission spaces.</p>
 <p></p>
 <p><b> Primary Responsibilities</b></p>
 <ul>
  <li> Work directly with the customer to capture and formalize &#x2018;Baseline&#x2019; and &#x2018;To Be&#x2019; business user stories, use cases, requirements, system architectures, and data schema.</li>
  <li> Track and communicate solution and system development, production and maintenance activities.</li>
  <li> Design and execute test plans for solution and system testing.</li>
  <li> Help support the identification and considerations for alternative application solutions to meet business customer needs.</li>
  <li> Work with the broader development, including SQL developers and Azure engineers.</li>
  <li> Consider security and infrastructure implications of requested changes.</li>
 </ul>
 <p></p>
 <p><b> Basic Qualifications</b></p>
 <ul>
  <li> Bachelor&#x2019;s Degree in Systems Engineering, Computer Science, Information Technology, Business Science, or related field required</li>
  <li> 8+ years&#x2019; of combined experience in system engineering, software development or business analysis required.</li>
  <li> Candidate must demonstrate a willing initiative to solicit customer requirements and translate them into formalized structured technical products. Candidate must have strong verbal and written communication skills. Concise writing and ability to communicate technical content to broad audiences are critical candidate abilities.</li>
  <li> Candidate must be a US Citizen and be able to obtain and maintain a high-risk public trust clearance.</li>
  <li> Must have experience or familiarity with: executing Agile, Scrum, and Kanban methodologies. Soliciting and documenting use cases and designs for system requirements (User Stories, Use Cases, Requirements, Specifications, Data Schema, Business Process Workflows). Use of system management tools such as Azure DevOps, Jira, Redmine, or similar system. Developing and executing testing and acceptance plans. Tracking bugs, issues and resolutions. Performing data analysis and reporting. Knowledge of government system security policies (ATO process) Ability to flexibly pivot to varying needs of the project while maintaining situational awareness</li>
 </ul>
 <p></p>
 <p><b> Preferred Qualifications</b></p>
 <ul>
  <li> The preferred candidate will possess broader Systems Engineering knowledge and can readily execute low-level engineering tasks as well as high-level technical project management tasks.</li>
  <li> The preferred candidate will have a MS in Systems Engineering or a related discipline.</li>
  <li> Preferred candidates will have experience or familiarity with:</li>
  <li> Knowledge of the Microsoft Azure ecosystem</li>
  <li> Experience with Data Lakes and preferably Data Lakehouses</li>
  <li> Experience working with federal IT systems</li>
  <li> Experience working with SQL developers (or knowledge of SQL)</li>
  <li> Prioritizing and communicating requirements and system development activities.</li>
  <li> Communicating impact of requirement changes on active development activities.</li>
  <li> Executing and presenting trade space of alternatives with cost benefit analysis.</li>
  <li> Supporting Authority to Operate (ATO) activities and other production system processes.</li>
  <li> Experience in developing advanced data visualizations.</li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Pay Range:</b></h2> Pay Range &#x24;97,500.00 - &#x24;176,250.00
 <p></p>
 <p> The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.</p>
 <p> #Remote</p>
</div>",https://careers.leidos.com/jobs/13492238-data-lakehouse-engineer?tm_job=R-00121192&tm_event=view&tm_company=2502,e2317b3154109084,,Full-time,,,Remote,Data Lakehouse Engineer,Today,2023-10-18T13:31:38.939Z,3.7,1699.0,"$97,500 - $176,250 a year",2023-10-18T13:31:38.941Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e2317b3154109084&from=jasx&tk=1hd1fn4d0i3at800&vjs=3
28,Arista Networks,"Company Description
  Arista Networks is an industry leader in Cognitive Cloud Networking for mission critical data center and campus environments. Our award winning open source platforms deliver ultra low latency, high availability, automated analytics and secure network solutions.
  Our culture is one that is founded on our core key values which resonate across all of our employee and include respect, integrity, teamwork, innovation, passion, trust and quality.
 
 

 Job Description
  The Data Center Site Operations Engineer will help customers successfully plan and optimize their Data Center physical space, mechanical, cabling and other site infrastructure to receive Arista equipment and operate it in the highest performing and most efficient manner.
  Arista Professional Services team helps customers throughout the entire technology lifecycle from design to implementation, migration and ongoing operation, to ensure a seamless transition from initial exposure to being able to operate and realize tangible business outcomes from their network infrastructure.
  In this role, you would function as the subject matter expert in this area of the Arista PS team, interfacing with mechanical engineers in the Arista product team to understand detailed requirements, translate these into best practices for field installation, then take a customer facing role in training and advisory.
  Responsibilities will be to provide expert consulting to customers including site design assistance, proactive site prep. for installation, demonstration of best practices, creating training material and delivering training to customers and partiers, site surveys and oversight of physical installation and commissioning. Areas of expertise should include:
  Site requirements for all Arista Data Center products including:
 
   Equipment installation logistics including floorspace and rack preparation, moving and lifting
   Proper handling of all components from receiving, storage, unboxing and assembly
   Size, Weight and Rack mounting considerations for safe operation and serviceability
   Airflow and cooling requirements
   Power requirements including power resiliency
   Cable management considerations
   Copper and Fiber cable types, connector types, pluggable modules
 
  Understand overall DC design and optimization, including:
 
   Floorplan and rack layouts
   Power distribution and resiliency
   Cooling strategies including liquid cooling
   Cable layout strategies for different DC network topologies
   Equipment density and cable length considerations
 
 
 

 Qualifications
  
 
   An engineering, IT or technology focused degree.
   Mechanical Engineering degree a plus.
   10+ years of industry experience designing, constructing and overseeing operations in complex datacenter environments.
   Experience with the handling, installation and servicing of large scale IT equipment in the DC.
   Experience creating technical drawings, documenting methodologies and training DC staff to execute operations.
   Excellent written and verbal skills and ability to effectively communicate with all Arista, partner and customer levels.
   Prior experience with DC site operations as well as with customer facing services or support will be a plus
   This position is remote with a potential for 50% travel globally.
 
  Compensation Information:
  The new hire base pay for this role has a pay range of $103,000 to $158,000 across the US, within California, the base pay range for this role is $124,000 to 158,000.
  Arista offers different pay ranges based on work location, so that we can offer consistent and competitive pay appropriate to the market. The actual base pay offered will be based on a wide range of factors, including skills, qualifications, relevant experience, and work location. The pay range provided reflects base pay only and in addition certain roles may also be eligible for discretionary Arista bonuses and equity. Employees in Sales roles are eligible to participate in Arista’s Sales Incentive Plan, which pays commissions calculated as a percentage of eligible sales. US-based employees are also entitled to benefits including medical, dental, vision, wellbeing, tax savings and income protection. The recruiting team can share more details during the hiring process specific to the role and location.
  Additional Information
  All your information will be kept confidential according to EEO guidelines.","<div>
 Company Description
 <p><br> Arista Networks is an industry leader in Cognitive Cloud Networking for mission critical data center and campus environments. Our award winning open source platforms deliver ultra low latency, high availability, automated analytics and secure network solutions.</p>
 <p> Our culture is one that is founded on our core key values which resonate across all of our employee and include respect, integrity, teamwork, innovation, passion, trust and quality.</p>
</div> 
<br> 
<div>
 Job Description
 <p><br> The Data Center Site Operations Engineer will help customers successfully plan and optimize their Data Center physical space, mechanical, cabling and other site infrastructure to receive Arista equipment and operate it in the highest performing and most efficient manner.</p>
 <p> Arista Professional Services team helps customers throughout the entire technology lifecycle from design to implementation, migration and ongoing operation, to ensure a seamless transition from initial exposure to being able to operate and realize tangible business outcomes from their network infrastructure.</p>
 <p> In this role, you would function as the subject matter expert in this area of the Arista PS team, interfacing with mechanical engineers in the Arista product team to understand detailed requirements, translate these into best practices for field installation, then take a customer facing role in training and advisory.</p>
 <p> Responsibilities will be to provide expert consulting to customers including site design assistance, proactive site prep. for installation, demonstration of best practices, creating training material and delivering training to customers and partiers, site surveys and oversight of physical installation and commissioning. Areas of expertise should include:</p>
 <p><b> Site requirements for all Arista Data Center products including:</b></p>
 <ul>
  <li> Equipment installation logistics including floorspace and rack preparation, moving and lifting</li>
  <li> Proper handling of all components from receiving, storage, unboxing and assembly</li>
  <li> Size, Weight and Rack mounting considerations for safe operation and serviceability</li>
  <li> Airflow and cooling requirements</li>
  <li> Power requirements including power resiliency</li>
  <li> Cable management considerations</li>
  <li> Copper and Fiber cable types, connector types, pluggable modules</li>
 </ul>
 <p><b> Understand overall DC design and optimization, including:</b></p>
 <ul>
  <li> Floorplan and rack layouts</li>
  <li> Power distribution and resiliency</li>
  <li> Cooling strategies including liquid cooling</li>
  <li> Cable layout strategies for different DC network topologies</li>
  <li> Equipment density and cable length considerations</li>
 </ul>
</div> 
<br> 
<div>
 Qualifications
 <br> 
 <ul>
  <li> An engineering, IT or technology focused degree.</li>
  <li> Mechanical Engineering degree a plus.</li>
  <li> 10+ years of industry experience designing, constructing and overseeing operations in complex datacenter environments.</li>
  <li> Experience with the handling, installation and servicing of large scale IT equipment in the DC.</li>
  <li> Experience creating technical drawings, documenting methodologies and training DC staff to execute operations.</li>
  <li> Excellent written and verbal skills and ability to effectively communicate with all Arista, partner and customer levels.</li>
  <li> Prior experience with DC site operations as well as with customer facing services or support will be a plus</li>
  <li> This position is remote with a potential for 50% travel globally.</li>
 </ul>
 <p><b> Compensation Information:</b></p>
 <p> The new hire base pay for this role has a pay range of &#x24;103,000 to &#x24;158,000 across the US, within California, the base pay range for this role is &#x24;124,000 to 158,000.</p>
 <p> Arista offers different pay ranges based on work location, so that we can offer consistent and<br> competitive pay appropriate to the market. The actual base pay offered will be based on a wide range of factors, including skills, qualifications, relevant experience, and work location.<br> The pay range provided reflects base pay only and in addition certain roles may also be eligible for discretionary Arista bonuses and equity. Employees in Sales roles are eligible to participate in Arista&#x2019;s Sales Incentive Plan, which pays commissions calculated as a percentage of eligible sales. US-based employees are also entitled to benefits including medical, dental, vision, wellbeing, tax savings and income protection. The recruiting team can share more details during the hiring process specific to the role and location.</p>
 <br> Additional Information
 <p><br> All your information will be kept confidential according to EEO guidelines.</p>
</div>",https://jobs.smartrecruiters.com/AristaNetworks/743999937616147-data-center-site-operations-engineer-professional-services,6492b9f12e5769ae,,Full-time,,,"5453 Great America Pkwy, Santa Clara, CA 95054",Data Center Site Operations Engineer - Professional Services,1 day ago,2023-10-17T13:31:51.900Z,4.0,40.0,"$103,000 - $158,000 a year",2023-10-18T13:31:51.902Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=6492b9f12e5769ae&from=jasx&tk=1hd1foafmk78p800&vjs=3
35,Canyon Associates,"Role and Responsibilities
Must be a hands on Data Engineer with lead/management experience. Oversee 3 and design, create, test, deploy and support SQL code. Monitor database systems and daily ETL processes. Looking for 5+ years prior experience (hands-on)
AWS, AZURE Cloud, Azure Databricks, Azure SQL Database, Data Structure, Power BI, Snowflake, Relational databases
Job Type: Full-time
Pay: $140,000.00 - $175,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee discount
 Flexible spending account
 Health insurance
 Health savings account
 Paid time off
 Parental leave
 Vision insurance

Schedule:

 Monday to Friday

People with a criminal record are encouraged to apply
Education:

 Bachelor's (Preferred)

Work Location: Remote","<p><b>Role and Responsibilities</b></p>
<p>Must be a hands on Data Engineer with lead/management experience. Oversee 3 and design, create, test, deploy and support SQL code. Monitor database systems and daily ETL processes. Looking for 5+ years prior experience (hands-on)</p>
<p>AWS, AZURE Cloud, Azure Databricks, Azure SQL Database, Data Structure, Power BI, Snowflake, Relational databases</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;140,000.00 - &#x24;175,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee discount</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Paid time off</li>
 <li>Parental leave</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>People with a criminal record are encouraged to apply</p>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,e3a2b78dd10fb62c,,Full-time,,,"New York, NY",Lead Data Engineer,1 day ago,2023-10-17T13:32:19.755Z,,,"$140,000 - $175,000 a year",2023-10-18T13:32:19.758Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e3a2b78dd10fb62c&from=jasx&tk=1hd1foafmk78p800&vjs=3
36,Braintrust,"ABOUT US: 
   Braintrust is a user-owned talent network that connects top-tier professionals with the world's leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality. 
   ABOUT THE HIRING PROCESS: 
   The hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match. 
   Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.
 
  
  
  
  JOB TYPE: Freelance/ Contract Position (no agencies/C2C - see notes below) 
  LOCATION: Hybrid - Chicago, IL 
  SALARY RANGE: $55 - $64 /hr 
  ESTIMATED DURATION: 40hr/week - Long term 
  EXPERIENCE: 3-5 years 
  BRAINTRUST JOB ID: 9746 
  
 THE OPPORTUNITY 
  
  
   
    
     Requirements 
      
      
       ***MUST BE LOCATED IN CHICAGO ARE AND ABLE TO GO TO THE OFFICE 3 DAYS A WEEK***
        
        
        The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data.
        
        
        Job Responsibilities: 
        
        Develop and test data pipelines with ETL and store data in Snowflake, debug errors and make necessary modifications. 
        Modify existing databases and tables and work with data engineers and analysts to make changes. 
        Write code/scripts to validate data between different databases and work on designing new data pipelines. 
        
       Skills: 
        
        Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. 
        Ability to work independently and manage one's time. 
        Good knowledge of data modeling. 
        Expertise with SQL, Python and Snowflake 
        Basic knowledge of computer software, such as Visual Basic, Oracle, etc. 
        
       Education/Experience: 
        
        Associate's degree in computer programming or a relevant field required. Bachelor's/Master degree preferred. 
        2-4 years experience required 
       
      
     
    
   
   
    
     What you'll be working on 
      
      
       Position's Contributions to Work Group: 
        
        The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. 
        A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data. 
        This position is working with a team responsible for managing Customer Value Agreements (CVA) reporting process and providing insights to business partners to increasing sales 
        
       Typical task breakdown: 
        
        Develop and test Airflow DAGs, debug errors and make necessary modifications during our migration from SAS to Airflow and Snowflake. 
        Maintaining and enhancing tables in Snowflake 
        Provide support to business partners to answer questions regarding CVA metrics. 
        Working on migrating Airflow DAGs to AWS 
        
       Interaction with team: 
        
        Working with colleagues in Chicago, Michigan, and Geneva 
        Will be interacting with business stakeholders to explain the CVA logic and metrics impact. 
        
       Technical Skills (Required) 
        
        2+ years exp in Python 
        2+ years exp with SQL 
        2+ years exp in Snowflake 
        
       (Desired) 
        
        Project management exp specifically Agile methodology 
        Airflow experience 
        AWS experience 
        Data analytics 
        Good understanding of Software Development Life Cycle 
        Tableau, Power BI experience 
        
       Soft Skills (Required) 
        
        Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. 
        Ability to work independently and manage one's time. Critical thinking skills a must. 
        Able to work well in a team in a collaborative workspace. 
        Being proactive and take initiative. 
        Able to work off hours when required to support production issues. 
        
       (Desired) 
        
        Being able to create connection with people outside the team and business partners. 
        Organized
       
      
       
      
    
   
  
 
 
 
  Notes: 
   Our employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we'd welcome your application. 
   Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.","<div>
 <div>
  <p><b>ABOUT US</b>:</p> 
  <p> Braintrust is a user-owned talent network that connects top-tier professionals with the world&apos;s leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality.</p> 
  <p><b> ABOUT THE HIRING PROCESS:</b></p> 
  <p> The hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match.</p> 
  <p> Our process isn&apos;t for everyone, that&apos;s intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.</p>
 </div>
 <br> 
 <p></p> 
 <ul> 
  <li><b>JOB TYPE</b>: Freelance/ Contract Position (no agencies/C2C - see notes below)</li> 
  <li><b>LOCATION</b>: Hybrid - Chicago, IL</li> 
  <li><b>SALARY RANGE</b>: &#x24;55 - &#x24;64 /hr</li> 
  <li><b>ESTIMATED DURATION</b>: 40hr/week - Long term</li> 
  <li><b>EXPERIENCE</b>: 3-5 years</li> 
  <li><b>BRAINTRUST JOB ID: </b><b>9746</b></li> 
 </ul> 
 <p><b>THE OPPORTUNITY</b></p> 
 <div> 
  <div>
   <div>
    <div>
     <h2 class=""jobSectionHeader""><b>Requirements</b></h2> 
     <div> 
      <div>
       <p><b>***MUST BE LOCATED IN CHICAGO ARE AND ABLE TO GO TO THE OFFICE 3 DAYS A WEEK***</b></p>
       <br> 
       <p></p> 
       <p> The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data.</p>
       <br> 
       <p></p> 
       <p><b> Job Responsibilities:</b></p> 
       <ul> 
        <li>Develop and test data pipelines with ETL and store data in Snowflake, debug errors and make necessary modifications.</li> 
        <li>Modify existing databases and tables and work with data engineers and analysts to make changes.</li> 
        <li>Write code/scripts to validate data between different databases and work on designing new data pipelines.</li> 
       </ul> 
       <p><b>Skills:</b></p> 
       <ul> 
        <li>Verbal and written communication skills, problem solving skills, customer service and interpersonal skills.</li> 
        <li>Ability to work independently and manage one&apos;s time.</li> 
        <li>Good knowledge of data modeling.</li> 
        <li>Expertise with SQL, Python and Snowflake</li> 
        <li>Basic knowledge of computer software, such as Visual Basic, Oracle, etc.</li> 
       </ul> 
       <p><b>Education/Experience:</b></p> 
       <ul> 
        <li>Associate&apos;s degree in computer programming or a relevant field required. Bachelor&apos;s/Master degree preferred.</li> 
        <li>2-4 years experience required</li> 
       </ul>
      </div>
     </div>
    </div>
   </div>
   <div>
    <div>
     <h2 class=""jobSectionHeader""><b>What you&apos;ll be working on</b></h2> 
     <div> 
      <div>
       <p><b>Position&apos;s Contributions to Work Group:</b></p> 
       <ul> 
        <li>The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal.</li> 
        <li>A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data.</li> 
        <li>This position is working with a team responsible for managing Customer Value Agreements (CVA) reporting process and providing insights to business partners to increasing sales</li> 
       </ul> 
       <p><b>Typical task breakdown:</b></p> 
       <ul> 
        <li>Develop and test Airflow DAGs, debug errors and make necessary modifications during our migration from SAS to Airflow and Snowflake.</li> 
        <li>Maintaining and enhancing tables in Snowflake</li> 
        <li>Provide support to business partners to answer questions regarding CVA metrics.</li> 
        <li>Working on migrating Airflow DAGs to AWS</li> 
       </ul> 
       <p><b>Interaction with team:</b></p> 
       <ul> 
        <li>Working with colleagues in Chicago, Michigan, and Geneva</li> 
        <li>Will be interacting with business stakeholders to explain the CVA logic and metrics impact.</li> 
       </ul> 
       <p><b>Technical Skills</b><br> (Required)</p> 
       <ul> 
        <li>2+ years exp in Python</li> 
        <li>2+ years exp with SQL</li> 
        <li>2+ years exp in Snowflake</li> 
       </ul> 
       <p>(Desired)</p> 
       <ul> 
        <li>Project management exp specifically Agile methodology</li> 
        <li>Airflow experience</li> 
        <li>AWS experience</li> 
        <li>Data analytics</li> 
        <li>Good understanding of Software Development Life Cycle</li> 
        <li>Tableau, Power BI experience</li> 
       </ul> 
       <p><b>Soft Skills</b><br> (Required)</p> 
       <ul> 
        <li>Verbal and written communication skills, problem solving skills, customer service and interpersonal skills.</li> 
        <li>Ability to work independently and manage one&apos;s time. Critical thinking skills a must.</li> 
        <li>Able to work well in a team in a collaborative workspace.</li> 
        <li>Being proactive and take initiative.</li> 
        <li>Able to work off hours when required to support production issues.</li> 
       </ul> 
       <p>(Desired)</p> 
       <ul> 
        <li>Being able to create connection with people outside the team and business partners.</li> 
        <li>Organized</li>
       </ul>
      </div>
      <br> 
     </div> 
    </div>
   </div>
  </div>
 </div>
 <div></div>
 <div>
  <p>Notes:</p> 
  <p> Our employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we&apos;d welcome your application.</p> 
  <p> Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.</p>
 </div>
</div>",https://boards.greenhouse.io/braintrust/jobs/4998692004?gh_src=d894da014us,a500c31bd71fbb3a,,Full-time,Contract,Freelance,"Chicago, IL",Data Engineer 2 (CHICAGO/HYBRID) - Freelance,1 day ago,2023-10-17T13:32:17.825Z,4.5,4.0,$55 - $64 an hour,2023-10-18T13:32:17.828Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=a500c31bd71fbb3a&from=jasx&tk=1hd1foafmk78p800&vjs=3
37,Comfort Keepers,"Job Summary:
   Comfort Keepers is seeking a skilled and experienced Data Engineer to join our data team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to ensure the availability and accessibility of high-quality data for analysis and reporting purposes. The ideal candidate is passionate about data, governance, has strong programming skills, is a creative problem solver and is proficient in building efficient and scalable data solutions.
   Be part of a transformational experience as we meet the challenges of today’s business landscape and lay the foundation for future growth.
   Location: Remote-U.S. or Irvine, CA Expected Salary Range: $100k
   Responsibilities:
  
    Build Azure data factory pipeline, Azure Data Lake, Datawarehouse, Power BI Reports and dashboards.
    Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
    Optimize and fine-tune data processes to ensure high performance and reliability.
    Implement data validation, testing, and quality assurance processes to maintain data accuracy and consistency.
    Work with large and complex datasets, both structured and unstructured, and employ appropriate data storage solutions.
    Develop and maintain documentation for data pipelines, processes, and data dictionaries.
    Monitor and troubleshoot data pipeline issues, resolving them in a timely manner to minimize downtime.
    Collaborate with data scientists, analysts, agency partners and other stakeholders to understand data requirements and ensure data integrity, accuracy, and availability.
    Additional responsibilities will be supporting applications and projects as appropriate.
  
  
   Qualifications:
  
    Microsoft SQL Server Database 2019 and above, including Azure SQL
    Azure Data Factory, Azure Data Lake, Azure Devops , Azure Data warehouse
    Experience with Repository and version control, GitHub
    Microsoft Power BI and Power Query with DAX and M Language
  
 
  
 
 
   Azure Analysis service—Tabular Cube
   Amazon Athena and Redshift
   Development tools (Visual Studio)
   Designing and implementing data models, and data lake solutions
   Working with data scientists and digital teams to meet strategic data needs through project management tools like Microsoft Teams, JIRA, are desired
   Strong problem-solving skills and attention to detail.
   Excellent communication and teamwork skills.
 
  Preferred:
 
   C# Experience
   Python, and/or R applications and languages while managing work using software version control like GitHub and/or Dev Ops
 
 
  Work Environment: Remote
  An Equal Opportunity and Affirmative Action employer, Comfort Keepers considers applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, or any legally protected status. We will make reasonable accommodations for qualified individuals with known disabilities unless doing so would result in an undue hardship.
  
 Xwc2axfoet","<div>
 <div>
  <h1 class=""jobSectionHeader""><b>Job </b><b>Summary:</b></h1>
  <p> Comfort Keepers is seeking a skilled and experienced Data Engineer to join our data team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to ensure the availability and accessibility of high-quality data for analysis and reporting purposes. The ideal candidate is passionate about data, governance, has strong programming skills, is a creative problem solver and is proficient in building efficient and scalable data solutions.</p>
  <p> Be part of a transformational experience as we meet the challenges of today&#x2019;s business landscape and lay the foundation for future growth.</p>
  <p> Location: Remote-U.S. or Irvine, CA<br> Expected Salary Range: &#x24;100k</p>
  <h1 class=""jobSectionHeader""><b> Responsibilities:</b></h1>
  <ul>
   <li> Build Azure data factory pipeline, Azure Data Lake, Datawarehouse, Power BI Reports and dashboards.</li>
   <li> Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.</li>
   <li> Optimize and fine-tune data processes to ensure high performance and reliability.</li>
   <li> Implement data validation, testing, and quality assurance processes to maintain data accuracy and consistency.</li>
   <li> Work with large and complex datasets, both structured and unstructured, and employ appropriate data storage solutions.</li>
   <li> Develop and maintain documentation for data pipelines, processes, and data dictionaries.</li>
   <li> Monitor and troubleshoot data pipeline issues, resolving them in a timely manner to minimize downtime.</li>
   <li> Collaborate with data scientists, analysts, agency partners and other stakeholders to understand data requirements and ensure data integrity, accuracy, and availability.</li>
   <li> Additional responsibilities will be supporting applications and projects as appropriate.</li>
  </ul>
  <p></p>
  <h1 class=""jobSectionHeader""><b><br> Qualifications:</b></h1>
  <ul>
   <li> Microsoft SQL Server Database 2019 and above, including Azure SQL</li>
   <li> Azure Data Factory, Azure Data Lake, Azure Devops , Azure Data warehouse</li>
   <li> Experience with Repository and version control, GitHub</li>
   <li> Microsoft Power BI and Power Query with DAX and M Language</li>
  </ul>
 </div>
 <br> 
 <p></p>
 <ul>
  <li> Azure Analysis service&#x2014;Tabular Cube</li>
  <li> Amazon Athena and Redshift</li>
  <li> Development tools (Visual Studio)</li>
  <li> Designing and implementing data models, and data lake solutions</li>
  <li> Working with data scientists and digital teams to meet strategic data needs through project management tools like Microsoft Teams, JIRA, are desired</li>
  <li> Strong problem-solving skills and attention to detail.</li>
  <li> Excellent communication and teamwork skills.</li>
 </ul>
 <p> Preferred:</p>
 <ul>
  <li> C# Experience</li>
  <li> Python, and/or R applications and languages while managing work using software version control like GitHub and/or Dev Ops</li>
 </ul>
 <p></p>
 <p><b><br> Work Environment: Remote</b></p>
 <p> An Equal Opportunity and Affirmative Action employer, Comfort Keepers considers applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, or any legally protected status. We will make reasonable accommodations for qualified individuals with known disabilities unless doing so would result in an undue hardship.</p>
 <p> </p>
 <p>Xwc2axfoet</p>
</div>",https://sdxhomecareoperationsllc.applytojob.com/apply/Xwc2axfoet/Data-Engineer?source=INDE&utm_source=Indeed&utm_medium=organic&utm_campaign=Indeed,daada383998fad4a,,Full-time,,,Remote,Data Engineer,1 day ago,2023-10-17T13:32:20.583Z,3.6,4807.0,"$100,000 a year",2023-10-18T13:32:20.586Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=daada383998fad4a&from=jasx&tk=1hd1foafmk78p800&vjs=3
39,Booz Allen Hamilton,"Job Description 
  
 
 
  
   
    
     
      
       
        
         Location: 
        
        
         Washington,DC,US 
        
       
       
        
         Remote Work: 
        
        
         Hybrid 
        
       
       
        
         Job Number: 
        
        
         R0181926
        
       
      
     
    
    
    
      
     
       
      
     
    
    
    
     
      
       
        
         Data Engineer
          The Opportunity:
          Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there’s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it’s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions—from fraud detection to cancer research to national intelligence.
         
          As a big data engineer at Booz Allen, you’ll implement data engineering activities on some of the most mission-driven projects in the industry. You’ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you’ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good.
         
          Join us. The world can’t wait.
         
          You Have:
         
           5+ years of experience in application development
           5+ years of experience designing, developing, operationalizing and maintaining complex data applications at enterprise scale
           3+ years of experience creating software for retrieving, parsing and processing structured and unstructured data
           3+ years of experience building scalable ETL/ELT workflows for reporting and analytics
           Experience creating solutions within a collaborative, cross-functional team environment
           Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms
           Ability to obtain and maintain a Public Trust or Suitability/Fitness determination based on client requirements
           Bachelor’s degree
         
         
          Nice If You Have:
         
           Experience with Python, SQL, Scala, or Java
           Experience with Unix/Linux, including basic commands and Shell scripting
           Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud
           Experience with distributed data, computing tools including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka
           Experience working on real-time data and streaming applications
           Experience with NoSQL implementation, including MongoDB or Cassandra
           Experience with data warehousing using AWS Redshift, MySQL, or Snowflake
           Experience with Agile engineering practices
           Experience with HR system data
         
         
          Vetting:
          Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client.
         
          Create Your Career:
          Grow With Us
          Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
         
          A Place Where You Belong
          Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll build your community in no time.
         
          Support Your Well-Being
          Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
         
          Your Candidate Journey
          At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
         
          Compensation
          At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
          Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $73,100.00 to $166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
         
          Work Model Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
         
           If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
           If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
         
         
          EEO Commitment
          We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.","<div>
 <div>
  <div>
   <h2 class=""jobSectionHeader""><b>Job Description</b></h2> 
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Location: 
        </div>
        <div>
         Washington,DC,US 
        </div>
       </div>
       <div>
        <div>
         Remote Work: 
        </div>
        <div>
         Hybrid 
        </div>
       </div>
       <div>
        <div>
         Job Number: 
        </div>
        <div>
         R0181926
        </div>
       </div>
      </div>
     </div>
    </div>
    <p></p>
    <div>
     <br> 
     <div>
      <div> 
      </div>
     </div>
    </div>
    <div></div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Data Engineer
         <p><b> The Opportunity:</b></p>
         <p> Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there&#x2019;s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it&#x2019;s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions&#x2014;from fraud detection to cancer research to national intelligence.</p>
         <p></p>
         <p> As a big data engineer at Booz Allen, you&#x2019;ll implement data engineering activities on some of the most mission-driven projects in the industry. You&#x2019;ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you&#x2019;ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You&#x2019;ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good.</p>
         <p></p>
         <p> Join us. The world can&#x2019;t wait.</p>
         <p></p>
         <p><b> You Have:</b></p>
         <ul>
          <li><p> 5+ years of experience in application development</p></li>
          <li><p> 5+ years of experience designing, developing, operationalizing and maintaining complex data applications at enterprise scale</p></li>
          <li><p> 3+ years of experience creating software for retrieving, parsing and processing structured and unstructured data</p></li>
          <li><p> 3+ years of experience building scalable ETL/ELT workflows for reporting and analytics</p></li>
          <li><p> Experience creating solutions within a collaborative, cross-functional team environment</p></li>
          <li><p> Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms</p></li>
          <li><p> Ability to obtain and maintain a Public Trust or Suitability/Fitness determination based on client requirements</p></li>
          <li><p> Bachelor&#x2019;s degree</p></li>
         </ul>
         <p></p>
         <p><b> Nice If You Have:</b></p>
         <ul>
          <li><p> Experience with Python, SQL, Scala, or Java</p></li>
          <li><p> Experience with Unix/Linux, including basic commands and Shell scripting</p></li>
          <li><p> Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud</p></li>
          <li><p> Experience with distributed data, computing tools including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka</p></li>
          <li><p> Experience working on real-time data and streaming applications</p></li>
          <li><p> Experience with NoSQL implementation, including MongoDB or Cassandra</p></li>
          <li><p> Experience with data warehousing using AWS Redshift, MySQL, or Snowflake</p></li>
          <li><p> Experience with Agile engineering practices</p></li>
          <li><p> Experience with HR system data</p></li>
         </ul>
         <p></p>
         <p><b><br> Vetting:</b></p>
         <p> Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client.</p>
         <p></p>
         <p><b> Create Your Career:</b></p>
         <p><b> Grow With Us</b></p>
         <p> Your growth matters to us&#x2014;that&#x2019;s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.</p>
         <p></p>
         <p><b> A Place Where You Belong</b></p>
         <p> Diverse perspectives cultivate collective ingenuity. Booz Allen&#x2019;s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you&#x2019;ll build your community in no time.</p>
         <p></p>
         <p><b> Support Your Well-Being</b></p>
         <p> Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we&#x2019;ll support you as you pursue a balanced, fulfilling life&#x2014;at work and at home.</p>
         <p></p>
         <p><b> Your Candidate Journey</b></p>
         <p> At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we&#x2019;ve compiled a list of resources so you&#x2019;ll know what to expect as we forge a connection with you during your journey as a candidate with us.</p>
         <p></p>
         <p><b> Compensation</b></p>
         <p> At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen&#x2019;s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.</p>
         <p></p> Salary at Booz Allen is determined by various factors, including but not limited to location, the individual&#x2019;s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is &#x24;73,100.00 to &#x24;166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen&#x2019;s total compensation package for employees.
         <p></p>
         <p><b> Work Model</b><br> Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.</p>
         <ul>
          <li> If this position is listed as remote or hybrid, you&#x2019;ll periodically work from a Booz Allen or client site facility.</li>
          <li> If this position is listed as onsite, you&#x2019;ll work with colleagues and clients in person, as needed for the specific role.</li>
         </ul>
         <p></p>
         <p><b> EEO Commitment</b></p>
         <p> We&#x2019;re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change &#x2013; no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.</p>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<p></p>",https://careers.boozallen.com/jobs/JobDetail/Washington-Data-Engineer-R0181926/86617?source=JB-14400,814ba5de32777d5d,,,,,"Washington, DC",Data Engineer,1 day ago,2023-10-17T13:32:24.125Z,3.9,2512.0,"$73,100 - $166,000 a year",2023-10-18T13:32:24.148Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=814ba5de32777d5d&from=jasx&tk=1hd1fo8m3llq3800&vjs=3
41,eTeam Inc,"Post1
Job Title: Big Data EngineerDuration: 6 Months (Contract to hire)Location: Remote
Job Opportunity:

 This job designs and engineers solutions associated with analytic data for the organization and, working closely with the business, analytic and IT teams, assists with the build and upkeep for these solutions. This includes coding data ingestion, transformation, and delivery programs/locic for analysts to access operational, derived, and external data sets.
 Expected deliverables will include; coding of delivery frameworks to load and transform raw source data into enhanced analytic assets, being a key resource for analytical and big data efforts, working with architects, analysts and data scientists as needed.
 The incumbent is responsible for the operation and execution of projects related to Big Data or other analytic platforms. Leverages experience in analyzing and delivering large data sets by using a variety of delivery tools to perform tasks.
 Works in cross-functional teams from different organizations (both technical and non-technical) on projects. Provides guidance and education to Seniors and Intermediate level staff. Responsible for maintaining customer relationships. Technologies such as, but not limited to: Hadoop, Hive, NoSQL, Spark, Python, SAS, Teradata, Oracle, Informatica.
 Work closely with IT, architect and engineer solutions that provide views for Enterprise Data Objects or other analytic ecosystems. This would include working with the appropriate teams, leading the design and building out the design, and providing upkeep for the solution. Contribute to creating high performance Big Data (and traditional) systems to be used with analytic applications.
 Code, test, process, and maintain data resources for the analytics organizations. This will include working to maintain data sourcing, transformation and delivery, for key analytic platforms throughout the organization. (ETL/ELT)
 Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. Work with data science teams and strategic partners on capabilities of core platform. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams.
 Contribute to large functional efforts for programs across multiple projects. Manage relationships with customers of the function. Attend meetings with customers on a stand-alone basis or with team as needed.
 Follow standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. Keep current with Big Data technologies in order to recommend best tools in order to perform current and future work.

Required:

 Bachelor's Degree in Computer Systems Analysis, Computer Engineering, Data Processing, Healthcare Informatics or Management Information Systems

Preferred:

 Master's Degree in Management Information Systems, Healthcare Informatics or Computer Engineering .

Experience:Required -

 2 years in Data platform development, data engineering, software development, or data science
 2 years in Big data or cloud data platform

Preferred -

 2 years in the Healthcare Industry
 2 years in Data Warehousing
 2 years in Database Administration

Licenses And Certifications:

 Cloud certification (GCP, Azure, AWS)

Required Skills:

 Demonstrated skills in SQL
 Data Warehousing
 Problem-Solving
 Communication Skills
 Analytical Skills
 Demonstrated skills in Spark or Python or related tool
 Demonstrated skills in Google Cloud Technologies
 Databricks

Job Type: Contract
Salary: $50.00 - $55.00 per hour
Benefits:

 Referral program

Experience level:

 6 years

Schedule:

 8 hour shift

Experience:

 Data science: 5 years (Preferred)
 Big data: 5 years (Required)
 Data warehouse: 5 years (Required)
 Healthcare: 5 years (Required)
 SQL: 5 years (Required)

Work Location: Remote","<p>Post1</p>
<p><b>Job Title: Big Data Engineer</b><br><b>Duration: 6 Months (Contract to hire)</b><br><b>Location: Remote</b></p>
<p><b>Job Opportunity:</b></p>
<ul>
 <li>This job designs and engineers solutions associated with analytic data for the organization and, working closely with the business, analytic and IT teams, assists with the build and upkeep for these solutions. This includes coding data ingestion, transformation, and delivery programs/locic for analysts to access operational, derived, and external data sets.</li>
 <li>Expected deliverables will include; coding of delivery frameworks to load and transform raw source data into enhanced analytic assets, being a key resource for analytical and big data efforts, working with architects, analysts and data scientists as needed.</li>
 <li>The incumbent is responsible for the operation and execution of projects related to Big Data or other analytic platforms. Leverages experience in analyzing and delivering large data sets by using a variety of delivery tools to perform tasks.</li>
 <li>Works in cross-functional teams from different organizations (both technical and non-technical) on projects. Provides guidance and education to Seniors and Intermediate level staff. Responsible for maintaining customer relationships. Technologies such as, but not limited to: Hadoop, Hive, NoSQL, Spark, Python, SAS, Teradata, Oracle, Informatica.</li>
 <li>Work closely with IT, architect and engineer solutions that provide views for Enterprise Data Objects or other analytic ecosystems. This would include working with the appropriate teams, leading the design and building out the design, and providing upkeep for the solution. Contribute to creating high performance Big Data (and traditional) systems to be used with analytic applications.</li>
 <li>Code, test, process, and maintain data resources for the analytics organizations. This will include working to maintain data sourcing, transformation and delivery, for key analytic platforms throughout the organization. (ETL/ELT)</li>
 <li>Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. Work with data science teams and strategic partners on capabilities of core platform. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams.</li>
 <li>Contribute to large functional efforts for programs across multiple projects. Manage relationships with customers of the function. Attend meetings with customers on a stand-alone basis or with team as needed.</li>
 <li>Follow standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. Keep current with Big Data technologies in order to recommend best tools in order to perform current and future work.</li>
</ul>
<p><b>Required:</b></p>
<ul>
 <li>Bachelor&apos;s Degree in Computer Systems Analysis, Computer Engineering, Data Processing, Healthcare Informatics or Management Information Systems</li>
</ul>
<p><b>Preferred:</b></p>
<ul>
 <li>Master&apos;s Degree in Management Information Systems, Healthcare Informatics or Computer Engineering .</li>
</ul>
<p><b>Experience:</b><br><b>Required -</b></p>
<ul>
 <li>2 years in Data platform development, data engineering, software development, or data science</li>
 <li>2 years in Big data or cloud data platform</li>
</ul>
<p><b>Preferred </b>-</p>
<ul>
 <li>2 years in the Healthcare Industry</li>
 <li>2 years in Data Warehousing</li>
 <li>2 years in Database Administration</li>
</ul>
<p><b>Licenses And Certifications:</b></p>
<ul>
 <li>Cloud certification (GCP, Azure, AWS)</li>
</ul>
<p><b>Required Skills:</b></p>
<ul>
 <li>Demonstrated skills in SQL</li>
 <li>Data Warehousing</li>
 <li>Problem-Solving</li>
 <li>Communication Skills</li>
 <li>Analytical Skills</li>
 <li>Demonstrated skills in Spark or Python or related tool</li>
 <li>Demonstrated skills in Google Cloud Technologies</li>
 <li>Databricks</li>
</ul>
<p>Job Type: Contract</p>
<p>Salary: &#x24;50.00 - &#x24;55.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Referral program</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>6 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data science: 5 years (Preferred)</li>
 <li>Big data: 5 years (Required)</li>
 <li>Data warehouse: 5 years (Required)</li>
 <li>Healthcare: 5 years (Required)</li>
 <li>SQL: 5 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,66298b9c49650d80,,Contract,,,Remote,Big Data Engineer,1 day ago,2023-10-17T13:32:35.659Z,2.8,99.0,$50 - $55 an hour,2023-10-18T13:32:35.662Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dtmpfj98iB4C0jJJOWen3Era3IQfJzNZ4PFwBIKpo80CvlYmJYyffHOwy26mz3iNMuQaWfydZNaRB4htGwveySDwsdOtMP0srpXKAxJfXqhZah1I5V6HTeCm_4CjUcUkWhOaQhoZKcFqsUbmE_W_ayfXwg4C-EWaJkU9g2Kfk9G3u7iBrFylhZqkga7RYx9ahLi0QvkLMWN_gRZIwjTjhIeNJ8IlmHnFowBIayWeA93U4Rc7Yf15lcS_KFTVIcCzvdiFRWHQWgi0ytqgMQrajhE_T4WGznOiRaE79uVDEBEEDtNmz5ZWCy0L7uZDh9mrYXciIs2ySd29L-pb9GPtcUlh-1EhGrznwEpoCbmA-K72FOEAwr00CyV4SOGmTVdxZTYK539qXOhuQSpQTNbKhrXpMy4TD0J8ZOQE9iuCwqsaH6lEgeUo0wpAxwlsWNy776Fd0hl_k14YnoxDJI3acxiERcR32PMC1wvkNORwrZjiOV0Ehtx0saHbtstN95PhYj3W7gP6ZcxlQBh3_kcWDqQ4SF_E0ELLY01D_U6Y-U_hsnZOhXSoku7mKn6Qu-DvhxiCU9zolWfrOBvpNxowTcXoRxdGY8TNm-i7sJIyFg_w%3D%3D&xkcb=SoC8-_M3JhXpNEW-cB0LbzkdCdPP&p=0&fvj=1&vjs=3&jsa=9925&tk=1hd1fpnavjm7b800&from=jasx&wvign=1
43,Verizon,"When you join Verizon
  Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect around the world. We’re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together—lifting up our communities and striving to make an impact to move the world forward. If you’re fueled by purpose, and powered by persistence, explore a career with us. Here, you’ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.
 
  What you’ll be doing...
  As a part of our Big Data Product team, the Data Solutions Engineer will be responsible for developing and validating Big data products and applications which runs on the large Hadoop cluster and Cloud. The qualified engineer will be developing and testing ETL process, Migrating different applications to cloud , developing Data validation tools used for performing quality assessments and measurements on different data sets that feed data & AI products.
  Building big data and batch/real-time analytical solutions that leverage emerging technologies.
 
   Performing data migration and conversion activities on different applications and platforms.
   Designing, developing and testing of data ingestion pipelines, perform end to end automation of ETL process for various datasets that are being ingested into the big data platform.
   Performing data profiling/analysis, discovery, analysis, suitability and coverage of data, and identifying the various data types, formats, and data quality issues which exist within a given data source.
   Developing transformation logic, interfaces and reports as needed to meet project requirements.
   Participating in discussion for technical architecture, data modeling and ETL standards, collaborate with Product Managers, Architects and Senior Developers to establish the physical application framework (e.g. libraries, modules, execution environments).
   Improving and performance-tuning the optimization of data pipelines.
   Developing unit and integrated automated test suites to validate end to end data pipeline flow, data transformation rules, and data integrity.
   Developing tools to measure the data quality and visualize the anomaly pattern in source and processed data.
   Integrating automated processes into continuous integration workflows.
   Contributing to data quality assurance standards and procedures.
 
 
  What we’re looking for...
  You’re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems. You thrive in a fast-paced, innovative environment working as a phenomenal teammate to drive the best results and business outcomes.
 
  You'll need to have:
 
   Bachelor's degree or four or more years of work experience.
   Four or more years of relevant work experience.
   Programming experience in Scala, Java or Python
   Experience in data engineering, preferably in Google Cloud Platform with BigQuery
   Hands-on experience in designing, building, testing and deploying data pipelines in Teradata and Hadoop platform with experience in, HDFS, Hive, Spark, Streaming, HBase, Kafka, Oozie etc.
   Good organizational skills and strong written and verbal communication skills.
 
 
  Even better if you have one or more of the following:
 
   Bachelor’s degree in Computer Science or equivalent education/training
   Five or more years of Software development and Testing experience.
   Experience with development and automated testing in a CI/CD environment.
   Hands on experience in dashboard development using Looker/Tableau
   Knowledge of GIT/Jenkins and pipeline automation is a must.
   Experience with developing and testing real-time data-processing and Analytics Application Systems.
   Strong knowledge in SQL development on Database and/or BI/DW
   Strong knowledge in shell scripting Experience in Web Services - API development and testing.
   A solid understanding of common software development practices and tools.
   Strong analytical skills with a methodical approach to problem solving applied to the Big Data/AI domain.
   Experience with incident management tools (Opsgenie, PagerDuty etc) is a plus
 
 
  If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.
 
  This role is eligible to be considered for the Department of Defense SkillBridge Program.
 
 
  
   
    
     
      
       
         Where you’ll be working
       
      
     
    
   
  
  In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
 
  Scheduled Weekly Hours 40
 
  Equal Employment Opportunity
  We’re proud to be an equal opportunity employer - and celebrate our employees’ differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.
  Our benefits are designed to help you move forward in your career, and in areas of your life outside of Verizon. From health and wellness benefits, short term incentives, 401(k) Savings Plan, stock incentive programs, paid time off, parental leave, adoption assistance and tuition assistance, plus other incentives, we’ve got you covered with our award-winning total rewards package. For part-timers, your coverage will vary as you may be eligible for some of these benefits depending on your individual circumstances.
  If you are hired into a California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington work location, the compensation range for this position is between $113,000.00 and $210,000.00 annually based on a full-time schedule. The salary will vary depending on your location and confirmed job-related skills and experience. This is an incentive based position with the potential to earn more. For part time roles, your compensation will be adjusted to reflect your hours.","<div>
 <h3 class=""jobSectionHeader""><b>When you join Verizon</b></h3>
 <p> Verizon is one of the world&#x2019;s leading providers of technology and communications services, transforming the way we connect around the world. We&#x2019;re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together&#x2014;lifting up our communities and striving to make an impact to move the world forward. If you&#x2019;re fueled by purpose, and powered by persistence, explore a career with us. Here, you&#x2019;ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.</p>
 <p></p>
 <h3 class=""jobSectionHeader""><b> What you&#x2019;ll be doing...</b></h3>
 <p> As a part of our Big Data Product team, the Data Solutions Engineer will be responsible for developing and validating Big data products and applications which runs on the large Hadoop cluster and Cloud. The qualified engineer will be developing and testing ETL process, Migrating different applications to cloud , developing Data validation tools used for performing quality assessments and measurements on different data sets that feed data &amp; AI products.</p>
 <p> Building big data and batch/real-time analytical solutions that leverage emerging technologies.</p>
 <ul>
  <li><p> Performing data migration and conversion activities on different applications and platforms.</p></li>
  <li><p> Designing, developing and testing of data ingestion pipelines, perform end to end automation of ETL process for various datasets that are being ingested into the big data platform.</p></li>
  <li><p> Performing data profiling/analysis, discovery, analysis, suitability and coverage of data, and identifying the various data types, formats, and data quality issues which exist within a given data source.</p></li>
  <li><p> Developing transformation logic, interfaces and reports as needed to meet project requirements.</p></li>
  <li><p> Participating in discussion for technical architecture, data modeling and ETL standards, collaborate with Product Managers, Architects and Senior Developers to establish the physical application framework (e.g. libraries, modules, execution environments).</p></li>
  <li><p> Improving and performance-tuning the optimization of data pipelines.</p></li>
  <li><p> Developing unit and integrated automated test suites to validate end to end data pipeline flow, data transformation rules, and data integrity.</p></li>
  <li><p> Developing tools to measure the data quality and visualize the anomaly pattern in source and processed data.</p></li>
  <li><p> Integrating automated processes into continuous integration workflows.</p></li>
  <li><p> Contributing to data quality assurance standards and procedures.</p></li>
 </ul>
 <p></p>
 <h3 class=""jobSectionHeader""><b> What we&#x2019;re looking for...</b></h3>
 <p> You&#x2019;re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems. You thrive in a fast-paced, innovative environment working as a phenomenal teammate to drive the best results and business outcomes.</p>
 <p></p>
 <p> You&apos;ll need to have:</p>
 <ul>
  <li><p> Bachelor&apos;s degree or four or more years of work experience.</p></li>
  <li><p> Four or more years of relevant work experience.</p></li>
  <li><p> Programming experience in Scala, Java or Python</p></li>
  <li><p> Experience in data engineering, preferably in Google Cloud Platform with BigQuery</p></li>
  <li><p> Hands-on experience in designing, building, testing and deploying data pipelines in Teradata and Hadoop platform with experience in, HDFS, Hive, Spark, Streaming, HBase, Kafka, Oozie etc.</p></li>
  <li><p> Good organizational skills and strong written and verbal communication skills.</p></li>
 </ul>
 <p></p>
 <p> Even better if you have one or more of the following:</p>
 <ul>
  <li><p> Bachelor&#x2019;s degree in Computer Science or equivalent education/training</p></li>
  <li><p> Five or more years of Software development and Testing experience.</p></li>
  <li><p> Experience with development and automated testing in a CI/CD environment.</p></li>
  <li><p> Hands on experience in dashboard development using Looker/Tableau</p></li>
  <li><p> Knowledge of GIT/Jenkins and pipeline automation is a must.</p></li>
  <li><p> Experience with developing and testing real-time data-processing and Analytics Application Systems.</p></li>
  <li><p> Strong knowledge in SQL development on Database and/or BI/DW</p></li>
  <li><p> Strong knowledge in shell scripting Experience in Web Services - API development and testing.</p></li>
  <li><p> A solid understanding of common software development practices and tools.</p></li>
  <li><p> Strong analytical skills with a methodical approach to problem solving applied to the Big Data/AI domain.</p></li>
  <li><p> Experience with incident management tools (Opsgenie, PagerDuty etc) is a plus</p></li>
 </ul>
 <p></p>
 <p> If Verizon and this role sound like a fit for you, we encourage you to apply even if you don&#x2019;t meet every &#x201c;even better&#x201d; qualification listed above.</p>
 <p></p>
 <p> This role is eligible to be considered for the Department of Defense SkillBridge Program.</p>
 <p></p>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <h3 class=""jobSectionHeader""><b> Where you&#x2019;ll be working</b></h3>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div> In this hybrid role, you&apos;ll have a defined work location that includes work from home and assigned office days set by your manager.
 <p></p>
 <h3 class=""jobSectionHeader""><b> Scheduled Weekly Hours</b></h3> 40
 <p></p>
 <h3 class=""jobSectionHeader""><b> Equal Employment Opportunity</b></h3>
 <p> We&#x2019;re proud to be an equal opportunity employer - and celebrate our employees&#x2019; differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.</p>
 <p></p> Our benefits are designed to help you move forward in your career, and in areas of your life outside of Verizon. From health and wellness benefits, short term incentives, 401(k) Savings Plan, stock incentive programs, paid time off, parental leave, adoption assistance and tuition assistance, plus other incentives, we&#x2019;ve got you covered with our award-winning total rewards package. For part-timers, your coverage will vary as you may be eligible for some of these benefits depending on your individual circumstances.
 <p></p> If you are hired into a California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington work location, the compensation range for this position is between &#x24;113,000.00 and &#x24;210,000.00 annually based on a full-time schedule. The salary will vary depending on your location and confirmed job-related skills and experience. This is an incentive based position with the potential to earn more. For part time roles, your compensation will be adjusted to reflect your hours.
</div>
<p></p>",https://mycareer.verizon.com/jobs/r-1018721/senior-data-engineer/?source=jb-indeed&dclid=CL2MoeTa_4EDFYbMwAodcIQDxQ,b15ec8b380fddd07,,Full-time,,,"340 Washington St, Boston, MA 02108",Senior Data Engineer,1 day ago,2023-10-17T13:32:36.095Z,3.8,32072.0,"$113,000 - $210,000 a year",2023-10-18T13:32:36.098Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=b15ec8b380fddd07&from=jasx&tk=1hd1fpkd42j42000&vjs=3
44,Reality Defender,"** No firms - we cannot work with firms due to regulatory reasons.**
 
 
 
   Reality Defender seeks a forward deployed data engineer to join the Data Engineering team. You would work on product-oriented data infrastructure development for in-the-wild deepfake media detection, with an emphasis on engaging directly with clients and facilitating communication between technical and non-technical teams.
 
 
 
   #LI-Remote
  
 Responsibilities
 
   Building scalable robust infrastructure for data ingestion, storage, and sampling.
   Communicate complex technical concepts effectively to client executives, ensuring alignment between technical implementations and organizational objectives.
   Develop custom data tools tailored to meet specific client requirements, including adapting internally developed solutions to meet clients' needs.
   Create and maintain comprehensive technical documentation, including APIs, algorithms, and system architecture.
   Provide technical support to resolve complex issues escalated from customer support teams. Collaborate with cross-functional teams to diagnose and troubleshoot production incidents, and report results back to the customer in clear non-technical language.
 
  Requirements
 
   We encourage candidates who may not meet all the specified requirements to still apply. We value diverse perspectives and skills, and believe that unique experiences can contribute significantly to our team. If you are passionate about the role and confident in your ability to make a meaningful impact, we welcome your application. Your enthusiasm, adaptability, and potential for growth are equally important to us. Please use your cover letter to elaborate on how your background and experience make you an ideal fit for this role!
 
 
 
   Required
 
 
   3+ years of professional experience in software/data science and a bachelor's or master's degree in computer science, engineering, math, or STEM discipline.
   Strong communication skills.
   Proficiency in Python, NodeJs, Typescript, with a strong emphasis on adapting scalable software solutions to customer needs.
   Database experience, particularly NoSQL databases (MongoDB, DynamoDB, etc).
 
 
 
   Nice to have
 
 
   Interest in data exploration, visualization, cleaning, and analytics for real-world data modeling.
   Solid understanding of linear algebra, statistics and deep learning concepts.
   Experience working with audio, visual, and/or text datasets and models.
   Experience with AWS, Google Cloud, Azure, and On-Premises.
   Experience working with very large databases and deep learning APIs, including Pandas, PyTorch, PySpark, etc. 
  Highly organized, detail-oriented, and possess a proven ability to thrive under deadline pressure.
 
 
 
   Additional Requirements
 
 
   Willing to work extended hours when needed.
   Willing to occasionally work from or travel to client’s location.","<div>
 <div>
  <b>** No firms - we cannot work with firms due to regulatory reasons.**</b>
 </div>
 <div></div>
 <div>
  <br> Reality Defender seeks a forward deployed data engineer to join the Data Engineering team. You would work on product-oriented data infrastructure development for in-the-wild deepfake media detection, with an emphasis on engaging directly with clients and facilitating communication between technical and non-technical teams.
 </div>
 <div></div>
 <div>
  <br> #LI-Remote
 </div> 
 <h3 class=""jobSectionHeader""><b>Responsibilities</b></h3>
 <ul>
  <li> Building scalable robust infrastructure for data ingestion, storage, and sampling.</li>
  <li> Communicate complex technical concepts effectively to client executives, ensuring alignment between technical implementations and organizational objectives.</li>
  <li> Develop custom data tools tailored to meet specific client requirements, including adapting internally developed solutions to meet clients&apos; needs.</li>
  <li> Create and maintain comprehensive technical documentation, including APIs, algorithms, and system architecture.</li>
  <li> Provide technical support to resolve complex issues escalated from customer support teams. Collaborate with cross-functional teams to diagnose and troubleshoot production incidents, and report results back to the customer in clear non-technical language.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Requirements</b></h3>
 <ul>
  <li> We encourage candidates who may not meet all the specified requirements to still apply. We value diverse perspectives and skills, and believe that unique experiences can contribute significantly to our team. If you are passionate about the role and confident in your ability to make a meaningful impact, we welcome your application. Your enthusiasm, adaptability, and potential for growth are equally important to us. Please use your cover letter to elaborate on how your background and experience make you an ideal fit for this role!</li>
 </ul>
 <div></div>
 <div>
  <b><br> Required</b>
 </div>
 <ul>
  <li> 3+ years of professional experience in software/data science and a bachelor&apos;s or master&apos;s degree in computer science, engineering, math, or STEM discipline.</li>
  <li> Strong communication skills.</li>
  <li> Proficiency in Python, NodeJs, Typescript, with a strong emphasis on adapting scalable software solutions to customer needs.</li>
  <li> Database experience, particularly NoSQL databases (MongoDB, DynamoDB, etc).</li>
 </ul>
 <div></div>
 <div>
  <b><br> Nice to have</b>
 </div>
 <ul>
  <li> Interest in data exploration, visualization, cleaning, and analytics for real-world data modeling.</li>
  <li> Solid understanding of linear algebra, statistics and deep learning concepts.</li>
  <li> Experience working with audio, visual, and/or text datasets and models.</li>
  <li> Experience with AWS, Google Cloud, Azure, and On-Premises.</li>
  <li> Experience working with very large databases and deep learning APIs, including Pandas, PyTorch, PySpark, etc. </li>
  <li>Highly organized, detail-oriented, and possess a proven ability to thrive under deadline pressure.</li>
 </ul>
 <div></div>
 <div>
  <b><br> Additional Requirements</b>
 </div>
 <ul>
  <li> Willing to work extended hours when needed.</li>
  <li> Willing to occasionally work from or travel to client&#x2019;s location.</li>
 </ul>
</div>",https://jobs.lever.co/realitydefender/73112d98-7a9a-4800-9921-6576fdd1f0df?lever-source=Indeed,4eb3da80458c4c7f,,Full-time,,,"New York, NY",Forward Deployed Data Engineer - Remote,1 day ago,2023-10-17T13:32:38.660Z,,,"$100,000 - $200,000 a year",2023-10-18T13:32:38.748Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=4eb3da80458c4c7f&from=jasx&tk=1hd1fpkd42j42000&vjs=3
45,ServiceNow,"Company Description
  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. 
 With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. 
 Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. 
 Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.
  Job Description
  As a Senior Staff Data and Software Engineer, you will be responsible for developing and implementing cutting-edge technical solutions that align with our organization's business objectives. You will work closely with stakeholders to understand their needs, assess existing systems and infrastructure, and design robust and scalable data and mircroservices solutions that drive innovation and efficiency. Your role will require a combination of technical expertise, strategic thinking, and effective communication to bridge the gap between business and technology.
 
  Key Responsibilities:
 
   Solution Design: Collaborate with business leaders, project managers, and technical teams to understand requirements and design holistic technical solutions that address current and future needs.
   Architecture Planning: Develop and maintain technology roadmaps, ensuring alignment with organizational goals and industry best practices.
   Technical Leadership: Provide technical leadership and guidance to development teams, ensuring adherence to architectural standards and best practices.
   Risk Assessment: Identify and evaluate technical risks and propose mitigation strategies to ensure project success and data security.
   Documentation: Create and maintain comprehensive architecture documentation, including diagrams, guidelines, and standards for development teams to follow.
   Vendor Evaluation: Assess and recommend third-party tools, products, and services that can enhance our technical solutions.
   Prototyping: Develop proof-of-concept and prototype solutions to validate architectural decisions and demonstrate feasibility.
   Performance Optimization: Continuously monitor and analyze system performance, identifying areas for improvement and optimizing existing solutions.
   Security and Compliance: Ensure that solutions comply with industry regulations and security standards, and proactively address security vulnerabilities.
   Collaboration: Foster collaboration and effective communication between cross-functional teams, promoting a culture of innovation and excellence.
 
  
  Qualifications
  Qualifications: 
 
  Bachelor's degree in Computer Science, Information Technology, or related field (Master's degree preferred). 
  Proven experience as a Lead Engineer and Solution Architect or a similar role. 
  Strong knowledge of enterprise architecture principles and best practices. 
  Proficiency in designing and implementing solutions using various technologies and platforms. 
  Excellent problem-solving and analytical skills. 
  Outstanding communication and interpersonal abilities. 
  Project management skills and experience in managing complex technical projects. 
  Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus. 
 
 Preferred Skills: 
 
  Cloud computing expertise (e.g., AWS, Azure, Google Cloud Platform). 
  Knowledge of DevOps practices and tools. 
  Familiarity with microservices architecture, expertise a plus. 
  Familiarity with graph databases, expertise a plus. 
  Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes). 
  Strong understanding of data architecture and database technologies. 
  Knowledge of cybersecurity best practices. 
  Excellent presentation and facilitation skills. 
 For positions in the Bay Area, we offer a base pay of $184,700 - $323,300, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.
  
 Additional Information
  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. 
 At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. 
 If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. 
 For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. 
 Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.
  
 From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license. 
 Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.","<div>
 <b>Company Description</b>
 <p><br> At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can&#x2019;t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.</p> 
 <p>With more than 7,700+ customers, we serve approximately 85% of the Fortune 500&#xae;, and we&apos;re proud to be one of FORTUNE 100 Best Companies to Work For&#xae; and World&apos;s Most Admired Companies&#x2122;.</p> 
 <p>Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.</p> 
 <p>Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.</p>
 <b><br> Job Description</b>
 <p><br> As a Senior Staff Data and Software Engineer, you will be responsible for developing and implementing cutting-edge technical solutions that align with our organization&apos;s business objectives. You will work closely with stakeholders to understand their needs, assess existing systems and infrastructure, and design robust and scalable data and mircroservices solutions that drive innovation and efficiency. Your role will require a combination of technical expertise, strategic thinking, and effective communication to bridge the gap between business and technology.</p>
 <p></p>
 <p><b><br> Key Responsibilities:</b></p>
 <ul>
  <li><b> Solution Design:</b> Collaborate with business leaders, project managers, and technical teams to understand requirements and design holistic technical solutions that address current and future needs.</li>
  <li><b> Architecture Planning:</b> Develop and maintain technology roadmaps, ensuring alignment with organizational goals and industry best practices.</li>
  <li><b> Technical Leadership:</b> Provide technical leadership and guidance to development teams, ensuring adherence to architectural standards and best practices.</li>
  <li><b> Risk Assessment:</b> Identify and evaluate technical risks and propose mitigation strategies to ensure project success and data security.</li>
  <li><b> Documentation:</b> Create and maintain comprehensive architecture documentation, including diagrams, guidelines, and standards for development teams to follow.</li>
  <li><b> Vendor Evaluation:</b> Assess and recommend third-party tools, products, and services that can enhance our technical solutions.</li>
  <li><b> Prototyping:</b> Develop proof-of-concept and prototype solutions to validate architectural decisions and demonstrate feasibility.</li>
  <li><b> Performance Optimization:</b> Continuously monitor and analyze system performance, identifying areas for improvement and optimizing existing solutions.</li>
  <li><b> Security and Compliance:</b> Ensure that solutions comply with industry regulations and security standards, and proactively address security vulnerabilities.</li>
  <li><b> Collaboration:</b> Foster collaboration and effective communication between cross-functional teams, promoting a culture of innovation and excellence.</li>
 </ul>
 <br> 
 <b> Qualifications</b>
 <p><b><br> Qualifications:</b></p> 
 <ul>
  <li>Bachelor&apos;s degree in Computer Science, Information Technology, or related field (Master&apos;s degree preferred).</li> 
  <li>Proven experience as a Lead Engineer and Solution Architect or a similar role.</li> 
  <li>Strong knowledge of enterprise architecture principles and best practices.</li> 
  <li>Proficiency in designing and implementing solutions using various technologies and platforms.</li> 
  <li>Excellent problem-solving and analytical skills.</li> 
  <li>Outstanding communication and interpersonal abilities.</li> 
  <li>Project management skills and experience in managing complex technical projects.</li> 
  <li>Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus.</li> 
 </ul>
 <p><b>Preferred Skills:</b></p> 
 <ul>
  <li>Cloud computing expertise (e.g., AWS, Azure, Google Cloud Platform).</li> 
  <li>Knowledge of DevOps practices and tools.</li> 
  <li>Familiarity with microservices architecture, expertise a plus.</li> 
  <li>Familiarity with graph databases, expertise a plus.</li> 
  <li>Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes).</li> 
  <li>Strong understanding of data architecture and database technologies.</li> 
  <li>Knowledge of cybersecurity best practices.</li> 
  <li>Excellent presentation and facilitation skills.</li> 
 </ul>For positions in the Bay Area, we offer a base pay of &#x24;184,700 - &#x24;323,300, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.
 <br> 
 <b>Additional Information</b>
 <p><br> ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.</p> 
 <p>At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.</p> 
 <p>If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.</p> 
 <p>For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.</p> 
 <p>Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.</p>
 <p><br> </p>
 <p>From Fortune. &#xa9; 2022 Fortune Media IP Limited All rights reserved. Used under license.</p> 
 <p>Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.</p>
</div>",https://careers.servicenow.com/careers/jobs/743999937606270EXT?lang=en-us&trid=35ab2906-b356-4a56-8472-f60d30d2e2f0,c9f696550a62823e,,Full-time,,,"Chicago, IL 60607",Senior Staff Data and Services Software Engineer,1 day ago,2023-10-17T13:32:41.640Z,3.7,239.0,"$184,700 - $323,300 a year",2023-10-18T13:32:41.642Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=c9f696550a62823e&from=jasx&tk=1hd1fpkd42j42000&vjs=3
46,LaTronic Solutions LLC,"DATA ENGINEER
***Candidate Must have an Active DoD Secret clearance***
Responsible for the engineering of big data solutions and multi-tiered data environments. Experience with large scale big data deployments in Government and large commercial environments preferred.
Key requirements include:

 Leads initiatives utilizing big data solutions to provide actionable insights for addressing strategic and tactical mission objectives.
 Experience with Hadoop-based technologies (Cloudera, Hortonworks, MapReduce, Hive, HDFS).
 Experience with NoSQL technologies (e.g., MongoDB, Cassandra) (d) Ability to design data models using Enterprise Data modeling tools (ERWin, Visio).
 Builds high-performance algorithms, prototypes, and data models using required programing languages (e.g., Python, C/C++, Java, Perl, Scala).
 Analyzes and develops data set processes for data ingestion, modeling, mining. Experience integrating Big Data solutions with SAP technologies.

Minimum Experience:
Senior Level: 5+ years of relevant experience.
Must be experienced in the following or equivalent technologies:

 Python
 Flask, FastAPI, or other microweb framework equivalent
 An Object Relational Mapping toolkit like SQLAlchemy or equivalent
 Github
 Data analysis libraries such as Pandas.
 AWS S3 - Used as code triggers and intermediate result storage
 CloudTrail - Used to analyze logs
 Lambda - Used as serverless compute for small functions
 ECS Fargate - Used as serverless compute for large functions. Infrastructure:
 Terraform - Used to manage AWS infrastructure.

Minimum Education:
Undergraduate degree required.
Required:
Must have DoD Secret clearance
Must possess IT-II security clearance or have a current National Agency Check with Local Agency Check and Credit Check (NACLC
Job Types: Full-time, Contract
Pay: From $130,000.00 per year
Compensation package:

 1099 contract

Experience level:

 5 years

Schedule:

 8 hour shift

Experience:

 ETL: 4 years (Preferred)
 Big data: 4 years (Preferred)
 Data science: 4 years (Preferred)

Security clearance:

 Secret (Required)

Ability to Commute:

 Remote (Preferred)

Work Location: Remote","<p><b>DATA ENGINEER</b></p>
<p><b>***Candidate Must have an Active DoD Secret clearance***</b></p>
<p>Responsible for the engineering of big data solutions and multi-tiered data environments. Experience with large scale big data deployments in Government and large commercial environments preferred.</p>
<p>Key requirements include:</p>
<ul>
 <li>Leads initiatives utilizing big data solutions to provide actionable insights for addressing strategic and tactical mission objectives.</li>
 <li>Experience with Hadoop-based technologies (Cloudera, Hortonworks, MapReduce, Hive, HDFS).</li>
 <li>Experience with NoSQL technologies (e.g., MongoDB, Cassandra) (d) Ability to design data models using Enterprise Data modeling tools (ERWin, Visio).</li>
 <li>Builds high-performance algorithms, prototypes, and data models using required programing languages (e.g., Python, C/C++, Java, Perl, Scala).</li>
 <li>Analyzes and develops data set processes for data ingestion, modeling, mining. Experience integrating Big Data solutions with SAP technologies.</li>
</ul>
<p>Minimum Experience:</p>
<p>Senior Level: 5+ years of relevant experience.</p>
<p>Must be experienced in the following or equivalent technologies:</p>
<ul>
 <li>Python</li>
 <li>Flask, FastAPI, or other microweb framework equivalent</li>
 <li>An Object Relational Mapping toolkit like SQLAlchemy or equivalent</li>
 <li>Github</li>
 <li>Data analysis libraries such as Pandas.</li>
 <li>AWS S3 - Used as code triggers and intermediate result storage</li>
 <li>CloudTrail - Used to analyze logs</li>
 <li>Lambda - Used as serverless compute for small functions</li>
 <li>ECS Fargate - Used as serverless compute for large functions. Infrastructure:</li>
 <li>Terraform - Used to manage AWS infrastructure.</li>
</ul>
<p>Minimum Education:</p>
<p>Undergraduate degree required.</p>
<p>Required:</p>
<p>Must have DoD Secret clearance</p>
<p>Must possess IT-II security clearance or have a current National Agency Check with Local Agency Check and Credit Check (NACLC</p>
<p>Job Types: Full-time, Contract</p>
<p>Pay: From &#x24;130,000.00 per year</p>
<p>Compensation package:</p>
<ul>
 <li>1099 contract</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>ETL: 4 years (Preferred)</li>
 <li>Big data: 4 years (Preferred)</li>
 <li>Data science: 4 years (Preferred)</li>
</ul>
<p>Security clearance:</p>
<ul>
 <li>Secret (Required)</li>
</ul>
<p>Ability to Commute:</p>
<ul>
 <li>Remote (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,d9235c7fdf0bf5f3,,Full-time,Contract,,Remote,Data Engineer,4 days ago,2023-10-14T13:32:55.697Z,,,"From $130,000 a year",2023-10-18T13:32:55.700Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d9235c7fdf0bf5f3&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
47,INTEL,"Job Description
  Intel is on a multi-year transformational journey to unlock the full potential of enterprise data to strengthen and grow the business. The Core Data Solutions (CDS) group within the Intel IT business segment is chartered with delivering and maintaining world-class data and analytics capabilities that equip the business to capture value through consistent, accurate, reliable, and high-quality data and cutting-edge data, reporting, and analytical solutions.
 
  Key focus areas for the Data and Analytics Group include: maturing enterprise data governance, leveraging master data management, establishing an enterprise data foundation, and delivering strategic data assets and insights.
 
  We are seeking a passionate senior MDM software engineer to focus on designing Enterprise Master Data systems using Reltio, SAP and middle layer technologies consistent with the overall design of the organization's information systems architecture including:
 
 
   Collecting and analyzing information from users to formulate the scope and objectives of the system
   Preparing flow charts, models, and procedures and conducting feasibility studies to design possible system solutions
   Preparing and maintaining technical documentation to guide system users and to assist with the ongoing operation, maintenance, and development of the system
   Leverages Agile methodology to design and configure solutions. Works closely with the business to develop required training, change management, and operations processes
   Develop use cases, customer scenarios, and-or demos, plan and coordinate testing of the newly developed or enhanced applications and provide support
   Looks for opportunities to automate business processes with technology, and may provide consultation to users in the area of automated systems
   Information transformation engagements
   Presenting technical or new concepts across multiple levels of a global organization
   Working with and collaborating with all IT and biz teams toward developing a best practice data creation maintenance governance quality and efficiencies
 
  Qualifications
  You must possess the below minimum qualifications to be initially considered for this position. Preferred qualifications are in addition to the minimum requirements and are considered a plus factor in identifying top candidates.
 
  Minimum Qualifications
 
  The candidate must have a Bachelor's Degree in Computer Science, Information Management, or any other related field with 6+ years of experience -OR- a Master's Degree in Computer Science, Information Management, or any other related field and 4+ years of experience -OR- a PhD in Computer Science, Information Management, or any other related field and 2+ years of experience in: 
 
  System configuration and / or development on Cloud MDM, Reltio or SAP
   SaaS, Cloud Software, Software Configuration, Software Development, SAP MDG, Master Data Management
 
 
  Preferred Qualifications
 
   Managing or executing data cleansing data mapping and data governance areas as well as integration across complex ERP landscapes
   Assisting in developing normalizing and maintaining data standards and definitions.
   Developing processes tools and integration for Master Data Processes including data cleansing and data validation
   Mapping of Master data and integration from the legacy environment to the target environment
   Configuring and managing MDM entities for one or more master data domain
   Denodo and / or Mulesoft
 
  Inside this Business Group
  Intel's Information Technology Group (IT) designs, deploys and supports the information technology architecture and hardware/software applications for Intel. This includes the LAN, WAN, telephony, data centers, client PCs, backup and restore, and enterprise applications. IT is also responsible for e-Commerce development, data hosting and delivery of Web content and services.
  
  Covid Statement
  Intel strongly encourages employees to be vaccinated against COVID-19. Intel aligns to federal, state, and local laws and as a contractor to the U.S. Government is subject to government mandates that may be issued. Intel policies for COVID-19 including guidance about testing and vaccination are subject to change over time.
  
  Posting Statement
  All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.
  
  Benefits
  We offer a total compensation package that ranks among the best in the industry. It consists of competitive pay, stock, bonuses, as well as, benefit programs which include health, retirement, and vacation. Find more information about all of our Amazing Benefits here: https://www.intel.com/content/www/us/en/jobs/benefits.html
  
  Annual Salary Range for jobs which could be performed in US, Colorado, New York, Washington, California: $139,480.00-$209,760.00
  
 
  Salary range dependent on a number of factors including location and experience
 
  Working Model
  This role is available as a fully home-based and generally would require you to attend Intel sites only occasionally based on business need. This role may also be available as our hybrid work model which allows employees to split their time between working on-site at their assigned Intel site and off-site. 
 In certain circumstances the work model may change to accommodate business needs.
  JobType 
 Fully Remote","<p></p>
<div>
 <h2 class=""jobSectionHeader""><b>Job Description</b></h2>
 <p><br> Intel is on a multi-year transformational journey to unlock the full potential of enterprise data to strengthen and grow the business. The Core Data Solutions (CDS) group within the Intel IT business segment is chartered with delivering and maintaining world-class data and analytics capabilities that equip the business to capture value through consistent, accurate, reliable, and high-quality data and cutting-edge data, reporting, and analytical solutions.</p>
 <p></p>
 <p> Key focus areas for the Data and Analytics Group include: maturing enterprise data governance, leveraging master data management, establishing an enterprise data foundation, and delivering strategic data assets and insights.</p>
 <p></p>
 <p> We are seeking a passionate senior MDM software engineer to focus on designing Enterprise Master Data systems using Reltio, SAP and middle layer technologies consistent with the overall design of the organization&apos;s information systems architecture including:</p>
 <p></p>
 <ul>
  <li><p> Collecting and analyzing information from users to formulate the scope and objectives of the system</p></li>
  <li><p> Preparing flow charts, models, and procedures and conducting feasibility studies to design possible system solutions</p></li>
  <li><p> Preparing and maintaining technical documentation to guide system users and to assist with the ongoing operation, maintenance, and development of the system</p></li>
  <li><p> Leverages Agile methodology to design and configure solutions. Works closely with the business to develop required training, change management, and operations processes</p></li>
  <li><p> Develop use cases, customer scenarios, and-or demos, plan and coordinate testing of the newly developed or enhanced applications and provide support</p></li>
  <li><p> Looks for opportunities to automate business processes with technology, and may provide consultation to users in the area of automated systems</p></li>
  <li><p> Information transformation engagements</p></li>
  <li><p> Presenting technical or new concepts across multiple levels of a global organization</p></li>
  <li><p> Working with and collaborating with all IT and biz teams toward developing a best practice data creation maintenance governance quality and efficiencies</p></li>
 </ul>
 <h2 class=""jobSectionHeader""><b><br> Qualifications</b></h2>
 <p><br> You must possess the below minimum qualifications to be initially considered for this position. Preferred qualifications are in addition to the minimum requirements and are considered a plus factor in identifying top candidates.</p>
 <p></p>
 <p><b><br> Minimum Qualifications</b></p>
 <p></p>
 <p><br> The candidate must have a Bachelor&apos;s Degree in Computer Science, Information Management, or any other related field with 6+ years of experience -OR- a Master&apos;s Degree in Computer Science, Information Management, or any other related field and 4+ years of experience -OR- a PhD in Computer Science, Information Management, or any other related field and 2+ years of experience in: </p>
 <ul>
  <li><p>System configuration and / or development on Cloud MDM, Reltio or SAP</p></li>
  <li><p> SaaS, Cloud Software, Software Configuration, Software Development, SAP MDG, Master Data Management</p></li>
 </ul>
 <p></p>
 <p><b> Preferred Qualifications</b></p>
 <ul>
  <li><p> Managing or executing data cleansing data mapping and data governance areas as well as integration across complex ERP landscapes</p></li>
  <li><p> Assisting in developing normalizing and maintaining data standards and definitions.</p></li>
  <li><p> Developing processes tools and integration for Master Data Processes including data cleansing and data validation</p></li>
  <li><p> Mapping of Master data and integration from the legacy environment to the target environment</p></li>
  <li><p> Configuring and managing MDM entities for one or more master data domain</p></li>
  <li><p> Denodo and / or Mulesoft</p></li>
 </ul>
 <h2 class=""jobSectionHeader""><b><br> Inside this Business Group</b></h2>
 <br> Intel&apos;s Information Technology Group (IT) designs, deploys and supports the information technology architecture and hardware/software applications for Intel. This includes the LAN, WAN, telephony, data centers, client PCs, backup and restore, and enterprise applications. IT is also responsible for e-Commerce development, data hosting and delivery of Web content and services.
 <br> 
 <h2 class=""jobSectionHeader""><b> Covid Statement</b></h2>
 <br> Intel strongly encourages employees to be vaccinated against COVID-19. Intel aligns to federal, state, and local laws and as a contractor to the U.S. Government is subject to government mandates that may be issued. Intel policies for COVID-19 including guidance about testing and vaccination are subject to change over time.
 <br> 
 <h2 class=""jobSectionHeader""><b> Posting Statement</b></h2>
 <br> All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.
 <br> 
 <h2 class=""jobSectionHeader""><b> Benefits</b></h2>
 <br> We offer a total compensation package that ranks among the best in the industry. It consists of competitive pay, stock, bonuses, as well as, benefit programs which include health, retirement, and vacation. Find more information about all of our Amazing Benefits here: https://www.intel.com/content/www/us/en/jobs/benefits.html
 <br> 
 <br> Annual Salary Range for jobs which could be performed in US, Colorado, New York, Washington, California: &#x24;139,480.00-&#x24;209,760.00
 <br> 
 <ul>
  <li>Salary range dependent on a number of factors including location and experience</li>
 </ul>
 <h2 class=""jobSectionHeader""><b><br> Working Model</b></h2>
 <br> This role is available as a fully home-based and generally would require you to attend Intel sites only occasionally based on business need. This role may also be available as our hybrid work model which allows employees to split their time between working on-site at their assigned Intel site and off-site. 
 <b>In certain circumstances the work model may change to accommodate business needs.</b>
 <p><br> JobType </p>
 <p>Fully Remote</p>
</div>",https://jobs.intel.com/en/job/-/-/599/55687681968,705fe8793dda2171,,Full-time,,,Remote,Customer and Supplier Master Data Software Engineer,4 days ago,2023-10-14T13:32:52.075Z,4.1,6140.0,"$139,480 - $209,760 a year",2023-10-18T13:32:52.078Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=705fe8793dda2171&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
48,Epsilon Inc.,"Big Data Engineer   Who is Epsilon:  Epsilon is an IT Services company that was founded in 2009 and has become an established leader in providing Information Technology services to both Federal Government and Commercial businesses across the United States. Epsilon is known for its solution-focused and innovative approach, aligning technology systems, tools, and processes with the missions and objectives of its customers.  Epsilon’s headquarters are in Weaverville, NC with other corporate offices in Greenville, SC, Crystal City, VA, and Denver, CO. We have employees in 30+ States across the U.S.  Why work for Epsilon: In joining Epsilon’s team, you will have the opportunity to contribute to Epsilon’s business and customer initiatives, as well as influence our brand culture through people interaction and technology advancements.  Epsilon invests in our employees by promoting from within and enabling employees to elevate their knowledge and skill set in their profession by allocating $3,000 annually in Professional Development funds. We also offer competitive pay, comprehensive benefits through one of the largest national carriers, Paid Time Off (PTO) that increases with tenure and has a generous rollover, 11 company paid Holidays, and 401(k) with immediate contribution.  Where you’ll work: This fully remote opportunity allows you the flexibility to work from home in support of the USDA ENS program.  Our Customer’s Mission: At USDA Enterprise Network Services (ENS), our customers mission is to drive the modernization of telecommunication systems throughout the USDA, redefining the delivery of services, enhancing capabilities, and evolving into a world-class digital and business intelligence organization. ENS is committed to building strategic partnerships that connect the USDA with the global community, while providing adaptable solutions for the strategic delivery of support services. ENS ultimate goal is to deliver high-quality results that support the ongoing IT modernization efforts within the USDA, paving the way for a more efficient and innovative future.  An average day:  As Big Data Engineer, you will develop Agile environments leveraging advanced engineering practices to deliver architecturally-aligned, scalable solutions. You will develop generic data frameworks and data products that maintain the highest availability, performance, and strive for simplicity. Additionally, you will implement data engineering solutions using: AWS, Spark, Scala, Python, Airflow, EMR, Redshift, Athena, Snowflake, ECS, DevOps Automation, Integration, Docker, Build and Deployment Tools. In this position you will: 
 
  Create data warehouse architecture and associated diagrams and maintain the data, in collaboration with SMVB, to support the approval process and documentation requirements for the ENS Data Warehouse and associated dashboards. 
  Deliver a business requirements document (BRD) for each request business requirements document includes planning and documenting the government’s requirements of BI&A technical work, meeting with subject matter experts to determine business justification, project risks, constraints, assumptions, dependencies, estimated completion time, a list of user stories and acceptance criteria, and architecture diagrams. 
  Analyze and document efficiency improvements on problematic, long running data warehouse queries, to ensure improvement to query return time. 
  Shall implement cost effective, efficient, and innovative methods to migrate the existing data analytics platform to any newly identified platform [such as Amazon Web Services (AWS)]. 
  Develop and deliver a plan of action and milestones (POAM) to migrate existing siloed systems to a software as a service cloud architecture, in collaboration with SMVB. 
  Migrate or implement the services/system as described in any applicable federally approved POAM. 
  Configure BI&A continuous monitoring / report systems that track all events such as errors, ingestion / batch run reports, and unit / integration test results in a single location. 
  Maintain separate development and production environments for ETL and other analytics development and ensure the two environments are as similar as possible, as well as documenting a process in the Team Wiki (currently Confluence) for proper separation of development lifecycle activities. 
  Develop and integrate government-approved Extract, Transform, and Load (ETL) data engineering pipelines. 
  Create, execute, and document technical test plans consisting of test cases describing steps taken and what is being tested. Incorporate automated testing processes, as applicable, to eliminate manual steps. 
  Perform data governance functions to assess quality and risk; Track data steward roles over specific data sets of interest, Maintenance of metadata in the Enterprise Data Catalogue (Informatica), Assign data stewards to meet with internal customers to assess quality standards in technical work (ETL, Dashboards), Track all of ENS' data sources in a collaborative ENS-wide data source tracking list, Assess risks on data sources, Assess quality controls on data sets (such as formats for columns known only to product owners), Consider the appropriate level of technical testing required per the risk assessment. 
  Create user acceptance test plans and cases for user acceptance testing for each developed product and each feature of the developed product. 
  Create knowledge base articles and store the articles on the BI&A Team wiki (currently Confluence) to capture repeatable processes and SOPs related to BI&A development. 
  Develop, test, and integrate data visualizations (such as dashboards) using approved industry standard tools. 
  Maintain ENS BI&A’s version control (currently Bitbucket/git); Perform administration of the repositories using git, Host weekly or ad-hoc code review/pull request meetings. 
  Participate in planning and execution of tasks, in collaboration with SMVB and ENS’ teams, to develop BI&A capabilities and accommodate evolving requirements. 
  Administer Alteryx Server, Tableau Server, and analytics tools used in Amazon Web Services (AWS), to include the following tasks: Provide user administration, Create usage and performance monitoring, Schedule ETL batch runs and keep current. 
  Compile and deliver a quarterly report of the data product that uses data science methodology to identify problems and potential solutions to problems, to provide improvement opportunities to ENS. 
  Maintain an inventory listing of the BI&A tool licenses (currently Alteryx and Tableau) across ENS and its branches to track usage trends, forecast future licensing needs and ensure ENS does not overprovision licenses. 
 
 Basic Qualifications: 
 
  As a requirement of this position, all candidates must be a U.S. Citizen. In accordance with 8 U.S.C. 1324b(a)(2)(C), Epsilon will not consider candidates for this position who do not meet the aforementioned conditions. 
  Strong written and oral communication skills in the English language. All contractor employees must be able to read, write, speak, and understand English. 
  Contractor personnel performing in a leadership capacity shall be capable of directing contractor personnel and interfacing with the Government and customers. 
  Exceptional customer service skills. 
  Strong time-management and prioritization skills. 
  Ability to communicate applicable technical subject matter expertise to management and others. 
  ITIL v4 foundation knowledge 
  Ability to apply and provide feedback on service operation model and practices. 
 
 Other Requirements: 
 
  Must be able to pass federal background investigation and obtain a Public Trust
 
   
 Epsilon is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applications will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. EEO/AA: Minorities/Females/Disabled/Vets.  Please click here to review your rights under EEO policy.  If you are an individual with a disability and need special assistance or reasonable accommodation in applying for employment with Epsilon, Inc., please contact our Recruiting department by phone 828-398-5414 or by email careers@epsilon-inc.com.
    #LI-DNI","<div>
 <p><b>Big Data Engineer </b><br> <br> <b>Who is Epsilon: </b><br> Epsilon is an IT Services company that was founded in 2009 and has become an established leader in providing Information Technology services to both Federal Government and Commercial businesses across the United States. Epsilon is known for its solution-focused and innovative approach, aligning technology systems, tools, and processes with the missions and objectives of its customers.<br> <br> Epsilon&#x2019;s headquarters are in Weaverville, NC with other corporate offices in Greenville, SC, Crystal City, VA, and Denver, CO. We have employees in 30+ States across the U.S.<br> <br> <b>Why work for Epsilon:</b><br> In joining Epsilon&#x2019;s team, you will have the opportunity to contribute to Epsilon&#x2019;s business and customer initiatives, as well as influence our brand culture through people interaction and technology advancements.<br> <br> Epsilon invests in our employees by promoting from within and enabling employees to elevate their knowledge and skill set in their profession by allocating &#x24;3,000 annually in Professional Development funds. We also offer competitive pay, comprehensive benefits through one of the largest national carriers, Paid Time Off (PTO) that increases with tenure and has a generous rollover, 11 company paid Holidays, and 401(k) with immediate contribution.<br> <br> <b>Where you&#x2019;ll work:</b><br> This fully remote opportunity allows you the flexibility to work from home in support of the USDA ENS program.<br> <br> <b>Our Customer&#x2019;s Mission</b>:<br> At USDA Enterprise Network Services (ENS), our customers mission is to drive the modernization of telecommunication systems throughout the USDA, redefining the delivery of services, enhancing capabilities, and evolving into a world-class digital and business intelligence organization. ENS is committed to building strategic partnerships that connect the USDA with the global community, while providing adaptable solutions for the strategic delivery of support services. ENS ultimate goal is to deliver high-quality results that support the ongoing IT modernization efforts within the USDA, paving the way for a more efficient and innovative future.<br> <br> <b>An average day: </b><br> As Big Data Engineer, you will develop Agile environments leveraging advanced engineering practices to deliver architecturally-aligned, scalable solutions. You will develop generic data frameworks and data products that maintain the highest availability, performance, and strive for simplicity. Additionally, you will implement data engineering solutions using: AWS, Spark, Scala, Python, Airflow, EMR, Redshift, Athena, Snowflake, ECS, DevOps Automation, Integration, Docker, Build and Deployment Tools. In this position you will:</p> 
 <ul>
  <li>Create data warehouse architecture and associated diagrams and maintain the data, in collaboration with SMVB, to support the approval process and documentation requirements for the ENS Data Warehouse and associated dashboards.</li> 
  <li>Deliver a business requirements document (BRD) for each request business requirements document includes planning and documenting the government&#x2019;s requirements of BI&amp;A technical work, meeting with subject matter experts to determine business justification, project risks, constraints, assumptions, dependencies, estimated completion time, a list of user stories and acceptance criteria, and architecture diagrams.</li> 
  <li>Analyze and document efficiency improvements on problematic, long running data warehouse queries, to ensure improvement to query return time. </li>
  <li>Shall implement cost effective, efficient, and innovative methods to migrate the existing data analytics platform to any newly identified platform [such as Amazon Web Services (AWS)].</li> 
  <li>Develop and deliver a plan of action and milestones (POAM) to migrate existing siloed systems to a software as a service cloud architecture, in collaboration with SMVB.</li> 
  <li>Migrate or implement the services/system as described in any applicable federally approved POAM.</li> 
  <li>Configure BI&amp;A continuous monitoring / report systems that track all events such as errors, ingestion / batch run reports, and unit / integration test results in a single location.</li> 
  <li>Maintain separate development and production environments for ETL and other analytics development and ensure the two environments are as similar as possible, as well as documenting a process in the Team Wiki (currently Confluence) for proper separation of development lifecycle activities.</li> 
  <li>Develop and integrate government-approved Extract, Transform, and Load (ETL) data engineering pipelines.</li> 
  <li>Create, execute, and document technical test plans consisting of test cases describing steps taken and what is being tested. Incorporate automated testing processes, as applicable, to eliminate manual steps.</li> 
  <li>Perform data governance functions to assess quality and risk; Track data steward roles over specific data sets of interest, Maintenance of metadata in the Enterprise Data Catalogue (Informatica), Assign data stewards to meet with internal customers to assess quality standards in technical work (ETL, Dashboards), Track all of ENS&apos; data sources in a collaborative ENS-wide data source tracking list, Assess risks on data sources, Assess quality controls on data sets (such as formats for columns known only to product owners), Consider the appropriate level of technical testing required per the risk assessment.</li> 
  <li>Create user acceptance test plans and cases for user acceptance testing for each developed product and each feature of the developed product.</li> 
  <li>Create knowledge base articles and store the articles on the BI&amp;A Team wiki (currently Confluence) to capture repeatable processes and SOPs related to BI&amp;A development.</li> 
  <li>Develop, test, and integrate data visualizations (such as dashboards) using approved industry standard tools.</li> 
  <li>Maintain ENS BI&amp;A&#x2019;s version control (currently Bitbucket/git); Perform administration of the repositories using git, Host weekly or ad-hoc code review/pull request meetings.</li> 
  <li>Participate in planning and execution of tasks, in collaboration with SMVB and ENS&#x2019; teams, to develop BI&amp;A capabilities and accommodate evolving requirements.</li> 
  <li>Administer Alteryx Server, Tableau Server, and analytics tools used in Amazon Web Services (AWS), to include the following tasks: Provide user administration, Create usage and performance monitoring, Schedule ETL batch runs and keep current.</li> 
  <li>Compile and deliver a quarterly report of the data product that uses data science methodology to identify problems and potential solutions to problems, to provide improvement opportunities to ENS.</li> 
  <li>Maintain an inventory listing of the BI&amp;A tool licenses (currently Alteryx and Tableau) across ENS and its branches to track usage trends, forecast future licensing needs and ensure ENS does not overprovision licenses.</li> 
 </ul>
 <b>Basic Qualifications: </b>
 <ul>
  <li>As a requirement of this position, all candidates must be a U.S. Citizen. In accordance with 8 U.S.C. 1324b(a)(2)(C), Epsilon will not consider candidates for this position who do not meet the aforementioned conditions.</li> 
  <li>Strong written and oral communication skills in the English language. All contractor employees must be able to read, write, speak, and understand English.</li> 
  <li>Contractor personnel performing in a leadership capacity shall be capable of directing contractor personnel and interfacing with the Government and customers.</li> 
  <li>Exceptional customer service skills.</li> 
  <li>Strong time-management and prioritization skills.</li> 
  <li>Ability to communicate applicable technical subject matter expertise to management and others.</li> 
  <li>ITIL v4 foundation knowledge</li> 
  <li>Ability to apply and provide feedback on service operation model and practices.</li> 
 </ul>
 <b>Other Requirements: </b>
 <ul>
  <li>Must be able to pass federal background investigation and obtain a Public Trust</li>
 </ul>
 <p><br> <br> </p>
 <p>Epsilon is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applications will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. EEO/AA: Minorities/Females/Disabled/Vets.<br> <br> Please click here to review your rights under EEO policy.<br> <br> If you are an individual with a disability and need special assistance or reasonable accommodation in applying for employment with Epsilon, Inc., please contact our Recruiting department by phone 828-398-5414 or by email careers@epsilon-inc.com.</p>
 <p><br> <br> <br> #LI-DNI</p>
</div>",https://www2.jobdiva.com/portal/?a=etjdnw1kw4ry751omqcryv2jkphzlh03e5juym948sy1kv7hh74xv9c2ccsuyfmh&jobid=20657059#/jobs/20657059?compid=0&SearchString=&StatesString=&id=20657059&source=indeed.com,979d7c241a393d79,,,,,"363 W Drake Rd Ste 5, Fort Collins, CO 80526",Big Data Engineer,4 days ago,2023-10-14T13:32:54.594Z,4.2,10.0,"$110,000 - $125,000 a year",2023-10-18T13:32:54.599Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=979d7c241a393d79&from=jasx&tk=1hd1fq96gk6pu800&vjs=3
49,Medidata Solutions,"Requisition ID 
   533295
  
 
 
  
    
  
 
 
  Medidata: Powering Smarter Treatments and Healthier People 
   Medidata, a Dassault Systèmes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world's most trusted platform for clinical development, commercial, and real-world data. Known for its ground-breaking technological innovations, Medidata has supported more than 30,000 clinical trials and 9 million study participants. And Medidata’s ongoing commitment to infusing the patient voice into trial designs and solutions is helping to create a better and more inclusive experience for all participants in clinical studies. Medidata is involved in nearly 40% of company-initiated trial starts globally, with studies conducted in more than 140 countries. More than 70% of novel drugs approved by the Food and Drug Administration (FDA) in 2022 were developed with Medidata software. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us @medidata. 
   Our Team: 
   Medidata is looking for individuals who will help us tackle some of the most complex questions facing the industry today using our proprietary platform and advanced analytics. At Medidata, we never work alone. This role will partner heavily with all of the key stakeholder functions including product, delivery, data science, engineering, partnerships, and biostatistics. Successful Medidata AI candidates will be skilled in analytical/quantitative thinking, structured communication, and excited about building the next horizon of Medidata’s mission to power smarter treatments and healthier people. 
   Who We're Looking For: 
   
   Advanced skills in modern data architecture, data science engineering, data modeling and data quality using state-of-art cloud computing technologies (AWS). 
   Hands-on experience in the latest breed of data ETL, automation and 
   CICD technologies including Python, SQL and Git in a cloud setting. 
   2+ years of experience with cloud-native data warehouse technologies like Snowflake. 
   Skills in data analysis, insight generation and manipulation of structured and unstructured data sources. Experience with automated data quality frameworks. 
   Collaborate with all levels of data science engineering technology personnel and senior leadership. 
   Document and present work to all levels of technical and non-technical audiences. 
   Commitment to creating rigorous, high-quality insights from data, at scale. 
   You should be flexible / willing to work across matrixed delivery landscape which includes and not limited to Agile Applications 
   Development, Support and Deployment. 
   Design and implement secure data pipelines into a Snowflake data warehouse from on premise and cloud data sources. 
   Guide and review off-shore development team work providing coaching and coding feedback aligning to best practices set by the Data Science Engineering team. 
   
  Requirements (Education & Experience): 
   
   Undergraduate degree in a technical or scientific field, such as Statistics, Data Science, Computer Science, or similar  
   8+ years professional experience as a data scientist, data engineer, data analyst, or related role  
   Experience with clinical trial data is not required, but interest to learn and understand how these data improve medical research is paramount  
  
  Medidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation. Nine of the top 10 best-selling drugs in 2017 were developed on the Medidata platform. 
   Medidata Solutions have powered close to 30,000 clinical trials giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner. 
   As with all roles, Medidata sets ranges based on a number of factors including function, level, candidate expertise and experience, and geographic location. 
   The salary range for positions that will be physically based in the NYC Metro Area is $135,000 - $180,000 
   Base pay is one part of the Total Rewards that Medidata provides to compensate and recognize employees for their work. Most sales positions are eligible for a commission on the terms of applicable plan documents, and many of Medidata’s non-sales positions are eligible for annual bonuses. Medidata believes that benefits should connect you to the support you need when it matters most and provides best-in-class benefits, including medical, dental, life and disability insurance; 401(k) matching; unlimited paid time off; and 10 paid holidays per year. 
   #LI-ME1 #LI-Remote 
  
  Equal Employment Opportunity In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Medidata are based on merit, qualifications and abilities. Medidata is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age, disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. Medidata will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.","<p></p>
<div>
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>Requisition ID </b></h3>
   <p>533295</p>
  </div>
 </div>
 <div>
  <div>
   <br> 
  </div>
 </div>
 <div>
  <p>Medidata: Powering Smarter Treatments and Healthier People</p> 
  <p> Medidata, a Dassault Syst&#xe8;mes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world&apos;s most trusted platform for clinical development, commercial, and real-world data. Known for its ground-breaking technological innovations, Medidata has supported more than 30,000 clinical trials and 9 million study participants. And Medidata&#x2019;s ongoing commitment to infusing the patient voice into trial designs and solutions is helping to create a better and more inclusive experience for all participants in clinical studies. Medidata is involved in nearly 40% of company-initiated trial starts globally, with studies conducted in more than 140 countries. More than 70% of novel drugs approved by the Food and Drug Administration (FDA) in 2022 were developed with Medidata software. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us @medidata.</p> 
  <p><b> Our Team:</b></p> 
  <p> Medidata is looking for individuals who will help us tackle some of the most complex questions facing the industry today using our proprietary platform and advanced analytics. At Medidata, we never work alone. This role will partner heavily with all of the key stakeholder functions including product, delivery, data science, engineering, partnerships, and biostatistics. Successful Medidata AI candidates will be skilled in analytical/quantitative thinking, structured communication, and excited about building the next horizon of Medidata&#x2019;s mission to power smarter treatments and healthier people.</p> 
  <p><b> Who We&apos;re Looking For:</b></p> 
  <ul> 
   <li>Advanced skills in modern data architecture, data science engineering, data modeling and data quality using state-of-art cloud computing technologies (AWS).</li> 
   <li>Hands-on experience in the latest breed of data ETL, automation and</li> 
   <li>CICD technologies including Python, SQL and Git in a cloud setting.</li> 
   <li>2+ years of experience with cloud-native data warehouse technologies like Snowflake.</li> 
   <li>Skills in data analysis, insight generation and manipulation of structured and unstructured data sources. Experience with automated data quality frameworks.</li> 
   <li>Collaborate with all levels of data science engineering technology personnel and senior leadership.</li> 
   <li>Document and present work to all levels of technical and non-technical audiences.</li> 
   <li>Commitment to creating rigorous, high-quality insights from data, at scale.</li> 
   <li>You should be flexible / willing to work across matrixed delivery landscape which includes and not limited to Agile Applications</li> 
   <li>Development, Support and Deployment.</li> 
   <li>Design and implement secure data pipelines into a Snowflake data warehouse from on premise and cloud data sources.</li> 
   <li>Guide and review off-shore development team work providing coaching and coding feedback aligning to best practices set by the Data Science Engineering team.</li> 
  </ul> 
  <p><b>Requirements (Education &amp; Experience):</b></p> 
  <ul> 
   <li><p>Undergraduate degree in a technical or scientific field, such as Statistics, Data Science, Computer Science, or similar</p> </li> 
   <li><p>8+ years professional experience as a data scientist, data engineer, data analyst, or related role</p> </li> 
   <li><p>Experience with clinical trial data is not required, but interest to learn and understand how these data improve medical research is paramount</p> </li> 
  </ul>
  <p>Medidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation. Nine of the top 10 best-selling drugs in 2017 were developed on the Medidata platform.</p> 
  <p> Medidata Solutions have powered close to 30,000 clinical trials giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner.</p> 
  <p> As with all roles, Medidata sets ranges based on a number of factors including function, level, candidate expertise and experience, and geographic location.</p> 
  <p> The salary range for positions that will be physically based in the NYC Metro Area is &#x24;135,000 - &#x24;180,000</p> 
  <p> Base pay is one part of the Total Rewards that Medidata provides to compensate and recognize employees for their work. Most sales positions are eligible for a commission on the terms of applicable plan documents, and many of Medidata&#x2019;s non-sales positions are eligible for annual bonuses. Medidata believes that benefits should connect you to the support you need when it matters most and provides best-in-class benefits, including medical, dental, life and disability insurance; 401(k) matching; unlimited paid time off; and 10 paid holidays per year.</p> 
  <p> #LI-ME1<br> #LI-Remote</p> 
  <p></p>
  <h3 class=""jobSectionHeader""><b>Equal Employment Opportunity</b></h3> In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Medidata are based on merit, qualifications and abilities. Medidata is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age, disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. Medidata will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.
 </div>
</div>
<p></p>",https://www.indeed.com/rc/clk?jk=dbf9b007f625f71b&atk=&xpse=SoBP67I3JhXlT9yU8J0LbzkdCdPP,dbf9b007f625f71b,,,,,"New York, NY",Lead Data Science Engineer,3 days ago,2023-10-15T13:32:58.592Z,3.6,94.0,"$135,000 - $180,000 a year",2023-10-18T13:32:58.595Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=dbf9b007f625f71b&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
51,Amaze systems,"Location – Alpharetta, GA/Armonk, NY/Boston, MA/Fort Wayne, IN/Miami, FL/NYC, NY/Windsor, CT/ Washington, DC 
Job Title - Data Engineer
6 months C2h with client
Role Description:
Looking for a data engineer to design, develop and maintain pipelines and workflows and create analytics to digitally transform the current NA Accident and Health reporting and business decision processes.
Responsibilities
Development and support of data pipelines that produce data assets for various A&H workstreams including UW, UA, Actuarial, Claims
Handle data pipelines while testing for data curation, parsing, cleaning, transformation and enrichment of data
Work with fundamentals of data processing, data pipeline, data lineage and ETL (Extract-Transform-Load) methodologies
Implement the project according to the Software Development Life Cycle (SDLC) and programming by using fast paced agile methodology, involving task completion, user stories
Utilize knowledge of database management system software, object oriented programming development, system architecture and components and various programming languages
Review and analyze business workflows and user data needs
Design and implement business performance dashboards
Write customized queries/programs to generate automatic periodical reports highlighting all the Key Performance Indicators (KPIs)
Build applications using SQL and/or Python scripts to manipulate data, monitor and help to improve data quality
Design, build and maintain end-to-end data solutions supporting our processes with the right data architecture
Have working knowledge of Apache Spark, big data processing and building products on distributed cluster-computing framework
Construct workflow charts and diagrams and writing specifications.
Documentation of end-to-end data pipeline process.
Documentation of data assets for information management purposes.
Ad hoc team / business support as needed.
Exploration and evaluation of new technologies and platforms.
Requirements
Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field
5+ years as a data engineer
Sound Python and SQL skills with ability to query and analyze data, understand complexity and data structuresExperience with data and analytics technology, including but not limited to Hadoop, Spark, Java, Python, R, ElasticSearch, and others
Familiarity with relational database concepts
Detail-oriented, analytical, and inquisitiveGood communication skills
Highly organized with strong time-management skills
Ability to work independently and collaborate well with others
Ability to affect smooth organizational transformations
Job Type: Contract
Salary: From $60.00 per hour
Experience level:

 11+ years

Experience:

 Data Engineer: 10 years (Required)
 Python: 7 years (Required)
 Insurance Domain: 5 years (Required)

Work Location: Remote","<p><b>Location &#x2013; Alpharetta, GA/Armonk, NY/Boston, MA/Fort Wayne, IN/Miami, FL/NYC, NY/Windsor, CT/ Washington, DC </b></p>
<p><b>Job Title - Data Engineer</b></p>
<p><b>6 months C2h with client</b></p>
<p><b>Role Description:</b></p>
<p>Looking for a data engineer to design, develop and maintain pipelines and workflows and create analytics to digitally transform the current NA Accident and Health reporting and business decision processes.</p>
<p>Responsibilities</p>
<p>Development and support of data pipelines that produce data assets for various A&amp;H workstreams including UW, UA, Actuarial, Claims</p>
<p>Handle data pipelines while testing for data curation, parsing, cleaning, transformation and enrichment of data</p>
<p>Work with fundamentals of data processing, data pipeline, data lineage and ETL (Extract-Transform-Load) methodologies</p>
<p>Implement the project according to the Software Development Life Cycle (SDLC) and programming by using fast paced agile methodology, involving task completion, user stories</p>
<p>Utilize knowledge of database management system software, object oriented programming development, system architecture and components and various programming languages</p>
<p>Review and analyze business workflows and user data needs</p>
<p>Design and implement business performance dashboards</p>
<p>Write customized queries/programs to generate automatic periodical reports highlighting all the Key Performance Indicators (KPIs)</p>
<p>Build applications using SQL and/or Python scripts to manipulate data, monitor and help to improve data quality</p>
<p>Design, build and maintain end-to-end data solutions supporting our processes with the right data architecture</p>
<p>Have working knowledge of Apache Spark, big data processing and building products on distributed cluster-computing framework</p>
<p>Construct workflow charts and diagrams and writing specifications.</p>
<p>Documentation of end-to-end data pipeline process.</p>
<p>Documentation of data assets for information management purposes.</p>
<p>Ad hoc team / business support as needed.</p>
<p>Exploration and evaluation of new technologies and platforms.</p>
<p>Requirements</p>
<p>Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field</p>
<p>5+ years as a data engineer</p>
<p>Sound Python and SQL skills with ability to query and analyze data, understand complexity and data structures<br>Experience with data and analytics technology, including but not limited to Hadoop, Spark, Java, Python, R, ElasticSearch, and others</p>
<p>Familiarity with relational database concepts</p>
<p>Detail-oriented, analytical, and inquisitive<br>Good communication skills</p>
<p>Highly organized with strong time-management skills</p>
<p>Ability to work independently and collaborate well with others</p>
<p>Ability to affect smooth organizational transformations</p>
<p>Job Type: Contract</p>
<p>Salary: From &#x24;60.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>11+ years</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Engineer: 10 years (Required)</li>
 <li>Python: 7 years (Required)</li>
 <li>Insurance Domain: 5 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,22cd2b66172d9b52,,Contract,,,Remote,Data Engineer,3 days ago,2023-10-15T13:33:04.365Z,,,From $60 an hour,2023-10-18T13:33:04.373Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=22cd2b66172d9b52&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
53,Redcloud Consulting,"RedCloud Consulting is a business and IT consulting company with local Puget Sound Enterprise and Mid-sized clients. RedCloud seeks a 
 Senior Engineer – Data Center Infrastructure to support immediate client operations. Seattle Business Magazine has recognized us, ranked #1 on their ""Best Companies to Work for in Washington"" for Mid-Sized Businesses list, awarded #1 Fastest Growing Company in Washington by Puget Sound Business Journal, and named on the Inc. 500/5000 list.
  
  
 Job Description:
  We are seeking a highly skilled and experienced Senior Engineer specializing in Data Center Infrastructure to join our dynamic team. The ideal candidate will possess a strong background in Data Center Infrastructure Management (DCIM) with a focus on Sunbird DC Track software. This individual will play a crucial role in managing, maintaining, and optimizing our data center operations.
  
  
 Responsibilities include but are not limited to: 
 
  Serve as the primary owner and administrator of the DCIM platform, responsible for its day-to-day operations, maintenance, and periodic audits. 
  Collaborate with cross-functional teams to ensure seamless integration and operation of data center infrastructure. 
  Stay updated with industry best practices and emerging technologies to recommend and implement improvements in data center operations. 
  
  
 Required Knowledge, Skills, and Abilities: 
 
  Demonstrated expertise in Data Center Infrastructure Management (DCIM) with a deep understanding of data center operations, including power and cooling systems, rack and power configuration, and device management. 
  Proficient in using DCIM software, with a specific emphasis on Sunbird DC Track, to monitor and manage data center resources effectively. 
  Experience with ServiceNow is highly desirable, as it will be advantageous in streamlining operational processes. 
  Expertise in Subird DC Track DCIM Software is a critical factor for this role. The ability to leverage this specialization will be a substantial advantage in the hiring process. 
  Familiarity with other DCIM software solutions, such as PowerIQ and TigerEyes, is considered a plus. 
  
  
 Qualifications: 
 
  Bachelor's degree in data science, IT infrastructure, computer science, or other related field. 
  8+ years of proven experience in Data Center Infrastructure Management. 
  Extensive experience with Sunbird DC Track software is essential. 
  Strong proficiency in DCIM software applications. 
  Knowledge of physical hardware components and their integration within a data center environment. 
  Excellent problem-solving skills and attention to detail. 
  Effective communication and collaboration skills.
 
  
  
  Compensation range for position is $135,000 – 170,500 DOE.
  Benefits and bonus information can be found at 
 https://www.redcloudconsulting.com/careers.html
  
  RedCloud requires employees maintain permanent residency within the United States during their employment period. During onboarding, proof of eligibility to work in the United States will be requested. RedCloud does not provide visa sponsorship.
  
  
 About Us:
  RedCloud is a boutique, business and technology consulting firm providing local companies with expert-level support for over two decades. Whether it’s to solve a specific business challenge or to provide additional support for an ambitious project, we can help bring even the most visionary endeavors to fruition.
  
  Anchored by a foundation of ""integrity-based consulting"", the RedCloud team of subject matter experts collaborate closely with clients to develop and implement high-level solutions, bringing stability, growth, and innovation together for long-term success. We provide a broad array of business and technology consulting services through RedCloud’s core services: Empower Operations, Empower Sales and Marketing, Empower Customers, Empower Security and Privacy. 
  
  Visit 
 http://www.redcloudconsulting.com/ for more info. 
  #LI-Remote","<div>
 RedCloud Consulting is a business and IT consulting company with local Puget Sound Enterprise and Mid-sized clients. RedCloud seeks a 
 <b>Senior Engineer &#x2013; Data Center Infrastructure</b> to support immediate client operations. Seattle Business Magazine has recognized us, ranked #1 on their &quot;Best Companies to Work for in Washington&quot; for Mid-Sized Businesses list, awarded #1 Fastest Growing Company in Washington by Puget Sound Business Journal, and named on the Inc. 500/5000 list.
 <br> 
 <br> 
 <b>Job Description:</b>
 <br> We are seeking a highly skilled and experienced Senior Engineer specializing in Data Center Infrastructure to join our dynamic team. The ideal candidate will possess a strong background in Data Center Infrastructure Management (DCIM) with a focus on Sunbird DC Track software. This individual will play a crucial role in managing, maintaining, and optimizing our data center operations.
 <br> 
 <br> 
 <b>Responsibilities include but are not limited to:</b> 
 <ul>
  <li>Serve as the primary owner and administrator of the DCIM platform, responsible for its day-to-day operations, maintenance, and periodic audits. </li>
  <li>Collaborate with cross-functional teams to ensure seamless integration and operation of data center infrastructure. </li>
  <li>Stay updated with industry best practices and emerging technologies to recommend and implement improvements in data center operations. </li>
 </ul> 
 <br> 
 <b>Required Knowledge, Skills, and Abilities:</b> 
 <ul>
  <li>Demonstrated expertise in Data Center Infrastructure Management (DCIM) with a deep understanding of data center operations, including power and cooling systems, rack and power configuration, and device management. </li>
  <li>Proficient in using DCIM software, with a specific emphasis on Sunbird DC Track, to monitor and manage data center resources effectively. </li>
  <li>Experience with ServiceNow is highly desirable, as it will be advantageous in streamlining operational processes. </li>
  <li>Expertise in Subird DC Track DCIM Software is a critical factor for this role. The ability to leverage this specialization will be a substantial advantage in the hiring process. </li>
  <li>Familiarity with other DCIM software solutions, such as PowerIQ and TigerEyes, is considered a plus. </li>
 </ul> 
 <br> 
 <b>Qualifications:</b> 
 <ul>
  <li>Bachelor&apos;s degree in data science, IT infrastructure, computer science, or other related field. </li>
  <li>8+ years of proven experience in Data Center Infrastructure Management. </li>
  <li>Extensive experience with Sunbird DC Track software is essential. </li>
  <li>Strong proficiency in DCIM software applications. </li>
  <li>Knowledge of physical hardware components and their integration within a data center environment. </li>
  <li>Excellent problem-solving skills and attention to detail. </li>
  <li>Effective communication and collaboration skills.</li>
 </ul>
 <br> 
 <br> 
 <br> Compensation range for position is &#x24;135,000 &#x2013; 170,500 DOE.
 <br> Benefits and bonus information can be found at 
 <b>https://www.redcloudconsulting.com/careers.html</b>
 <br> 
 <br> RedCloud requires employees maintain permanent residency within the United States during their employment period. During onboarding, proof of eligibility to work in the United States will be requested. RedCloud does not provide visa sponsorship.
 <br> 
 <br> 
 <b>About Us:</b>
 <br> RedCloud is a boutique, business and technology consulting firm providing local companies with expert-level support for over two decades. Whether it&#x2019;s to solve a specific business challenge or to provide additional support for an ambitious project, we can help bring even the most visionary endeavors to fruition.
 <br> 
 <br> Anchored by a foundation of &quot;integrity-based consulting&quot;, the RedCloud team of subject matter experts collaborate closely with clients to develop and implement high-level solutions, bringing stability, growth, and innovation together for long-term success. We provide a broad array of business and technology consulting services through RedCloud&#x2019;s core services: Empower Operations, Empower Sales and Marketing, Empower Customers, Empower Security and Privacy. 
 <br> 
 <br> Visit 
 <b>http://www.redcloudconsulting.com/</b> for more info. 
 <br> #LI-Remote
</div>",http://careerportal.redcloudconsulting.com/#/jobs/1517,15a24cf7d36918c8,,,,,"Seattle, WA","Senior Engineer, Data Center Infrastructure",3 days ago,2023-10-15T13:33:02.827Z,4.7,6.0,"$135,000 - $170,500 a year",2023-10-18T13:33:02.829Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=15a24cf7d36918c8&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
54,Clari,"Clari’s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here…are you ready to achieve remarkable with us?
 
 
 
   About the Team
 
 
   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari’s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can’t wait to meet you.
 
 
 
   About the Role
 
 
   We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari’s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.
 
 
 
   You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari’s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don’t believe us? Hear what our customers have to say
   
 
 
  
 
  Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.
   
 
 
  
 
  This is a fully remote opportunity and can be worked from any location in the United States.
  
 Responsibilities
 
   Design and evolve the architecture for the query layer that powers Clari’s product suite and platform
   Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion
   Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers
   Write scalable, robust, and fully tested software for deployment in mission-critical production environments
   Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization
   Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent
 
  Qualifications
 
   10+ years of software development experience using Java or similar object-oriented languages for backend development
   Deep expertise with relational database skills and concepts
   Experience having led multiple projects from inception through deployment, maintenance, and support
   Experience with Postgres and non-relational databases like MongoDB
   Experience with AWS
   Experience building scalable systems and architectures
   Experience with database performance tuning
 
  Perks and Benefits @ Clari 
 
  Remote-first with opportunities to work and celebrate in person
   Medical, dental, vision, short & long-term disability, Life insurance, and EAP
   Mental health support provided by Modern Health
   Pre-IPO stock options
   Well-being and professional development funds
   Retirement 401(k) plan
   100% paid parental leave, plus fertility and family planning support provided by Maven
   Discretionary paid time off, monthly ‘take a break’ days, and Focus Fridays
   Focus on culture: Charitable giving match, plus in-person and virtual events
 
 
 
   It is Clari’s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari’s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.
 
 
 
   Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.
 
 
 
   The salary range for this position is $191,300 to $286,900. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.
 
 
   #BI-Remote #LI-Remote
 
 
 
   You’ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We’d love to have you join us on our journey to remarkable!
 
 
 
   If you feel you don’t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you.
 
 
 
   Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you’ll bring your whole self to the job.","<div>
 <div>
  Clari&#x2019;s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here&#x2026;are you ready to achieve remarkable with us?
 </div>
 <div></div>
 <div>
  <b><br> About the Team</b>
 </div>
 <div>
   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari&#x2019;s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can&#x2019;t wait to meet you.
 </div>
 <div></div>
 <div>
  <b><br> About the Role</b>
 </div>
 <div>
   We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari&#x2019;s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.
 </div>
 <div></div>
 <div>
  <br> You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari&#x2019;s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don&#x2019;t believe us? Hear what our customers have to say
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  <i>This is a fully remote opportunity and can be worked from any location in the United States.</i>
 </div> 
 <h3 class=""jobSectionHeader""><b>Responsibilities</b></h3>
 <ul>
  <li> Design and evolve the architecture for the query layer that powers Clari&#x2019;s product suite and platform</li>
  <li> Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion</li>
  <li> Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers</li>
  <li> Write scalable, robust, and fully tested software for deployment in mission-critical production environments</li>
  <li> Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization</li>
  <li> Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Qualifications</b></h3>
 <ul>
  <li> 10+ years of software development experience using Java or similar object-oriented languages for backend development</li>
  <li> Deep expertise with relational database skills and concepts</li>
  <li> Experience having led multiple projects from inception through deployment, maintenance, and support</li>
  <li> Experience with Postgres and non-relational databases like MongoDB</li>
  <li> Experience with AWS</li>
  <li> Experience building scalable systems and architectures</li>
  <li> Experience with database performance tuning</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Perks and Benefits @ Clari </b></h3>
 <ul>
  <li>Remote-first with opportunities to work and celebrate in person</li>
  <li> Medical, dental, vision, short &amp; long-term disability, Life insurance, and EAP</li>
  <li> Mental health support provided by Modern Health</li>
  <li> Pre-IPO stock options</li>
  <li> Well-being and professional development funds</li>
  <li> Retirement 401(k) plan</li>
  <li> 100% paid parental leave, plus fertility and family planning support provided by Maven</li>
  <li> Discretionary paid time off, monthly &#x2018;take a break&#x2019; days, and Focus Fridays</li>
  <li> Focus on culture: Charitable giving match, plus in-person and virtual events</li>
 </ul>
 <div></div>
 <div>
  <br> It is Clari&#x2019;s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari&#x2019;s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.
 </div>
 <div></div>
 <div>
  <br> Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.
 </div>
 <div></div>
 <div>
  <br> The salary range for this position is &#x24;191,300 to &#x24;286,900. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.
 </div>
 <div>
   #BI-Remote #LI-Remote
 </div>
 <div></div>
 <div>
  <br> You&#x2019;ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We&#x2019;d love to have you join us on our journey to remarkable!
 </div>
 <div></div>
 <div>
  <b><br> If you feel you don&#x2019;t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you.</b>
 </div>
 <div></div>
 <div>
  <br> Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you&#x2019;ll bring your whole self to the job.
 </div>
</div>",https://jobs.lever.co/clari/3b2a2738-2090-42de-b748-acfc28b25609?lever-source=Indeed,416b51eb49b3f599,,Full-time,,,"Seattle, WA","Principal Software Engineer, Data Platform - Remote",1 day ago,2023-10-17T13:33:04.910Z,4.0,3.0,"$191,300 - $286,900 a year",2023-10-18T13:33:04.928Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=416b51eb49b3f599&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
57,Internet Archive,"Interested in a mission-driven job ensuring perpetual open access to information for a global audience? Enjoy helping scale the use of services and products critical to hundreds of national and international non-profits, libraries, universities, cultural heritage institutions, and mission-driven organizations? If so, the Internet Archive is seeking a Software Engineer for our Archiving & Data Services team.
  Internet Archive (IA) is a non-profit digital library, top 200 website at archive.org, and an archive of over 99 petabytes of digital information running in many self-owned and operated data centers. Internet Archive also provides mission-aligned services to thousands of organizations working collaboratively to advance our shared goal of “Universal Access to All Knowledge.” The Archiving & Data Services group provides a suite of paid, SaaS, and free products, as well as community programs, focused on the archiving, management, analysis, and accessibility of digital information. Its services are used by over 1,500 organizations around the world.
  We are looking for a motivated, detail-oriented Software Engineer to join our team. The role will focus on Archive-It (archive-it.org), our platform for building, sharing, and preserving web archive collections. This position offers the opportunity to work with a range of technologies and gain deep knowledge about web crawling, archival replay, and large-scale distributed systems. Our services work with petabytes of archived data and facilitate the discovery and use of large-scale digital collections. The Software Engineer will have the unique opportunity to build things that further open access to information and advance the public good.
  Key Responsibilities:
 
   Collaborate with team members to understand user needs, design new features, support web crawling and preservation, and improve the performance and reliability of Archive-It and other department products.
  
   
     Implement, test, and maintain software across our stack (Python, Elasticsearch, Postgres, Temporal, HTML/CSS/JS/TS).
   
  
   
     Develop, monitor, and maintain the Archive-It partner application, where web crawls are configured, scheduled, and reported.
   
  
   
     Improve a distributed system orchestrating web crawls and post-processing them for long term preservation, indexing for retrieval, deduplication, and reporting.
   
  
   
     Participate in code reviews to ensure the quality and stability of our software and diffusion of knowledge across the team.
   
   Document architecture, software, and features for internal and external users.
 
  Qualification and Skills:
 
   Degree in Computer Science or a related field, or equivalent experience, strongly preferred.
  
   
     Proficiency in Python, with familiarity in Postgres, Elasticsearch, and HTML/CSS/JS preferred.
   
  
   
     A strong understanding of web services and distributed systems.
   
  
   
     Excellent problem-solving skills, attention to detail, and ability to work both independently and collaboratively.
   
  
   
     Experience with web crawling, Django, workflow systems (e.g. Temporal, Airflow), distributed databases (e.g. Cassandra, Scylla), Hadoop, and Ansible are a plus
   
  
   
     GitLab, GitHub, Sentry, Grafana, JIRA, are other tools we use.
   
  
   
     Our independently operated data centers run Ubuntu Linux VMs and our department runs everything from the VM up, so Linux experience is preferred.
   
   An interest in the Internet Archive’s mission to provide Universal Access to All Knowledge is expected.
 
  Job Details:
  Remote applicants preferred. We have headquarters in San Francisco and Vancouver and candidates in those locations will have the option for hybrid remote/in-office arrangements. Candidates will need to have some time overlap with primarily North America (and largely Pacific Time) based colleagues. Compensation and title will be commensurate with experience and the role is open to candidates of varying seniority with a general, but negotiable, salary range of $90,000 to $115,000 based on living in the San Francisco, CA region. Compensation may be adjusted based on the geographic location of the finalist.
  Benefits & Perks:
  The Internet Archive is a remote first workplace and provides a comprehensive benefits package including; PTO, paid holidays, and medical benefits. Depending on where you live, we also provide these additional benefits; dental, vision, health savings accounts, flex spending accounts, commuter benefits, short term disability, long term disability and retirement programs.
  At the Internet Archive, we believe we do our best work when our employees bring together diverse ideas. Members of all groups under represented in the tech industry and library world are strongly encouraged to apply. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law.","<p></p>
<div>
 <p>Interested in a mission-driven job ensuring perpetual open access to information for a global audience? Enjoy helping scale the use of services and products critical to hundreds of national and international non-profits, libraries, universities, cultural heritage institutions, and mission-driven organizations? If so, the Internet Archive is seeking a Software Engineer for our Archiving &amp; Data Services team.</p>
 <p> Internet Archive (IA) is a non-profit digital library, top 200 website at archive.org, and an archive of over 99 petabytes of digital information running in many self-owned and operated data centers. Internet Archive also provides mission-aligned services to thousands of organizations working collaboratively to advance our shared goal of &#x201c;Universal Access to All Knowledge.&#x201d; The Archiving &amp; Data Services group provides a suite of paid, SaaS, and free products, as well as community programs, focused on the archiving, management, analysis, and accessibility of digital information. Its services are used by over 1,500 organizations around the world.</p>
 <p> We are looking for a motivated, detail-oriented Software Engineer to join our team. The role will focus on Archive-It (archive-it.org), our platform for building, sharing, and preserving web archive collections. This position offers the opportunity to work with a range of technologies and gain deep knowledge about web crawling, archival replay, and large-scale distributed systems. Our services work with petabytes of archived data and facilitate the discovery and use of large-scale digital collections. The Software Engineer will have the unique opportunity to build things that further open access to information and advance the public good.</p>
 <p> Key Responsibilities:</p>
 <ul>
  <li><p> Collaborate with team members to understand user needs, design new features, support web crawling and preservation, and improve the performance and reliability of Archive-It and other department products.</p></li>
  <li>
   <div>
     Implement, test, and maintain software across our stack (Python, Elasticsearch, Postgres, Temporal, HTML/CSS/JS/TS).
   </div></li>
  <li>
   <div>
     Develop, monitor, and maintain the Archive-It partner application, where web crawls are configured, scheduled, and reported.
   </div></li>
  <li>
   <div>
     Improve a distributed system orchestrating web crawls and post-processing them for long term preservation, indexing for retrieval, deduplication, and reporting.
   </div></li>
  <li>
   <div>
     Participate in code reviews to ensure the quality and stability of our software and diffusion of knowledge across the team.
   </div></li>
  <li><p> Document architecture, software, and features for internal and external users.</p></li>
 </ul>
 <p> Qualification and Skills:</p>
 <ul>
  <li><p> Degree in Computer Science or a related field, or equivalent experience, strongly preferred.</p></li>
  <li>
   <div>
     Proficiency in Python, with familiarity in Postgres, Elasticsearch, and HTML/CSS/JS preferred.
   </div></li>
  <li>
   <div>
     A strong understanding of web services and distributed systems.
   </div></li>
  <li>
   <div>
     Excellent problem-solving skills, attention to detail, and ability to work both independently and collaboratively.
   </div></li>
  <li>
   <div>
     Experience with web crawling, Django, workflow systems (e.g. Temporal, Airflow), distributed databases (e.g. Cassandra, Scylla), Hadoop, and Ansible are a plus
   </div></li>
  <li>
   <div>
     GitLab, GitHub, Sentry, Grafana, JIRA, are other tools we use.
   </div></li>
  <li>
   <div>
     Our independently operated data centers run Ubuntu Linux VMs and our department runs everything from the VM up, so Linux experience is preferred.
   </div></li>
  <li><p> An interest in the Internet Archive&#x2019;s mission to provide Universal Access to All Knowledge is expected.</p></li>
 </ul>
 <p> Job Details:</p>
 <p> Remote applicants preferred. We have headquarters in San Francisco and Vancouver and candidates in those locations will have the option for hybrid remote/in-office arrangements. Candidates will need to have some time overlap with primarily North America (and largely Pacific Time) based colleagues. Compensation and title will be commensurate with experience and the role is open to candidates of varying seniority with a general, but negotiable, salary range of &#x24;90,000 to &#x24;115,000 based on living in the San Francisco, CA region. Compensation may be adjusted based on the geographic location of the finalist.</p>
 <p> Benefits &amp; Perks:</p>
 <p> The Internet Archive is a remote first workplace and provides a comprehensive benefits package including; PTO, paid holidays, and medical benefits. Depending on where you live, we also provide these additional benefits; dental, vision, health savings accounts, flex spending accounts, commuter benefits, short term disability, long term disability and retirement programs.</p>
 <p> At the Internet Archive, we believe we do our best work when our employees bring together diverse ideas. Members of all groups under represented in the tech industry and library world are strongly encouraged to apply. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law.</p>
</div>
<p></p>",https://app.trinethire.com/companies/32967-internet-archive/jobs/83694-software-engineer-archiving-and-data-services?source=indeed,438a072845b56b63,,Full-time,,,"San Francisco, CA",Software Engineer - Archiving and Data Services,1 day ago,2023-10-17T13:33:09.063Z,3.6,27.0,"$90,000 - $115,000 a year",2023-10-18T13:33:09.067Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=438a072845b56b63&from=jasx&tk=1hd1fq7bmjm7k800&vjs=3
59,Accrete.AI,"The U.S. Government agencies we work with have contracts that require all personnel working on their corresponding contracts to have U.S. citizenship – do you meet this requirement?
 
  
  Accrete is looking for a Data Platform Engineer who will play a critical role in designing, building, and maintaining our data platform to support the organization's data processing and analytics needs. You will collaborate with data engineers, data scientists, and other cross-functional teams to create scalable and efficient data solutions. The ideal candidate should have a strong background in data engineering, cloud technologies, and data architecture, along with a passion for innovation and problem-solving.
  
 Accrete is an AI prime defense contractor with the U.S. government that creates AI software, enabling its customers to make better decisions, faster. Accrete is on a mission to create AI so powerful it amplifies human reasoning and enables enterprises to grow in previously unimaginable ways. Prior to launching Accrete in 2017, Prashant Bhuyan, Accrete’s Founder and CEO, spent over a decade in high-frequency trading where he and a core team experimented with and developed AI technology that ultimately became the early underpinnings of Accrete.
  Accrete’s solutions enable the Department of Defense to predict covert behavior from foreign adversaries seeking to influence the supply chain; the U.S. Air Force to identify vulnerabilities in microprocessor firmware; major music labels to identify superstars before competitors; auto dealers to automatically generate marketing content from vehicle feature lists; employee benefits brokers to identify the shortest path to the hottest leads; and more. 
  To learn more about Accrete, please visit our website: Accrete.ai 
  
 Responsibilities: 
 
  Lead the design and development of our data platform to support the ingestion, organization, and retrieval of data. 
  Work closely with stakeholders to ensure the timely arrival of data that is prepared for downstream use.
   Operate and maintain technologies associated with the platform.
   Optimize data storage and retrieval strategies, ensuring data availability, consistency, and reliability.
   Implement data governance and data security practices to protect sensitive data and ensure compliance with regulatory requirements.
   Develop and maintain data platform documentation, including data models, data dictionaries, and system architecture.
   Conduct performance tuning and optimization of data processing workflows for enhanced efficiency and scalability.
   Stay up-to-date with emerging data technologies, industry trends, and best practices, and evaluate their potential to enhance our data platform.
   Mentor and provide guidance to junior data engineers, fostering a culture of knowledge sharing and skill development.
  
  
 Requirements: 
 
   Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
   2-3+ years of experience in data engineering and data platform development.
   Proven expertise in designing and building scalable data platforms, data warehouses, and data lakes.
   Strong proficiency in data modeling, ETL processes, and data integration techniques.
   Extensive experience with cloud platforms such as AWS, Azure, or Google Cloud, and cloud-based data services.
   Proficiency in programming languages commonly used in data engineering, such as Python, Java, Rust, C, or Go.
   Familiarity with data governance, data security, and data compliance practices.
   Operational knowledge of common data systems like: Kafka, Airflow, Spark, Trino, Ranger.
   Knowledge of containerization technologies, such as Docker and Kubernetes, for data platform deployment.
   Strong problem-solving and analytical skills, with the ability to troubleshoot and resolve complex data issues.
   Excellent communication and collaboration skills to work effectively with cross-functional teams.
   A proactive and self-driven approach to learning and staying updated with evolving data technologies and tools.
  
 
 The base salary range for this role is $145,000 to 160,000. 
  
 Benefits: 
 We offer a competitive salary, benefits package, and opportunities for growth and advancement within the company. If you are an innovative and results-driven leader, we encourage you to apply for this exciting opportunity. 
  Accrete is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.","<div>
 <ul>
  <li><i>The U.S. Government agencies we work with have contracts that require all personnel working on their corresponding contracts to have U.S. citizenship &#x2013; do you meet this requirement?</i></li>
 </ul>
 <p></p> 
 <p> Accrete is looking for a Data Platform Engineer who will play a critical role in designing, building, and maintaining our data platform to support the organization&apos;s data processing and analytics needs. You will collaborate with data engineers, data scientists, and other cross-functional teams to create scalable and efficient data solutions. The ideal candidate should have a strong background in data engineering, cloud technologies, and data architecture, along with a passion for innovation and problem-solving.</p>
 <p></p> 
 <p>Accrete is an AI prime defense contractor with the U.S. government that creates AI software, enabling its customers to make better decisions, faster. Accrete is on a mission to create AI so powerful it amplifies human reasoning and enables enterprises to grow in previously unimaginable ways. Prior to launching Accrete in 2017, Prashant Bhuyan, Accrete&#x2019;s Founder and CEO, spent over a decade in high-frequency trading where he and a core team experimented with and developed AI technology that ultimately became the early underpinnings of Accrete.</p>
 <p><br> Accrete&#x2019;s solutions enable the Department of Defense to predict covert behavior from foreign adversaries seeking to influence the supply chain; the U.S. Air Force to identify vulnerabilities in microprocessor firmware; major music labels to identify superstars before competitors; auto dealers to automatically generate marketing content from vehicle feature lists; employee benefits brokers to identify the shortest path to the hottest leads; and more.</p> 
 <p> To learn more about Accrete, please visit our website: Accrete.ai</p> 
 <p></p> 
 <p><b>Responsibilities: </b></p>
 <ul>
  <li>Lead the design and development of our data platform to support the ingestion, organization, and retrieval of data. </li>
  <li>Work closely with stakeholders to ensure the timely arrival of data that is prepared for downstream use.</li>
  <li> Operate and maintain technologies associated with the platform.</li>
  <li> Optimize data storage and retrieval strategies, ensuring data availability, consistency, and reliability.</li>
  <li> Implement data governance and data security practices to protect sensitive data and ensure compliance with regulatory requirements.</li>
  <li> Develop and maintain data platform documentation, including data models, data dictionaries, and system architecture.</li>
  <li> Conduct performance tuning and optimization of data processing workflows for enhanced efficiency and scalability.</li>
  <li> Stay up-to-date with emerging data technologies, industry trends, and best practices, and evaluate their potential to enhance our data platform.</li>
  <li> Mentor and provide guidance to junior data engineers, fostering a culture of knowledge sharing and skill development.</li>
 </ul> 
 <p></p> 
 <p><b>Requirements:</b></p> 
 <ul>
  <li> Bachelor&apos;s or Master&apos;s degree in Computer Science, Data Engineering, or a related field.</li>
  <li> 2-3+ years of experience in data engineering and data platform development.</li>
  <li> Proven expertise in designing and building scalable data platforms, data warehouses, and data lakes.</li>
  <li> Strong proficiency in data modeling, ETL processes, and data integration techniques.</li>
  <li> Extensive experience with cloud platforms such as AWS, Azure, or Google Cloud, and cloud-based data services.</li>
  <li> Proficiency in programming languages commonly used in data engineering, such as Python, Java, Rust, C, or Go.</li>
  <li> Familiarity with data governance, data security, and data compliance practices.</li>
  <li> Operational knowledge of common data systems like: Kafka, Airflow, Spark, Trino, Ranger.</li>
  <li> Knowledge of containerization technologies, such as Docker and Kubernetes, for data platform deployment.</li>
  <li> Strong problem-solving and analytical skills, with the ability to troubleshoot and resolve complex data issues.</li>
  <li> Excellent communication and collaboration skills to work effectively with cross-functional teams.</li>
  <li> A proactive and self-driven approach to learning and staying updated with evolving data technologies and tools.</li>
 </ul> 
 <p></p>
 <p>The base salary range for this role is &#x24;145,000 to 160,000.</p> 
 <p></p> 
 <p><b>Benefits: </b></p>
 <p><i>We offer a competitive salary, benefits package, and opportunities for growth and advancement within the company. If you are an innovative and results-driven leader, we encourage you to apply for this exciting opportunity.</i></p> 
 <p><i> Accrete is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.</i></p>
</div>",https://www.indeed.com/applystart?jk=77845bc5ebc6aae6&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXjPLcA7DAVOeBgupB7PgHT0-BhdrQogdzP3xc9-PmOQTQ&vjfrom=serp&astse=7845a5fa38e75c67&assa=6120,77845bc5ebc6aae6,,Full-time,,,Remote,Data Platform Engineer,20 days ago,2023-09-28T13:33:16.463Z,,,"$145,000 - $160,000 a year",2023-10-18T13:33:16.465Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=77845bc5ebc6aae6&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
60,ServiceNow,"Company Description
  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. 
 With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. 
 Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. 
 Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.
  Job Description
  Team:
  The Data Streaming group has teams that provide streaming API for higher-layer applications and/or work to scale our application platforms. Depending on the nature of the data, the storage systems include data in motion, such as time-series databases or message bus systems. Our largest customers are constantly pushing the limits of the backend storage in terms of size of the data, speed of IO, and the number of concurrent transactions. Performance, reliability, and scalability are always at the core of our work.
  Our largest customers are constantly pushing the limits of the backend storage in terms of size of the data, speed of IO, and several concurrent transactions. Performance, reliability, and scalability are always at the core of our work.
   What you get to do in this role:
 
   Design and build highly innovative interactive high performant solutions with scalability and quality.
   Design software that is simple to use to allow customers to extend and customize the functionality to meet their specific needs.
   Build high-quality, clean, scalable, and reusable code by enforcing best practices around software engineering architecture and processes (Code Reviews, Unit testing, etc.)
   Design and develop tools, libraries, and frameworks with long term platform mindset thinking for high modularity, extensibility, configurability, and maintainability.
   Lead and coordinate work cross functionally to improve architecture, developing process and mentoring junior members in the team.
   Work with the product owners to understand detailed requirements and own your code from design, implementation, test automation and delivery of high-quality product to our users.
   Contribute to the design and implementation of new products and features while also enhancing the existing product suite.
   Manage projects with material technical risk at a team level.
   Explorer and evaluate new technology and innovation to continuously improve platform capability and functionality.
   Be a mentor for colleagues and help promote knowledge-sharing.
 
  To be successful in the role:
 
   Advanced knowledge and experience with fundamentals in distributed systems design and development
   Strong fluency with Java programming as well as good understanding of Java memory model and garbage collection
   Experience with JVM performance tuning and optimization as well as experience in diagnosing performance bottlenecks.
   Advanced working knowledge of concurrency, sockets, networking, operating systems, memory management, runtimes, portability, etc
   Has the experience and ability to diagnose issues and troubleshoot.
 
  Nice to have:
 
   Experience with streaming systems (Kafka, Pulsar, etc.)
   Experience working with Kafka.
   Experience working in a DevOps environment.
   Experience with relational databases: Oracle, MySQL, MariaDB, MS SQLServer
 
  
  Qualifications
  
 
   10+ years of experience with Java or a similar OO language
   Experience building and operating large-scale systems.
   Experience with data structures, algorithms, object-oriented design, design patterns, and performance/scale considerations
 
  For positions in California (outside of the Bay Area), we offer a base pay of $166,230 - $290,970, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location. For individuals who will be working in the Bay Area, there is a pay enhancement for positions located in that geographical area; please contact your recruiter for additional information.
  Additional Information
  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.
  At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.
  If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.
  For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.
  Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.
  From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license.
  Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.","<div>
 <b>Company Description</b>
 <p><br> At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can&#x2019;t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.</p> 
 <p>With more than 7,700+ customers, we serve approximately 85% of the Fortune 500&#xae;, and we&apos;re proud to be one of FORTUNE 100 Best Companies to Work For&#xae; and World&apos;s Most Admired Companies&#x2122;.</p> 
 <p>Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.</p> 
 <p>Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.</p>
 <b><br> Job Description</b>
 <p><b><br> Team:</b></p>
 <p> The Data Streaming group has teams that provide streaming API for higher-layer applications and/or work to scale our application platforms. Depending on the nature of the data, the storage systems include data in motion, such as time-series databases or message bus systems. Our largest customers are constantly pushing the limits of the backend storage in terms of size of the data, speed of IO, and the number of concurrent transactions. Performance, reliability, and scalability are always at the core of our work.</p>
 <p> Our largest customers are constantly pushing the limits of the backend storage in terms of size of the data, speed of IO, and several concurrent transactions. Performance, reliability, and scalability are always at the core of our work.</p>
 <p><br> <b> What you get to do in this role:</b></p>
 <ul>
  <li> Design and build highly innovative interactive high performant solutions with scalability and quality.</li>
  <li> Design software that is simple to use to allow customers to extend and customize the functionality to meet their specific needs.</li>
  <li> Build high-quality, clean, scalable, and reusable code by enforcing best practices around software engineering architecture and processes (Code Reviews, Unit testing, etc.)</li>
  <li> Design and develop tools, libraries, and frameworks with long term platform mindset thinking for high modularity, extensibility, configurability, and maintainability.</li>
  <li> Lead and coordinate work cross functionally to improve architecture, developing process and mentoring junior members in the team.</li>
  <li> Work with the product owners to understand detailed requirements and own your code from design, implementation, test automation and delivery of high-quality product to our users.</li>
  <li> Contribute to the design and implementation of new products and features while also enhancing the existing product suite.</li>
  <li> Manage projects with material technical risk at a team level.</li>
  <li> Explorer and evaluate new technology and innovation to continuously improve platform capability and functionality.</li>
  <li> Be a mentor for colleagues and help promote knowledge-sharing.</li>
 </ul>
 <p><b> To be successful in the role:</b></p>
 <ul>
  <li> Advanced knowledge and experience with fundamentals in distributed systems design and development</li>
  <li> Strong fluency with Java programming as well as good understanding of Java memory model and garbage collection</li>
  <li> Experience with JVM performance tuning and optimization as well as experience in diagnosing performance bottlenecks.</li>
  <li> Advanced working knowledge of concurrency, sockets, networking, operating systems, memory management, runtimes, portability, etc</li>
  <li> Has the experience and ability to diagnose issues and troubleshoot.</li>
 </ul>
 <p><b> Nice to have:</b></p>
 <ul>
  <li> Experience with streaming systems (Kafka, Pulsar, etc.)</li>
  <li> Experience working with Kafka.</li>
  <li> Experience working in a DevOps environment.</li>
  <li> Experience with relational databases: Oracle, MySQL, MariaDB, MS SQLServer</li>
 </ul>
 <br> 
 <b> Qualifications</b>
 <br> 
 <ul>
  <li> 10+ years of experience with Java or a similar OO language</li>
  <li> Experience building and operating large-scale systems.</li>
  <li> Experience with data structures, algorithms, object-oriented design, design patterns, and performance/scale considerations</li>
 </ul>
 <p> For positions in California (outside of the Bay Area), we offer a base pay of &#x24;166,230 - &#x24;290,970, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location. For individuals who will be working in the Bay Area, there is a pay enhancement for positions located in that geographical area; please contact your recruiter for additional information.</p>
 <b><br> Additional Information</b>
 <p><br> ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.</p>
 <p> At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.</p>
 <p> If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.</p>
 <p> For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.</p>
 <p> Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.</p>
 <p> From Fortune. &#xa9; 2022 Fortune Media IP Limited All rights reserved. Used under license.</p>
 <p> Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.</p>
</div>",https://careers.servicenow.com/careers/jobs/743999935084528EXT?lang=en-us&trid=35ab2906-b356-4a56-8472-f60d30d2e2f0,e56b7ba5eba022d3,,Full-time,,,"4810 Eastgate Mall, San Diego, CA 92121",Sr Staff Data Platform Software Engineer - Data Stream,12 days ago,2023-10-06T13:33:21.665Z,3.7,239.0,"$166,230 - $290,970 a year",2023-10-18T13:33:21.668Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e56b7ba5eba022d3&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
61,Jobscan,"At Jobscan, we’re passionate about empowering job seekers to land more interviews with AI technology. We have helped over 2 million job seekers get more interviews in 100+ countries. Jobscan’s platform benefits job seekers, employers, universities, and communities. We're a fast-growing remote startup. We are completely customer-funded, profitable, and growing exponentially!
  
  
  
    We handle vast amounts of data to help job seekers succeed, and we need an experienced Data Engineer to optimize our pipelines for reliability, efficiency, and quality. As part of our distributed engineering team, you will play a crucial role in shaping the future of our data assets.
  
  
 
  
   What you'll be doing
   
    
      Diagnose and Resolve Issues: Troubleshoot and fix data issues within our existing pipeline, which is built on Segment.
      ETL Development: Design, implement, and maintain ETL processes tailored for BigQuery and MySQL while adhering to privacy and governance principles.
      Data Cleansing: Develop and implement data validation and transformation solutions as an integral part of our ETL workflows.
      Data Integration: Utilize Segment for optimized data collection, integration, and management.
      Stakeholder Collaboration: Work closely with stakeholders to tackle specific data integrity and quality issues.
      Teamwork: Collaborate with our Senior Data Analyst and engineering team to refine data models and architectures.
      SQL Optimization: Write and fine-tune SQL queries for performance and scalability in BigQuery and MySQL environments.
      Documentation: Maintain meticulous documentation for all data processes and updates.
    
   
  
  
 
  
   What you'll need
   
    
      Bachelor’s degree in Computer Science, Engineering, or a related field.
      7+ years of relevant experience in data engineering, especially in data pipeline cleanup and ETL processes.
      Direct experience with Customer Data Platforms (CDP) such as Segment, Rudderstack, or Treasure Data.
      Mastery of SQL with hands-on experience in BigQuery and MySQL.
      Proficient in Google Cloud Platform services, particularly BigQuery and Google Analytics 4.
      Experience with modern programming languages like Python, R, JavaScript, and PHP.
      Exceptional problem-solving and communication skills.
      Proven expertise in data schemas and data cleaning principles.
    
   
  
  
 
  
   Preferred qualifications
   
    
      Specific prior experience with Segment for data integration is a strong plus.
      Capability to read and understand PHP and JavaScript code to collaborate effectively with our engineering team.
      Proven track record in tackling data quality and integrity issues in team settings.
    
   
  
  
 
  
   $140,000 - $175,000 a year
  
  
 
  
   Benefits
  
  
    - Remote work - we trust you to get your work done and make it to your meetings‍‍
  
  
    - Competitive salary + stock options - you should have a piece of what we're building here️
  
  
    - Flexible schedule - we make it easy to take care of the important things, like your family and health‍ ️
  
  
    - Unlimited PTO + 14 Paid Holidays + Paid Sick Days - we want our employees to have time to care for their personal wellness and mental health‍ ️
  
  
    - Paid maternal/parental leave - enjoy time with your family's new addition‍‍‍
  
  
    401(k) + employer match
   Medical, dental, vision, and life insurance with generous employer contributions
   Health savings accounts
   Life insurance
   $1000 office stipend; monthly education and internet stipend
  
  
    - Wellness stipend - use for yoga class, gym membership, or anything that improves your personal wellness‍ ️
  
  
   Apple computer or PC of your choice
   Bi-annual company retreats
  
  
  
    Jobscan is committed to equal pay; diversity, equity, and inclusion; and As we continue to grow, we are always adding more benefits and perks for our team.
  
  
  
    Jobscan provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.","<div>
 <div>
  <div>
   At Jobscan, we&#x2019;re passionate about empowering job seekers to land more interviews with AI technology. We have helped over 2 million job seekers get more interviews in 100+ countries. Jobscan&#x2019;s platform benefits job seekers, employers, universities, and communities. We&apos;re a fast-growing remote startup. We are completely customer-funded, profitable, and growing exponentially!
  </div>
  <div></div>
  <div>
   <br> We handle vast amounts of data to help job seekers succeed, and we need an experienced Data Engineer to optimize our pipelines for reliability, efficiency, and quality. As part of our distributed engineering team, you will play a crucial role in shaping the future of our data assets.
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>What you&apos;ll be doing</b></h3>
   <ul>
    <ul>
     <li><b> Diagnose and Resolve Issues:</b> Troubleshoot and fix data issues within our existing pipeline, which is built on Segment.</li>
     <li><b> ETL Development:</b> Design, implement, and maintain ETL processes tailored for BigQuery and MySQL while adhering to privacy and governance principles.</li>
     <li><b> Data Cleansing:</b> Develop and implement data validation and transformation solutions as an integral part of our ETL workflows.</li>
     <li><b> Data Integration:</b> Utilize Segment for optimized data collection, integration, and management.</li>
     <li><b> Stakeholder Collaboration:</b> Work closely with stakeholders to tackle specific data integrity and quality issues.</li>
     <li><b> Teamwork:</b> Collaborate with our Senior Data Analyst and engineering team to refine data models and architectures.</li>
     <li><b> SQL Optimization:</b> Write and fine-tune SQL queries for performance and scalability in BigQuery and MySQL environments.</li>
     <li><b> Documentation:</b> Maintain meticulous documentation for all data processes and updates.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>What you&apos;ll need</b></h3>
   <ul>
    <ul>
     <li> Bachelor&#x2019;s degree in Computer Science, Engineering, or a related field.</li>
     <li> 7+ years of relevant experience in data engineering, especially in data pipeline cleanup and ETL processes.</li>
     <li> Direct experience with Customer Data Platforms (CDP) such as Segment, Rudderstack, or Treasure Data.</li>
     <li> Mastery of SQL with hands-on experience in BigQuery and MySQL.</li>
     <li> Proficient in Google Cloud Platform services, particularly BigQuery and Google Analytics 4.</li>
     <li> Experience with modern programming languages like Python, R, JavaScript, and PHP.</li>
     <li> Exceptional problem-solving and communication skills.</li>
     <li> Proven expertise in data schemas and data cleaning principles.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>Preferred qualifications</b></h3>
   <ul>
    <ul>
     <li> Specific prior experience with Segment for data integration is a strong plus.</li>
     <li> Capability to read and understand PHP and JavaScript code to collaborate effectively with our engineering team.</li>
     <li> Proven track record in tackling data quality and integrity issues in team settings.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   &#x24;140,000 - &#x24;175,000 a year
  </div>
 </div> 
 <div>
  <div>
   <b>Benefits</b>
  </div>
  <div>
   <b> - Remote work</b> - we trust you to get your work done and make it to your meetings&#x200d;&#x200d;
  </div>
  <div>
   <b> - Competitive salary + stock options</b> - you should have a piece of what we&apos;re building here&#xfe0f;
  </div>
  <div>
   <b> - Flexible schedule</b> - we make it easy to take care of the important things, like your family and health&#x200d; &#xfe0f;
  </div>
  <div>
   <b> - Unlimited PTO + 14 Paid Holidays + Paid Sick Days </b>- we want our employees to have time to care for their personal wellness and mental health&#x200d; &#xfe0f;
  </div>
  <div>
   <b> - Paid maternal/parental leave</b> - enjoy time with your family&apos;s new addition&#x200d;&#x200d;&#x200d;
  </div>
  <ul>
   <li> <b>401(k) + employer match</b></li>
   <li><b>Medical, dental, vision, and life insurance </b>with generous employer contributions</li>
   <li><b>Health savings accounts</b></li>
   <li><b>Life insurance</b></li>
   <li><b>&#x24;1000 office stipend; monthly education and internet stipend</b></li>
  </ul>
  <div>
   <b> - Wellness stipend </b>- use for yoga class, gym membership, or anything that improves your personal wellness&#x200d; &#xfe0f;
  </div>
  <ul>
   <li><b>Apple computer or PC of your choice</b></li>
   <li><b>Bi-annual company retreats</b></li>
  </ul>
  <div></div>
  <div>
   <br> Jobscan is committed to equal pay; diversity, equity, and inclusion; and As we continue to grow, we are always adding more benefits and perks for our team.
  </div>
  <div></div>
  <div>
   <i><br> Jobscan provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws</i>.
  </div>
 </div>
</div>",https://jobs.lever.co/jobscan-2/88ce689e-7f61-4027-bfc7-e727038e90f5,0213c2c18b4e7a68,,Full-time,,,Remote,Senior Data Engineer,19 days ago,2023-09-29T13:33:18.821Z,,,"$140,000 - $175,000 a year",2023-10-18T13:33:18.824Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=0213c2c18b4e7a68&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
63,iRhythm Technologies,"About Us: 
  iRhythm is a leading digital healthcare company focused on the way cardiac arrhythmias are clinically diagnosed by combining our wearable biosensing technology with powerful cloud-based data analytics and machine-learning capabilities. Our goal is to be the leading provider of first-line ambulatory ECG monitoring for patients at risk for arrhythmias. iRhythm's continuous ambulatory monitoring has already put over 6 million patients and their doctors on a shorter path to what they both need – answers. 
  About this Role: 
  As a Sr Research Data Engineer within the iRhythm Research & Systems Engineering team, you will be exposed to a diverse set of engineering problems spanning Machine Learning, Big Data, Multi-Modal data, Data Privacy, AWS, Batch Processing and Mobile technologies in the context of addressing unmet clinical needs. You will have the opportunity to work with a talented team to develop a deep understanding of our end-to-end systems, helping to drive successful delivery of software projects throughout its development life cycle. 
  Note: This role will work remote from a US based home office. We are unable to offer any sort of sponsorship for this role. 
  About You 
  
  You are a quality-minded individual with ability to go detail-oriented and scientific while having the big picture in mind 
  You enjoy investigative and troubleshooting processes using your excellent analytical and problem-solving skills, along with a strong sense of urgency 
  You are self-motivated and demonstrate initiative in helping others 
  You are independent and thrive in the face of ambiguous, open-ended challenges 
  You have effective presentation and interpersonal skills communicating technical information to a variety of stakeholders 
  You enjoy collaborating with cross functional groups 
  
 Responsibilities 
  
  Work with engineering and cross-functional stakeholders to research, define, and write technical requirements, figure out data or operational workflows, and plan for V&V/release activities 
  Design and code pipelines that efficiently transform our raw data into formats and structures that best serve our researchers (Python and algorithm design) 
  Perform exploratory research with physiological data, with good software practices in mind 
  Develop, test, and document proof-of-concept algorithms and/or software tools 
  Participate and support project management, software development, and software QA in the software development life cycle and regulatory submissions as system expert 
  Update and maintain Design History File, participate in risk management/hazards analysis activities 
  Develop, document, and monitor testing regimes that ensure performant code and data integrity 
  Maintain a clean and well-documented code base and audit trail (git, Bitbucket, Confluence, JIRA) 
  
 Basic Qualifications 
  
  Combined 5+ years of algorithm/software development and systems engineering experience in the Healthcare, Medical Device, or other regulated software industry. 
  Software development experience with one or more of Python, Java, C#, or C/C++ 
  Understanding of statistics and proficiency with data analysis. Proficiency in Linux/Unix/Windows environments 
  BA/BS in science or technical field 
  Experience processing and integrating developmental data, clinical study data, EHR, and claims data. 
  Experience selecting and organizing data for training and validating algorithms, managing constraints and requirements. 
  Demonstrated ability to work with structured and unstructured data across multiple modalities. 
  Hands on experience with technologies leveraging multiple servers for data intensive tasks. 
  Experience with software development in Python, including libraries such as pandas and scikit-learn 
  Experience creating data pipelines that transform raw data into insights. 
  Hands on Experience on Data Analytics Services including Athena, Glue Data Catalog and Quicksight 
  Accessing and parsing data from S3 through python API calls 
  Experience and familiarity with AWS Sagemaker 
  Extract, transform and load data from different formats like JSON, databases and integrate results for algorithm training and validation 
  Database familiarity including Oracle, MySQL, Redshift, DynamoDB and Elastic Cache 
  
 Desired/Preferred Technical Qualifications 
  
  MS/PhD in science or technical field 
  Experience with FDA 21 CFR part 820 and IEC standards 62304, 14971, and 62366 
  Experience working with large amounts of data and tools to handle them e.g. AWS, Spark, SQL 
  Working knowledge of signal processing 
  Experience using or managing Splunk, Tableau 
  
 
  What's in it for you: 
   This is a full-time position with competitive compensation package, excellent benefits including medical, dental, and vision insurance (all of which start on your first day), paid holidays, and PTO! 
   iRhythm also provides additional benefits including 401K (with company match), an Employee Stock Purchase Plan, paid parental leave, pet insurance discount, Cultural Committee/Charity events, and so much more! 
   
   FLSA Status: Exempt 
    As a part of our core values, we ensure a diverse and inclusive workforce. We welcome and celebrate people of all backgrounds, experiences, skills, and perspectives. iRhythm Technologies, Inc. is an Equal Opportunity Employer (M/F/V/D). We will consider for employment all qualified applicants with arrest and conviction records in accordance with all applicable laws. 
    Make iRhythm your path forward. 
    #LI-AR1 
    #LI-Remote 
   
 
 
 
  
   Actual compensation may vary depending on job-related factors including knowledge, skills, experience, and work location.
  
  
   Estimated Pay Range
  
    $119,800—$174,500 USD","<div>
 <p><b>About Us:</b></p> 
 <p> iRhythm is a leading digital healthcare company focused on the way cardiac arrhythmias are clinically diagnosed by combining our wearable biosensing technology with powerful cloud-based data analytics and machine-learning capabilities. Our goal is to be the leading provider of first-line ambulatory ECG monitoring for patients at risk for arrhythmias. iRhythm&apos;s continuous ambulatory monitoring has already put over 6 million patients and their doctors on a shorter path to what they both need &#x2013; answers.</p> 
 <p><b> About this Role:</b></p> 
 <p> As a<b> Sr Research Data Engineer </b>within the iRhythm Research &amp; Systems Engineering team, you will be exposed to a diverse set of engineering problems spanning Machine Learning, Big Data, Multi-Modal data, Data Privacy, AWS, Batch Processing and Mobile technologies in the context of addressing unmet clinical needs. You will have the opportunity to work with a talented team to develop a deep understanding of our end-to-end systems, helping to drive successful delivery of software projects throughout its development life cycle.</p> 
 <p> Note: This role will work remote from a US based home office. We are unable to offer any sort of sponsorship for this role.</p> 
 <p><b> About You</b></p> 
 <ul> 
  <li>You are a quality-minded individual with ability to go detail-oriented and scientific while having the big picture in mind</li> 
  <li>You enjoy investigative and troubleshooting processes using your excellent analytical and problem-solving skills, along with a strong sense of urgency</li> 
  <li>You are self-motivated and demonstrate initiative in helping others</li> 
  <li>You are independent and thrive in the face of ambiguous, open-ended challenges</li> 
  <li>You have effective presentation and interpersonal skills communicating technical information to a variety of stakeholders</li> 
  <li>You enjoy collaborating with cross functional groups</li> 
 </ul> 
 <p><b>Responsibilities</b></p> 
 <ul> 
  <li>Work with engineering and cross-functional stakeholders to research, define, and write technical requirements, figure out data or operational workflows, and plan for V&amp;V/release activities</li> 
  <li>Design and code pipelines that efficiently transform our raw data into formats and structures that best serve our researchers (Python and algorithm design)</li> 
  <li>Perform exploratory research with physiological data, with good software practices in mind</li> 
  <li>Develop, test, and document proof-of-concept algorithms and/or software tools</li> 
  <li>Participate and support project management, software development, and software QA in the software development life cycle and regulatory submissions as system expert</li> 
  <li>Update and maintain Design History File, participate in risk management/hazards analysis activities</li> 
  <li>Develop, document, and monitor testing regimes that ensure performant code and data integrity</li> 
  <li>Maintain a clean and well-documented code base and audit trail (git, Bitbucket, Confluence, JIRA)</li> 
 </ul> 
 <p><b>Basic Qualifications</b></p> 
 <ul> 
  <li>Combined 5+ years of algorithm/software development and systems engineering experience in the Healthcare, Medical Device, or other regulated software industry.</li> 
  <li>Software development experience with one or more of Python, Java, C#, or C/C++</li> 
  <li>Understanding of statistics and proficiency with data analysis. Proficiency in Linux/Unix/Windows environments</li> 
  <li>BA/BS in science or technical field</li> 
  <li>Experience processing and integrating developmental data, clinical study data, EHR, and claims data.</li> 
  <li>Experience selecting and organizing data for training and validating algorithms, managing constraints and requirements.</li> 
  <li>Demonstrated ability to work with structured and unstructured data across multiple modalities.</li> 
  <li>Hands on experience with technologies leveraging multiple servers for data intensive tasks.</li> 
  <li>Experience with software development in Python, including libraries such as pandas and scikit-learn</li> 
  <li>Experience creating data pipelines that transform raw data into insights.</li> 
  <li>Hands on Experience on Data Analytics Services including Athena, Glue Data Catalog and Quicksight</li> 
  <li>Accessing and parsing data from S3 through python API calls</li> 
  <li>Experience and familiarity with AWS Sagemaker</li> 
  <li>Extract, transform and load data from different formats like JSON, databases and integrate results for algorithm training and validation</li> 
  <li>Database familiarity including Oracle, MySQL, Redshift, DynamoDB and Elastic Cache</li> 
 </ul> 
 <p><b>Desired/Preferred Technical Qualifications</b></p> 
 <ul> 
  <li>MS/PhD in science or technical field</li> 
  <li>Experience with FDA 21 CFR part 820 and IEC standards 62304, 14971, and 62366</li> 
  <li>Experience working with large amounts of data and tools to handle them e.g. AWS, Spark, SQL</li> 
  <li>Working knowledge of signal processing</li> 
  <li>Experience using or managing Splunk, Tableau</li> 
 </ul> 
 <div>
  <p><b>What&apos;s in it for you:</b></p> 
  <p> This is a full-time position with competitive compensation package, excellent benefits including medical, dental, and vision insurance (all of which start on your first day), paid holidays, and PTO!</p> 
  <p> iRhythm also provides additional benefits including 401K (with company match), an Employee Stock Purchase Plan, paid parental leave, pet insurance discount, Cultural Committee/Charity events, and so much more!</p> 
  <div> 
   <p><b>FLSA Status: </b>Exempt</p> 
   <p><i> As a part of our core values, we ensure a diverse and inclusive workforce. We welcome and celebrate people of all backgrounds, experiences, skills, and perspectives. iRhythm Technologies, Inc. is an Equal Opportunity Employer (M/F/V/D). We will consider for employment all qualified applicants with arrest and conviction records in accordance with all applicable laws.</i></p> 
   <p> Make iRhythm your path forward.</p> 
   <p> #LI-AR1</p> 
   <p> #LI-Remote</p> 
  </div> 
 </div>
 <p></p>
 <div>
  <div>
   <p>Actual compensation may vary depending on job-related factors including knowledge, skills, experience, and work location.</p>
  </div>
  <p></p>
  <p><b><br> Estimated Pay Range</b></p>
  <div>
    &#x24;119,800&#x2014;&#x24;174,500 USD
  </div>
 </div>
</div>",https://www.irhythmtech.com/company/job-openings?gh_jid=5400674&gh_src=c3f698341us,700c104561e30304,,Full-time,,,"Deerfield, IL",Senior Research Data Engineer,12 days ago,2023-10-06T13:33:23.359Z,3.2,95.0,"$119,800 - $174,500 a year",2023-10-18T13:33:23.362Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=700c104561e30304&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
67,CrowdStrike,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We work on large scale distributed systems, processing over 1 trillion events a day with a petabyte of RAM deployed in our Cassandra clusters - and this traffic is growing daily. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to developing and shaping our cybersecurity platform. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
 
 
 
   About the Role
   CrowdStrike is looking to hire a Senior Engineer to the Data Services team to help us take our database systems to the next level. We’re looking for a highly-technical, hands-on engineer, who loves to work with data plane services like Cassandra, ElasticSearch, and Kafka, and is comfortable building automation around large-scale cloud-based critical systems. We’ll be looking at candidate CVs with an eye on achievement - what you’ve accomplished in the past tells us the most about what you can do for us in the future.
 
 
   What You'll Do:
 
 
  
   
     Maintain a deep understanding of the data components - including Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark, and use that understanding to operate and automate properly configured clusters.
   
  
   
     Work with Engineering to roll out new products and features.
   
  
   
     Develop infrastructure services to support the CrowdStrike engineering team’s pursuit of a full devops model.
   
  
   
     Work closely with Engineering and Customer Support to troubleshoot time-sensitive production issues, regardless of when they happen.
   
  
   
     Keep petabytes of critical business data safe, secure, and available.
   
 
 
 
   What You’ll Need:
 
 
  
   
     Experience with large scale datastores using technologies like Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark.
   
  
   
     Experience with large-scale, business-critical Linux environments
   
  
   
     Experience operating within the cloud, preferably Amazon Web Services
   
  
   
     Proven ability to work effectively with both local and remote teams
   
  
   
     Track record of making great decisions, particularly when it matters most
   
  
   
     Rock solid communication skills, verbal and written
   
  
   
     A combination of confidence and independence... with the prudence to know when to ask for help from the rest of the team
   
  
   
     Experience in the information security industry preferred, but not required
   
  
   
     Bachelor’s degree in an applicable field, such as CS, CIS or Engineering
   
 
 
   #LI-SS1
 
 
   #LI-MW1
 
 
   #LI-Remote
 
 
   #HTF
 
 
 
   Benefits of Working at CrowdStrike:
 
 
  
   
     Remote-first culture
   
  
   
     Market leader in compensation and equity awards
   
  
   
     Competitive vacation and flexible working arrangements
   
  
   
     Comprehensive and inclusive health benefits
   
  
   
     Physical and mental wellness programs
   
  
   
     Paid parental leave, including adoption
   
  
   
     A variety of professional development and mentorship opportunities
   
  
   
     Offices with stocked kitchens when you need to fuel innovation and collaboration
   
 
 
 
  
    We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
  
 
 
 
   CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact 
  
   Recruiting@crowdstrike.com
   for further assistance.
 
 
 
   CrowdStrike participates in the E-Verify program.
 
 
  
    Notice of E-Verify Participation
  
 
 
  
    Right to Work
  
  CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $105,000 - $195,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.","<div>
 <div>
  #WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We work on large scale distributed systems, processing over 1 trillion events a day with a petabyte of RAM deployed in our Cassandra clusters - and this traffic is growing daily. We&#x2019;re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to developing and shaping our cybersecurity platform. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
 </div>
 <div></div>
 <div>
   About the Role
  <br> CrowdStrike is looking to hire a Senior Engineer to the Data Services team to help us take our database systems to the next level. We&#x2019;re looking for a highly-technical, hands-on engineer, who loves to work with data plane services like Cassandra, ElasticSearch, and Kafka, and is comfortable building automation around large-scale cloud-based critical systems. We&#x2019;ll be looking at candidate CVs with an eye on achievement - what you&#x2019;ve accomplished in the past tells us the most about what you can do for us in the future.
 </div>
 <div>
  <br> What You&apos;ll Do:
 </div>
 <ul>
  <li>
   <div>
     Maintain a deep understanding of the data components - including Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark, and use that understanding to operate and automate properly configured clusters.
   </div></li>
  <li>
   <div>
     Work with Engineering to roll out new products and features.
   </div></li>
  <li>
   <div>
     Develop infrastructure services to support the CrowdStrike engineering team&#x2019;s pursuit of a full devops model.
   </div></li>
  <li>
   <div>
     Work closely with Engineering and Customer Support to troubleshoot time-sensitive production issues, regardless of when they happen.
   </div></li>
  <li>
   <div>
     Keep petabytes of critical business data safe, secure, and available.
   </div></li>
 </ul>
 <div></div>
 <div>
   What You&#x2019;ll Need:
 </div>
 <ul>
  <li>
   <div>
     Experience with large scale datastores using technologies like Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark.
   </div></li>
  <li>
   <div>
     Experience with large-scale, business-critical Linux environments
   </div></li>
  <li>
   <div>
     Experience operating within the cloud, preferably Amazon Web Services
   </div></li>
  <li>
   <div>
     Proven ability to work effectively with both local and remote teams
   </div></li>
  <li>
   <div>
     Track record of making great decisions, particularly when it matters most
   </div></li>
  <li>
   <div>
     Rock solid communication skills, verbal and written
   </div></li>
  <li>
   <div>
     A combination of confidence and independence... with the prudence to know when to ask for help from the rest of the team
   </div></li>
  <li>
   <div>
     Experience in the information security industry preferred, but not required
   </div></li>
  <li>
   <div>
     Bachelor&#x2019;s degree in an applicable field, such as CS, CIS or Engineering
   </div></li>
 </ul>
 <div>
  <br> #LI-SS1
 </div>
 <div>
   #LI-MW1
 </div>
 <div>
   #LI-Remote
 </div>
 <div>
   #HTF
 </div>
 <div></div>
 <div>
   Benefits of Working at CrowdStrike:
 </div>
 <ul>
  <li>
   <div>
     Remote-first culture
   </div></li>
  <li>
   <div>
     Market leader in compensation and equity awards
   </div></li>
  <li>
   <div>
     Competitive vacation and flexible working arrangements
   </div></li>
  <li>
   <div>
     Comprehensive and inclusive health benefits
   </div></li>
  <li>
   <div>
     Physical and mental wellness programs
   </div></li>
  <li>
   <div>
     Paid parental leave, including adoption
   </div></li>
  <li>
   <div>
     A variety of professional development and mentorship opportunities
   </div></li>
  <li>
   <div>
     Offices with stocked kitchens when you need to fuel innovation and collaboration
   </div></li>
 </ul>
 <div></div>
 <div>
  <div>
    We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
  </div>
 </div>
 <div></div>
 <div>
   CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact 
  <div>
   Recruiting@crowdstrike.com
  </div> for further assistance.
 </div>
 <div></div>
 <div>
   CrowdStrike participates in the E-Verify program.
 </div>
 <div>
  <div>
    Notice of E-Verify Participation
  </div>
 </div>
 <div>
  <div>
    Right to Work
  </div>
 </div> CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is &#x24;105,000 - &#x24;195,000 per year + variable/incentive compensation + equity + benefits. A candidate&#x2019;s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
</div>",https://crowdstrike.wd5.myworkdayjobs.com/en-US/crowdstrikecareers/job/USA---Remote/Sr-Systems-Engineer---Data-Services--Remote-_R14626?indeed_organic,86616a549b160eb2,,Full-time,,,Remote,Sr. Systems Engineer - Data Services (Remote),11 days ago,2023-10-07T13:33:32.605Z,3.6,36.0,"$105,000 - $195,000 a year",2023-10-18T13:33:32.608Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=86616a549b160eb2&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
68,Cypress Consulting,"Sr. Data Center Engineer/Network Security Consultant
We are seeking an Advanced Services Engineer to provide advanced support, guidance and assistance to address specific customer needs. In this position, you will be working as a technology expert in the Routing & Switching space to design, implement, and support (troubleshoot) the deployments within a customer infrastructure. The ideal candidate will also have a level of comfort communicating across all functions within the organization, as well as with clients and partners. Previous Service Provider experience is a huge plus.
Essential Functions of the Job:

 You will provide advanced post-sales support for a large Data Center networking deployment for a large customer.
 Review customer network designs for an EVPN, VxLAN, leaf-spine architecture and make recommendations for deployment
 Migrate or interconnect to/from Cisco, Juniper and other vendors to Arista infrastructure
 Assist with configuration build outs including creating network provisioning automation using Python and tools such as Chef or Ansible
 Assist with implementation and change controls
 You will assist with proof of concepts (POC) and in-depth testing to validate design scenario
 Provide bug scrubs and code recommendations
 Provide interface to TAC and internal development teams and the customer
 You will provide customer advice regarding architectural questions, product prerequisites, product features, etc.
 Translate complex business requirements into Leaf-Spine Network solutions
 Designing Network solutions utilizing extensive experience with routing protocols including advanced BGP design as well as extensive EVPN/VXLAN experience.
 Establish and maintaining strong relationships with key partners
 Attend key partner events, training sessions, and provide ongoing training with the customer teams globally
 Continue training to maintain expertise
 Ability to understand the client’s business objectives and technical needs
 Ability to meet Service Level Agreements (SLAs) for sales and clients
 Regularly exercises discretion and independent judgment
 Maintain professional relationships with teammates, partners, and clients
 Some travel may be required within assigned territory

Qualifications
Required Skills and Experience

 Bachelor’s degree in Computer Science or equivalent years of experience
 Network Industry Certification preferred (e.g. CCIE (R&S), JNCIE)
 5+ years’ working experience with network technologies including network design and deployments of Campus and Data Center networks. Knowledge of leaf-spine architectures highly desired.
 5+ years’ minimum experience with Cisco-based technologies focusing on infrastructure and voice
 Demonstrated experience in technical post-sales, as either a Network Consulting Engineer or as an Advanced Systems (AS) Engineer
 Experience with Cisco enterprise routing/switching within large data center enterprise customers (Catalyst, Nexus, ASR)
 Expert knowledge in the following areas: Ethernet, VLANs, VxLAN, EVPN, IP Routing, TCP/IP, OSPF, BGP, eBGP, Multicast, QoS
 Expertise in at least one area of Data Center related technologies - Openstack, SDN, NFV, Load Balancers, Virtualization, Linux tools
 Expert level knowledge of industry-standard CLI
 Ability to write white papers a plus
 Background in Perl, Python, Scripting for creating network automation is highly desired
 Excellent customer service and verbal communication skills
 Excellent written skills and the ability to do related documentation and ticket tracking of opportunities/meeting follow-up

Job Type: Full-time
Pay: $115.00 - $125.00 per hour
Benefits:

 Dental insurance
 Health insurance
 Paid time off
 Professional development assistance
 Referral program
 Retirement plan
 Vision insurance

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Experience:

 Data Center Design: 5 years (Required)
 BGP, SD-WAN, VXLAN: 5 years (Required)
 Consultative/Customer-facing: 3 years (Required)
 Layer 2/3 switching and routing: 2 years (Required)
 Service Provider: 2 years (Preferred)

Work Location: Remote","<p><b>Sr. Data Center Engineer/Network Security Consultant</b></p>
<p>We are seeking an Advanced Services Engineer to provide advanced support, guidance and assistance to address specific customer needs. In this position, you will be working as a technology expert in the Routing &amp; Switching space to design, implement, and support (troubleshoot) the deployments within a customer infrastructure. The ideal candidate will also have a level of comfort communicating across all functions within the organization, as well as with clients and partners. Previous Service Provider experience is a huge plus.</p>
<p><b>Essential Functions of the Job:</b></p>
<ul>
 <li>You will provide advanced post-sales support for a large Data Center networking deployment for a large customer.</li>
 <li>Review customer network designs for an EVPN, VxLAN, leaf-spine architecture and make recommendations for deployment</li>
 <li>Migrate or interconnect to/from Cisco, Juniper and other vendors to Arista infrastructure</li>
 <li>Assist with configuration build outs including creating network provisioning automation using Python and tools such as Chef or Ansible</li>
 <li>Assist with implementation and change controls</li>
 <li>You will assist with proof of concepts (POC) and in-depth testing to validate design scenario</li>
 <li>Provide bug scrubs and code recommendations</li>
 <li>Provide interface to TAC and internal development teams and the customer</li>
 <li>You will provide customer advice regarding architectural questions, product prerequisites, product features, etc.</li>
 <li>Translate complex business requirements into Leaf-Spine Network solutions</li>
 <li>Designing Network solutions utilizing extensive experience with routing protocols including advanced BGP design as well as extensive EVPN/VXLAN experience.</li>
 <li>Establish and maintaining strong relationships with key partners</li>
 <li>Attend key partner events, training sessions, and provide ongoing training with the customer teams globally</li>
 <li>Continue training to maintain expertise</li>
 <li>Ability to understand the client&#x2019;s business objectives and technical needs</li>
 <li>Ability to meet Service Level Agreements (SLAs) for sales and clients</li>
 <li>Regularly exercises discretion and independent judgment</li>
 <li>Maintain professional relationships with teammates, partners, and clients</li>
 <li>Some travel may be required within assigned territory</li>
</ul>
<p><b>Qualifications</b></p>
<p><b>Required Skills and Experience</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in Computer Science or equivalent years of experience</li>
 <li>Network Industry Certification preferred (e.g. CCIE (R&amp;S), JNCIE)</li>
 <li>5+ years&#x2019; working experience with network technologies including network design and deployments of Campus and Data Center networks. Knowledge of leaf-spine architectures highly desired.</li>
 <li>5+ years&#x2019; minimum experience with Cisco-based technologies focusing on infrastructure and voice</li>
 <li>Demonstrated experience in technical post-sales, as either a Network Consulting Engineer or as an Advanced Systems (AS) Engineer</li>
 <li>Experience with Cisco enterprise routing/switching within large data center enterprise customers (Catalyst, Nexus, ASR)</li>
 <li>Expert knowledge in the following areas: Ethernet, VLANs, VxLAN, EVPN, IP Routing, TCP/IP, OSPF, BGP, eBGP, Multicast, QoS</li>
 <li>Expertise in at least one area of Data Center related technologies - Openstack, SDN, NFV, Load Balancers, Virtualization, Linux tools</li>
 <li>Expert level knowledge of industry-standard CLI</li>
 <li>Ability to write white papers a plus</li>
 <li>Background in Perl, Python, Scripting for creating network automation is highly desired</li>
 <li>Excellent customer service and verbal communication skills</li>
 <li>Excellent written skills and the ability to do related documentation and ticket tracking of opportunities/meeting follow-up</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;115.00 - &#x24;125.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Referral program</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Center Design: 5 years (Required)</li>
 <li>BGP, SD-WAN, VXLAN: 5 years (Required)</li>
 <li>Consultative/Customer-facing: 3 years (Required)</li>
 <li>Layer 2/3 switching and routing: 2 years (Required)</li>
 <li>Service Provider: 2 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,94a5ae6b3646efe1,,Full-time,,,Remote,Sr. Data Center Design Engineer/Consultant -100% Remote,11 days ago,2023-10-07T13:33:40.341Z,,,$115 - $125 an hour,2023-10-18T13:33:40.343Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=94a5ae6b3646efe1&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
69,Cypress Consulting,"Sr. Data Center Engineer/Deployment Engineer
We are seeking a Deployment Engineer to provide advanced support, guidance and assistance to address specific customer needs. In this position, you will be working as a technology expert in the Routing & Switching space to design, implement, and support (troubleshoot) the deployments within a customer infrastructure. The ideal candidate will also have a level of comfort communicating across all functions within the organization, as well as with clients and partners. Arista configuration and CloudVision usage experience is ideal. Should be familiar with EVPN/VXLAN and proven deployment models. Should also be comfortable with documentation and tools like MS Excel/Viso and word.
Essential Functions of the Job:

 You will provide advanced post-sales support for a large Data Center networking deployment for a large customer.
 Review customer network designs for an EVPN, VxLAN, leaf-spine architecture and make recommendations for deployment
 Migrate or interconnect to/from Cisco, Juniper and other vendors to Arista infrastructure
 Assist with implementation and change controls
 You will assist with proof of concepts (POC) and in-depth testing to validate design scenario
 Provide interface to TAC and internal development teams and the customer
 Designing Network solutions utilizing extensive experience with routing protocols including advanced BGP design as well as extensive EVPN/VXLAN experience.
 Establish and maintaining strong relationships with key partners
 Continue training to maintain expertise
 Ability to understand the client’s business objectives and technical needs
 Ability to meet Service Level Agreements (SLAs) for sales and clients
 Regularly exercises discretion and independent judgment
 Maintain professional relationships with teammates, partners, and clients
 Some travel may be required within assigned territory

Qualifications
Required Skills and Experience

 Bachelor’s degree in Computer Science or equivalent years of experience
 5+ years’ working experience with network technologies including network design and deployments of Campus and Data Center networks. Knowledge of leaf-spine architectures highly desired.
 5+ Years’ minimum experience with Cisco-based technologies focusing on infrastructure and voice
 Experience with Cisco enterprise routing/switching within large data center enterprise customers (Catalyst, Nexus, ASR)
 Expert knowledge in the following areas: Ethernet, VLANs, VxLAN, EVPN, IP Routing, TCP/IP, OSPF, BGP, eBGP, Multicast, QoS
 Background in Perl, Python, Scripting for creating network automation is highly desired
 Excellent customer service and verbal communication skills
 Excellent written skills and the ability to do related documentation and ticket tracking of opportunities/meeting follow-up

Job Type: Full-time
Pay: $115.00 - $125.00 per hour
Benefits:

 Dental insurance
 Health insurance
 Paid time off
 Professional development assistance
 Referral program
 Retirement plan
 Vision insurance

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Experience:

 Layer 2/3 switching and routing: 2 years (Required)
 Data Center Design: 3 years (Required)
 BGP, SD-WAN, VXLAN: 3 years (Required)
 Consultative/Customer-facing: 2 years (Required)

Work Location: Remote","<p><b>Sr. Data Center Engineer/Deployment Engineer</b></p>
<p>We are seeking a Deployment Engineer to provide advanced support, guidance and assistance to address specific customer needs. In this position, you will be working as a technology expert in the Routing &amp; Switching space to design, implement, and support (troubleshoot) the deployments within a customer infrastructure. The ideal candidate will also have a level of comfort communicating across all functions within the organization, as well as with clients and partners. Arista configuration and CloudVision usage experience is ideal. Should be familiar with EVPN/VXLAN and proven deployment models. Should also be comfortable with documentation and tools like MS Excel/Viso and word.</p>
<p><b>Essential Functions of the Job:</b></p>
<ul>
 <li>You will provide advanced post-sales support for a large Data Center networking deployment for a large customer.</li>
 <li>Review customer network designs for an EVPN, VxLAN, leaf-spine architecture and make recommendations for deployment</li>
 <li>Migrate or interconnect to/from Cisco, Juniper and other vendors to Arista infrastructure</li>
 <li>Assist with implementation and change controls</li>
 <li>You will assist with proof of concepts (POC) and in-depth testing to validate design scenario</li>
 <li>Provide interface to TAC and internal development teams and the customer</li>
 <li>Designing Network solutions utilizing extensive experience with routing protocols including advanced BGP design as well as extensive EVPN/VXLAN experience.</li>
 <li>Establish and maintaining strong relationships with key partners</li>
 <li>Continue training to maintain expertise</li>
 <li>Ability to understand the client&#x2019;s business objectives and technical needs</li>
 <li>Ability to meet Service Level Agreements (SLAs) for sales and clients</li>
 <li>Regularly exercises discretion and independent judgment</li>
 <li>Maintain professional relationships with teammates, partners, and clients</li>
 <li>Some travel may be required within assigned territory</li>
</ul>
<p><b>Qualifications</b></p>
<p><b>Required Skills and Experience</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in Computer Science or equivalent years of experience</li>
 <li>5+ years&#x2019; working experience with network technologies including network design and deployments of Campus and Data Center networks. Knowledge of leaf-spine architectures highly desired.</li>
 <li>5+ Years&#x2019; minimum experience with Cisco-based technologies focusing on infrastructure and voice</li>
 <li>Experience with Cisco enterprise routing/switching within large data center enterprise customers (Catalyst, Nexus, ASR)</li>
 <li>Expert knowledge in the following areas: Ethernet, VLANs, VxLAN, EVPN, IP Routing, TCP/IP, OSPF, BGP, eBGP, Multicast, QoS</li>
 <li>Background in Perl, Python, Scripting for creating network automation is highly desired</li>
 <li>Excellent customer service and verbal communication skills</li>
 <li>Excellent written skills and the ability to do related documentation and ticket tracking of opportunities/meeting follow-up</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;115.00 - &#x24;125.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Referral program</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Layer 2/3 switching and routing: 2 years (Required)</li>
 <li>Data Center Design: 3 years (Required)</li>
 <li>BGP, SD-WAN, VXLAN: 3 years (Required)</li>
 <li>Consultative/Customer-facing: 2 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,019fe648ac2dfa54,,Full-time,,,Remote,Sr. Data Center Design Engineer/Deployment Engineer -100% Remote,11 days ago,2023-10-07T13:33:40.403Z,,,$115 - $125 an hour,2023-10-18T13:33:40.405Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=019fe648ac2dfa54&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
70,Blackhawk Network,"About Blackhawk Network: 
 
   Blackhawk Network (BHN) is the leader in global branded payment technologies. We strengthen relationships between brands and their customers, employees, and partners by transforming transactions into connections. BHN’s portfolio includes: Gift Card & eGift products, promotions and distribution that grow revenue faster; Rewards & Incentives that build loyalty and acquisition and are integrated into today’s leading platforms; and Payments that enable businesses and customers to access and disburse funds in convenient and innovative ways. BHN’s network spans across the globe with over 400,000 consumer touchpoints. Learn more at BHN.com.
 
 
 
   This position may be performed remotely anywhere within the United States except for the State of Alaska, North Dakota, or South Dakota.
  Overview: 
 
   We are looking for an experienced data scientist to lead the initiatives we’re taking, to improve the product offerings of Blackhawk Network, from the perspective of risk modelling and business forecasting (prescriptive & predictive).
 
 
 
   As a team member in core data science team, you will own the research charter for data and decision science to enable the business stakeholders to be data-driven and deterministic, by providing insights into the decisions at-hand and also roadmap planning.
 
 
 
   You will be working in a team of Data Scientists to enable a culture of 360-degree analysis of business, with ownership of the modelling environments and ML models. You will get the support to evangelize sound practices for prototyping of concepts, to fail-fast and/or maintain continuum of persistent research.
 
 
 
   You will collaborate with multi-disciplinary teams of engineers, product owners & business stakeholders to solve complex & ambiguous problems, in the domains of:
 
 
   Gift Cards E-Commerce (B2B & B2C) 
  Forecasting of Inventory, Traffic & Breakage 
  Risk Modelling (Scorecards) 
  Fraud Detection & Prevention 
  Loyalty & Rewards Programs Modelling 
 Responsibilities: 
 
   You will:
 
 
   Partner closely with Product, Engineering, Marketing, Sales, Finance and Data Science teams to shape product strategy using rigorous scientific solutions
   Apply statistical, machine learning and econometric models on large datasets to: i) measure results and outcomes of our current models and product strategies, ii) optimize user experience while minimizing fraud/risks
   Design, conduct, and analyze experiments to quantify the impact of product and operation changes
   Develop metrics to guide product development, and create dashboards for key performance indicators and deep dive to understand the drivers
   Drive the collection of new data and the refinement of existing data sources
  Qualifications: 
 
  Masters (or equivalent) degree in a quantitative discipline (Statistics, Operations Research, Data Science, Mathematics, Physics, Engineering etc.). 
  A strong background in advanced mathematics, in particular statistics & probability theory, data mining, and machine learning. 
  5+ years of overall professional experience with 3+ years in data science, doing exploratory data analysis, testing hypotheses, and building prescriptive & predictive models.
   Demonstrated experience in creation, deployment and performance evaluation of supervised and unsupervised machine learning models.
   Proficiency in Python. 
  Experience handling terabyte size datasets, diving into data to discover hidden patterns, using data visualization tools, writing SQL.
   Strong Communication: ability to articulate clearly, navigate & adapt across different seniority levels. 
  Ability to use statistical, algorithmic, data mining, and visualization techniques to model complex problems, find opportunities, discover solutions, and deliver actionable business insights. 
  Be passionate about collaborating daily with your team and other groups while working via a distributed global operating model. 
  Be eager to help your teammates, share your knowledge with them, and learn from them. 
 
 
  Preferred Qualifications
 
 
   Experience with MLOPs frameworks
   Experience with creating explainable ML model
   Experience with source code control systems like Git
   Experience with data visualization and business intelligence tools like PowerBI
   Knowledge of experimental design and A/B testing
   Experience in an Agile development environment
   Experience with AWS cloud environment
   Proven ability to quickly learn and apply new techniques
   Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment 
  Must be a team-player and capable of handling multi-tasks in a dynamic environment
   Excellent business writing, verbal communication, and presentation skills
  Benefits: 
 
   Salary Range for all U.S. Residents (excluding Alaska, California, North Dakota, South Dakota): $125,430.00 to $149,000.00
 
 
   Salary Range for California Residents Only: $99,530.00 to $149,000.00
 
 
 
   Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Blackhawk Network offers benefits including 401k with employer match, medical, dental, vision, 12 paid holidays in the year 2023, sick pay accrual according to state law, parental leave, life insurance, disability insurance, accident and illness insurance, health and dependent care flexible spending accounts, wellness benefits, and flexible time off for all full-time employees. 
 EEO Statement: 
 
   Blackhawk Network provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Blackhawk Network believes that diversity leads to strength. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
 
 
 
   Blackhawk Network encourages applicants with previous criminal records to apply to all positions and, pursuant to the San Francisco and Los Angeles Fair Chance Acts (and other “Fair Chance” laws), Blackhawk Network will consider for employment qualified applicants with arrest and conviction records. For Philadelphia applicants or jobs, please see a copy of Philadelphia’s ordinance on this topic by clicking this link: https://codelibrary.amlegal.com/codes/philadelphia/latest/philadelphia_pa/0-0-0-280104.","<div>
 About Blackhawk Network: 
 <div>
   Blackhawk Network (BHN) is the leader in global branded payment technologies. We strengthen relationships between brands and their customers, employees, and partners by transforming transactions into connections. BHN&#x2019;s portfolio includes: Gift Card &amp; eGift products, promotions and distribution that grow revenue faster; Rewards &amp; Incentives that build loyalty and acquisition and are integrated into today&#x2019;s leading platforms; and Payments that enable businesses and customers to access and disburse funds in convenient and innovative ways. BHN&#x2019;s network spans across the globe with over 400,000 consumer touchpoints. Learn more at BHN.com.
 </div>
 <div></div>
 <div>
  <br> This position may be performed remotely anywhere within the United States except for the State of Alaska, North Dakota, or South Dakota.
 </div> Overview: 
 <div>
   We are looking for an experienced data scientist to lead the initiatives we&#x2019;re taking, to improve the product offerings of Blackhawk Network, from the perspective of risk modelling and business forecasting (prescriptive &amp; predictive).
 </div>
 <div></div>
 <div>
  <br> As a team member in core data science team, you will own the research charter for data and decision science to enable the business stakeholders to be data-driven and deterministic, by providing insights into the decisions at-hand and also roadmap planning.
 </div>
 <div></div>
 <div>
  <br> You will be working in a team of Data Scientists to enable a culture of 360-degree analysis of business, with ownership of the modelling environments and ML models. You will get the support to evangelize sound practices for prototyping of concepts, to fail-fast and/or maintain continuum of persistent research.
 </div>
 <div></div>
 <div>
  <br> You will collaborate with multi-disciplinary teams of engineers, product owners &amp; business stakeholders to solve complex &amp; ambiguous problems, in the domains of:
 </div>
 <ul>
  <li> Gift Cards E-Commerce (B2B &amp; B2C) </li>
  <li>Forecasting of Inventory, Traffic &amp; Breakage </li>
  <li>Risk Modelling (Scorecards) </li>
  <li>Fraud Detection &amp; Prevention </li>
  <li>Loyalty &amp; Rewards Programs Modelling </li>
 </ul>Responsibilities: 
 <div>
   You will:
 </div>
 <ul>
  <li> Partner closely with Product, Engineering, Marketing, Sales, Finance and Data Science teams to shape product strategy using rigorous scientific solutions</li>
  <li> Apply statistical, machine learning and econometric models on large datasets to: i) measure results and outcomes of our current models and product strategies, ii) optimize user experience while minimizing fraud/risks</li>
  <li> Design, conduct, and analyze experiments to quantify the impact of product and operation changes</li>
  <li> Develop metrics to guide product development, and create dashboards for key performance indicators and deep dive to understand the drivers</li>
  <li> Drive the collection of new data and the refinement of existing data sources</li>
 </ul> Qualifications: 
 <ul>
  <li>Masters (or equivalent) degree in a quantitative discipline (Statistics, Operations Research, Data Science, Mathematics, Physics, Engineering etc.). </li>
  <li>A strong background in advanced mathematics, in particular statistics &amp; probability theory, data mining, and machine learning. </li>
  <li>5+ years of overall professional experience with 3+ years in data science, doing exploratory data analysis, testing hypotheses, and building prescriptive &amp; predictive models.</li>
  <li> Demonstrated experience in creation, deployment and performance evaluation of supervised and unsupervised machine learning models.</li>
  <li> Proficiency in Python. </li>
  <li>Experience handling terabyte size datasets, diving into data to discover hidden patterns, using data visualization tools, writing SQL.</li>
  <li> Strong Communication: ability to articulate clearly, navigate &amp; adapt across different seniority levels. </li>
  <li>Ability to use statistical, algorithmic, data mining, and visualization techniques to model complex problems, find opportunities, discover solutions, and deliver actionable business insights. </li>
  <li>Be passionate about collaborating daily with your team and other groups while working via a distributed global operating model. </li>
  <li>Be eager to help your teammates, share your knowledge with them, and learn from them. </li>
 </ul>
 <div>
  <b>Preferred Qualifications</b>
 </div>
 <ul>
  <li> Experience with MLOPs frameworks</li>
  <li> Experience with creating explainable ML model</li>
  <li> Experience with source code control systems like Git</li>
  <li> Experience with data visualization and business intelligence tools like PowerBI</li>
  <li> Knowledge of experimental design and A/B testing</li>
  <li> Experience in an Agile development environment</li>
  <li> Experience with AWS cloud environment</li>
  <li> Proven ability to quickly learn and apply new techniques</li>
  <li> Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment </li>
  <li>Must be a team-player and capable of handling multi-tasks in a dynamic environment</li>
  <li> Excellent business writing, verbal communication, and presentation skills</li>
 </ul> Benefits: 
 <div>
   Salary Range for all U.S. Residents (excluding Alaska, California, North Dakota, South Dakota): &#x24;125,430.00 to &#x24;149,000.00
 </div>
 <div>
   Salary Range for California Residents Only: &#x24;99,530.00 to &#x24;149,000.00
 </div>
 <div></div>
 <div>
  <br> Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Blackhawk Network offers benefits including 401k with employer match, medical, dental, vision, 12 paid holidays in the year 2023, sick pay accrual according to state law, parental leave, life insurance, disability insurance, accident and illness insurance, health and dependent care flexible spending accounts, wellness benefits, and flexible time off for all full-time employees. 
 </div>EEO Statement: 
 <div>
   Blackhawk Network provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Blackhawk Network believes that diversity leads to strength. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
 </div>
 <div></div>
 <div>
  <br> Blackhawk Network encourages applicants with previous criminal records to apply to all positions and, pursuant to the San Francisco and Los Angeles Fair Chance Acts (and other &#x201c;Fair Chance&#x201d; laws), Blackhawk Network will consider for employment qualified applicants with arrest and conviction records. For Philadelphia applicants or jobs, please see a copy of Philadelphia&#x2019;s ordinance on this topic by clicking this link: https://codelibrary.amlegal.com/codes/philadelphia/latest/philadelphia_pa/0-0-0-280104.
 </div>
</div>",https://careers-blackhawknetwork.icims.com/jobs/20300/job?utm_source=indeed_integration&iis=Job%20Board&iisn=Indeed&indeed-apply-token=73a2d2b2a8d6d5c0a62696875eaebd669103652d3f0c2cd5445d3e66b1592b0f,8205e7f76522b355,,Full-time,,,Remote,Staff Software Engineer - Data science and Machine Learning (Remote),12 days ago,2023-10-06T13:33:37.343Z,3.5,206.0,"$99,530 - $149,000 a year",2023-10-18T13:33:37.345Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=8205e7f76522b355&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
71,Baylor Scott & White Health,"The Lead Data Protection Engineer will be responsible for overseeing and implementing CIS Control 3 (Data Protection) strategies and solutions to safeguard our organization’s critical data assets. This role will play a pivotal role in enhancing data protection measures across the organization, developing information security policies and standards, introducing security best practices, and supporting the implementation and refinement of selected technologies to support a continuous growth model. This role will also be responsible for building and maintaining strong relationships with service line leaders, vendors, and other departments to help collectively further BSWH’s strategic initiatives. They will assist respective IS Directors and/or leadership with understanding the role the data protection applications play in relation to the IS Application environment and managing architecture of the technology as needed to support the data protection controls. 
  SALARY 
  The pay range for this position is $116,521 (entry-level qualifications) - $209,560 (highly experienced) The specific rate will depend upon the successful candidate’s specific qualifications and prior experience. 
  ESSENTIAL FUNCTIONS OF THE ROLE 
  Responsible for the day-to-day operational activities of the Data Loss Prevention team that includes implementation, support and maintenance of the technology and management of the managed service provider. 
  Ensures that all system platforms are functional and secure. 
  Ensures fulfillment of legal and contractual information security and privacy mandates. 
  Creates and drives long term planning and strategic vision creation for continuous improvement to the information management program, with oversight from Director as needed. 
  Establishes budget and tracks to ensure budget is on track and aligned with strategic goals. 
  Understanding of interdependencies of healthcare landscape and its influence on portfolio. 
  Owns assorted departmental responsibilities as assigned (ie. contract management staffing) 
  Partners with and leads team towards the identification of problems opportunities from a digital innovation perspective, oversee and perform the development and documentation of business requirements, objectives, deliverables, and specifications, and collaboration with customers employees, and support staff. 
  Actively maintains a pulse on digital disruption, innovation, and healthcare. 
  Provides recommendations for improvements that meet team objectives. 
  Presents and explains findings to leadership team and individuals. 
  KEY SUCCESS FACTORS 
  
  Superior leadership, problem solving, team building, and decision-making skills. 
  Skilled project manager with ability to articulate business needs. 
  Excellent written, verbal, and interpersonal communication skills. 
  Proficient computer software and database skills, including Microsoft Products (Excel, SharePoint, Teams, Forms, etc.). 
  Ability to focus and prioritize strategic objectives and work in a growing and challenging environment. 
  Maintains a broad knowledge of state-of-the-art technology equipment and systems. 
  
 LOCATION: Remote 
  SCHEDULE: Full Time 
  BENEFITS 
  Our competitive benefits package includes the following 
  
  Immediate eligibility for health and welfare benefits 
  401(k) savings plan with dollar-for-dollar match up to 5% 
  Tuition Reimbursement 
  PTO accrual beginning Day 1 
  
 Note: Benefits may vary based upon position type and/or level
  QUALIFICATIONS 
 
  EDUCATION - Bachelor's or 4 years of work experience above the minimum qualification 
  EXPERIENCE - 7 Years of Experience","<div>
 <p>The <b>Lead Data Protection Engineer</b> will be responsible for overseeing and implementing CIS Control 3 (Data Protection) strategies and solutions to safeguard our organization&#x2019;s critical data assets. This role will play a pivotal role in enhancing data protection measures across the organization, developing information security policies and standards, introducing security best practices, and supporting the implementation and refinement of selected technologies to support a continuous growth model. This role will also be responsible for building and maintaining strong relationships with service line leaders, vendors, and other departments to help collectively further BSWH&#x2019;s strategic initiatives. They will assist respective IS Directors and/or leadership with understanding the role the data protection applications play in relation to the IS Application environment and managing architecture of the technology as needed to support the data protection controls.</p> 
 <p><b> SALARY</b></p> 
 <p> The pay range for this position is &#x24;116,521 (entry-level qualifications) - &#x24;209,560 (highly experienced) The specific rate will depend upon the successful candidate&#x2019;s specific qualifications and prior experience.</p> 
 <p><b> ESSENTIAL FUNCTIONS OF THE ROLE</b></p> 
 <p> Responsible for the day-to-day operational activities of the Data Loss Prevention team that includes implementation, support and maintenance of the technology and management of the managed service provider.</p> 
 <p> Ensures that all system platforms are functional and secure.</p> 
 <p> Ensures fulfillment of legal and contractual information security and privacy mandates.</p> 
 <p> Creates and drives long term planning and strategic vision creation for continuous improvement to the information management program, with oversight from Director as needed.</p> 
 <p> Establishes budget and tracks to ensure budget is on track and aligned with strategic goals.</p> 
 <p> Understanding of interdependencies of healthcare landscape and its influence on portfolio.</p> 
 <p> Owns assorted departmental responsibilities as assigned (ie. contract management staffing)</p> 
 <p> Partners with and leads team towards the identification of problems opportunities from a digital innovation perspective, oversee and perform the development and documentation of business requirements, objectives, deliverables, and specifications, and collaboration with customers employees, and support staff.</p> 
 <p> Actively maintains a pulse on digital disruption, innovation, and healthcare.</p> 
 <p> Provides recommendations for improvements that meet team objectives.</p> 
 <p> Presents and explains findings to leadership team and individuals.</p> 
 <p><b> KEY SUCCESS FACTORS</b></p> 
 <ul> 
  <li>Superior leadership, problem solving, team building, and decision-making skills.</li> 
  <li>Skilled project manager with ability to articulate business needs.</li> 
  <li>Excellent written, verbal, and interpersonal communication skills.</li> 
  <li>Proficient computer software and database skills, including Microsoft Products (Excel, SharePoint, Teams, Forms, etc.).</li> 
  <li>Ability to focus and prioritize strategic objectives and work in a growing and challenging environment.</li> 
  <li>Maintains a broad knowledge of state-of-the-art technology equipment and systems.</li> 
 </ul> 
 <p><b>LOCATION: Remote</b></p> 
 <p><b> SCHEDULE: Full Time</b></p> 
 <p><b> BENEFITS</b></p> 
 <p> Our competitive benefits package includes the following</p> 
 <ul> 
  <li>Immediate eligibility for health and welfare benefits</li> 
  <li>401(k) savings plan with dollar-for-dollar match up to 5%</li> 
  <li>Tuition Reimbursement</li> 
  <li>PTO accrual beginning Day 1</li> 
 </ul> 
 <p>Note: Benefits may vary based upon position type and/or level</p>
 <p><b> QUALIFICATIONS</b></p> 
 <ul>
  <li>EDUCATION - Bachelor&apos;s or 4 years of work experience above the minimum qualification</li> 
  <li>EXPERIENCE - 7 Years of Experience</li>
 </ul>
</div>",https://jobs.bswhealth.com/us/en/job/23018360/Lead-Data-Protection-Engineer?rx_campaign=indeed0&rx_ch=jobp4p&rx_group=119123&rx_job=23018360&rx_r=none&rx_source=Indeed&rx_ts=20231018T120047Z&rx_vp=cpc&utm_source=indeedorganic&rx_p=N1RFXMVXVV&rx_viewer=f260f8ff6dba11ee904659a2e7370c6b44634e17d02e43f4b49efd1fe7534128,780bbe8f6d47e19d,,Full-time,,,Remote,Lead Data Protection Engineer,11 days ago,2023-10-07T13:33:34.053Z,3.8,4045.0,"$116,521 a year",2023-10-18T13:33:34.055Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=780bbe8f6d47e19d&from=jasx&tk=1hd1fqt7tk7ar805&vjs=3
72,Emerson United Inc,"Overall 7-8 years of experience
Must Have :
Databricks/py-spark – 4+ years of experience
Cloud experience – 4+ years of cloud experience
Nice to have:
Preferable Software engineering background
Preferable AWS experience
Experience with Typescript
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:

 7 years

Schedule:

 8 hour shift

Experience:

 AWS: 6 years (Required)
 Databricks: 4 years (Required)
 Pyspark: 4 years (Required)

Work Location: Remote","<p>Overall 7-8 years of experience</p>
<p>Must Have :</p>
<p>Databricks/py-spark &#x2013; 4+ years of experience</p>
<p>Cloud experience &#x2013; 4+ years of cloud experience</p>
<p>Nice to have:</p>
<p>Preferable Software engineering background</p>
<p>Preferable AWS experience</p>
<p>Experience with Typescript</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;75.00 - &#x24;80.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>AWS: 6 years (Required)</li>
 <li>Databricks: 4 years (Required)</li>
 <li>Pyspark: 4 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,5f5de83e70b15c74,,Contract,,,Remote,AWS Data Engineer,5 days ago,2023-10-13T13:33:48.811Z,,,$75 - $80 an hour,2023-10-18T13:33:48.812Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BSRaD9lfi1M4GEnG4pnFsPi92N_zg6XQTKM2RXCnXf0V5919SIZyeemH_4jgA_bdpI9HgUwgut0jGaaF2iDLdiYuuvWAuwswCdYF4kR-bJDD_O6Q2L82HD6YXZ3q7Qi33t3scop2VRFHBIB2Q10usrg63hXNqu_Ee38ReI34fzeiOOjPfK-lIfTud6ULu-M1hDvbwRx5LGmO8zabL5QD2y6M-SwEPH3A7dTqmoVZLiQwolSnrYfX0hOehpP07eWDwM4a9EzJXpyhrD_6xsSO3f221I5IZDW1apUJzIKikQbvPlYBg38zrKVHIAwTmvv4jbpS1QrqiOGb3LWHTFceJ4S_lEladPHH6nNb5c91RkYSQC6-suCChSGhszn-k_cAKswpOf_tp2T6MiFHzp5h0gTZISaPmJLHuHzodQqntJ1W6si_Krb-Ag2KTJCYpf4l7yC5FymY4yzrRgMdzKXrQBBvakSyNGiGeQPTzq7qqA_FKTSkpw7WUd0513akBX6lFmemaFun-hor6VrpjjZDHUiKogEx0l_LpWd1MH6q48xQiGhlw1iPdM-XVc0S0-0uEuyekRngLW7Or46HuJNo5n&xkcb=SoBs-_M3JhX_1MWas50FbzkdCdPP&p=14&fvj=1&vjs=3&jsa=4707&tk=1hd1fs0bnk7b3800&from=jasx&wvign=1
73,Lucayan Technology Solutions,"Description:
10 Months Contract
Required skills:Data Engineering – They need to be stronger.Azure – high need – should have experience here.Kubernetes – have been very stringent with here.Machine Learning.Python – must have.Spark - also required.Jupterhub/ mlflow/ databricks/ kubeflow – Have been deployed and worked on any of this tool.
Secondary Skills - Nice to Haves:HELMArgo Workflow
Job Description:The Engineer is responsible for developing, implementing, and maintaining technical software applications and provides a combination of technical and business leadership while being the primary trusted and capable owner of one or more high priority, high visibility, complex initiatives.This Engineer will typically lead and coach a small number of team members and provide guidance to team members and multiple.You will design, develop, test, deploy, maintain, and enhance Machine Learning Pipelines using K8s/AKS based Argo Workflow Orchestration solutions.Participate and contribute to design reviews with platform engineering team to decide the design, technologies, project priorities, deadlines, and deliverables.You will work closely with Data Lake and Data Science team to understand their data structure and machine learning algorithms.Understanding of ETL pipelines, and ingress / egress methodologies and design patternsImplement real time argo workflow pipelines, integrate pipelines with machine learning models, and translate data and model results into business stakeholders Data LakeDevelop distributed Machine Learning Pipeline for training & inferencing using Argo, Spark & AKSBuild highly scalable backend REST APIs to collect data from Data Lake and other use-cases / scenarios.Deploy Application in Azure Kubernetes Service using GitLab CICD, Jenkins, Docker, Kubectl, Helm and MainfestExperience in branching, tagging, and maintaining the versions across the different environments in GitLab.Review code developed by other developers and provided feedback to ensure best practices (e.g., checking code in, accuracy, testability, and efficiency)Debug/track/resolve by analyzing the sources of issues and the impact on application, network, or service operations and quality.Functional, benchmark & performance testing and tuning for the built workflows.Assess, design & optimize the resources capacities (e.g. Memory, GPU etc.) for ML based resource intensive workloads.
Education: Bachelors Degree
Job Type: Contract
Pay: $48.00 per hour
Experience level:

 7 years

Work Location: Remote","<p><b>Description:</b></p>
<p>10 Months Contract</p>
<p><b>Required skills:</b><br>Data Engineering &#x2013; They need to be stronger.<br>Azure &#x2013; high need &#x2013; should have experience here.<br>Kubernetes &#x2013; have been very stringent with here.<br>Machine Learning.<br>Python &#x2013; must have.<br>Spark - also required.<br>Jupterhub/ mlflow/ databricks/ kubeflow &#x2013; Have been deployed and worked on any of this tool.</p>
<p><b>Secondary Skills - Nice to Haves:</b><br>HELM<br>Argo Workflow</p>
<p><b>Job Description:</b><br>The Engineer is responsible for developing, implementing, and maintaining technical software applications and provides a combination of technical and business leadership while being the primary trusted and capable owner of one or more high priority, high visibility, complex initiatives.<br>This Engineer will typically lead and coach a small number of team members and provide guidance to team members and multiple.<br>You will design, develop, test, deploy, maintain, and enhance Machine Learning Pipelines using K8s/AKS based Argo Workflow Orchestration solutions.<br>Participate and contribute to design reviews with platform engineering team to decide the design, technologies, project priorities, deadlines, and deliverables.<br>You will work closely with Data Lake and Data Science team to understand their data structure and machine learning algorithms.<br>Understanding of ETL pipelines, and ingress / egress methodologies and design patterns<br>Implement real time argo workflow pipelines, integrate pipelines with machine learning models, and translate data and model results into business stakeholders Data Lake<br>Develop distributed Machine Learning Pipeline for training &amp; inferencing using Argo, Spark &amp; AKS<br>Build highly scalable backend REST APIs to collect data from Data Lake and other use-cases / scenarios.<br>Deploy Application in Azure Kubernetes Service using GitLab CICD, Jenkins, Docker, Kubectl, Helm and Mainfest<br>Experience in branching, tagging, and maintaining the versions across the different environments in GitLab.<br>Review code developed by other developers and provided feedback to ensure best practices (e.g., checking code in, accuracy, testability, and efficiency)<br>Debug/track/resolve by analyzing the sources of issues and the impact on application, network, or service operations and quality.<br>Functional, benchmark &amp; performance testing and tuning for the built workflows.<br>Assess, design &amp; optimize the resources capacities (e.g. Memory, GPU etc.) for ML based resource intensive workloads.</p>
<p><b>Education:</b> Bachelors Degree</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;48.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Work Location: Remote</p>",,84bd4ff5bed36cd7,,Contract,,,Remote,Data Engineer (Remote),5 days ago,2023-10-13T13:33:51.293Z,,,$48 an hour,2023-10-18T13:33:51.295Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=84bd4ff5bed36cd7&from=jasx&tk=1hd1fru0j2f35000&vjs=3
74,Emerson United Inc,"Overall 7-8 years of experience
Must Have :
Databricks/py-spark – 4+ years of experience
Cloud experience – 4+ years of cloud experience
Nice to have:
Preferable Software engineering background
Preferable AWS experience
Experience with Typescript
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:

 7 years

Schedule:

 8 hour shift

Experience:

 AWS: 6 years (Required)
 Databricks: 4 years (Required)
 Pyspark: 4 years (Required)

Work Location: Remote","<p>Overall 7-8 years of experience</p>
<p>Must Have :</p>
<p>Databricks/py-spark &#x2013; 4+ years of experience</p>
<p>Cloud experience &#x2013; 4+ years of cloud experience</p>
<p>Nice to have:</p>
<p>Preferable Software engineering background</p>
<p>Preferable AWS experience</p>
<p>Experience with Typescript</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;75.00 - &#x24;80.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>AWS: 6 years (Required)</li>
 <li>Databricks: 4 years (Required)</li>
 <li>Pyspark: 4 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,5f5de83e70b15c74,,Contract,,,Remote,AWS Data Engineer,5 days ago,2023-10-13T13:33:51.930Z,,,$75 - $80 an hour,2023-10-18T13:33:51.932Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BSRaD9lfi1M4GEnG4pnFsPi92N_zg6XQTKM2RXCnXf0V5919SIZyeemH_4jgA_bdpI9HgUwgut0jGaaF2iDLdiYuuvWAuwswCdYF4kR-bJDD_O6Q2L82HD6YXZ3q7Qi33t3scop2VRFHBIB2Q10usrg63hXNqu_Ee38ReI34fzeiOOjPfK-lIfGwSkA7owJ8clHoNrP8gwcHXdLtGf94gfpR5uceYMwGrxpPjg5fB8p6QGgWuDTze5141UjYeaQFaUh9XFmffCu5tKNzykPYSfv-1J5BiNqG2tmC_InjwQnJ0VS24Nrlz8XK6DphxbkEni-P2Ddh2AEunISc9UBU5uNm5eVTiqwI7a_dQ74AuSAx_ZmI9FcXwFdubyuEdTqboV_UhiYWIa1eBh2nnADZhfQ0ITnUecOo1HU6PXg6jTVr2mHF3cVrnQR5l1V1PckuaM7OkEiQjgAZSPfFaBRoYtqgKV5h_FJc2D7OZMPOqIJ9vvN5BDxmFeyPsiT0e0ApZF_7iWXLBPKrq0gy37ntETPNIPcA6uYQczgzHRvtl0b5JQZtMYkY0t60d0wfuxSbpuK_Lrv5P-InG9MVofbEnD&xkcb=SoCU-_M3JhXgCYzRm50bbzkdCdPP&p=14&fvj=1&vjs=3&jsa=2783&tk=1hd1frufmlens800&from=jasx&wvign=1
75,Emerson United Inc,"Overall 7-8 years of experience
Must Have :
Databricks/py-spark – 4+ years of experience
Cloud experience – 4+ years of cloud experience
Nice to have:
Preferable Software engineering background
Preferable AWS experience
Experience with Typescript
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:

 7 years

Schedule:

 8 hour shift

Experience:

 AWS: 6 years (Required)
 Databricks: 4 years (Required)
 Pyspark: 4 years (Required)

Work Location: Remote","<p>Overall 7-8 years of experience</p>
<p>Must Have :</p>
<p>Databricks/py-spark &#x2013; 4+ years of experience</p>
<p>Cloud experience &#x2013; 4+ years of cloud experience</p>
<p>Nice to have:</p>
<p>Preferable Software engineering background</p>
<p>Preferable AWS experience</p>
<p>Experience with Typescript</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;75.00 - &#x24;80.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>AWS: 6 years (Required)</li>
 <li>Databricks: 4 years (Required)</li>
 <li>Pyspark: 4 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,5f5de83e70b15c74,,Contract,,,Remote,AWS Data Engineer,5 days ago,2023-10-13T13:33:52.894Z,,,$75 - $80 an hour,2023-10-18T13:33:52.896Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BSRaD9lfi1M4GEnG4pnFsPi92N_zg6XQTKM2RXCnXf0V5919SIZyeemH_4jgA_bdpI9HgUwgut0jGaaF2iDLdiYuuvWAuwswCdYF4kR-bJDD_O6Q2L82HD6YXZ3q7Qi33t3scop2VRFHBIB2Q10usrg63hXNqu_Ee38ReI34fzeiOOjPfK-lIfCjcFTC5AXcJpPEWx6ir0xSGQUBKwX9dEQAGPmvqyqIoMc04OcAlC09TtRcvUojkvY1ryhZ-uGAa0b5CDbZIyBWzPsOwmIRuFEjT4MgUtMQ8Y0illdkO6WPZV3c1OgMhobM7qv2gnLz44Nutkv8toStQnmKcDrI8M9D147ptWH0KA6gt1lA9XttSv876k8G228MQfaguALaMzl_jTlNPNXNzwgV_iuM5yYvdG_3haflu-GuqYRpy3CPN2n5NJRVtuKewBuzQuP5IjLqoN6RHo4onIo3TBO6tbR-EWHHJVMiwQqYX_QKoHt1UTLr_CaY9mVuoc5jZW8VtoVagp8JpcDPdf_ZF9a5HKVAO28dyxEEaCAyddC_gEWmIS5P8qZZxdaYsk9o9lABEqrp1NtOy-2_2aCV6tp08o&xkcb=SoDW-_M3JhXgGPyit50GbzkdCdPP&p=13&fvj=1&vjs=3&jsa=2296&tk=1hd1fru0j2f35000&from=jasx&wvign=1
78,ICF,"Senior Data Engineer at ICF
**This role can be completely remote, sitting anywhere within the US**
We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!
You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians’ work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.
Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.
Tools & Technology:
o Spark, Hadoop, Scala, Python, and AWS EMR
o Airflow, Jenkins
o AWS Redshift and Teradata
o Git and Github
o Confluence
Key Responsibilities:
o Write complex unit and integration tests for all data processing code
o Work with DevOps engineers on CI, CD, and IaC
o Read specs and translate them into test designs and test automation
o Perform code reviews and develop processes for improving code quality
Basic Qualifications:
o Bachelor’s degree
o 5+ years of high-volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API
o 2+ years of experience with Agile methodology
o 2+ years of experience performing data pipeline and data validation
o Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain a Public Trust Clearance.
Preferred Qualifications:
o MS and 3+ years of Data Engineering experience
o Experience working in the healthcare industry with PHI/PII
o Federal Government contracting work experience
o Expertise working as part of a dynamic, interactive Agile team
o Strong written and verbal communication skills
o Demonstrated time management skills.
o Strong organizational skills with attention to detail
o Curiosity about how things work, ability to look out for potential risks
Job Type: Full-time
Pay: $100,000.00 - $160,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Flexible schedule
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Tuition reimbursement
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 10 years
 8 years
 9 years

Schedule:

 8 hour shift

Education:

 Bachelor's (Required)

Experience:

 Informatica: 1 year (Preferred)
 Spark: 2 years (Required)
 SQL: 3 years (Preferred)
 Cloud: 1 year (Required)
 Scala: 3 years (Required)

Work Location: Remote","<p><b>Senior Data Engineer at ICF</b></p>
<p>**This role can be completely remote, sitting anywhere within the US**</p>
<p>We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!</p>
<p>You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians&#x2019; work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.</p>
<p>Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.</p>
<p><b>Tools &amp; Technology:</b></p>
<p>o Spark, Hadoop, Scala, Python, and AWS EMR</p>
<p>o Airflow, Jenkins</p>
<p>o AWS Redshift and Teradata</p>
<p>o Git and Github</p>
<p>o Confluence</p>
<p><b>Key Responsibilities:</b></p>
<p>o Write complex unit and integration tests for all data processing code</p>
<p>o Work with DevOps engineers on CI, CD, and IaC</p>
<p>o Read specs and translate them into test designs and test automation</p>
<p>o Perform code reviews and develop processes for improving code quality</p>
<p><b>Basic Qualifications:</b></p>
<p>o Bachelor&#x2019;s degree</p>
<p>o <b>5+ years of high-volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API</b></p>
<p>o 2+ years of experience with Agile methodology</p>
<p>o 2+ years of experience performing data pipeline and data validation</p>
<p>o Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain a Public Trust Clearance.</p>
<p><b>Preferred Qualifications:</b></p>
<p>o MS and 3+ years of Data Engineering experience</p>
<p>o Experience working in the healthcare industry with PHI/PII</p>
<p>o Federal Government contracting work experience</p>
<p>o Expertise working as part of a dynamic, interactive Agile team</p>
<p>o Strong written and verbal communication skills</p>
<p>o Demonstrated time management skills.</p>
<p>o Strong organizational skills with attention to detail</p>
<p>o Curiosity about how things work, ability to look out for potential risks</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;100,000.00 - &#x24;160,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Tuition reimbursement</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Required)</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>Spark: 2 years (Required)</li>
 <li>SQL: 3 years (Preferred)</li>
 <li>Cloud: 1 year (Required)</li>
 <li>Scala: 3 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,032e33bd776de85e,,Full-time,,,Remote,Senior Data Engineer - Scala/Spark,4 days ago,2023-10-14T13:34:00.445Z,3.4,666.0,"$100,000 - $160,000 a year",2023-10-18T13:34:00.447Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=032e33bd776de85e&from=jasx&tk=1hd1fs0bnk7b3800&vjs=3
82,hire IT people,"Job Summary:
We are seeking an experienced Azure Cloud Solutions Architect to design, develop and implement cloud solutions based on the Azure platform. The ideal candidate will have a deep understanding of cloud computing principles and be able to design scalable, secure, and highly available cloud-based solutions. As an Azure Cloud Solutions Architect, you will work closely with clients, project managers, and other technical teams to deliver cloud solutions that meet business needs.
Here is the feedback on the position for the customer. Do you have someone who fits the description? They need about 60% development and 40% operations
For this position we need more of a data developer/engineer. Skills I’d like to see (in order of importance): SQL, ELT/ETL experience, Azure generalist, Azure Snapse, Powershell/bash, Python. SQL is a must have –
I would ask them to outline a CTE (alternatively a subquery) as an interview question. That’s indicative of a reasonable amount of DB experience. Experience directly with Azure and the MS DB stack is great too, but less important than being able to write a SQL statement.
Responsibilities:

 Design and architect cloud-based solutions on the Azure platform, including infrastructure, networking, security, and application layers
 Collaborate with clients and project managers to understand business requirements and translate them into technical solutions
 Develop technical architecture and design documents that outline the proposed solution, including diagrams, specifications, and cost estimates
 Evaluate existing systems and applications to determine if they can be migrated to the cloud, and design migration plans as needed
 Develop and implement cloud automation scripts using PowerShell or other scripting tools
 Configure and manage Azure services such as Virtual Machines, Storage Accounts, Virtual Networks, and App Services
 Implement and manage Azure security measures, including Azure Active Directory, Azure Security Center, and Network Security Groups
 Monitor and optimize Azure resources to ensure maximum performance, availability, and scalability
 Provide guidance and mentorship to other technical team members on Azure technologies and best practices
 Stay up-to-date with the latest cloud technologies and trends, and evaluate their potential use in our solutions

Requirements:

 Bachelor's degree in Computer Science, Information Technology, or related field
 5+ years of experience in Azure cloud architecture and design
 Strong understanding of Azure Container solutions like Azure Kubernetes Service, Azure Container Apps, and Azure Container Registry
 Experience with implementing Azure SQL Database
 String understanding
 Strong understanding of cloud computing principles and architectures, including IaaS, PaaS, and SaaS models
 Experience with Azure services such as Virtual Machines, Storage Accounts, Databases, Virtual Networks, and App Services
 Proficiency in cloud automation tools and scripting languages, such as Terraform, Ansible,PowerShell, Python, or Azure CLI
 Experience with Azure DevOps, including Continuous Integration and Continuous Deployment
 Understanding of cloud security principles and best practices, including Azure Security Center and Network Security Groups
 Strong communication and collaboration skills, and ability to work in a team environment
 Ability to adapt to new technologies and learn quickly
 Azure certifications such as AZ-303 and AZ-304 are a plus

Job Type: Full-time
Salary: From $75.00 per hour
Experience level:

 10 years
 11+ years

Schedule:

 8 hour shift

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p><b>Job Summary:</b></p>
<p>We are seeking an experienced Azure Cloud Solutions Architect to design, develop and implement cloud solutions based on the Azure platform. The ideal candidate will have a deep understanding of cloud computing principles and be able to design scalable, secure, and highly available cloud-based solutions. As an Azure Cloud Solutions Architect, you will work closely with clients, project managers, and other technical teams to deliver cloud solutions that meet business needs.</p>
<p>Here is the feedback on the position for the customer. Do you have someone who fits the description? They need about 60% development and 40% operations</p>
<p>For this position we need more of a data developer/engineer. Skills I&#x2019;d like to see (in order of importance): SQL, ELT/ETL experience, Azure generalist, Azure Snapse, Powershell/bash, Python. SQL is a must have &#x2013;</p>
<p>I would ask them to outline a CTE (alternatively a subquery) as an interview question. That&#x2019;s indicative of a reasonable amount of DB experience. Experience directly with Azure and the MS DB stack is great too, but less important than being able to write a SQL statement.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Design and architect cloud-based solutions on the Azure platform, including infrastructure, networking, security, and application layers</li>
 <li>Collaborate with clients and project managers to understand business requirements and translate them into technical solutions</li>
 <li>Develop technical architecture and design documents that outline the proposed solution, including diagrams, specifications, and cost estimates</li>
 <li>Evaluate existing systems and applications to determine if they can be migrated to the cloud, and design migration plans as needed</li>
 <li>Develop and implement cloud automation scripts using PowerShell or other scripting tools</li>
 <li>Configure and manage Azure services such as Virtual Machines, Storage Accounts, Virtual Networks, and App Services</li>
 <li>Implement and manage Azure security measures, including Azure Active Directory, Azure Security Center, and Network Security Groups</li>
 <li>Monitor and optimize Azure resources to ensure maximum performance, availability, and scalability</li>
 <li>Provide guidance and mentorship to other technical team members on Azure technologies and best practices</li>
 <li>Stay up-to-date with the latest cloud technologies and trends, and evaluate their potential use in our solutions</li>
</ul>
<p><b>Requirements:</b></p>
<ul>
 <li>Bachelor&apos;s degree in Computer Science, Information Technology, or related field</li>
 <li>5+ years of experience in Azure cloud architecture and design</li>
 <li><b>Strong understanding of Azure Container solutions like Azure Kubernetes Service, Azure Container Apps, and Azure Container Registry</b></li>
 <li><b>Experience with implementing Azure SQL Database</b></li>
 <li><b>String understanding</b></li>
 <li>Strong understanding of cloud computing principles and architectures, including IaaS, PaaS, and SaaS models</li>
 <li>Experience with Azure services such as Virtual Machines, Storage Accounts, Databases, Virtual Networks, and App Services</li>
 <li>Proficiency in cloud automation tools and scripting languages, such as <b>Terraform, Ansible,</b>PowerShell, Python, or Azure CLI</li>
 <li>Experience with Azure DevOps, including Continuous Integration and Continuous Deployment</li>
 <li>Understanding of cloud security principles and best practices, including Azure Security Center and Network Security Groups</li>
 <li><b>Strong communication and collaboration skills, and ability to work in a team environment</b></li>
 <li>Ability to adapt to new technologies and learn quickly</li>
 <li>Azure certifications such as AZ-303 and AZ-304 are a plus</li>
</ul>
<p>Job Type: Full-time</p>
<p>Salary: From &#x24;75.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,88983c081a8a400b,,Full-time,,,Remote,Sr. Azure Data Engineer,4 days ago,2023-10-14T13:34:07.595Z,,,From $75 an hour,2023-10-18T13:34:07.597Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=88983c081a8a400b&from=jasx&tk=1hd1fru0j2f35000&vjs=3
89,ICF,"Senior Data Engineer - Scala
 
 
 
  
   
    
     
       Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth.
     
     
     
       **This role can be completely remote, sitting anywhere within the US**
     
     
     
       We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!
     
     
     
       You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians’ work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.
     
     
     
       Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.
     
     
     
       Tools & Technology:
     
     
       Spark, Hadoop, Scala, Python, and AWS EMR
       Airflow, Jenkins
       AWS Redshift and Teradata
       Git and Github
       Confluence
     
     
     
       Key Responsibilities:
     
     
       Write complex unit and integration tests for all data processing code
       Work with DevOps engineers on CI, CD, and IaC
       Read specs and translate them into test designs and test automation
       Perform code reviews and develop processes for improving code quality
     
     
     
       Basic Qualifications:
     
     
       Bachelor’s Degree
       5+ years of high volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API
       2+ years of experience with Agile methodology
       2+ years of experience performing data pipeline and data validation
       Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain and maintain a Public Trust Clearance.
     
     
     
       Preferred Qualifications:
     
     
       MS and 3+ years of technical experience
       Experience working in the healthcare industry with PHI/PII
       Federal Government contracting work experience
       Expertise working as part of a dynamic, interactive Agile team
       Strong written and verbal communication skills
       Demonstrated time management skills.
       Strong organizational skills with attention to detail
       Curiosity about how things work, ability to look out for potential risks
     
     
     
       #Indeed
     
     
       #LI-CC1
     
     
       #DMX
     
    
   
  
 
 
 
   Working at ICF
 
  ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
 
 
   We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our 
  
   EEO & AA policy
  .
 
 
 
   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email 
  
   icfcareercenter@icf.com
   and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: 
  
   Know Your Rights
   and 
  
   Pay Transparency Statement.
  
 
 
 
   Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
  $82,673.00 - $140,544.00
  Nationwide Remote Office (US99)","<div>
 <div>
  Senior Data Engineer - Scala
 </div>
 <div></div>
 <div>
  <div>
   <div>
    <div>
     <div>
       Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth.
     </div>
     <div></div>
     <div>
       **This role can be completely remote, sitting anywhere within the US**
     </div>
     <div></div>
     <div>
       We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!
     </div>
     <div></div>
     <div>
       You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians&#x2019; work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.
     </div>
     <div></div>
     <div>
       Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.
     </div>
     <div></div>
     <div>
       Tools &amp; Technology:
     </div>
     <ul>
      <li> Spark, Hadoop, Scala, Python, and AWS EMR</li>
      <li> Airflow, Jenkins</li>
      <li> AWS Redshift and Teradata</li>
      <li> Git and Github</li>
      <li> Confluence</li>
     </ul>
     <div></div>
     <div>
       Key Responsibilities:
     </div>
     <ul>
      <li> Write complex unit and integration tests for all data processing code</li>
      <li> Work with DevOps engineers on CI, CD, and IaC</li>
      <li> Read specs and translate them into test designs and test automation</li>
      <li> Perform code reviews and develop processes for improving code quality</li>
     </ul>
     <div></div>
     <div>
       Basic Qualifications:
     </div>
     <ul>
      <li> Bachelor&#x2019;s Degree</li>
      <li> 5+ years of high volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API</li>
      <li> 2+ years of experience with Agile methodology</li>
      <li> 2+ years of experience performing data pipeline and data validation</li>
      <li> Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain and maintain a Public Trust Clearance.</li>
     </ul>
     <div></div>
     <div>
       Preferred Qualifications:
     </div>
     <ul>
      <li> MS and 3+ years of technical experience</li>
      <li> Experience working in the healthcare industry with PHI/PII</li>
      <li> Federal Government contracting work experience</li>
      <li> Expertise working as part of a dynamic, interactive Agile team</li>
      <li> Strong written and verbal communication skills</li>
      <li> Demonstrated time management skills.</li>
      <li> Strong organizational skills with attention to detail</li>
      <li> Curiosity about how things work, ability to look out for potential risks</li>
     </ul>
     <div></div>
     <div>
       #Indeed
     </div>
     <div>
       #LI-CC1
     </div>
     <div>
       #DMX
     </div>
    </div>
   </div>
  </div>
 </div>
 <div></div>
 <div>
   Working at ICF
 </div>
 <div></div> ICF is a global advisory and technology services provider, but we&#x2019;re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
 <div></div>
 <div>
   We can only solve the world&apos;s toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our 
  <div>
   EEO &amp; AA policy
  </div>.
 </div>
 <div></div>
 <div>
   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email 
  <div>
   icfcareercenter@icf.com
  </div> and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: 
  <div>
   Know Your Rights
  </div> and 
  <div>
   Pay Transparency Statement.
  </div>
 </div>
 <div></div>
 <div>
  <br> Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
 </div> &#x24;82,673.00 - &#x24;140,544.00
 <div></div> Nationwide Remote Office (US99)
</div>",https://icf.wd5.myworkdayjobs.com/en-US/ICFExternal_Career_Site/job/Reston-VA/Senior-Data-Engineer---Scala--Remote-_R2304300?source=indeed&source=indeed,dc6d198d707948d7,,Full-time,,,"Reston, VA",Senior Data Engineer - Scala (Remote),4 days ago,2023-10-14T13:34:18.376Z,3.4,666.0,"$82,673 - $140,544 a year",2023-10-18T13:34:18.378Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=dc6d198d707948d7&from=jasx&tk=1hd1fsp38k6pu800&vjs=3
90,Manrco Inc,"Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment?
We are seeking Data Engineers who are passionate about marrying data with emerging technologies to join our team. A
What You’ll Do:

 Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies


 Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems


 Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake


 Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community


 Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment


 Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Basic Qualifications:

 Bachelor’s Degree


 At least 2 years of experience in application development


 At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)

Preferred Qualifications:

 Master's Degree


 3+ years of experience in application development


 1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink


 1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)


 1+ years of experience with Ansible / Terraform


 2+ years of experience with Agile engineering practices


 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)


 2+ years of experience with NoSQL implementation (Mongo, Cassandra)


 2+ years of experience developing Java based software solutions


 2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)


 2+ years of experience developing software solutions to solve complex business problems


 2+ years of experience with UNIX/Linux including basic commands and shell scripting

Job Type: Contract
Pay: $80.00 per hour
Expected hours: 40 per week
Benefits:

 Health insurance

Compensation package:

 Hourly pay

Experience level:

 5 years
 6 years
 7 years
 8 years

Schedule:

 12 hour shift
 Monday to Friday

Experience:

 Data warehouse: 1 year (Preferred)
 Informatica: 4 years (Preferred)
 Snowflake: 3 years (Preferred)

Work Location: Remote","<p>Data Engineer</p>
<p>Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment?</p>
<p>We are seeking Data Engineers who are passionate about marrying data with emerging technologies to join our team. A</p>
<p>What You&#x2019;ll Do:</p>
<ul>
 <li>Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies</li>
</ul>
<ul>
 <li>Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems</li>
</ul>
<ul>
 <li>Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake</li>
</ul>
<ul>
 <li>Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal &amp; external technology communities, and mentoring other members of the engineering community</li>
</ul>
<ul>
 <li>Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment</li>
</ul>
<ul>
 <li>Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance</li>
</ul>
<p>Basic Qualifications:</p>
<ul>
 <li>Bachelor&#x2019;s Degree</li>
</ul>
<ul>
 <li>At least 2 years of experience in application development</li>
</ul>
<ul>
 <li>At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)</li>
</ul>
<p>Preferred Qualifications:</p>
<ul>
 <li>Master&apos;s Degree</li>
</ul>
<ul>
 <li>3+ years of experience in application development</li>
</ul>
<ul>
 <li>1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink</li>
</ul>
<ul>
 <li>1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)</li>
</ul>
<ul>
 <li>1+ years of experience with Ansible / Terraform</li>
</ul>
<ul>
 <li>2+ years of experience with Agile engineering practices</li>
</ul>
<ul>
 <li>2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)</li>
</ul>
<ul>
 <li>2+ years of experience with NoSQL implementation (Mongo, Cassandra)</li>
</ul>
<ul>
 <li>2+ years of experience developing Java based software solutions</li>
</ul>
<ul>
 <li>2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)</li>
</ul>
<ul>
 <li>2+ years of experience developing software solutions to solve complex business problems</li>
</ul>
<ul>
 <li>2+ years of experience with UNIX/Linux including basic commands and shell scripting</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;80.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Hourly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>12 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data warehouse: 1 year (Preferred)</li>
 <li>Informatica: 4 years (Preferred)</li>
 <li>Snowflake: 3 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,7df0841eea60802f,,Contract,,,Remote,Data Engineer,5 days ago,2023-10-13T13:34:29.848Z,,,$80 an hour,2023-10-18T13:34:29.853Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=7df0841eea60802f&from=jasx&tk=1hd1ft1i8joou800&vjs=3
91,E Source,"Are you an experienced data engineer with a passion for data and innovation? Do you thrive on a cross-functional team of professionals?
 
  E Source is a leading provider of software solutions and consulting services for utilities and their customers. We use data science and machine learning to help utilities optimize their operations and achieve their sustainability goals. We process large amounts of data from our clients and uncover insights and patterns that they didn’t see before.
 
  Data engineering is a key function enabling a robust data science environment for our consulting services and as middleware in preparing our client’s data for processing within our software as a service (SaaS) solutions.
 
  We are seeking a Senior Data engineer to join our Machine Learning Engineering team and help us design and build scalable, reliable, and secure data pipelines, infrastructure, and systems for our consulting services and software as a service solutions. The Senior Data Engineer will work with a cross-functional team of Machine Learning engineers, software engineers, data scientists, and data analysts to deliver data products and solutions for our clients while also contributing to our internal data practices.
 
  Responsibilities:
  
 
 
   Design, develop and maintain data pipelines, infrastructure, and systems to support data products and solutions using technologies such as AWS, Python, Databricks, Spark, and SQL databases like PostgreSQL.
   Work with cross-functional teams to translate business problems into technical solutions and provide technical guidance and mentorship to junior data engineers.
   Develop and implement data engineering strategies and best practices that align with business objectives and customer needs.
   Monitor and troubleshoot data pipelines and systems to ensure data quality, integrity, and availability.
   Conduct research on industry trends and best practices to improve data engineering capabilities and evaluate new data technologies and tools.
   Build and maintain data lakes to support business intelligence needs.
   Manage the software development lifecycle and DevOps aspects of the code.
   Collaborate with data scientists and analysts to understand their data requirements and provide them with optimal data solutions.
   Create new data validation methods and data analysis tools.
 
 
  Requirements:
  
 
 
   Bachelor's degree in computer science, information technology, or a related field.
   4-7 years of experience in data engineering or a similar role.
   Expert-level skills in Python, SQL databases such as PostgreSQL, and big data technologies such as Databricks and Spark.
   Hands-on experience building cloud resident data pipelines in AWS.
   Strong understanding of data governance, security, privacy, and retention policies and procedures.
   Strong communication, collaboration, and problem-solving skills.
   High proficiency using agile software tools like Jira and following mature DevOps practices using GIT, Docker, and CI servers like Jenkins.
   Passion for data and innovation.
 
 
  Preferred Qualifications:
 
   Demonstrated capacity to work autonomously and proactively, with a proven track record of achieving results without constant supervision.
   Experience with ETL optimization, designing, coding, and tuning big data processes in Databricks.
   Sound knowledge of data lineage and data quality techniques.
   Experience in working with data science and machine learning models and frameworks
   Previous experience in the Energy or Utility industry in an analytic role.
   MS Degree in management information systems, computer programming, software engineering, data science, or an equivalent STEM field.
 
 
  A little about E Source
 
  Since 1986, E Source has focused on partnering with utilities to help their customers save electricity. That novel approach defined the art of electric end-use efficiency—better known as energy efficiency. We’ve expanded that concept to include a broader perspective of sustainability for utilities that deliver electricity, natural gas, and water.
 
  We work to enhance relationships with the people utilities serve, achieve the next generation of savings, and lead the carbon-reduction effort. We help utilities think differently, make data useful, and learn from the best strategies across the industry. Our people, our insights, and our network help our clients and give them the assurance that the programs they implement are the most effective.
 
  Benefits
  
 
 
   We offer excellent insurance packages, including medical, dental, and vision plans; company-paid life insurance; company-paid long- and short-term disability insurance; and medical and dependent-care flexible spending plans.
   We provide a flexible time off (FTO) program; E Source employees can take as many paid days off per year as they need, with manager approval, while fulfilling their work obligations and ensuring proper coverage of their responsibilities.
   We offer flexible schedules, flexible work locations, and a paid parental leave benefit.
   We provide a 401(k) plan with a 3% employer match.
 
 
  The budgeted salary for this position is $99,750 to $135,450 (includes base + annual bonus). Actual pay will be adjusted based on experience.
 
  This person can work remote, a hybrid schedule, or from one of our physical office locations.
 
  Applicants must be authorized to work for any employer in the US. We’re unable to sponsor or take over sponsorship of employment visas at this time.
 
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
 
  #LI-Remote
 
  #LI-AP1","<div>
 <p>Are you an experienced data engineer with a passion for data and innovation? Do you thrive on a cross-functional team of professionals?</p>
 <p></p>
 <p><br> E Source is a leading provider of software solutions and consulting services for utilities and their customers. We use data science and machine learning to help utilities optimize their operations and achieve their sustainability goals. We process large amounts of data from our clients and uncover insights and patterns that they didn&#x2019;t see before.</p>
 <p></p>
 <p><br> Data engineering is a key function enabling a robust data science environment for our consulting services and as middleware in preparing our client&#x2019;s data for processing within our software as a service (SaaS) solutions.</p>
 <p></p>
 <p><br> We are seeking a Senior Data engineer to join our Machine Learning Engineering team and help us design and build scalable, reliable, and secure data pipelines, infrastructure, and systems for our consulting services and software as a service solutions. The Senior Data Engineer will work with a cross-functional team of Machine Learning engineers, software engineers, data scientists, and data analysts to deliver data products and solutions for our clients while also contributing to our internal data practices.</p>
 <p></p>
 <p><b><br> Responsibilities:</b></p>
 <br> 
 <p></p>
 <ul>
  <li> Design, develop and maintain data pipelines, infrastructure, and systems to support data products and solutions using technologies such as AWS, Python, Databricks, Spark, and SQL databases like PostgreSQL.</li>
  <li> Work with cross-functional teams to translate business problems into technical solutions and provide technical guidance and mentorship to junior data engineers.</li>
  <li> Develop and implement data engineering strategies and best practices that align with business objectives and customer needs.</li>
  <li> Monitor and troubleshoot data pipelines and systems to ensure data quality, integrity, and availability.</li>
  <li> Conduct research on industry trends and best practices to improve data engineering capabilities and evaluate new data technologies and tools.</li>
  <li> Build and maintain data lakes to support business intelligence needs.</li>
  <li> Manage the software development lifecycle and DevOps aspects of the code.</li>
  <li> Collaborate with data scientists and analysts to understand their data requirements and provide them with optimal data solutions.</li>
  <li> Create new data validation methods and data analysis tools.</li>
 </ul>
 <p></p>
 <p><b><br> Requirements:</b></p>
 <br> 
 <p></p>
 <ul>
  <li> Bachelor&apos;s degree in computer science, information technology, or a related field.</li>
  <li> 4-7 years of experience in data engineering or a similar role.</li>
  <li> Expert-level skills in Python, SQL databases such as PostgreSQL, and big data technologies such as Databricks and Spark.</li>
  <li> Hands-on experience building cloud resident data pipelines in AWS.</li>
  <li> Strong understanding of data governance, security, privacy, and retention policies and procedures.</li>
  <li> Strong communication, collaboration, and problem-solving skills.</li>
  <li> High proficiency using agile software tools like Jira and following mature DevOps practices using GIT, Docker, and CI servers like Jenkins.</li>
  <li> Passion for data and innovation.</li>
 </ul>
 <p></p>
 <p><b><br> Preferred Qualifications:</b></p>
 <ul>
  <li> Demonstrated capacity to work autonomously and proactively, with a proven track record of achieving results without constant supervision.</li>
  <li> Experience with ETL optimization, designing, coding, and tuning big data processes in Databricks.</li>
  <li> Sound knowledge of data lineage and data quality techniques.</li>
  <li> Experience in working with data science and machine learning models and frameworks</li>
  <li> Previous experience in the Energy or Utility industry in an analytic role.</li>
  <li> MS Degree in management information systems, computer programming, software engineering, data science, or an equivalent STEM field.</li>
 </ul>
 <p></p>
 <p><b><br> A little about E Source</b></p>
 <p></p>
 <p><br> Since 1986, E Source has focused on partnering with utilities to help their customers save electricity. That novel approach defined the art of electric end-use efficiency&#x2014;better known as energy efficiency. We&#x2019;ve expanded that concept to include a broader perspective of sustainability for utilities that deliver electricity, natural gas, and water.</p>
 <p></p>
 <p><br> We work to enhance relationships with the people utilities serve, achieve the next generation of savings, and lead the carbon-reduction effort. We help utilities think differently, make data useful, and learn from the best strategies across the industry. Our people, our insights, and our network help our clients and give them the assurance that the programs they implement are the most effective.</p>
 <p></p>
 <p><b><br> Benefits</b></p>
 <br> 
 <p></p>
 <ul>
  <li> We offer excellent insurance packages, including medical, dental, and vision plans; company-paid life insurance; company-paid long- and short-term disability insurance; and medical and dependent-care flexible spending plans.</li>
  <li> We provide a flexible time off (FTO) program; E Source employees can take as many paid days off per year as they need, with manager approval, while fulfilling their work obligations and ensuring proper coverage of their responsibilities.</li>
  <li> We offer flexible schedules, flexible work locations, and a paid parental leave benefit.</li>
  <li> We provide a 401(k) plan with a 3% employer match.</li>
 </ul>
 <p></p>
 <p><br> The budgeted salary for this position is <b>&#x24;99,750 to &#x24;135,450 (includes base + annual bonus)</b>. Actual pay will be adjusted based on experience.</p>
 <p></p>
 <p><br> This person can work remote, a hybrid schedule, or from one of our physical office locations.</p>
 <p></p>
 <p><br> Applicants must be authorized to work for any employer in the US. We&#x2019;re unable to sponsor or take over sponsorship of employment visas at this time.</p>
 <p></p>
 <p><br> All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.</p>
 <p></p>
 <p><br> #LI-Remote</p>
 <p></p>
 <p><br> #LI-AP1</p>
</div>",https://www.paycomonline.net/v4/ats/web.php/jobs/ViewJobDetails?job=174420&clientkey=D091157AE56F3B54E83C2790C96E526E&source=Indeed&source=Indeed.com,cf276d4ea3c5fdcb,,Full-time,,,Hybrid remote,Data Engineer III - REMOTE,5 days ago,2023-10-13T13:34:25.387Z,4.0,10.0,"$99,750 - $135,450 a year",2023-10-18T13:34:25.389Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=cf276d4ea3c5fdcb&from=jasx&tk=1hd1ft1i8joou800&vjs=3
92,Enigma,"The Opportunity: 
  Join Enigma at a pivotal moment as we continue to provide valuable solutions for small businesses. We're seeking an experienced Data Product Engineer to join our team and help develop and build the iteration of small business data products . Your work will directly impact the accuracy of small business profiles, which influence decisions for companies that employ half the U.S. workforce! 
  The Role: 
  As a Data Product Engineer, you will design and develop data products that solve critical customer pain points. Your impact will be measured by your ability to deliver scalable, high-quality data products that customers love. To succeed in this role, you will bring together three distinct capabilities 
  
  Understand acute customer needs and extract common problem structures across customers 
  Analyze and extract value from data at scale 
  Build efficient, maintainable production-grade data pipelines 
  
 We are looking for someone who: 
  
  Operates with a bias for action and knows how to deliver value in the short, medium and long term 
  Loves talking to customers and works hard to solve their problems in a repeatable way 
  Adopts a principled, metrics-driven approach to difficult data problems and demonstrates excellence in their analytics and engineering craft 
  Operates transparently, collaboratively and with low ego—loves learning from others and having their ideas questioned and challenged 
  
 What Makes This Job Exciting: 
  
  Impact: Develop products that take an innovative data-first approach to solving high-value customer problems. 
  Technical Challenge: Tackle complex data and engineering problems while balancing customer impact, reliability, scalability, data quality, and an ambitious forward development plan. 
  Ownership: You'll work directly with customers. You and your teammates will design and build products based on your learnings . 
  
 Bonus Points If You: 
  
  Have experience building data products at scale. 
  Bring prior experience in Databricks or Spark 
  Have worked on data products in the marketing, kyb or credit underwriting space. 
  
 About Us: 
  At Enigma, we're building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values – generosity, curiosity, ingenuity, & drive – guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We're a team of curious, driven individuals with diverse backgrounds and skills, but we're all passionate about engineering deeper understanding through data—together. If this resonates, we would love to hear from you! 
  We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. 
  Salary Range: $160,000-$210,000 
  A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we're creating together","<div>
 <p><b>The Opportunity:</b></p> 
 <p> Join Enigma at a pivotal moment as we continue to provide valuable solutions for small businesses. We&apos;re seeking an experienced Data Product Engineer to join our team and help develop and build the iteration of small business data products . Your work will directly impact the accuracy of small business profiles, which influence decisions for companies that employ half the U.S. workforce!</p> 
 <p><b> The Role:</b></p> 
 <p> As a Data Product Engineer, you will design and develop data products that solve critical customer pain points. Your impact will be measured by your ability to deliver scalable, high-quality data products that customers love. To succeed in this role, you will bring together three distinct capabilities</p> 
 <ul> 
  <li>Understand acute customer needs and extract common problem structures across customers</li> 
  <li>Analyze and extract value from data at scale</li> 
  <li>Build efficient, maintainable production-grade data pipelines</li> 
 </ul> 
 <p><b>We are looking for someone who:</b></p> 
 <ul> 
  <li>Operates with a bias for action and knows how to deliver value in the short, medium and long term</li> 
  <li>Loves talking to customers and works hard to solve their problems in a repeatable way</li> 
  <li>Adopts a principled, metrics-driven approach to difficult data problems and demonstrates excellence in their analytics and engineering craft</li> 
  <li>Operates transparently, collaboratively and with low ego&#x2014;loves learning from others and having their ideas questioned and challenged</li> 
 </ul> 
 <p><b>What Makes This Job Exciting:</b></p> 
 <ul> 
  <li>Impact: Develop products that take an innovative data-first approach to solving high-value customer problems.</li> 
  <li>Technical Challenge: Tackle complex data and engineering problems while balancing customer impact, reliability, scalability, data quality, and an ambitious forward development plan.</li> 
  <li>Ownership: You&apos;ll work directly with customers. You and your teammates will design and build products based on your learnings .</li> 
 </ul> 
 <p><b>Bonus Points If You:</b></p> 
 <ul> 
  <li>Have experience building data products at scale.</li> 
  <li>Bring prior experience in Databricks or Spark</li> 
  <li>Have worked on data products in the marketing, kyb or credit underwriting space.</li> 
 </ul> 
 <p><b>About Us:</b></p> 
 <p> At Enigma, we&apos;re building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values &#x2013; generosity, curiosity, ingenuity, &amp; drive &#x2013; guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We&apos;re a team of curious, driven individuals with diverse backgrounds and skills, but we&apos;re all passionate about engineering deeper understanding through data&#x2014;together. If this resonates, we would love to hear from you!</p> 
 <p> We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.</p> 
 <p><i> Salary Range: &#x24;160,000-&#x24;210,000</i></p> 
 <p><i> A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we&apos;re creating together</i></p>
</div>",https://www.indeed.com/rc/clk?jk=5e030fc53b9ee4f1&atk=&xpse=SoBm67I3JhX6bryKLx0LbzkdCdPP,5e030fc53b9ee4f1,,,,,Remote,"Senior Software Engineer, Data Product",6 days ago,2023-10-12T13:34:29.530Z,,,"$160,000 - $210,000 a year",2023-10-18T13:34:29.531Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=5e030fc53b9ee4f1&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
94,United Talent Agency (UTA),"UTA is committed to building high performance data and application platforms by utilizing the most effective cutting-edge technology we can. UTA's Engineering Team's mission is to use Continuous Integration and Continuous Delivery methods to create a sustainable and secure pipeline in delivering solutions to its business stakeholders. We do this by continuously analyzing areas of improvement and identifying areas of opportunity to automate, secure and codify our environment.
 
 
 
   As a Data Infrastructure Engineer at UTA, you will play a crucial role in optimizing workload, query performance, and distributed query execution. You'll work in a fast-paced development and operational environment, contributing to world-class tooling, automation, and infrastructure for our SaaS platform.
 
 
 
   The salary range for this role is $170,000 to $200,000 commensurate with experience and skills. This role will be eligible for a discretionary bonus.
 
 
   What You Will Do
 
 
   Create a framework that enables the development team to understand the full impact of their features for stakeholders, including testing before it is enabled in production Providing insights into performance and reliability on actual production, guaranteeing no customer impact
   Design a visualization framework that provides the ability to visualize all queries in all environments, including production, while also designing improvements for better insights into potential issues and query plan manipulation
   Develop a service that automatically finds and resolves data corruption in the system, at all stages of development, including in production
   Responsible for creating a testing platform, meant to find correctness and reliability issues in pre-production environments
   Creation of automated system to safely orchestrate the enablement of features in production that will automatically detect and mitigate production issues, for rapid end-to-end feature rollout process at scale
   Identify infrastructure gaps, for which you can design and implement automated solutions
   Contribute to the design, development, and maintenance of some of our existing projects
   Help ensure data governance and security are effectively managed and maintained in their organizations.
 
 
   What You Will Need
 
 
   5+ years hands-on software engineering experience
   Advanced CS fundamentals including data structures, algorithms, and distributed systems
   Good understanding of database fundamentals
   Background in database tooling, database internals, schema design, or building components for large scale data processing systems
   Systems programming skills with fluency in Java, JavaScript or Python
   Track record of identifying and implementing creative solutions with data from multiple sources
 
 
 
   What You Will Get
 
 
   The unique and exciting opportunity to work at one of the leading global entertainment companies
   Access to the tools, leadership, and resources you will need to create and drive a center of excellence
   The opportunity to do the best work of your career
   Work in an inclusive and diverse company culture
   Competitive benefits and programs to support your well-being
   Experience working in a collaborative environment with room to grow
 
 
 
   About UTA
 
 
 
   UTA unites ideas, opportunities and talent. The company represents some of the world's most iconic, barrier-breaking artists, creators and changemakers—from actors, athletes and musicians to writers, gamers and digital influencers. One of the most influential companies in global entertainment, UTA's business spans talent representation, content production, as well as strategic advisory and marketing work with some of the world's biggest brands. Affiliated companies include Digital Brand Architects, KLUTCH Sports Group, Curtis Brown Group, and MediaLink. UTA is headquartered in Los Angeles with offices in Atlanta, Chicago, Nashville, New York and London.
 
 
 
   For more information: 
  
   https://www.unitedtalent.com/about/
  
   
   UTA and its Affiliated Companies are Equal Employment Opportunity employers and welcome all job seekers including individuals with disabilities and veterans with disabilities.
 
 
 
   #LI-CB1","<div>
 <div>
  UTA is committed to building high performance data and application platforms by utilizing the most effective cutting-edge technology we can. UTA&apos;s Engineering Team&apos;s mission is to use Continuous Integration and Continuous Delivery methods to create a sustainable and secure pipeline in delivering solutions to its business stakeholders. We do this by continuously analyzing areas of improvement and identifying areas of opportunity to automate, secure and codify our environment.
 </div>
 <div></div>
 <div>
   As a Data Infrastructure Engineer at UTA, you will play a crucial role in optimizing workload, query performance, and distributed query execution. You&apos;ll work in a fast-paced development and operational environment, contributing to world-class tooling, automation, and infrastructure for our SaaS platform.
 </div>
 <div></div>
 <div>
   The salary range for this role is &#x24;170,000 to &#x24;200,000 commensurate with experience and skills. This role will be eligible for a discretionary bonus.
 </div>
 <div>
  <br> What You Will Do
 </div>
 <ul>
  <li> Create a framework that enables the development team to understand the full impact of their features for stakeholders, including testing before it is enabled in production Providing insights into performance and reliability on actual production, guaranteeing no customer impact</li>
  <li> Design a visualization framework that provides the ability to visualize all queries in all environments, including production, while also designing improvements for better insights into potential issues and query plan manipulation</li>
  <li> Develop a service that automatically finds and resolves data corruption in the system, at all stages of development, including in production</li>
  <li> Responsible for creating a testing platform, meant to find correctness and reliability issues in pre-production environments</li>
  <li> Creation of automated system to safely orchestrate the enablement of features in production that will automatically detect and mitigate production issues, for rapid end-to-end feature rollout process at scale</li>
  <li> Identify infrastructure gaps, for which you can design and implement automated solutions</li>
  <li> Contribute to the design, development, and maintenance of some of our existing projects</li>
  <li> Help ensure data governance and security are effectively managed and maintained in their organizations.</li>
 </ul>
 <div>
  <br> What You Will Need
 </div>
 <ul>
  <li> 5+ years hands-on software engineering experience</li>
  <li> Advanced CS fundamentals including data structures, algorithms, and distributed systems</li>
  <li> Good understanding of database fundamentals</li>
  <li> Background in database tooling, database internals, schema design, or building components for large scale data processing systems</li>
  <li> Systems programming skills with fluency in Java, JavaScript or Python</li>
  <li> Track record of identifying and implementing creative solutions with data from multiple sources</li>
 </ul>
 <div></div>
 <div>
   What You Will Get
 </div>
 <ul>
  <li> The unique and exciting opportunity to work at one of the leading global entertainment companies</li>
  <li> Access to the tools, leadership, and resources you will need to create and drive a center of excellence</li>
  <li> The opportunity to do the best work of your career</li>
  <li> Work in an inclusive and diverse company culture</li>
  <li> Competitive benefits and programs to support your well-being</li>
  <li> Experience working in a collaborative environment with room to grow</li>
 </ul>
 <div></div>
 <div>
   About UTA
 </div>
 <div></div>
 <div>
   UTA unites ideas, opportunities and talent. The company represents some of the world&apos;s most iconic, barrier-breaking artists, creators and changemakers&#x2014;from actors, athletes and musicians to writers, gamers and digital influencers. One of the most influential companies in global entertainment, UTA&apos;s business spans talent representation, content production, as well as strategic advisory and marketing work with some of the world&apos;s biggest brands. Affiliated companies include Digital Brand Architects, KLUTCH Sports Group, Curtis Brown Group, and MediaLink. UTA is headquartered in Los Angeles with offices in Atlanta, Chicago, Nashville, New York and London.
 </div>
 <div></div>
 <div>
   For more information: 
  <div>
   https://www.unitedtalent.com/about/
  </div>
  <br> 
  <br> UTA and its Affiliated Companies are Equal Employment Opportunity employers and welcome all job seekers including individuals with disabilities and veterans with disabilities.
 </div>
 <div></div>
 <div>
   #LI-CB1
 </div>
</div>
<div></div>",https://unitedtalent.wd5.myworkdayjobs.com/en-US/UTA/job/Remote/Data-Infrastructure-Engineer_R2861,bc15c50a09fd93ff,,Full-time,,,Remote,Data Infrastructure Engineer,13 days ago,2023-10-05T13:34:34.771Z,,,"$170,000 - $200,000 a year",2023-10-18T13:34:34.772Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=bc15c50a09fd93ff&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
99,For People,"For People is a team of skilled technologists improving government digital services for disadvantaged and vulnerable populations. We embed directly in government agencies to modernize software, systems, and platforms so that they better serve people.
  Your Impact We are a dedicated team focused on creating and managing an extensive Medicare data warehouse at the Centers for Medicare & Medicaid Services (CMS) to serve Medicare beneficiaries' demographic, enrollment, and claims data in a FHIR (Fast Healthcare Interoperability Resources) format. We are responsible for providing data to several Medicare APIs so that those systems can seamlessly exchange data between various healthcare providers, insurers, and patients. You will directly impact the quality of healthcare that over 65 million Medicare enrollees nationwide receive.
  Our Culture For People is a team of humans. We place a significant amount of emphasis on positive work-life balance, setting healthy expectations, and making sure our loved ones are taken care of first. That means picking a child up from school during the day or going for a mid-day walk is okay!
  This position is 100% remote. Our entire team is remote across the United States, from the West Coast to the East Coast. There will never be a return-to-office, as we have none!
  This position's published base salary range is between $125,000 and $160,000 annually, plus generous benefits (e.g., For People pays 100% of Gold-tier employee health insurance premiums) and annual company profit sharing.
  Your Opportunities
 
   Lead the implementation of FHIR (Fast Healthcare Interoperability Resources) standards within the data warehouse, ensuring accuracy and efficient data exchange across the ecosystem of partner APIs.
   Ingest healthcare data from source systems into FHIR resources and profiles through developing ETL (Extract, Transform, Load) processes, checking for data quality and integrity.
   Create and maintain data mapping specifications to transform non-FHIR data formats into FHIR-compliant data.
   Design and maintain the data warehouse's FHIR-based data model to meet the needs of downstream API systems.
   Implement security measures and access controls to protect sensitive healthcare data and comply with healthcare data privacy regulations, such as HIPAA.
   Maintain comprehensive documentation of FHIR implementations, data transformation processes, and data flows.
   Stay informed about industry best practices and evolving FHIR standards.
 
  You Bring
 
   A humble and caring attitude
   In-depth knowledge and experience with FHIR standards and resource types.
   Expert-level Java programming abilities, alongside some familiarity with Python and Bash scripts.
   Proficiency in designing and implementing data ingestion and transformation processes.
   Strong database design and data modeling skills, with experience creating and maintaining data models in a healthcare context.
   A systematic approach to identifying and resolving issues related to FHIR data integration, data quality, and performance.
   Demonstrated commitment to staying updated on industry best practices, evolving FHIR standards, and opportunities for process improvement.
 
  If you're passionate about healthcare technology and ready to positively impact the quality of healthcare for millions of Medicare enrollees nationwide, we encourage you to apply. Join us in revolutionizing healthcare data accessibility.
  Some fine print. You will be working on a United States government platform, and they have a few basic requirements for contractors like ourselves. You must perform all work physically within the United States at all times. In addition, you must be a United States citizen and be able to pass a government-performed public trust background check.
  For People is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, and/or veteran status. 
  
 1D2WJ7jBEl","<div>
 <p>For People is a team of skilled technologists improving government digital services for disadvantaged and vulnerable populations. We embed directly in government agencies to modernize software, systems, and platforms so that they better serve people.</p>
 <p><b> Your Impact</b><br> We are a dedicated team focused on creating and managing an extensive Medicare data warehouse at the Centers for Medicare &amp; Medicaid Services (CMS) to serve Medicare beneficiaries&apos; demographic, enrollment, and claims data in a FHIR (Fast Healthcare Interoperability Resources) format. We are responsible for providing data to several Medicare APIs so that those systems can seamlessly exchange data between various healthcare providers, insurers, and patients. You will <i>directly impact the quality of healthcare that over 65 million Medicare enrollees nationwide receive</i>.</p>
 <p><b> Our Culture</b><br> For People is a team of humans. We place a significant amount of emphasis on positive work-life balance, setting healthy expectations, and making sure our loved ones are taken care of first. That means picking a child up from school during the day or going for a mid-day walk is okay!</p>
 <p> This position is <b><i>100% remote</i></b>. Our entire team is remote across the United States, from the West Coast to the East Coast. There will never be a return-to-office, as we have none!</p>
 <p> This position&apos;s published base salary range is between <i>&#x24;125,000 and &#x24;160,000</i> annually, plus generous benefits (e.g., For People pays 100% of Gold-tier employee health insurance premiums) and annual company profit sharing.</p>
 <p><b> Your Opportunities</b></p>
 <ul>
  <li> Lead the implementation of FHIR (Fast Healthcare Interoperability Resources) standards within the data warehouse, ensuring accuracy and efficient data exchange across the ecosystem of partner APIs.</li>
  <li> Ingest healthcare data from source systems into FHIR resources and profiles through developing ETL (Extract, Transform, Load) processes, checking for data quality and integrity.</li>
  <li> Create and maintain data mapping specifications to transform non-FHIR data formats into FHIR-compliant data.</li>
  <li> Design and maintain the data warehouse&apos;s FHIR-based data model to meet the needs of downstream API systems.</li>
  <li> Implement security measures and access controls to protect sensitive healthcare data and comply with healthcare data privacy regulations, such as HIPAA.</li>
  <li> Maintain comprehensive documentation of FHIR implementations, data transformation processes, and data flows.</li>
  <li> Stay informed about industry best practices and evolving FHIR standards.</li>
 </ul>
 <p><b> You Bring</b></p>
 <ul>
  <li> A humble and caring attitude</li>
  <li> In-depth knowledge and experience with <b>FHIR</b> <b>standards</b> and resource types.</li>
  <li> Expert-level <b>Java</b> programming abilities, alongside some familiarity with Python and Bash scripts.</li>
  <li> Proficiency in designing and implementing data ingestion and transformation processes.</li>
  <li> Strong database design and data modeling skills, with experience creating and maintaining data models in a healthcare context.</li>
  <li> A systematic approach to identifying and resolving issues related to FHIR data integration, data quality, and performance.</li>
  <li> Demonstrated commitment to staying updated on industry best practices, evolving FHIR standards, and opportunities for process improvement.</li>
 </ul>
 <p> If you&apos;re passionate about healthcare technology and ready to positively impact the quality of healthcare for millions of Medicare enrollees nationwide, we encourage you to apply. Join us in revolutionizing healthcare data accessibility.</p>
 <p><b><i> Some fine print.</i></b> You will be working on a United States government platform, and they have a few basic requirements for contractors like ourselves. You must perform all work physically within the United States at all times. In addition, you must be a United States citizen and be able to pass a government-performed public trust background check.</p>
 <p> For People is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, and/or veteran status.<br> </p>
 <p> </p>
 <p>1D2WJ7jBEl</p>
</div>",https://forpeople.applytojob.com/apply/1D2WJ7jBEl/Senior-Backend-Engineer-FHIR-Data?source=INDE,b151e97f7474d1c8,,Full-time,,,Remote,"Senior Backend Engineer, FHIR Data",6 days ago,2023-10-12T13:34:40.341Z,,,"$125,000 - $160,000 a year",2023-10-18T13:34:40.343Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=b151e97f7474d1c8&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
101,"Gevo, Inc.","About the role: 
  The team at Gevo works with experts in software engineering and data science. sustainability, agronomy, carbon accounting, and data science to help farmers create new value for sustainable practices. We are looking for a team member with excellent geospatial programming, analytics, and data engineering skills for the position of Geospatial Agronomist, Data Engineer. To be successful in this role you will: 
 
  Lead the overall geospatial data management program 
  Design scalable processes and programs to help acquire, organize, analyze, and display layered and temporal geospatial data sets. 
  Be the interface with both growers and external partners to lead collection and transfer of field and crop management boundaries files and other shapefiles from the farm into Verity Tracking. 
  Conduct manual (and developing automated processes for the) clean-up, corrections, and adjustments of geometries as well as attributional data, to fit into the platforms overall data flow. 
  Review, compare, validate, and normalize naming and acreage discrepancies between in-platform maps, USDA shapefiles, and third-party shapefiles. 
  Ensure maps are of acceptable quality and format with the appropriate tract, field, and farm boundaries demarcated. 
  Manage geospatial data associated with projects related to remote sensing, including data acquisition, processing, analysis, and storytelling with data. 
  Integrating remote sensing data with GIS databases and other geospatial information to provide an integrated and comprehensive understanding of the area(s) of interest. 
  Work with the data science and sustainability teams to analyze and interpret remote sensing data. This includes identifying features, objects, characteristics, and patterns within the imagery, such as land cover classification, vegetation indices, change detection, and anomaly detection. 
  Resolve problems by assisting, coaching, and training dealers/distributors on technical problem investigation and solution implementation. 
  Present and promote data management best practices to retailers, agronomy partners, and farmers. 
  Collaborate with the data engineering and data science teams to build automated geospatial workflows. 
  Identification of geospatial trends and insights to solve key tactical and strategic business problems 
  Write clean, testable, and modularized code 
  Work with key stakeholders to identify opportunities to enhance the flow, analysis, and presentation of multiple different geospatial data sets into and across the company 
  Transparently communicate priorities, obstacles, and progress on a regular basis to the Product, Sustainability, and Marketing teams. 
  Comfortably lead technical geospatial operations while following agile principles 
  
 Who you are:
  
  
  Degree or equivalent experience in a quantitative field such as data science, data analytics, geospatial engineering, or computer science; with applied experience in technical geospatial data implementation (programming and visualization) 
  Expert in common geospatial platforms and programming languages such as Python and proficient with data management systems (e.g. ArcGis, GEOJSON etc.) 
  Experience with databases / SQL dialects is preferred 
  Experience in agriculture sales and/or business development and/or crop production preferred. 
  Comfortable in data validation, verification, and quality assurance methods for geospatial data - strong analytical skills and an eye for detail are crucial for fulfilling your duties. 
  Logic-driven critical thinker committed to accuracy, precision, and increasing understanding though geospatial data visualization. 
  Strong ability to manage projects, ask the right questions, and propose product solutions. 
  Passionate about environmental sustainability with domain knowledge in agricultural practices, industrial processes, and data. 
  Experienced in building trusted business relationships. 
  Customer-oriented, with a demonstrated ability to respect and earn the respect of farmers and retail distribution partners 
  Strong written and verbal communicator 
  Enjoy attending farm shows to promote the use of products and services 
  Proficient at managing time, priorities, and expenses. 
  Self-starter, reliable, and able to work independently. 
  Computer Skills: To perform this job successfully, an individual should be proficient in Google Suite applications (and similar office suites). 
 
 Who We Are 
  
  We are People First 
  We are Mission-Focused 
  We are Agile 
  We are Innovators 
  
 What Gevo Offers You 
 
  Free health, dental, vision, life and disability insurance for employee and family 
  21 days of paid time off plus 10 paid holidays 
  401k contribution plan 
  Annual incentive plan, based on Company performance 
  Paid community service time 
  Dog friendly office 
  Be part of a smart, high performing, passionate team 
  Work-from-home stipend, if remote 
  
 Commitment to Diversity and Inclusion
  
  Gevo, Inc. is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.","<p></p>
<div>
 <p><b>About the role:</b></p> 
 <p> The team at Gevo works with experts in software engineering and data science. sustainability, agronomy, carbon accounting, and data science to help farmers create new value for sustainable practices. We are looking for a team member with excellent geospatial programming, analytics, and data engineering skills for the position of Geospatial Agronomist, Data Engineer. To be successful in this role you will: </p>
 <ul>
  <li>Lead the overall geospatial data management program </li>
  <li>Design scalable processes and programs to help acquire, organize, analyze, and display layered and temporal geospatial data sets. </li>
  <li>Be the interface with both growers and external partners to lead collection and transfer of field and crop management boundaries files and other shapefiles from the farm into Verity Tracking. </li>
  <li>Conduct manual (and developing automated processes for the) clean-up, corrections, and adjustments of geometries as well as attributional data, to fit into the platforms overall data flow. </li>
  <li>Review, compare, validate, and normalize naming and acreage discrepancies between in-platform maps, USDA shapefiles, and third-party shapefiles. </li>
  <li>Ensure maps are of acceptable quality and format with the appropriate tract, field, and farm boundaries demarcated. </li>
  <li>Manage geospatial data associated with projects related to remote sensing, including data acquisition, processing, analysis, and storytelling with data. </li>
  <li>Integrating remote sensing data with GIS databases and other geospatial information to provide an integrated and comprehensive understanding of the area(s) of interest. </li>
  <li>Work with the data science and sustainability teams to analyze and interpret remote sensing data. This includes identifying features, objects, characteristics, and patterns within the imagery, such as land cover classification, vegetation indices, change detection, and anomaly detection. </li>
  <li>Resolve problems by assisting, coaching, and training dealers/distributors on technical problem investigation and solution implementation. </li>
  <li>Present and promote data management best practices to retailers, agronomy partners, and farmers. </li>
  <li>Collaborate with the data engineering and data science teams to build automated geospatial workflows. </li>
  <li>Identification of geospatial trends and insights to solve key tactical and strategic business problems </li>
  <li>Write clean, testable, and modularized code </li>
  <li>Work with key stakeholders to identify opportunities to enhance the flow, analysis, and presentation of multiple different geospatial data sets into and across the company </li>
  <li>Transparently communicate priorities, obstacles, and progress on a regular basis to the Product, Sustainability, and Marketing teams. </li>
  <li>Comfortably lead technical geospatial operations while following agile principles</li> 
 </ul> 
 <p><b>Who you are:</b></p>
 <br> 
 <ul> 
  <li>Degree or equivalent experience in a quantitative field such as data science, data analytics, geospatial engineering, or computer science; with applied experience in technical geospatial data implementation (programming and visualization) </li>
  <li>Expert in common geospatial platforms and programming languages such as Python and proficient with data management systems (e.g. ArcGis, GEOJSON etc.) </li>
  <li>Experience with databases / SQL dialects is preferred </li>
  <li>Experience in agriculture sales and/or business development and/or crop production preferred. </li>
  <li>Comfortable in data validation, verification, and quality assurance methods for geospatial data - strong analytical skills and an eye for detail are crucial for fulfilling your duties. </li>
  <li>Logic-driven critical thinker committed to accuracy, precision, and increasing understanding though geospatial data visualization. </li>
  <li>Strong ability to manage projects, ask the right questions, and propose product solutions. </li>
  <li>Passionate about environmental sustainability with domain knowledge in agricultural practices, industrial processes, and data. </li>
  <li>Experienced in building trusted business relationships. </li>
  <li>Customer-oriented, with a demonstrated ability to respect and earn the respect of farmers and retail distribution partners </li>
  <li>Strong written and verbal communicator </li>
  <li>Enjoy attending farm shows to promote the use of products and services </li>
  <li>Proficient at managing time, priorities, and expenses. </li>
  <li>Self-starter, reliable, and able to work independently. </li>
  <li>Computer Skills: To perform this job successfully, an individual should be proficient in Google Suite applications (and similar office suites). </li>
 </ul>
 <p><b>Who We Are</b></p> 
 <ul> 
  <li>We are People First </li>
  <li>We are Mission-Focused </li>
  <li>We are Agile </li>
  <li>We are Innovators</li> 
 </ul> 
 <p><b>What Gevo Offers You</b> </p>
 <ul>
  <li>Free health, dental, vision, life and disability insurance for employee and family </li>
  <li>21 days of paid time off plus 10 paid holidays </li>
  <li>401k contribution plan </li>
  <li>Annual incentive plan, based on Company performance </li>
  <li>Paid community service time </li>
  <li>Dog friendly office </li>
  <li>Be part of a smart, high performing, passionate team </li>
  <li>Work-from-home stipend, if remote</li> 
 </ul> 
 <p><b>Commitment to Diversity and Inclusion</b></p>
 <br> 
 <p> Gevo, Inc. is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.</p>
</div>",https://gevo-inc-careers.rippling-ats.com/job/679776/geospatial-data-engineer?s=in,96205e6a1d6acd47,,,,,Remote,Geospatial Data Engineer,6 days ago,2023-10-12T13:34:45.202Z,,,"$75,000 - $114,000 a year",2023-10-18T13:34:45.204Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=96205e6a1d6acd47&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
102,The Dedham Group,"About Us:
 Our mission at Pulse Analytics is to help decision-makers in oncology and other specialty therapeutic areas, identify and reduce access barriers across the healthcare industry, ensuring patients have access to the treatments they need. Our web-based decision support application connects healthcare industry organizations and key influencers to deliver targeted quality insights to support our client’s customer engagement strategies. We are currently growing and are looking to bring on talented and driven individuals to help build the future of B2B healthcare data analytics products 

 This role is for someone who is excited about architecting data infrastructure and building out data platforms from the ground up. As a Data Engineer, you will have the opportunity to work on key data delivery initiatives – automating data extraction processes and enhancing data sources for our clients. If you are passionate about leveraging data to drive business decisions and thrive in a dynamic environment, we want to hear from you. 

 
In this role you will:
 Design, build, and maintain efficient data pipelines from various sources to support key business initiatives. 
Perform data cleansing and validation processes to ensure the integrity and quality of data used for analysis. 
Act as a data evangelist within the company, promoting the value and impact of data science and engineering initiatives. 
Collaborate closely with leadership, software engineers, and product managers to understand data requirements and align data solutions with business objectives. 
Work alongside Business Analysts to identify opportunities for automated data acquisition and deliver high-quality data that drives actionable insights. 
Develop and implement data governance policies and procedures to ensure the security, privacy, and quality of data. 
Requirements 

 
Minimum Qualifications:
 3+ years of experience as a Data Engineer or in a similar role 
Proficiency in Python and SQL, with the ability to write complex scripts and queries 
Strong knowledge of data modeling, data warehousing, and ETL pipeline development 
Understanding of data management fundamentals and data storage principles 
Exceptional problem-solving and communication skills 
Experience with cloud platforms such as AWS, Azure, or Google Cloud 
Proficiency in Cloud Orchestration tools such as Airflow, Dagster, or Prefect 

 
Preferred Qualifications:
 Bachelors degree in Computer Science or related field 
Experience with Big Data Technologies (e.g. Hadoop, Hive, Spark) 
Familiarity with the AWS Ecosystem (e.g. AWS S3, AWS Athena, AWS Glue) 
Knowledge of Distributed Systems 
Benefits 

 
Company Culture and Values:
 At Pulse Analytics, we foster a collaborative and innovative environment where your ideas are heard and valued. Our core values: 

 
Service: We adopt a client-first approach to solutions 

Agile: Being flexible in our approach allows us to adapt and iterate quickly to client demands or needs 

Innovation: We strive for excellence, embrace risk-taking, and take bold actions to innovate and improve 

Transparency: When everyone is on the same page, we produce our best work 

Ownership: Everyone is a product owner and operates with integrity and self-accountability 

 
Perks and Benefits:
 401k match 
Medical, Vision, and Dental Insurance Coverage 
Casual dress code 
Remote work flexibility 
Generous PTO/Vacation 
Stipend for conferences 

 The expected base salary for this position ranges from $115,000 - $150,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus. 

 The Dedham Group is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people’s differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual’s abilities, skills, performance and behavior and our business requirements. The Dedham Group operates a zero tolerance policy to any form of discrimination, abuse or harassment. 

 #LI-REMOTE 

 #LI-YK1","<b>About Us:</b>
<br> Our mission at Pulse Analytics is to help decision-makers in oncology and other specialty therapeutic areas, identify and reduce access barriers across the healthcare industry, ensuring patients have access to the treatments they need. Our web-based decision support application connects healthcare industry organizations and key influencers to deliver targeted quality insights to support our client&#x2019;s customer engagement strategies. We are currently growing and are looking to bring on talented and driven individuals to help build the future of B2B healthcare data analytics products 
<br>
<br> This role is for someone who is excited about architecting data infrastructure and building out data platforms from the ground up. As a Data Engineer, you will have the opportunity to work on key data delivery initiatives &#x2013; automating data extraction processes and enhancing data sources for our clients. If you are passionate about leveraging data to drive business decisions and thrive in a dynamic environment, we want to hear from you. 
<br>
<br> 
<b>In this role you will:</b>
<br> Design, build, and maintain efficient data pipelines from various sources to support key business initiatives. 
<br>Perform data cleansing and validation processes to ensure the integrity and quality of data used for analysis. 
<br>Act as a data evangelist within the company, promoting the value and impact of data science and engineering initiatives. 
<br>Collaborate closely with leadership, software engineers, and product managers to understand data requirements and align data solutions with business objectives. 
<br>Work alongside Business Analysts to identify opportunities for automated data acquisition and deliver high-quality data that drives actionable insights. 
<br>Develop and implement data governance policies and procedures to ensure the security, privacy, and quality of data. 
<br>Requirements 
<br>
<br> 
<b>Minimum Qualifications:</b>
<br> 3+ years of experience as a Data Engineer or in a similar role 
<br>Proficiency in Python and SQL, with the ability to write complex scripts and queries 
<br>Strong knowledge of data modeling, data warehousing, and ETL pipeline development 
<br>Understanding of data management fundamentals and data storage principles 
<br>Exceptional problem-solving and communication skills 
<br>Experience with cloud platforms such as AWS, Azure, or Google Cloud 
<br>Proficiency in Cloud Orchestration tools such as Airflow, Dagster, or Prefect 
<br>
<br> 
<b>Preferred Qualifications:</b>
<br> Bachelors degree in Computer Science or related field 
<br>Experience with Big Data Technologies (e.g. Hadoop, Hive, Spark) 
<br>Familiarity with the AWS Ecosystem (e.g. AWS S3, AWS Athena, AWS Glue) 
<br>Knowledge of Distributed Systems 
<br>Benefits 
<br>
<br> 
<b>Company Culture and Values:</b>
<br> At Pulse Analytics, we foster a collaborative and innovative environment where your ideas are heard and valued. Our core values: 
<br>
<br> 
<b>Service:</b> We adopt a client-first approach to solutions 
<br>
<b>Agile:</b> Being flexible in our approach allows us to adapt and iterate quickly to client demands or needs 
<br>
<b>Innovation:</b> We strive for excellence, embrace risk-taking, and take bold actions to innovate and improve 
<br>
<b>Transparency:</b> When everyone is on the same page, we produce our best work 
<br>
<b>Ownership:</b> Everyone is a product owner and operates with integrity and self-accountability 
<br>
<br> 
<b>Perks and Benefits:</b>
<br> 401k match 
<br>Medical, Vision, and Dental Insurance Coverage 
<br>Casual dress code 
<br>Remote work flexibility 
<br>Generous PTO/Vacation 
<br>Stipend for conferences 
<br>
<br> The expected base salary for this position ranges from &#x24;115,000 - &#x24;150,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus. 
<br>
<br> The Dedham Group is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people&#x2019;s differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual&#x2019;s abilities, skills, performance and behavior and our business requirements. The Dedham Group operates a zero tolerance policy to any form of discrimination, abuse or harassment. 
<br>
<br> #LI-REMOTE 
<br>
<br> #LI-YK1",https://apply.workable.com/the-dedham-group/j/D3C968FDA4/,38d10858e6384637,,Full-time,,,"New York, NY",Data Engineer,6 days ago,2023-10-12T13:34:54.381Z,,,"$115,000 - $150,000 a year",2023-10-18T13:34:54.384Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=38d10858e6384637&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
103,WorkCog,"Job Description:

 Minimum 9+ years exp Mandatory
 Mandatory Tech: We consider the following technologies as mandatory and should be familiar to the candidate:
 GCP
 Data Warehousing
 GIT
 Core Tech: Our ideal candidate should possess strong proficiency in the following technologies:
 Airflow 2years
 Python - 3 Years
 SQL
 Spark - 3 Years
 ETL

Job Type: Contract
Pay: $55.00 - $85.00 per hour
Expected hours: 40 per week
Benefits:

 Referral program

Compensation package:

 Hourly pay

Experience level:

 10 years
 11+ years
 9 years

Schedule:

 8 hour shift

Experience:

 Informatica: 5 years (Preferred)
 SQL: 6 years (Preferred)
 Data warehouse: 7 years (Preferred)

Work Location: Remote","<p><b>Job Description:</b></p>
<ul>
 <li>Minimum 9+ years exp Mandatory</li>
 <li>Mandatory Tech: We consider the following technologies as mandatory and should be familiar to the candidate:</li>
 <li>GCP</li>
 <li>Data Warehousing</li>
 <li>GIT</li>
 <li>Core Tech: Our ideal candidate should possess strong proficiency in the following technologies:</li>
 <li>Airflow 2years</li>
 <li>Python - 3 Years</li>
 <li>SQL</li>
 <li>Spark - 3 Years</li>
 <li>ETL</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;55.00 - &#x24;85.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Benefits:</p>
<ul>
 <li>Referral program</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Hourly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 5 years (Preferred)</li>
 <li>SQL: 6 years (Preferred)</li>
 <li>Data warehouse: 7 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,879c5ddd69953e54,,Contract,,,Remote,"Senior Data Engineer (Contract ""W2"". With 10+ Year's)",5 days ago,2023-10-13T13:35:01.469Z,,,$55 - $85 an hour,2023-10-18T13:35:01.472Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=879c5ddd69953e54&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
105,IBM,"Introduction
   At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.
   
   
  Your Role and Responsibilities
   Octo, an IBM company, is an industry-leading, award-winning provider of technical solutions for the federal government. At Octo, we specialize in providing agile software engineering, user experience design, cloud services, and digital strategy services that address government's most pressing missions. Octo delivers intelligent solutions and rapid results, yielding lower costs and measurable outcomes.
   Our team is what makes Octo great. At Octo you'll work beside some of the smartest and most accomplished staff you'll find in your career. Octo offers fantastic benefits and an amazing workplace culture where you will feel valued while you perform mission critical work for our government. Voted one of the region’s best places to work multiple times, Octo is an employer of choice!
   
  
  As a Data Engineer, you will work closely with architects, engineers, and integrators to assess customer requirements and to design and support our team to unlock insights from the massive amounts of data within the Veterans Affairs ecosystem. You will be tasked with overall onboarding, operationalizing, administration, and maintenance of key big data/data science/machine learning platforms like Databricks and other cutting-edge technologies.
   Previous experience with Veterans Affairs and/or health/clinical data is a major plus.
   Us...
   We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.
   Program Mission...
   This program supports Veterans Affairs' strategic mission of furthering efforts to modernize its data analytics platform and enhance accessibility to enterprise data and reporting tools.
   
  
  Responsibilities...
  
    Serves as a technical consultant to implement Analytics solutions and produce Data Domain ETL Scripts.
    Uses PowerBI/dashboards to support problem identification and resolution.
    Develops and maintains documentation on various operational and design aspects of the Platform. Assist in troubleshooting issues and resolving them.
    Builds awareness, increases knowledge and drives adoption of modern technologies, sharing user and engineering benefits to gain buy-in.
    Effectively communicates with and influences key stakeholders across the enterprise, at all levels of the organization.
    Operates as a trusted advisor for technology, platform, or capability domain, helping to shape use cases and implementation in a unified manner.
  
   Years of Experience: Must have at least 5 years of experience with Microsoft database and BI technologies, including at least 2-3 years of experience with Azure Data Lake Storage, Azure Data Factory, Azure SQL DW, Azure Synapse, Databricks, Spark, and/or Python
   Education: Bachelor's degree in computer science or related area OR 8 years of additional experience will be considered in lieu of degree. 
  Location: Remote within the United States.
   Clearance: Ability to obtain a Public Trust security clearance.
   Required Technical and Professional Expertise
   
  
    See below for experience and educational requirements.
    Experience defining and implementing strategies for extracting, transforming, and loading data from multiple data sources into analytic data stores.
    Knowledge of Cloud Data Analytics platforms 
   Experience programming in PowerShell, Python, SQL.
    Experience with cloud data storage formats such as Parquet, Avro. 
   Experience with data transformation techniques.
    Ability to test data integrity and develop tests and quality checks. 
   Experience preparing data for various types of data analysis: descriptive, diagnostic, predictive, prescriptive. 
   Performance analysis and tuning experience
    Experience with Data Warehouse or Big Data solutions
    Experience with ML models
    Experience with data modeling and database design
    Strong communication, interpersonal, and collaboration skills working in a team-oriented environment.
  
   
   Preferred Technical and Professional Expertise
   
  
    Experience supporting Department of Veterans Affairs (VA) and/or other federal organizations.
    Advanced SQL, NoSQL query, and scripting. Experience with Python, Java.
    Experience with Azure Data Lake Storage, Azure Data Factory, Azure SQL DW, Azure Synapse, Databricks, Spark, and/or Python
    Experience with relational database systems (i.e., DB2, SQL Server) and non-relational databases such as (Azure SQL, Amazon RDBS, MongoDB, Hadoop tools).
    Understanding of data design concepts (i.e., data modeling, data mapping, OLTP, and OLAP).
    Experience modeling data, message, and service interoperability.
    Azure PowerShell knowledge
  
 
 
  
    
    About Business Unit
   IBM Consulting is IBM’s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients’ businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.
 
 
  
    
    Your Life @ IBM
   In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.
   Being an IBMer means you’ll be able to learn and develop yourself and your career, you’ll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.
   Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.
   Are you ready to be an IBMer?
 
 
  
    About IBM
   IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.
   
   Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. 
   
   At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.
 
 
  
    
    Location Statement
   IBM offers a competitive and comprehensive benefits program. Eligible employees may have access to:
   
   
  
   Healthcare benefits including medical & prescription drug coverage, dental, vision, and mental health & well being
   - Financial programs such as 401(k), the IBM Employee Stock Purchase Plan, financial counseling, life insurance, short & long- term disability coverage, and opportunities for performance based salary incentive programs
   
  
   Generous paid time off including 12 holidays, minimum 56 hours sick time, 120 hours vacation, 12 weeks parental bonding leave in accordance with IBM Policy, and other Paid Care Leave programs. IBM also offers paid family leave benefits to eligible employees where required by applicable law
   Training and educational resources on our personalized, AI-driven learning platform where IBMers can grow skills and obtain industry-recognized certifications to achieve their career goals
   Diverse and inclusive employee resource groups, giving & volunteer opportunities, and discounts on retail products, services & experiences
  
   The compensation range and benefits for this position are based on a full-time schedule for a full calendar year. The salary will vary depending on your job-related skills, experience and location. Pay increment and frequency of pay will be in accordance with employment classification and applicable laws. For part time roles, your compensation and benefits will be adjusted to reflect your hours. Benefits may be pro-rated for those who start working during the calendar year. 
   
   We consider qualified applicants with criminal histories, consistent with applicable law.
 
 
  
    
    Being You @ IBM
   IBM is committed to creating a diverse environment and is proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, caste, genetics, pregnancy, disability, neurodivergence, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.","<div>
 <div>
  <b>Introduction</b>
  <br> At IBM, work is more than a job - it&apos;s a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you&apos;ve never thought possible. Are you ready to lead in this new era of technology and solve some of the world&apos;s most challenging problems? If so, lets talk.
  <br> 
  <br> 
  <b>Your Role and Responsibilities</b>
  <p><br> Octo, an IBM company, is an industry-leading, award-winning provider of technical solutions for the federal government. At Octo, we specialize in providing agile software engineering, user experience design, cloud services, and digital strategy services that address government&apos;s most pressing missions. Octo delivers intelligent solutions and rapid results, yielding lower costs and measurable outcomes.</p>
  <p> Our team is what makes Octo great. At Octo you&apos;ll work beside some of the smartest and most accomplished staff you&apos;ll find in your career. Octo offers fantastic benefits and an amazing workplace culture where you will feel valued while you perform mission critical work for our government. Voted one of the region&#x2019;s best places to work multiple times, Octo is an employer of choice!</p>
  <p> </p>
  <p></p>
  <p>As a <b>Data Engineer</b>, you will work closely with architects, engineers, and integrators to assess customer requirements and to design and support our team to unlock insights from the massive amounts of data within the Veterans Affairs ecosystem. You will be tasked with overall onboarding, operationalizing, administration, and maintenance of key big data/data science/machine learning platforms like Databricks and other cutting-edge technologies.</p>
  <p> Previous experience with Veterans Affairs and/or health/clinical data is a major plus.</p>
  <p><b> Us...</b></p>
  <p> We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client&#x2019;s missions.</p>
  <p><b> Program Mission...</b></p>
  <p> This program supports Veterans Affairs&apos; strategic mission of furthering efforts to modernize its data analytics platform and enhance accessibility to enterprise data and reporting tools.</p>
  <p> </p>
  <p></p>
  <p><b>Responsibilities...</b></p>
  <ul>
   <li> Serves as a technical consultant to implement Analytics solutions and produce Data Domain ETL Scripts.</li>
   <li> Uses PowerBI/dashboards to support problem identification and resolution.</li>
   <li> Develops and maintains documentation on various operational and design aspects of the Platform. Assist in troubleshooting issues and resolving them.</li>
   <li> Builds awareness, increases knowledge and drives adoption of modern technologies, sharing user and engineering benefits to gain buy-in.</li>
   <li> Effectively communicates with and influences key stakeholders across the enterprise, at all levels of the organization.</li>
   <li> Operates as a trusted advisor for technology, platform, or capability domain, helping to shape use cases and implementation in a unified manner.</li>
  </ul>
  <p><b> Years of Experience: </b>Must have at<b> </b>least 5 years of experience with Microsoft database and BI technologies, including at least 2-3 years of experience with Azure Data Lake Storage, Azure Data Factory, Azure SQL DW, Azure Synapse, Databricks, Spark, and/or Python</p>
  <p><b> Education: </b>Bachelor&apos;s degree in computer science or related area OR 8 years of additional experience will be considered in lieu of degree. </p>
  <p><b>Location: </b>Remote within the United States.</p>
  <p><b> Clearance:</b> Ability to obtain a Public Trust security clearance.</p>
  <b><br> Required Technical and Professional Expertise</b>
  <br> 
  <ul>
   <li> See below for experience and educational requirements.</li>
   <li> Experience defining and implementing strategies for extracting, transforming, and loading data from multiple data sources into analytic data stores.</li>
   <li> Knowledge of Cloud Data Analytics platforms </li>
   <li>Experience programming in PowerShell, Python, SQL.</li>
   <li> Experience with cloud data storage formats such as Parquet, Avro. </li>
   <li>Experience with data transformation techniques.</li>
   <li> Ability to test data integrity and develop tests and quality checks. </li>
   <li>Experience preparing data for various types of data analysis: descriptive, diagnostic, predictive, prescriptive. </li>
   <li>Performance analysis and tuning experience</li>
   <li> Experience with Data Warehouse or Big Data solutions</li>
   <li> Experience with ML models</li>
   <li> Experience with data modeling and database design</li>
   <li> Strong communication, interpersonal, and collaboration skills working in a team-oriented environment.</li>
  </ul>
  <br> 
  <b> Preferred Technical and Professional Expertise</b>
  <br> 
  <ul>
   <li> Experience supporting Department of Veterans Affairs (VA) and/or other federal organizations.</li>
   <li> Advanced SQL, NoSQL query, and scripting. Experience with Python, Java.</li>
   <li> Experience with Azure Data Lake Storage, Azure Data Factory, Azure SQL DW, Azure Synapse, Databricks, Spark, and/or Python</li>
   <li> Experience with relational database systems (i.e., DB2, SQL Server) and non-relational databases such as (Azure SQL, Amazon RDBS, MongoDB, Hadoop tools).</li>
   <li> Understanding of data design concepts (i.e., data modeling, data mapping, OLTP, and OLAP).</li>
   <li> Experience modeling data, message, and service interoperability.</li>
   <li> Azure PowerShell knowledge</li>
  </ul>
 </div>
 <div>
  <div>
   <br> 
   <b> About Business Unit</b>
  </div> IBM Consulting is IBM&#x2019;s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients&#x2019; businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.
 </div>
 <div>
  <div>
   <br> 
   <b> Your Life @ IBM</b>
  </div> In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.
  <p> Being an IBMer means you&#x2019;ll be able to learn and develop yourself and your career, you&#x2019;ll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.</p>
  <p> Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.</p>
  <p> Are you ready to be an IBMer?</p>
 </div>
 <div>
  <div>
   <b><br> About IBM</b>
  </div> IBM&#x2019;s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.
  <br> 
  <br> Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we&#x2019;re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. 
  <br> 
  <br> At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it&#x2019;s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.
 </div>
 <div>
  <div>
   <br> 
   <b> Location Statement</b>
  </div> IBM offers a competitive and comprehensive benefits program. Eligible employees may have access to:
  <br> 
  <br> 
  <ul>
   <li>Healthcare benefits including medical &amp; prescription drug coverage, dental, vision, and mental health &amp; well being</li>
  </ul> - Financial programs such as 401(k), the IBM Employee Stock Purchase Plan, financial counseling, life insurance, short &amp; long- term disability coverage, and opportunities for performance based salary incentive programs
  <br> 
  <ul>
   <li>Generous paid time off including 12 holidays, minimum 56 hours sick time, 120 hours vacation, 12 weeks parental bonding leave in accordance with IBM Policy, and other Paid Care Leave programs. IBM also offers paid family leave benefits to eligible employees where required by applicable law</li>
   <li>Training and educational resources on our personalized, AI-driven learning platform where IBMers can grow skills and obtain industry-recognized certifications to achieve their career goals</li>
   <li>Diverse and inclusive employee resource groups, giving &amp; volunteer opportunities, and discounts on retail products, services &amp; experiences</li>
  </ul>
  <br> The compensation range and benefits for this position are based on a full-time schedule for a full calendar year. The salary will vary depending on your job-related skills, experience and location. Pay increment and frequency of pay will be in accordance with employment classification and applicable laws. For part time roles, your compensation and benefits will be adjusted to reflect your hours. Benefits may be pro-rated for those who start working during the calendar year. 
  <br> 
  <br> We consider qualified applicants with criminal histories, consistent with applicable law.
 </div>
 <div>
  <div>
   <br> 
   <b> Being You @ IBM</b>
  </div> IBM is committed to creating a diverse environment and is proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, caste, genetics, pregnancy, disability, neurodivergence, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
 </div>
</div>",https://careers.ibm.com/job/19269307/data-engineer-remote/?codes=JB_Indeed&codes=1-INDEED,4e41a47b763034ef,,Full-time,,,"Washington, DC 20001",Data Engineer,6 days ago,2023-10-12T13:34:49.642Z,3.9,32635.0,"$59,000 - $112,000 a year",2023-10-18T13:34:49.644Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=4e41a47b763034ef&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
106,DGR Systems LLC,"DGR Systems is looking for a Sr. Engineer, Infrastructure Solutions, with experience in data solutions backup. You will be responsible for designing and implementing backup solutions with our clients using Cohesity DataProtect. You will collaborate with internal and external stakeholders to develop and maintain robust data protection strategies and solutions.
  The ideal candidate must be an excellent communicator with an ability to simplify complex topics into clear messages and must be consultative in focus with a growth mindset and passion for continuous learning to bring the best solutions to our clients. The candidate will bring technical experience, solutions leadership, and business acumen to DGR Systems.
  
  
 Why DGR? 
 DGR Systems helps solve the most complex business and operational challenges for their clients. Our team of top-level industry experts takes an innovative and straightforward approach to consulting, design, deployment, and ongoing Assurance Services to meet client needs. 
 At a glance, DGR Systems was founded in 2009 in Tampa, Florida and provides full-service solutions in the areas of Modern Workplace (Endpoint Solutions, Collaboration), Security (Identity and Access Management, Zero Trust, Information Protection) Modern Infrastructure and Cloud, and Applications (Collaboration Apps, SQL Reporting, Power Platform). With an impressive depth of experience across the Microsoft technology solution stack combined with our focus on integrating solutions from multiple leading vendors, we help organizations design and execute against their most challenging digital transformations. At DGR Systems, our culture is built around one simple standard: Excellence is our Baseline - and we deliver on that standard with every client, every day.
  Core Values
  DGR Systems core values are an essential and enduring tenant of our organization. They are a small set of timeless guiding principles describing who we are, how we treat people and how we run our business. 
 
  Passion - Love what you do and make it evident through your approach to your work and the attitude you display. 
  Ownership - Be accountable for outcomes. Take initiative to start and move things forward to make something better. 
  Integrity - Do the right thing. Always. Every time. Without exception. 
  Navigation - Find solutions to problems. Evolve, adapt, and embrace change around you for tomorrow will be different than today. 
  Teamwork - Be approachable and engage with the team around you constantly. We win or lose together. 
 
  
 Responsibilities 
 
  Provide service ownership through the understanding and evangelizing of how technology solutions can help solve business challenges and guide the development of services to meet those needs 
  Ensure the execution of service delivery for our clients to a standard of excellence expected by our clients 
  Engage with existing and prospective clients, partnered with our sales organization, to help design, implement, and maintain backup and recovery solutions, including data backup, replication, and restoration processes to solve client challenges 
  Develop backup and data retention policies that align with client’s business objectives and compliance requirements. 
  Implement security best practices for data protection and ensure compliance with industry regulations and data privacy laws 
  Identify and resolve backup and recovery issues promptly. Perform root cause analysis for failures and implement corrective actions 
  Assess storage capacity requirements and plan for scalability to accommodate future data growth 
  Create comprehensive documentation of backup and recovery processes, configurations, and procedures 
  Work closely with internal and external teams, including system administrators, network engineers, and database administrators, to ensure data protection is integrated seamlessly into the organization's infrastructure 
  All other duties as assigned. 
 
 Requirements
  
  5+ years of experience in backup solutions design and implementation 
  Strong knowledge of backup concepts, methodologies, and technologies 
  Understanding of disaster recovery principles and methodologies 
  Desired experience in Cohesity Backup products 
  Proficient in backup solutions for various platforms, applications, and databases, such as Windows, Linux, VMware, Hyper-V, SQL Server, Oracle, Exchange, SharePoint, etc. 
  Experience in backup solutions for cloud environments, such as AWS, Azure, GCP, etc. 
  Experience in backup solutions for hybrid and multi-cloud scenarios 
  Experience in backup solutions for large-scale and complex environments 
  Experience in backup solutions for disaster recovery and business continuity 
  Demonstrated client-focus solutions provider excelling at understanding customer needs and translating those needs to solutions 
  Excellent problem-solving, collaboration, organizational, presentation, product demonstration, writing, and verbal communication skills 
  Ability to work independently and collaboratively in a fast-paced and dynamic environment 
  Certification in Cohesity products is a plus 
  Certifications in relevant areas (e.g., Certified Information Systems Security Professional (CISSP), Certified Backup and Recovery Engineer (CBRE)) are a plus 
 
 Benefits
  
  Health Care Plans (Medical, Dental & Vision) 
  Health Savings Account 
  Retirement Plan (401k, IRA) 
  Life Insurance (Basic, Voluntary & AD&D) 
  Paid Time Off (Vacation, Sick & Public Holidays) 
  Family Leave (Maternity, Paternity) 
 
 
  Short-Term & Long-Term Disability 
  Training & Development 
  Work from Home Program 
 
 
 We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.","<div>
 <p>DGR Systems is looking for a Sr. Engineer, Infrastructure Solutions, with experience in data solutions backup. You will be responsible for designing and implementing backup solutions with our clients using Cohesity DataProtect. You will collaborate with internal and external stakeholders to develop and maintain robust data protection strategies and solutions.</p>
 <p> The ideal candidate must be an excellent communicator with an ability to simplify complex topics into clear messages and must be consultative in focus with a growth mindset and passion for continuous learning to bring the best solutions to our clients. The candidate will bring technical experience, solutions leadership, and business acumen to DGR Systems.</p>
 <br> 
 <p></p> 
 <h3 class=""jobSectionHeader""><b>Why DGR?</b></h3> 
 <p>DGR Systems helps solve the most complex business and operational challenges for their clients. Our team of top-level industry experts takes an innovative and straightforward approach to consulting, design, deployment, and ongoing Assurance Services to meet client needs.</p> 
 <p>At a glance, DGR Systems was founded in 2009 in Tampa, Florida and provides full-service solutions in the areas of Modern Workplace (Endpoint Solutions, Collaboration), Security (Identity and Access Management, Zero Trust, Information Protection) Modern Infrastructure and Cloud, and Applications (Collaboration Apps, SQL Reporting, Power Platform). With an impressive depth of experience across the Microsoft technology solution stack combined with our focus on integrating solutions from multiple leading vendors, we help organizations design and execute against their most challenging digital transformations. At DGR Systems, our culture is built around one simple standard: <b>Excellence is our Baseline</b> - and we deliver on that standard with every client, every day.</p>
 <p><b> Core Values</b></p>
 <p><br> DGR Systems core values are an essential and enduring tenant of our organization. They are a small set of timeless guiding principles describing who we are, how we treat people and how we run our business.</p> 
 <ul>
  <li><b>Passion -</b> Love what you do and make it evident through your approach to your work and the attitude you display.</li> 
  <li><b>Ownership -</b> Be accountable for outcomes. Take initiative to start and move things forward to make something better.</li> 
  <li><b>Integrity -</b> Do the right thing. Always. Every time. Without exception.</li> 
  <li><b>Navigation -</b> Find solutions to problems. Evolve, adapt, and embrace change around you for tomorrow will be different than today.</li> 
  <li><b>Teamwork -</b> Be approachable and engage with the team around you constantly. We win or lose together.</li> 
 </ul>
 <br> 
 <h3 class=""jobSectionHeader""><b>Responsibilities</b></h3> 
 <ul>
  <li>Provide service ownership through the understanding and evangelizing of how technology solutions can help solve business challenges and guide the development of services to meet those needs</li> 
  <li>Ensure the execution of service delivery for our clients to a standard of excellence expected by our clients</li> 
  <li>Engage with existing and prospective clients, partnered with our sales organization, to help design, implement, and maintain backup and recovery solutions, including data backup, replication, and restoration processes to solve client challenges </li>
  <li>Develop backup and data retention policies that align with client&#x2019;s business objectives and compliance requirements. </li>
  <li>Implement security best practices for data protection and ensure compliance with industry regulations and data privacy laws</li> 
  <li>Identify and resolve backup and recovery issues promptly. Perform root cause analysis for failures and implement corrective actions</li> 
  <li>Assess storage capacity requirements and plan for scalability to accommodate future data growth</li> 
  <li>Create comprehensive documentation of backup and recovery processes, configurations, and procedures</li> 
  <li>Work closely with internal and external teams, including system administrators, network engineers, and database administrators, to ensure data protection is integrated seamlessly into the organization&apos;s infrastructure</li> 
  <li>All other duties as assigned.</li> 
 </ul>
 <p><b>Requirements</b></p>
 <ul> 
  <li>5+ years of experience in backup solutions design and implementation</li> 
  <li>Strong knowledge of backup concepts, methodologies, and technologies </li>
  <li>Understanding of disaster recovery principles and methodologies</li> 
  <li>Desired experience in Cohesity Backup products</li> 
  <li>Proficient in backup solutions for various platforms, applications, and databases, such as Windows, Linux, VMware, Hyper-V, SQL Server, Oracle, Exchange, SharePoint, etc.</li> 
  <li>Experience in backup solutions for cloud environments, such as AWS, Azure, GCP, etc.</li> 
  <li>Experience in backup solutions for hybrid and multi-cloud scenarios</li> 
  <li>Experience in backup solutions for large-scale and complex environments</li> 
  <li>Experience in backup solutions for disaster recovery and business continuity</li> 
  <li>Demonstrated client-focus solutions provider excelling at understanding customer needs and translating those needs to solutions</li> 
  <li>Excellent problem-solving, collaboration, organizational, presentation, product demonstration, writing, and verbal communication skills</li> 
  <li>Ability to work independently and collaboratively in a fast-paced and dynamic environment</li> 
  <li>Certification in Cohesity products is a plus</li> 
  <li>Certifications in relevant areas (e.g., Certified Information Systems Security Professional (CISSP), Certified Backup and Recovery Engineer (CBRE)) are a plus</li> 
 </ul>
 <p><b>Benefits</b></p>
 <ul> 
  <li>Health Care Plans (Medical, Dental &amp; Vision) </li>
  <li>Health Savings Account</li> 
  <li>Retirement Plan (401k, IRA) </li>
  <li>Life Insurance (Basic, Voluntary &amp; AD&amp;D) </li>
  <li>Paid Time Off (Vacation, Sick &amp; Public Holidays) </li>
  <li>Family Leave (Maternity, Paternity) </li>
 </ul>
 <ul>
  <li>Short-Term &amp; Long-Term Disability </li>
  <li>Training &amp; Development </li>
  <li>Work from Home Program</li> 
 </ul>
 <p></p>
 <p><i>We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.</i></p>
</div>
<p></p>",https://apply.workable.com/dgrsystems/j/27F3E20EF9,8df3ef9ef08ef072,,Full-time,,,"Tampa, FL",Sr. Engineer- Infrastructure Solutions - Data Solutions Backup,5 days ago,2023-10-13T13:34:59.565Z,3.5,2.0,"$80,000 - $120,000 a year",2023-10-18T13:34:59.649Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=8df3ef9ef08ef072&from=jasx&tk=1hd1ft2vcjooh800&vjs=3
107,"Community Reinvestment Fund, USA","Posted on October 12, 2023 
       
      
     
    
   
  
 
 
  
   
    
     
      
       
        
         
          
           About the Position 
          
         
         
          
           The Data Engineer is responsible for supporting the implementation of projects focused on collecting, aggregating, storing, reconciling, and making data accessible from disparate sources to enable analysis and decision making. The Data Engineer will play a critical role in the data supply chain by ensuring stakeholders can access and manipulate data for routine and ad hoc analysis. The Data engineer will additionally support the full lifecycle of data from sources through analytics to action. 
            The Data Engineer must be an experienced user of Power BI. The Data Engineer will develop scalable, reliable, and high-impact solutions leveraging the Azure cloud platform to create a modern enterprise data platform. The Data Engineer will work closely with business stakeholders and data analysts to understand the data needs and design, implement, and maintain Power BI reports, dashboards, and visualizations. 
          
         
        
       
      
     
     
      
       
        
         
          
           Key Responsibilities & Accountabilities 
          
         
         
          
           
            Translate business requirements to technical solutions leveraging strong business acumen 
            Analyze current business practices, processes and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics PaaS Services 
            Support the planning and implementation of data design services, providing sizing and configuration assistance and performing needs assessments. 
            Deliver of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies. 
            Design and Build Azure Data Pipelines using Databricks. 
            Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics. 
            Expose data to end users using Power BI or any other modern visualization platform or experience 
            Implement effective metrics and monitoring processes 
           
          
         
        
       
      
     
     
      
       
        
         
          
           About You 
          
         
         
          
           
            Bachelor’s degree in Computer Science (or related field), or equivalent work experience 
            Minimum of 4 years data engineering experience 
            Demonstrated experience of turning business use cases and requirements to technical solutions 
            Experience in business processing mapping of data and analytics solutions. 
            Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows. 
            Strong team collaboration and experience working with remote teams 
            The ability to apply such methods to solve business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration 
            Experienced in Data Transformation using ETL/ELT tools such as AWS Glue, Azure Data Factory, Talend, EAI 
            Knowledge in business intelligence tools such as Power BI, Tableau, Qlik, Cognos TM1 
            Knowledge of Azure Data Factory, Azure Data Lake, Relational Databases SQL DW, and SQL, Azure App Service is required. Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics is a plus 
            Experienced in Cloud Data-related tool such as Microsoft Azure, Amazon S3 
            Ability to leverage on variety of programming languages & data management/processing tools to ensure data reliability, quality & efficiency 
            Knowledge of Python is a plus 
            Designing and building Data Pipelines using streams of IOT data 
            Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals 
           
          
         
         
         
          
           Organizational Policies 
          
         
         
          
           Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law. 
          
         
         
          
           Candidates must reside in and be authorized to work in the United States without sponsorship. 
          
         
         
          
           CRF’s Theory of Change CRF’s Theory of Change is the strategic framework used to guide its work. While CRF’s mission remains constant, the Theory of Change outlines how it contributes to the changes it seeks. 
          
         
         
          
           The challenge CRF works to address The current economic system, perpetuated by institutional racism, individual biases and disparities, is unjust. It fosters the inequities that are causing widening disparities in incomes, wealth, and opportunity gaps. 
          
         
         
          
           A just economy that works for all CRF’s Theory of Change is underpinned by the conviction that small businesses are the backbone of our economy, employing nearly half of the U.S. workforce and generating two-thirds of new jobs. CRF believes that providing small businesses equitable access to capital and support services is essential to creating a just economy that works for all. 
          
         
         
          
           CRF activates its mission by:
           
             Co-creating and deploying innovative financial products and services that address the barriers and inequities small businesses operated by historically excluded people face.
             Designing and managing financial programs that attract incremental impact capital to communities with a history of underinvestment.
             Orchestrating a network of trusted small business support organizations enabled by technology.
             Growing the capacity of community development finance organizations.
             Helping small businesses navigate the complexities of the small business support ecosystem.
            
          
         
         
          
           CRF STANDS IN SOLIDARITY Community Reinvestment Fund, USA (CRF) stands in solidarity with all fighting for social justice, equity, and transformational change. They know that the social changes taking place today will yield a lasting positive impact on the lives of millions. 
          
         
         
          
           DIVERSITY, EQUITY & INCLUSION: CRF is dedicated to building and sustaining a truly diverse, equitable, and inclusive culture. These are not just words on a page – Diversity, Equity & Inclusion are top priorities for the organization, and tie deeply to each of their core values and overall vision for the future. CRF is an equal opportunity employer that evaluate applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity, or other characteristics protected by law. 
          
         
         
          
           CORE VALUES Create Equitable Economic Opportunities, Lead Through Collaboration, Transform Through Innovation, Excel In All They Do, Act with Integrity. 
          
         
         
          
           How We Help Together with its partners – including community leaders, nonprofit lenders, financial institutions, foundations and more – CRF is creating new strategies and technologies that build stronger local economies, create jobs and support economic mobility. 
          
         
         
          
           CRF is headquartered in Minneapolis, Minnesota. For a more detailed description of the incredible work we do, how we do it, and who we are, please visit www.crfusa.com. 
          
         
        
       
      
     
    
   
   
    
     
      
       
        
         
          
           
            
             
              
               
                Department: 
               
              
             
             
              
               
                Data and Analytics
                
              
             
            
           
          
         
         
          
           
            
             
              
               
                Location:
                
              
             
             
              
               
                Minneapolis, MN – remote work is available
                
              
             
            
           
          
         
         
          
           
            
             
              
               
                Salary:
                
              
             
             
              
               
                $85,000 to $105,000 Annually (exempt)
                
              
             
            
           
          
         
        
       
      
     
     
     
      
       
        
         
          
           What We Offer 
          
         
         
          
           A collaborative working environment comprised of driven and highly engaged individuals committed to diversity, equity, and inclusion and the mission, vision and values of CRF. CRF is proud to extend its employees a wide array of benefits including, but not limited to: 
          
         
         
          
           
            Health and dental insurance 
            403B and Roth IRA 
            Paid Time Off (PTO) 
            Wellness Program 
            Educational Assistance 
            Long-Term and Short-Term Disability 
            Life Insurance 
            Flexible schedules and telecommuting options 
            10 Federal Holidays plus 2 Floating Holidays 
           
          
         
        
       
      
     
     
      
       
        
         
          
           How to Apply 
          
         
         
          
           To apply for this or other positions, please send your resume to: recruiting@crfusa.com 
          
         
        
       
      
     
    
   
  
 
 
  
   
    
     
      
       Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law. 
      
     
     
      
       CRF USA, Inc. requires all employees to be vaccinated for COVID-19. As a condition of hire with CRF USA, Inc. candidates must be able to show proof of vaccination for COVID-19 prior to an extension of an offer of employment. Accommodations will be considered for disabilities or sincerely held religious beliefs. 
      
     
     
      
       Candidates must reside in and be authorized to work in the United States without sponsorship.","<div></div>
<div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <ul>
        <li>Posted on October 12, 2023 </li>
       </ul>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h3 class=""jobSectionHeader""><b>About the Position</b></h3> 
          </div>
         </div>
         <div>
          <div>
           <p>The Data Engineer is responsible for supporting the implementation of projects focused on collecting, aggregating, storing, reconciling, and making data accessible from disparate sources to enable analysis and decision making. The Data Engineer will play a critical role in the data supply chain by ensuring stakeholders can access and manipulate data for routine and ad hoc analysis. The Data engineer will additionally support the full lifecycle of data from sources through analytics to action.</p> 
           <p> The Data Engineer must be an experienced user of Power BI. The Data Engineer will develop scalable, reliable, and high-impact solutions leveraging the Azure cloud platform to create a modern enterprise data platform. The Data Engineer will work closely with business stakeholders and data analysts to understand the data needs and design, implement, and maintain Power BI reports, dashboards, and visualizations. </p>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h2 class=""jobSectionHeader""><b>Key Responsibilities &amp; Accountabilities</b></h2> 
          </div>
         </div>
         <div>
          <div>
           <ul>
            <li>Translate business requirements to technical solutions leveraging strong business acumen </li>
            <li>Analyze current business practices, processes and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data &amp; Analytics PaaS Services</li> 
            <li>Support the planning and implementation of data design services, providing sizing and configuration assistance and performing needs assessments.</li> 
            <li>Deliver of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.</li> 
            <li>Design and Build Azure Data Pipelines using Databricks.</li> 
            <li>Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.</li> 
            <li>Expose data to end users using Power BI or any other modern visualization platform or experience</li> 
            <li>Implement effective metrics and monitoring processes</li> 
           </ul>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h2 class=""jobSectionHeader""><b>About You</b></h2> 
          </div>
         </div>
         <div>
          <div>
           <ul>
            <li>Bachelor&#x2019;s degree in Computer Science (or related field), or equivalent work experience</li> 
            <li>Minimum of 4 years data engineering experience </li>
            <li>Demonstrated experience of turning business use cases and requirements to technical solutions </li>
            <li>Experience in business processing mapping of data and analytics solutions. </li>
            <li>Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.</li> 
            <li>Strong team collaboration and experience working with remote teams</li> 
            <li>The ability to apply such methods to solve business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration</li> 
            <li>Experienced in Data Transformation using ETL/ELT tools such as AWS Glue, Azure Data Factory, Talend, EAI</li> 
            <li>Knowledge in business intelligence tools such as Power BI, Tableau, Qlik, Cognos TM1</li> 
            <li>Knowledge of Azure Data Factory, Azure Data Lake, Relational Databases SQL DW, and SQL, Azure App Service is required. Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics is a plus</li> 
            <li>Experienced in Cloud Data-related tool such as Microsoft Azure, Amazon S3</li> 
            <li>Ability to leverage on variety of programming languages &amp; data management/processing tools to ensure data reliability, quality &amp; efficiency</li> 
            <li>Knowledge of Python is a plus</li> 
            <li>Designing and building Data Pipelines using streams of IOT data</li> 
            <li>Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals</li> 
           </ul>
          </div>
         </div>
         <div></div>
         <div>
          <div>
           <h3 class=""jobSectionHeader""><b>Organizational Policies</b></h3> 
          </div>
         </div>
         <div>
          <div>
           <p>Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law.</p> 
          </div>
         </div>
         <div>
          <div>
           <p>Candidates must reside in and be authorized to work in the United States without sponsorship.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>CRF&#x2019;s Theory of Change</b><br> CRF&#x2019;s Theory of Change is the strategic framework used to guide its work. While CRF&#x2019;s mission remains constant, the Theory of Change outlines how it contributes to the changes it seeks.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>The challenge CRF works to address</b><br> The current economic system, perpetuated by institutional racism, individual biases and disparities, is unjust. It fosters the inequities that are causing widening disparities in incomes, wealth, and opportunity gaps.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>A just economy that works for all</b><br> CRF&#x2019;s Theory of Change is underpinned by the conviction that small businesses are the backbone of our economy, employing nearly half of the U.S. workforce and generating two-thirds of new jobs. CRF believes that providing small businesses equitable access to capital and support services is essential to creating a just economy that works for all.</p> 
          </div>
         </div>
         <div>
          <div>
           <p>CRF activates its mission by:</p>
           <ul>
            <li> Co-creating and deploying innovative financial products and services that address the barriers and inequities small businesses operated by historically excluded people face.</li>
            <li> Designing and managing financial programs that attract incremental impact capital to communities with a history of underinvestment.</li>
            <li> Orchestrating a network of trusted small business support organizations enabled by technology.</li>
            <li> Growing the capacity of community development finance organizations.</li>
            <li> Helping small businesses navigate the complexities of the small business support ecosystem.</li>
           </ul> 
          </div>
         </div>
         <div>
          <div>
           <p><b>CRF STANDS IN SOLIDARITY</b><br> Community Reinvestment Fund, USA (CRF) stands in solidarity with all fighting for social justice, equity, and transformational change. They know that the social changes taking place today will yield a lasting positive impact on the lives of millions.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>DIVERSITY, EQUITY &amp; INCLUSION:</b><br> CRF is dedicated to building and sustaining a truly diverse, equitable, and inclusive culture. These are not just words on a page &#x2013; Diversity, Equity &amp; Inclusion are top priorities for the organization, and tie deeply to each of their core values and overall vision for the future. CRF is an equal opportunity employer that evaluate applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity, or other characteristics protected by law.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>CORE VALUES</b><br> Create Equitable Economic Opportunities, Lead Through Collaboration, Transform Through Innovation, Excel In All They Do, Act with Integrity.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>How We Help</b><br> Together with its partners &#x2013; including community leaders, nonprofit lenders, financial institutions, foundations and more &#x2013; CRF is creating new strategies and technologies that build stronger local economies, create jobs and support economic mobility.</p> 
          </div>
         </div>
         <div>
          <div>
           <p>CRF is headquartered in Minneapolis, Minnesota. For a more detailed description of the incredible work we do, how we do it, and who we are, please visit www.crfusa.com.</p> 
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                Department: 
               </div>
              </div>
             </div>
             <div>
              <div>
               <div>
                Data and Analytics
               </div> 
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                Location:
               </div> 
              </div>
             </div>
             <div>
              <div>
               <div>
                Minneapolis, MN &#x2013; remote work is available
               </div> 
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                Salary:
               </div> 
              </div>
             </div>
             <div>
              <div>
               <div>
                &#x24;85,000 to &#x24;105,000 Annually (exempt)
               </div> 
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div></div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h3 class=""jobSectionHeader""><b>What We Offer</b></h3> 
          </div>
         </div>
         <div>
          <div>
           A collaborative working environment comprised of driven and highly engaged individuals committed to diversity, equity, and inclusion and the mission, vision and values of CRF. CRF is proud to extend its employees a wide array of benefits including, but not limited to: 
          </div>
         </div>
         <div>
          <div>
           <ul>
            <li>Health and dental insurance</li> 
            <li>403B and Roth IRA</li> 
            <li>Paid Time Off (PTO)</li> 
            <li>Wellness Program</li> 
            <li>Educational Assistance</li> 
            <li>Long-Term and Short-Term Disability</li> 
            <li>Life Insurance</li> 
            <li>Flexible schedules and telecommuting options</li> 
            <li>10 Federal Holidays plus 2 Floating Holidays</li> 
           </ul>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h2 class=""jobSectionHeader""><b>How to Apply</b></h2> 
          </div>
         </div>
         <div>
          <div>
           To apply for this or other positions, please send your resume to: recruiting@crfusa.com 
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law. 
      </div>
     </div>
     <div>
      <div>
       CRF USA, Inc. requires all employees to be vaccinated for COVID-19. As a condition of hire with CRF USA, Inc. candidates must be able to show proof of vaccination for COVID-19 prior to an extension of an offer of employment. Accommodations will be considered for disabilities or sincerely held religious beliefs. 
      </div>
     </div>
     <div>
      <div>
       Candidates must reside in and be authorized to work in the United States without sponsorship.
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<div></div>",https://crfusa.com/data-engineer/,16c48fc80404f2f3,,,,,"801 Nicollet Mall Ste 1700, Minneapolis, MN 55402",Data Engineer,5 days ago,2023-10-13T13:35:05.331Z,4.0,6.0,"$85,000 - $105,000 a year",2023-10-18T13:35:05.334Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=16c48fc80404f2f3&from=jasx&tk=1hd1ft1i8joou800&vjs=3
108,Gap Inc.,"About Gap Inc. 
 Our brands bridge the gaps we see in the world. Old Navy democratizes style to ensure everyone has access to quality fashion at every price point. Athleta unleashes the potential of every woman, regardless of body size, age or ethnicity. Banana Republic believes in sustainable luxury for all. And Gap inspires the world to bring individuality to modern, responsibly made essentials. 
 This simple idea—that we all deserve to belong, and on our own terms—is core to who we are as a company and how we make decisions. Our team is made up of thousands of people across the globe who take risks, think big, and do good for our customers, communities, and the planet. Ready to learn fast, create with audacity and lead boldly? Join our team.
  About the Role
  In this role, you will design highly scalable and high performing technology solutions in an Agile work environment and produce and deliver code and/or test cases using your knowledge of software development and Agile practice. You will collaborate closely with business support teams, product managers, security and architecture to assist in resolving critical production issues to help simplify and improve business processes through the latest in technology and automation. You are a technical expert that will lead through the requirements gathering, design, development, deployment, and support phases of a product. You are proficient in at least one core programming languages or packages.  As a Senior Engineer - ML/Data you will be contributing to the build of applications and services off of our Gap Data Platform. The tools will be KPI based, focusing on trend analysis of the different brands we are servicing: GAP, Banana Republic, Old Navy and Athleta. Strong understanding of CICD pipelines, data operations and Dev Ops are beneficial to this role.   This is a remote role is based out Dallas, TX. However, the Company may require you in the future to work in an on-site location designated by Gap Inc. on a full-time or part-time basis.
  What You'll Do
 
   Define technical specifications and development requirements that result in high performing technologies that are also domain specific.
   Develop and enhance product and/or applications with limited direction to solve business problems of medium complexity by keeping customer experience at the forefront.
   Adopt and model a DevOps mindset by applying automation, continuous integration and continuous delivery in everything we do.
   Foster innovation by applying best practices and learning from emerging technologies and through collaboration with cross functional stakeholders.
   Serve as application expert in support of domain areas.
   Communicate difficult concepts, providing technical and professional interpretations and recommendations.
   Advise and mentor junior team members and enable collaboration to help teams achieve their best.
 
  Who You Are
 
   Strong working experience utilizing Python and Databricks
   Software Development experience and understanding of security, secure coding/testing and data structures and aware of industry and competitor practices.
   Comprehensive knowledge of software development, practice, concepts and technology.
   Proficiency with various software languages and platforms such as Java, Oracle, Azure etc.
   Experience with related technology stack and platforms.
   Experience with building and sustaining effective relationships with immediate team and stakeholders.
 
  Benefits at Gap Inc. 
 
  Merchandise discount for our brands: 50% off regular-priced merchandise at Old Navy, Gap, Banana Republic and Athleta, and 30% off at Outlet for all employees. 
  One of the most competitive Paid Time Off plans in the industry.* 
  Employees can take up to five “on the clock” hours each month to volunteer at a charity of their choice.* 
  Extensive 401(k) plan with company matching for contributions up to four percent of an employee’s base pay.* 
  Employee stock purchase plan.* 
  Medical, dental, vision and life insurance.* 
  See more of the benefits we offer. 
 
 
  For eligible employees
 
  Gap Inc. is an equal-opportunity employer and is committed to providing a workplace free from harassment and discrimination. We are committed to recruiting, hiring, training and promoting qualified people of all backgrounds, and make all employment decisions without regard to any protected status. We have received numerous awards for our long-held commitment to equality and will continue to foster a diverse and inclusive environment of belonging. In 2022, we were recognized by Forbes as one of the World's Best Employers and one of the Best Employers for Diversity.  Salary Range: $95,300 - $138,200 USD Employee pay will vary based on factors such as qualifications, experience, skill level, competencies and work location. We will meet minimum wage or minimum of the pay range (whichever is higher) based on city, county and state requirements.  US Candidates Please note that effective, June 30, 2022, Gap Inc. will no longer require any of its employees to wear face masks or require proof of COVID vaccination, unless required by local or state/provincial mandates or as part of Gap Inc’s quarantine guidelines after being exposed to or testing positive for COVID. Therefore, please disregard any language in any job posting that refers to Gap Inc.’s face mask and proof of vaccination policy as said policy is no longer effective.","<div>
 <h2 class=""jobSectionHeader""><b>About Gap Inc.</b></h2> 
 <p>Our brands bridge the gaps we see in the world. Old Navy democratizes style to ensure everyone has access to quality fashion at every price point. Athleta unleashes the potential of every woman, regardless of body size, age or ethnicity. Banana Republic believes in sustainable luxury for all. And Gap inspires the world to bring individuality to modern, responsibly made essentials. </p>
 <p>This simple idea&#x2014;that we all deserve to belong, and on our own terms&#x2014;is core to who we are as a company and how we make decisions. Our team<b> </b>is made up of thousands of people across the globe who take risks, think big, and do good for our customers, communities, and the planet. Ready to learn fast, create with audacity and lead boldly? Join our team.</p>
 <h2 class=""jobSectionHeader""><b> About the Role</b></h2>
 <p> In this role, you will design highly scalable and high performing technology solutions in an Agile work environment and produce and deliver code and/or test cases using your knowledge of software development and Agile practice. You will collaborate closely with business support teams, product managers, security and architecture to assist in resolving critical production issues to help simplify and improve business processes through the latest in technology and automation. You are a technical expert that will lead through the requirements gathering, design, development, deployment, and support phases of a product. You are proficient in at least one core programming languages or packages.<br> <br> As a Senior Engineer - ML/Data you will be contributing to the build of applications and services off of our Gap Data Platform. The tools will be KPI based, focusing on trend analysis of the different brands we are servicing: GAP, Banana Republic, Old Navy and Athleta. Strong understanding of CICD pipelines, data operations and Dev Ops are beneficial to this role. <br> <br> This is a remote role is based out Dallas, TX. However, the Company may require you in the future to work in an on-site location designated by Gap Inc. on a full-time or part-time basis.</p>
 <h2 class=""jobSectionHeader""><b> What You&apos;ll Do</b></h2>
 <ul>
  <li> Define technical specifications and development requirements that result in high performing technologies that are also domain specific.</li>
  <li> Develop and enhance product and/or applications with limited direction to solve business problems of medium complexity by keeping customer experience at the forefront.</li>
  <li> Adopt and model a DevOps mindset by applying automation, continuous integration and continuous delivery in everything we do.</li>
  <li> Foster innovation by applying best practices and learning from emerging technologies and through collaboration with cross functional stakeholders.</li>
  <li> Serve as application expert in support of domain areas.</li>
  <li> Communicate difficult concepts, providing technical and professional interpretations and recommendations.</li>
  <li> Advise and mentor junior team members and enable collaboration to help teams achieve their best.</li>
 </ul>
 <h2 class=""jobSectionHeader""><b> Who You Are</b></h2>
 <ul>
  <li> Strong working experience utilizing Python and Databricks</li>
  <li> Software Development experience and understanding of security, secure coding/testing and data structures and aware of industry and competitor practices.</li>
  <li> Comprehensive knowledge of software development, practice, concepts and technology.</li>
  <li> Proficiency with various software languages and platforms such as Java, Oracle, Azure etc.</li>
  <li> Experience with related technology stack and platforms.</li>
  <li> Experience with building and sustaining effective relationships with immediate team and stakeholders.</li>
 </ul>
 <h2 class=""jobSectionHeader""><b> Benefits at Gap Inc.</b></h2> 
 <ul>
  <li>Merchandise discount for our brands: 50% off regular-priced merchandise at Old Navy, Gap, Banana Republic and Athleta, and 30% off at Outlet for all employees.</li> 
  <li>One of the most competitive Paid Time Off plans in the industry.*</li> 
  <li>Employees can take up to five &#x201c;on the clock&#x201d; hours each month to volunteer at a charity of their choice.*</li> 
  <li>Extensive 401(k) plan with company matching for contributions up to four percent of an employee&#x2019;s base pay.*</li> 
  <li>Employee stock purchase plan.*</li> 
  <li>Medical, dental, vision and life insurance.*</li> 
  <li>See more of the benefits we offer.</li> 
 </ul>
 <ul>
  <li><i>For eligible employees</i></li>
 </ul>
 <p> Gap Inc. is an equal-opportunity employer and is committed to providing a workplace free from harassment and discrimination. We are committed to recruiting, hiring, training and promoting qualified people of all backgrounds, and make all employment decisions without regard to any protected status. We have received numerous awards for our long-held commitment to equality and will continue to foster a diverse and inclusive environment of belonging. In 2022, we were recognized by Forbes as one of the World&apos;s Best Employers and one of the Best Employers for Diversity.<br> <br> Salary Range: &#x24;95,300 - &#x24;138,200 USD<br> Employee pay will vary based on factors such as qualifications, experience, skill level, competencies and work location. We will meet minimum wage or minimum of the pay range (whichever is higher) based on city, county and state requirements.<br> <br> <b>US Candidates</b><br> Please note that effective, June 30, 2022, Gap Inc. will no longer require any of its employees to wear face masks or require proof of COVID vaccination, unless required by local or state/provincial mandates or as part of Gap Inc&#x2019;s quarantine guidelines after being exposed to or testing positive for COVID. Therefore, please disregard any language in any job posting that refers to Gap Inc.&#x2019;s face mask and proof of vaccination policy as said policy is no longer effective.</p>
</div>","https://www.gapinc.com/en-us/jobs/w73/16/senior-engineer-ml-data-remote-dallas,-tx?rx_campaign=indeed0&rx_ch=jobp4p&rx_group=116953&rx_job=R137316&rx_r=none&rx_source=Indeed&rx_ts=20231018T080143Z&rx_vp=cpc&src=JB-12080&src=JB-10324&rx_p=CTNK5L83XE&rx_viewer=28bbc4b76dbb11ee91c7ddc6db95c7d1c5c6cf3fd1ba4e4a95fa1cde1a72997f",18da6255e1db9629,,Full-time,,,"San Francisco, CA 94105",Senior Engineer - ML/Data (Remote,5 days ago,2023-10-13T13:35:06.949Z,3.7,3496.0,"$95,300 - $138,200 a year",2023-10-18T13:35:06.952Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=18da6255e1db9629&from=jasx&tk=1hd1ft1i8joou800&vjs=3
110,IBR (Imagine Believe Realize),"The Senior Data Engineer must be able to meet the key criteria below:

 Location: 100% telework
 Years' Experience: 10+ years
 Education: Bachelor’s in IT related field
 Security Clearance: IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required.
 Work Authorization: Must show that applicant is legally permitted to work in the United States.
 Employment Type: Full-Time, W-2
 Key Skills:
 10+ years of IT experience focusing on enterprise data architecture and management
 Experience with Databricks required
 8+ years experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
 Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services
 Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
 Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus

Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities

 Plan, create, and maintain data architectures, ensuring alignment with business requirements
 Obtain data, formulate dataset processes, and store optimized data
 Identify problems and inefficiencies and apply solutions
 Determine tasks where manual participation can be eliminated with automation.
 Identify and optimize data bottlenecks, leveraging automation where possible
 Create and manage data lifecycle policies (retention, backups/restore, etc)
 Create, maintain, and manage ETL/ELT pipelines
 Create, maintain, and manage data transformations
 Maintain/update documentation
 Create, maintain, and manage data pipeline schedules
 Monitor data pipelines
 Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality
 Support AI/ML teams with optimizing feature engineering code
 Spark updates
 Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT
 Research existing data in the data lake to determine best sources for data
 Create, manage, and maintain ksqlDB and Kafka Streams queries/code
 Maintain and update Python-based data processing scripts executed on AWS Lambdas
 Unit tests for all the Spark, Python data processing and Lambda codes
 Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)

Qualifications

 10+ years of IT experience focusing on enterprise data architecture and management
 Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
 Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required
 Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark
 Data Lake concepts such as time travel and schema evolution and optimization
 Structured Streaming and Delta Live Tables with Databricks a bonus
 Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
 Advanced level understanding of streaming data pipelines and how they differ from batch systems
 Formalize concepts of how to handle late data, defining windows, and data freshness
 Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc
 Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.
 Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus
 Understanding of streaming data pipelines and batch systems
 Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness
 Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
 Indexing and partitioning strategy experience
 Debug, troubleshoot, design and implement solutions to complex technical issues
 Experience with large-scale, high-performance enterprise big data application deployment and solution
 Understanding how to create DAGs to define workflows
 Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required
 Architecture experience in AWS environment a bonus
 Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale (UNICORN) a bonus
 Experience with Docker, Jenkins, and CloudWatch
 Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines
 Experience working with AWS Lambdas for configuration and optimization
 Experience working with DynamoDB to query and write data
 Experience with S3
 Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus
 Familiarity with Pytest and Unittest a bonus
 Experience working with JSON and defining JSON Schemas a bonus
 Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus
 Familiarity with Schema Registry, message formats such as Avro, ORC, etc.
 Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams
 Ability to thrive in a team-based environment
 Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management

Physical Demands
Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.
Work Environment
Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.
About IBRImagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:

 Nationwide medical, dental, and vision insurance
 3 weeks of Paid Time Off and 11 Paid Federal Holidays
 401k matching
 Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees
 Flexible spending accounts and Dependent Care spending accounts
 Wellness incentives
 Reimbursement for professional development and certifications
 Training assistance opportunities

Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.
Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.”Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional contact information has been provided below:
info@teamibr.com​​​​​​​407.459.1830
Job Type: Full-time
Pay: $131,243.38 - $158,056.53 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Referral program
 Vision insurance

Experience level:

 10 years

Schedule:

 Monday to Friday

Work Location: Remote","<p><b>The Senior Data Engineer must be able to meet the key criteria below:</b></p>
<ul>
 <li><b>Location: </b>100% telework</li>
 <li><b>Years&apos; Experience: </b>10+ years</li>
 <li><b>Education: </b>Bachelor&#x2019;s in IT related field</li>
 <li><b>Security Clearance:</b> IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required.</li>
 <li><b>Work Authorization:</b> Must show that applicant is legally permitted to work in the United States.</li>
 <li><b>Employment Type:</b> Full-Time, W-2</li>
 <li><b>Key Skills:</b></li>
 <li>10+ years of IT experience focusing on enterprise data architecture and management</li>
 <li>Experience with Databricks required</li>
 <li>8+ years experience in Conceptual/Logical/Physical Data Modeling &amp; expertise in Relational and Dimensional Data Modeling</li>
 <li>Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services</li>
 <li>Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)</li>
 <li>Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus</li>
</ul>
<p><b>Overview</b></p>
<p>Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations &amp; Maintenance activities.</p>
<p><b>Responsibilities</b></p>
<ul>
 <li>Plan, create, and maintain data architectures, ensuring alignment with business requirements</li>
 <li>Obtain data, formulate dataset processes, and store optimized data</li>
 <li>Identify problems and inefficiencies and apply solutions</li>
 <li>Determine tasks where manual participation can be eliminated with automation.</li>
 <li>Identify and optimize data bottlenecks, leveraging automation where possible</li>
 <li>Create and manage data lifecycle policies (retention, backups/restore, etc)</li>
 <li>Create, maintain, and manage ETL/ELT pipelines</li>
 <li>Create, maintain, and manage data transformations</li>
 <li>Maintain/update documentation</li>
 <li>Create, maintain, and manage data pipeline schedules</li>
 <li>Monitor data pipelines</li>
 <li>Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality</li>
 <li>Support AI/ML teams with optimizing feature engineering code</li>
 <li>Spark updates</li>
 <li>Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT</li>
 <li>Research existing data in the data lake to determine best sources for data</li>
 <li>Create, manage, and maintain ksqlDB and Kafka Streams queries/code</li>
 <li>Maintain and update Python-based data processing scripts executed on AWS Lambdas</li>
 <li>Unit tests for all the Spark, Python data processing and Lambda codes</li>
 <li>Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)</li>
</ul>
<p><b>Qualifications</b></p>
<ul>
 <li>10+ years of IT experience focusing on enterprise data architecture and management</li>
 <li>Experience in Conceptual/Logical/Physical Data Modeling &amp; expertise in Relational and Dimensional Data Modeling</li>
 <li>Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required</li>
 <li>Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark</li>
 <li>Data Lake concepts such as time travel and schema evolution and optimization</li>
 <li>Structured Streaming and Delta Live Tables with Databricks a bonus</li>
 <li>Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support</li>
 <li>Advanced level understanding of streaming data pipelines and how they differ from batch systems</li>
 <li>Formalize concepts of how to handle late data, defining windows, and data freshness</li>
 <li>Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc</li>
 <li>Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.</li>
 <li>Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus</li>
 <li>Understanding of streaming data pipelines and batch systems</li>
 <li>Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness</li>
 <li>Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)</li>
 <li>Indexing and partitioning strategy experience</li>
 <li>Debug, troubleshoot, design and implement solutions to complex technical issues</li>
 <li>Experience with large-scale, high-performance enterprise big data application deployment and solution</li>
 <li>Understanding how to create DAGs to define workflows</li>
 <li>Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required</li>
 <li>Architecture experience in AWS environment a bonus</li>
 <li>Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale (UNICORN) a bonus</li>
 <li>Experience with Docker, Jenkins, and CloudWatch</li>
 <li>Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines</li>
 <li>Experience working with AWS Lambdas for configuration and optimization</li>
 <li>Experience working with DynamoDB to query and write data</li>
 <li>Experience with S3</li>
 <li>Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus</li>
 <li>Familiarity with Pytest and Unittest a bonus</li>
 <li>Experience working with JSON and defining JSON Schemas a bonus</li>
 <li>Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus</li>
 <li>Familiarity with Schema Registry, message formats such as Avro, ORC, etc.</li>
 <li>Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams</li>
 <li>Ability to thrive in a team-based environment</li>
 <li>Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management</li>
</ul>
<p><b>Physical Demands</b></p>
<p>Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.</p>
<p><b>Work Environment</b></p>
<p>Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.</p>
<p><b>About IBR</b><br>Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:</p>
<ul>
 <li>Nationwide medical, dental, and vision insurance</li>
 <li>3 weeks of Paid Time Off and 11 Paid Federal Holidays</li>
 <li>401k matching</li>
 <li>Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees</li>
 <li>Flexible spending accounts and Dependent Care spending accounts</li>
 <li>Wellness incentives</li>
 <li>Reimbursement for professional development and certifications</li>
 <li>Training assistance opportunities</li>
</ul>
<p>Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.</p>
<p>Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.&#x201d;<br>Learn more at <i>http://www.teamibr.com</i></p>
<p>If alternative methods of assistance are needed with the application process, additional contact information has been provided below:</p>
<p><i>info@teamibr.com</i><br>&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;407.459.1830</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;131,243.38 - &#x24;158,056.53 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Referral program</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,c1dc76acaaac3b9b,,Full-time,,,Remote,Senior Data Engineer,5 days ago,2023-10-13T13:35:17.517Z,,,"$131,243 - $158,057 a year",2023-10-18T13:35:17.521Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=c1dc76acaaac3b9b&from=jasx&tk=1hd1ft1i8joou800&vjs=3
111,PrismHR,"Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.
  We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:
 
   Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
   Defining streaming event data feeds required for real-time analytics and reporting
   Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance
 
  As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product. 
 Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!
  Responsibilities:
 
   Build our next generation data warehouse
   Build our event stream platform
   Translate user requirements for reporting and analysis into actionable deliverables
   Enhance automation, operation, and expansion of real-time and batch data environment
   Manage numerous projects in an ever-changing work environment
   Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
   Build processes for topnotch security, performance, reliability, and accuracy
   Provide mentorship and collaborate with fellow team members
 
 
  Requirements:
 
   Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
   3+ years of experience building data pipelines
   3+ years of experience building data frameworks for unit testing, data lineage tracking, and automation 
  Fluency in Scala is required
   Working knowledge of Apache Spark
   Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)
 
 
  Nice to Have:
 
   Experience with Machine Learning 
  Familiarity with Looker a plus
   Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)
 
 
 
   Please note: This position can be remote/telecommute. Notice for candidates located in the following states: CA, CO, NJ, NY, WA: The base salary range for this position is between $100,000 - $145,000 (salary is dependent on location, experience, knowledge, and skills based on the responsibilities outlined in the job description).
 
  #LI-REMOTE
  PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners. 
 Diversity, Equity and Inclusion Program/Affirmative Action Plan: We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.  Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.  As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.  The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers. 
 Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy. 
 PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response. 
 #LI-ML1
  
 vlsNrOjLD3","<div>
 <p>Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We&apos;re hiring Data Engineers to help build and optimize the foundational architecture of our product&apos;s data.</p>
 <p> We&#x2019;ve built a strong data engineering team to date, but have a lot of work ahead of us, including:</p>
 <ul>
  <li> Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds</li>
  <li> Defining streaming event data feeds required for real-time analytics and reporting</li>
  <li> Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance</li>
 </ul>
 <p> As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product. </p>
 <p>Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You&apos;ll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You&#x2019;ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!</p>
 <p><b> Responsibilities:</b></p>
 <ul>
  <li> Build our next generation data warehouse</li>
  <li> Build our event stream platform</li>
  <li> Translate user requirements for reporting and analysis into actionable deliverables</li>
  <li> Enhance automation, operation, and expansion of real-time and batch data environment</li>
  <li> Manage numerous projects in an ever-changing work environment</li>
  <li> Extract, transform, and load complex data into the data warehouse using cutting-edge technologies</li>
  <li> Build processes for topnotch security, performance, reliability, and accuracy</li>
  <li> Provide mentorship and collaborate with fellow team members</li>
 </ul>
 <p></p>
 <p><b><br> Requirements:</b></p>
 <ul>
  <li> Bachelor&#x2019;s or Master&#x2019;s degree in Computer Science, Information Systems, Operations Research, or related field required</li>
  <li> 3+ years of experience building data pipelines</li>
  <li> 3+ years of experience building data frameworks for unit testing, data lineage tracking, and automation </li>
  <li>Fluency in Scala is required</li>
  <li> Working knowledge of Apache Spark</li>
  <li> Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)</li>
 </ul>
 <p></p>
 <p><b><br> Nice to Have:</b></p>
 <ul>
  <li> Experience with Machine Learning </li>
  <li>Familiarity with Looker a plus</li>
  <li> Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)</li>
 </ul>
 <p></p>
 <ul>
  <li><br> Please note: This position can be remote/telecommute. Notice for candidates located in the following states: CA, CO, NJ, NY, WA: The base salary range for this position is between &#x24;100,000 - &#x24;145,000 (salary is dependent on location, experience, knowledge, and skills based on the responsibilities outlined in the job description).</li>
 </ul>
 <p> #LI-REMOTE</p>
 <p> PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company&#x2019;s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR&#x2019;s Marketplace Partners.</p> 
 <p><b>Diversity, Equity and Inclusion Program/Affirmative Action Plan:</b><br> We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.<br> <br> Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren&apos;t just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you&#x2019;re excited about our roles but your past experience doesn&#x2019;t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.<br> <br> As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual&#x2019;s qualifications as they relate to the job under consideration. The Company&#x2019;s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.<br> <br> The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers. </p>
 <p>Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy.</p> 
 <p><i>PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.</i></p> 
 <p>#LI-ML1</p>
 <p> </p>
 <p>vlsNrOjLD3</p>
</div>",https://prismhr.applytojob.com/apply/vlsNrOjLD3/Senior-Data-Engineer?source=INDE,3cf3060bcd436793,,Full-time,,,Remote,Senior Data Engineer,5 days ago,2023-10-13T13:35:12.830Z,3.2,38.0,"$100,000 - $145,000 a year",2023-10-18T13:35:12.833Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=3cf3060bcd436793&from=jasx&tk=1hd1ft1i8joou800&vjs=3
113,SourceMantra,"Hello,

 Job Title: Senior AWS Data Engineer with Strong Python Development(10+)
 Location: Reston, VA – Once in a month
 Duration: 12+ Months Contract position
 Experience : 10+ years

Need to be strong in AWS SNS, SQS, Lambda functions.
Job Description :

 • Strong knowledge on AWS services such as (S3, RDS, EC2, Lambda, SQS, SNS, Redshift)
 • Having prior working experience in Fannie Mae will be added advantage.
 • Good Knowledge on Java and Database (Oracle Postgres)

Thanks & RegardsSwarna - swarna@sourcemantra.com|Source Mantra IncTechnical Recruiter Desk No: (908-381-0321)LinkedIn: linkedin.com/in/swarna-yenumula-6512a1167295 Durham Ave, Suite # 201, South Plainfield, NJ 07080Source Mantra Inc | Certified Minority Business Enterprise (MBE)
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:

 Health insurance

Compensation package:

 Hourly pay

Experience level:

 10 years

Schedule:

 8 hour shift

Experience:

 Java: 4 years (Preferred)
 Python: 3 years (Preferred)
 Redshift: 3 years (Preferred)

Work Location: Remote","<p>Hello,</p>
<ul>
 <li><b>Job Title:</b> Senior AWS Data Engineer with Strong Python Development(10+)</li>
 <li><b>Location:</b> Reston, VA &#x2013; Once in a month</li>
 <li><b>Duration:</b> 12+ Months Contract position</li>
 <li><b>Experience :</b> 10+ years</li>
</ul>
<p>Need to be strong in AWS SNS, SQS, Lambda functions.</p>
<p><b>Job Description :</b></p>
<ul>
 <li>&#x2022; Strong knowledge on AWS services such as (S3, RDS, EC2, Lambda, SQS, SNS, Redshift)</li>
 <li>&#x2022; Having prior working experience in Fannie Mae will be added advantage.</li>
 <li>&#x2022; Good Knowledge on Java and Database (Oracle Postgres)</li>
</ul>
<p><b>Thanks &amp; Regards</b><br><b>Swarna - </b>swarna@sourcemantra.com|<b>Source Mantra Inc</b><br><b>Technical Recruiter </b><br><b>Desk No</b>: (908-381-0321)<br><b>LinkedIn</b>: linkedin.com/in/swarna-yenumula-6512a1167<br><b>295 Durham Ave, Suite # 201, South Plainfield, NJ 07080</b><br><b>Source Mantra Inc | Certified Minority Business Enterprise (MBE)</b></p>
<p>Job Type: Contract</p>
<p>Salary: &#x24;60.00 - &#x24;70.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Hourly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Java: 4 years (Preferred)</li>
 <li>Python: 3 years (Preferred)</li>
 <li>Redshift: 3 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,6154aa92888e85ea,,Contract,,,Remote,AWS Data Engineer with Python 10+Years,5 days ago,2023-10-13T13:35:21.881Z,,,$60 - $70 an hour,2023-10-18T13:35:21.883Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=6154aa92888e85ea&from=jasx&tk=1hd1ft1i8joou800&vjs=3
115,Hire IT People Inc,"**Position:** Azure Date Engineer
**Location:** [Remote] / Customer is located in Washington DC.
**Responsibilities:**
1. Provide support for Azure cloud-based applications and infrastructure.
2. Diagnose and troubleshoot technical issues related to Azure cloud services.
3. Collaborate with cross-functional teams to implement best practices and ensure service optimization.
4. Continuously monitor and manage cloud environment to prevent and mitigate risks.
5. Assist client in designing and implementing Azure cloud solutions based on their unique needs by moving forward siting applications into Cloud environment.
6. Keep up to date with the latest Azure updates and features, ensuring optimal utilization and implementation.
7. Offer guidance and recommendations on cost-management strategies within Azure.
8. Contribute to the development of internal tools and processes to enhance Azure cloud management and support.
**Requirements:**
1. Proven experience as a Cloud Support Engineer, preferably with a focus on Azure.
2. Strong knowledge of Azure services, including but not limited to Azure Active Directory, Azure DevOps, Azure Kubernetes Service, and more.
3. Excellent problem-solving skills and the ability to diagnose complex cloud-based issues.
4. Strong verbal and written communication skills.
5. Must be a US citizen.
6. Must successfully pass Level 4 Public Trust verification.
7. Relevant certifications in Azure, such as AZ-104 or AZ-303/304, are a plus.
Job Type: Full-time
Salary: From $140,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Vision insurance

Work Location: Remote","<p>**Position:** Azure Date Engineer</p>
<p>**Location:** [Remote] / Customer is located in Washington DC.</p>
<p>**Responsibilities:**</p>
<p>1. Provide support for Azure cloud-based applications and infrastructure.</p>
<p>2. Diagnose and troubleshoot technical issues related to Azure cloud services.</p>
<p>3. Collaborate with cross-functional teams to implement best practices and ensure service optimization.</p>
<p>4. Continuously monitor and manage cloud environment to prevent and mitigate risks.</p>
<p>5. Assist client in designing and implementing Azure cloud solutions based on their unique needs by moving forward siting applications into Cloud environment.</p>
<p>6. Keep up to date with the latest Azure updates and features, ensuring optimal utilization and implementation.</p>
<p>7. Offer guidance and recommendations on cost-management strategies within Azure.</p>
<p>8. Contribute to the development of internal tools and processes to enhance Azure cloud management and support.</p>
<p>**Requirements:**</p>
<p>1. Proven experience as a Cloud Support Engineer, preferably with a focus on Azure.</p>
<p>2. Strong knowledge of Azure services, including but not limited to Azure Active Directory, Azure DevOps, Azure Kubernetes Service, and more.</p>
<p>3. Excellent problem-solving skills and the ability to diagnose complex cloud-based issues.</p>
<p>4. Strong verbal and written communication skills.</p>
<p>5. Must be a US citizen.</p>
<p>6. Must successfully pass Level 4 Public Trust verification.</p>
<p>7. Relevant certifications in Azure, such as AZ-104 or AZ-303/304, are a plus.</p>
<p>Job Type: Full-time</p>
<p>Salary: From &#x24;140,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Work Location: Remote</p>",,649f0d12515402f8,,Full-time,,,Remote,Azure Data Engineer,5 days ago,2023-10-13T13:35:29.972Z,,,"From $140,000 a year",2023-10-18T13:35:30.158Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=649f0d12515402f8&from=jasx&tk=1hd1ft1i8joou800&vjs=3
118,Blueprint Technologies,"Senior Data Engineer, Azure 
  Senior Data Engineer 
  Remote-US Only 
  Who is Blueprint? 
  We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun. 
  What does Blueprint do? 
  Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies. 
  Why Blueprint? 
  At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint. 
  What will I be doing? 
  Blueprint is looking for an Azure, Senior Data Engineer to join us as we build cutting-edge technology solutions! This is a fast-paced role that needs a dedicated and passionate individual focused on team and client satisfaction! The ideal candidate will have a solid background in consulting, with demonstrating experience in leading clients through the process of building modern data estates. You will also be responsible for mentoring any junior developers on the engagement. 
  Responsibilities: 
  
  Develop and implement effective data architecture solutions using Databricks and Lakehouse 
  Optimize and tune data pipelines for performance and scalability 
  Monitor and troubleshoot data pipelines to ensure data availability and reliability 
  Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from data 
  Implement best practices for data governance, data security, and data quality to ensure data integrity across all data sources 
  Create and maintain documentation related to data architecture, data pipelines, and data models 
  Stay up to date with emerging technologies and best practices in data engineering and big data processing 
  Mentor and train other data engineers on best practices for data engineering and Databricks usage 
  Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally 
  
 Qualifications: 
  
  Bachelor's degree in computer science or equivalent experience 
  At least 5+ -years of experience as a data engineer 
  At least 5+ years of experience with SQL Development (ETL transformations, stored procedure) 
  Data Ingestion experience from inception to Gold Medallion 
 
 
  Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices 
 
 
  At least 3-5 -years of experience programming with PySpark performing various transformations 
  Design, build, implement and maintain our data infrastructure to power analytics and ML 
  Partner with engineers, producers and designers to deliver data insights that impact our players 
  Contribute to our investments into various open-source and 3rd party tools to build a system that scales with the company 
  Collaborate with our Data Scientists and Analytics Engineers to make pipeline implementation faster, more straightforward, and more trustworthy driven decisions that will shape our business 
  2-5+ years building large scale data infrastructure on Spark/Databricks or similar 
  3+ years experience working with real-time data ingestion/processing 
  Working knowledge of Databricks DLT(Delta Live Table) and Unity Catalog a plus 
  Experience with relational and non-relational database technologies (i.e., NoSQL, blob storage, etc.) 
  Experience with data wrangling skills with csv, tsv, parquet, and json 
  Experience designing, building and optimizing Big Data platforms that are robust, scalable and reliable 
  Excellent problem-solving and troubleshooting skills 
  
 Salary Range 
  Pay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $146,400 to $175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range. 
  Equal Opportunity Employer 
  Blueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law. 
  If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.com 
  Blueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes: 
  
  Medical, dental, and vision coverage 
  Flexible Spending Account 
  401k program 
  Competitive PTO offerings 
  Parental Leave 
  Opportunities for professional growth and development","<div>
 <p><b>Senior Data Engineer, Azure</b></p> 
 <p><b> Senior Data Engineer</b></p> 
 <p><b> Remote-US Only</b></p> 
 <p><b> Who is Blueprint?</b></p> 
 <p> We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We&apos;re bold, smart, agile, and fun.</p> 
 <p><b> What does Blueprint do?</b></p> 
 <p> Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.</p> 
 <p><b> Why Blueprint?</b></p> 
 <p> At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You&apos;ll focus on solving unique business problems while gaining hands-on experience with the world&apos;s best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you&apos;ll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won&apos;t find a better place to work and thrive than at Blueprint.</p> 
 <p><b> What will I be doing?</b></p> 
 <p> Blueprint is looking for an <b>Azure, Senior Data Engineer </b>to join us as we build cutting-edge technology solutions! This is a fast-paced role that needs a dedicated and passionate individual focused on team and client satisfaction! The ideal candidate will have a solid background in consulting, with demonstrating experience in leading clients through the process of building modern data estates. You will also be responsible for mentoring any junior developers on the engagement.</p> 
 <p><b> Responsibilities:</b></p> 
 <ul> 
  <li>Develop and implement effective data architecture solutions using Databricks and Lakehouse</li> 
  <li>Optimize and tune data pipelines for performance and scalability</li> 
  <li>Monitor and troubleshoot data pipelines to ensure data availability and reliability</li> 
  <li>Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from data</li> 
  <li>Implement best practices for data governance, data security, and data quality to ensure data integrity across all data sources</li> 
  <li>Create and maintain documentation related to data architecture, data pipelines, and data models</li> 
  <li>Stay up to date with emerging technologies and best practices in data engineering and big data processing</li> 
  <li>Mentor and train other data engineers on best practices for data engineering and Databricks usage</li> 
  <li>Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally</li> 
 </ul> 
 <p><b>Qualifications:</b></p> 
 <ul> 
  <li>Bachelor&apos;s degree in computer science or equivalent experience</li> 
  <li>At least 5+ -years of experience as a data engineer</li> 
  <li>At least 5+ years of experience with SQL Development (ETL transformations, stored procedure)</li> 
  <li>Data Ingestion experience from inception to Gold Medallion</li> 
 </ul>
 <ul>
  <li>Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices</li> 
 </ul>
 <ul>
  <li>At least 3-5 -years of experience programming with PySpark performing various transformations</li> 
  <li>Design, build, implement and maintain our data infrastructure to power analytics and ML</li> 
  <li>Partner with engineers, producers and designers to deliver data insights that impact our players</li> 
  <li>Contribute to our investments into various open-source and 3rd party tools to build a system that scales with the company</li> 
  <li>Collaborate with our Data Scientists and Analytics Engineers to make pipeline implementation faster, more straightforward, and more trustworthy driven decisions that will shape our business</li> 
  <li>2-5+ years building large scale data infrastructure on Spark/Databricks or similar</li> 
  <li>3+ years experience working with real-time data ingestion/processing</li> 
  <li>Working knowledge of Databricks DLT(Delta Live Table) and Unity Catalog a plus</li> 
  <li>Experience with relational and non-relational database technologies (i.e., NoSQL, blob storage, etc.)</li> 
  <li>Experience with data wrangling skills with csv, tsv, parquet, and json</li> 
  <li>Experience designing, building and optimizing Big Data platforms that are robust, scalable and reliable</li> 
  <li>Excellent problem-solving and troubleshooting skills</li> 
 </ul> 
 <p><b>Salary Range</b></p> 
 <p> Pay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: &#x24;146,400 to &#x24;175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate&apos;s qualifications and experience and may be outside this range.</p> 
 <p><b> Equal Opportunity Employer</b></p> 
 <p> Blueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.</p> 
 <p> If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.com</p> 
 <p> Blueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes:</p> 
 <ul> 
  <li>Medical, dental, and vision coverage</li> 
  <li>Flexible Spending Account</li> 
  <li>401k program</li> 
  <li>Competitive PTO offerings</li> 
  <li>Parental Leave</li> 
  <li>Opportunities for professional growth and development</li>
 </ul>
</div>
<p></p>",https://bpcs.com/careers/jobs/avail?gh_jid=5433144&gh_src=57f83ce11us,75fadf29bd2cd624,,,,,Remote,Sr. Data Engineer,5 days ago,2023-10-13T13:35:25.126Z,3.2,57.0,"$146,400 - $175,100 a year",2023-10-18T13:35:25.130Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=75fadf29bd2cd624&from=jasx&tk=1hd1ft1i8joou800&vjs=3
119,Emerson United Inc,"Overall 7-8 years of experience
Must Have :
Databricks/py-spark – 4+ years of experience
Cloud experience – 4+ years of cloud experience
Nice to have:
Preferable Software engineering background
Preferable AWS experience
Experience with Typescript
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:

 7 years

Schedule:

 8 hour shift

Experience:

 AWS: 6 years (Required)
 Databricks: 4 years (Required)
 Pyspark: 4 years (Required)

Work Location: Remote","<p>Overall 7-8 years of experience</p>
<p>Must Have :</p>
<p>Databricks/py-spark &#x2013; 4+ years of experience</p>
<p>Cloud experience &#x2013; 4+ years of cloud experience</p>
<p>Nice to have:</p>
<p>Preferable Software engineering background</p>
<p>Preferable AWS experience</p>
<p>Experience with Typescript</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;75.00 - &#x24;80.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>AWS: 6 years (Required)</li>
 <li>Databricks: 4 years (Required)</li>
 <li>Pyspark: 4 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,5f5de83e70b15c74,,Contract,,,Remote,AWS Data Engineer,5 days ago,2023-10-13T13:35:41.097Z,,,$75 - $80 an hour,2023-10-18T13:35:41.101Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=5f5de83e70b15c74&from=jasx&tk=1hd1fv9v4k5om800&vjs=3
123,eTeam Inc.,"Job Title :- Data engineer (10+ years candidate required) Job Location :- Remote   Job Description :- 
  LinkedIn is Mandatory 
  Description:- 
  
  
   Bachelors degree in the areas of Computer Science, Engineering, Information Systems, Business, or equivalent field of study required 
   7+ years of experience in working with data solutions. 
   3+ years of experience coding in Python, or Scala or similar scripting language. 
   3+ years of experience in developing data pipelines in AWS Cloud Platform (preferred), Azure, or Snowflake at scale. 
   2+ years Experience in designing and implementing data ingestion with real-time data streaming tools like Kafka, Kinesis or any similar tools.SAP/Client or other cloud integrations are preferred. 
   3+ years experience working with MPP databases such as Snowflake (Preferred) , Redshift or similar MPP databases. 
   2+ years experience working with Serverless ETL processes (Lambda, AWS Glue, Matillion or similar) 
   1+ years experience with big data technologies like EMR, Hadoop, Spark, Cassandra, MongoDB or other open source big data tools. 
   Knowledge of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations. 
   Experience designing, documenting, and defending designs for key components in large distributed computing systems 
   Demonstrated ability to learn new technologies quickly and independently 
   Demonstrated ability to achieve stretch goals in a very innovative and fast paced environment 
   Ability to handle multiple competing priorities in a fast-paced environment 
   Excellent verbal and written communication skills, especially in technical communications 
   Strong interpersonal skills and a desire to work collaboratively 
  
  Experience participating in an Agile software development team, e.g. SCRUM 
  
  Job Responsibilities : 
  
   Responsible for the building, deployment, and maintenance of critical scalable Data Pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements 
   Work closely with SMEs, Data Modeler, Architects, Analysts and other team members on requirements to build scalable real time/near real time/batch data solutions. 
   Contributes design, code, configurations, and documentation for components that manage data ingestion, real time streaming, batch processing, data extraction, transformation, and loading into Data Lake/Cloud Data Warehouse/MPP (Snowflake/Redshift/similar Technologies ) . 
   Owns one or more key components of the infrastructure and works to continually improve it, identifying gaps and improving the platforms quality, robustness, maintainability, and speed. 
   Cross-trains other team members on technologies being developed, while also continuously learning new technologies from other team members. 
   Interacts with technical teams across and ensures that solutions meet customer requirements in terms of functionality, performance, availability, scalability, and reliability. 
   Performs development, QA, and dev-ops roles as needed to ensure total end to end responsibility of solutions. 
   Keep up with current trends in big data and Analytics , evaluate tools and pace yourself for innovation. 
  
  Mentor Junior engineers ,create necessary documentation and Run-books while still being able to deliver on goals","<div>
 <div>
  <p><b>Job Title :- Data engineer (10+ years candidate required)</b><br> <b>Job Location :-</b><b> </b><b>Remote</b><b> </b><br> <br> <b>Job Description :-</b><b> </b></p>
  <p><b>LinkedIn is Mandatory</b><b> </b></p>
  <p><b>Description:-</b> </p>
  <p></p>
  <ul>
   <li>Bachelors degree in the areas of Computer Science, Engineering, Information Systems, Business, or equivalent field of study required</li> 
   <li>7+ years of experience in working with data solutions.</li> 
   <li>3+ years of experience coding in Python, or Scala or similar scripting language. </li>
   <li>3+ years of experience in developing data pipelines in AWS Cloud Platform (preferred), Azure, or Snowflake at scale.</li> 
   <li>2+ years Experience in designing and implementing data ingestion with real-time data streaming tools like Kafka, Kinesis or any similar tools.SAP/Client or other cloud integrations are preferred.</li> 
   <li>3+ years experience working with MPP databases such as Snowflake (Preferred) , Redshift or similar MPP databases.</li> 
   <li>2+ years experience working with Serverless ETL processes (Lambda, AWS Glue, Matillion or similar)</li> 
   <li>1+ years experience with big data technologies like EMR, Hadoop, Spark, Cassandra, MongoDB or other open source big data tools. </li>
   <li>Knowledge of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations.</li> 
   <li>Experience designing, documenting, and defending designs for key components in large distributed computing systems</li> 
   <li>Demonstrated ability to learn new technologies quickly and independently</li> 
   <li>Demonstrated ability to achieve stretch goals in a very innovative and fast paced environment</li> 
   <li>Ability to handle multiple competing priorities in a fast-paced environment</li> 
   <li>Excellent verbal and written communication skills, especially in technical communications</li> 
   <li>Strong interpersonal skills and a desire to work collaboratively</li> 
  </ul>
  <p>Experience participating in an Agile software development team, e.g. SCRUM</p> 
  <p></p>
  <p><b>Job Responsibilities</b><b> </b>: </p>
  <ul>
   <li>Responsible for the building, deployment, and maintenance of critical scalable Data Pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements</li> 
   <li>Work closely with SMEs, Data Modeler, Architects, Analysts and other team members on requirements to build scalable real time/near real time/batch data solutions.</li> 
   <li>Contributes design, code, configurations, and documentation for components that manage data ingestion, real time streaming, batch processing, data extraction, transformation, and loading into Data Lake/Cloud Data Warehouse/MPP (Snowflake/Redshift/similar Technologies ) .</li> 
   <li>Owns one or more key components of the infrastructure and works to continually improve it, identifying gaps and improving the platforms quality, robustness, maintainability, and speed.</li> 
   <li>Cross-trains other team members on technologies being developed, while also continuously learning new technologies from other team members.</li> 
   <li>Interacts with technical teams across and ensures that solutions meet customer requirements in terms of functionality, performance, availability, scalability, and reliability.</li> 
   <li>Performs development, QA, and dev-ops roles as needed to ensure total end to end responsibility of solutions.</li> 
   <li>Keep up with current trends in big data and Analytics , evaluate tools and pace yourself for innovation.</li> 
  </ul>
  <p>Mentor Junior engineers ,create necessary documentation and Run-books while still being able to deliver on goals</p>
 </div>
</div>",https://www2.jobdiva.com/portal/?a=svjdnwzkulao5hqo7t0ifgvj8s71sf01d7dtgdstyhdixakxt6ty85zljsdyhgz2&jobid=20650894#/jobs/20650894?compid=0&SearchString=&StatesString=&id=20650894&source=indeed.com,a03e6162d11ae269,,Contract,,,"Cary, NC",Data engineer,6 days ago,2023-10-12T13:35:44.281Z,,,$45 - $55 an hour,2023-10-18T13:35:44.282Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=a03e6162d11ae269&from=jasx&tk=1hd1fvcrnllot800&vjs=3
124,Sanametrix,"***US Citzenship is Required******
Sanametrix, Inc. is a fast-growing small business headquartered in Arlington, VA. We are dedicated to providing federal agencies with legendary customer service and focused solutions for their business and technology needs.
This role is responsible for building data pipelines for transferring data from source systems (virtual machines, Microsoft SQL Server) into AWS Cloud using AWS Native Tools. This resource has strong data modeling and scripting experience and has a strong knowledge of AWS Data Services.
Responsibilities:

 Perform data processing, algorithm / structures, pipeline orchestration, data quality, governance, discovery


 Work with structured and unstructured data, blob data


 Develop and work with APIs


 Collect and organize data using data warehousing technique and file storage technologies • Perform ELT and ETL processes


 Gather data requirements


 Develop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.


 Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization.


 Implement processes and systems to monitor data quality, to ensure production data accuracy, and ensure key stakeholder and business process access.


 Write unit/integration tests, contribute to engineering wiki, and documents.


 Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.


 Work closely with a team of front-end and back-end engineers, product managers, and analysts.


 Design data integrations and data quality framework based on established requirements.

Qualifications & Skills:
Scripting • SQL • Python • Spark • Linux / shell scripting
Services / Tools (six or more)

 S3


 Lambda


 Redshift


 Lake Formation


 Glue ETL


 Kinesis


 DMS


 Glue catalog/Crawlers


 Git


 Jira


 Airflow /Orchestration

Education, Experience, and Licensing Requirements:

 BS or MS degree in Computer Science or a related technical field


 4+ years of Python or Java development experience


 4+ years of SQL or NoSQL experience


 4+ years of experience with schema design and dimensional data modeling


 Ability in managing and communicating data warehouse plans to internal clients


 Experience designing, building, and maintaining data processing systems


 AWS Certified is preferred

Job Type: Full-time
Pay: $110,656.51 - $120,080.95 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible schedule
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Tuition reimbursement

Compensation package:

 Yearly pay

Experience level:

 4 years

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Experience:

 Informatica: 4 years (Preferred)
 SQL: 4 years (Required)
 Data warehouse: 4 years (Preferred)

Work Location: Remote","<p><b>***US Citzenship is Required******</b></p>
<p>Sanametrix, Inc. is a fast-growing small business headquartered in Arlington, VA. We are dedicated to providing federal agencies with legendary customer service and focused solutions for their business and technology needs.</p>
<p>This role is responsible for building data pipelines for transferring data from source systems (virtual machines, Microsoft SQL Server) into AWS Cloud using AWS Native Tools. This resource has strong data modeling and scripting experience and has a strong knowledge of AWS Data Services.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Perform data processing, algorithm / structures, pipeline orchestration, data quality, governance, discovery</li>
</ul>
<ul>
 <li>Work with structured and unstructured data, blob data</li>
</ul>
<ul>
 <li>Develop and work with APIs</li>
</ul>
<ul>
 <li>Collect and organize data using data warehousing technique and file storage technologies &#x2022; Perform ELT and ETL processes</li>
</ul>
<ul>
 <li>Gather data requirements</li>
</ul>
<ul>
 <li>Develop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.</li>
</ul>
<ul>
 <li>Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization.</li>
</ul>
<ul>
 <li>Implement processes and systems to monitor data quality, to ensure production data accuracy, and ensure key stakeholder and business process access.</li>
</ul>
<ul>
 <li>Write unit/integration tests, contribute to engineering wiki, and documents.</li>
</ul>
<ul>
 <li>Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.</li>
</ul>
<ul>
 <li>Work closely with a team of front-end and back-end engineers, product managers, and analysts.</li>
</ul>
<ul>
 <li>Design data integrations and data quality framework based on established requirements.</li>
</ul>
<p>Qualifications &amp; Skills:</p>
<p>Scripting &#x2022; SQL &#x2022; Python &#x2022; Spark &#x2022; Linux / shell scripting</p>
<p>Services / Tools (six or more)</p>
<ul>
 <li>S3</li>
</ul>
<ul>
 <li>Lambda</li>
</ul>
<ul>
 <li>Redshift</li>
</ul>
<ul>
 <li>Lake Formation</li>
</ul>
<ul>
 <li>Glue ETL</li>
</ul>
<ul>
 <li>Kinesis</li>
</ul>
<ul>
 <li>DMS</li>
</ul>
<ul>
 <li>Glue catalog/Crawlers</li>
</ul>
<ul>
 <li>Git</li>
</ul>
<ul>
 <li>Jira</li>
</ul>
<ul>
 <li>Airflow /Orchestration</li>
</ul>
<p>Education, Experience, and Licensing Requirements:</p>
<ul>
 <li>BS or MS degree in Computer Science or a related technical field</li>
</ul>
<ul>
 <li>4+ years of Python or Java development experience</li>
</ul>
<ul>
 <li>4+ years of SQL or NoSQL experience</li>
</ul>
<ul>
 <li>4+ years of experience with schema design and dimensional data modeling</li>
</ul>
<ul>
 <li>Ability in managing and communicating data warehouse plans to internal clients</li>
</ul>
<ul>
 <li>Experience designing, building, and maintaining data processing systems</li>
</ul>
<ul>
 <li>AWS Certified is preferred</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;110,656.51 - &#x24;120,080.95 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Flexible schedule</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Tuition reimbursement</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>4 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 4 years (Preferred)</li>
 <li>SQL: 4 years (Required)</li>
 <li>Data warehouse: 4 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,6116c05f42022336,,Full-time,,,Remote,Data Engineer,30+ days ago,2023-09-18T13:35:55.040Z,4.4,7.0,"$110,657 - $120,081 a year",2023-10-18T13:35:55.054Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CyQKdz8_lqdlgY-c-amsQST66Z8QjChsyYA8vzcGklWI54h1yaGRml5nZ8zCgFfjK0ZW-ufu-_JHCgbraA_8Fg5ATid4dqS3sKjF52i-1M3_0-M7jyZvHimBaYOdqijbr2o_GnQcYG7gNZLx4-WxgznWbghRNnlHq-GXY6nBkPNqceCYdURQ3TT3immrKviUaPlqf-c_Ib7N-Q0_i_sOXrXBN1vqLBBUi3-4BfTVVixgQFa21zTokCNHLrwz3Kny0Ad23IYAIYfeC_d-rEdT2v5haWsybFd3UNkJ0t79P5xuzLgUur1_DcxTIVo_zkOPIMUDEaZsnbN0I9AGB_So0CcTeVs6N061zI8As2fYAtew8Z5xKsM8ICPSUFtjxKk9wSnYeyfITOW7XC1XRbItQQkvB3GfRObW2uQ8CwkAbHPmVi3ksTfsplb2w2gX85ZpG77ZYLjDKNQ3DrsOeKYW9gA7fjHjPza3iu-T03BvSXuWj8cYVxjdjVKR5AaUXJY0c4qaJPllg0vvLJfrgSosp0otfqB5CqsLSuY3V30mAz5vrQpTRbjswWX0cleaIuMDtEmZGYX13rTMm-A_sJfjuvGutDwZCFKhLhvKVAeszbeA%3D%3D&xkcb=SoDC-_M3JhXw-q2as50FbzkdCdPP&p=14&fvj=1&vjs=3&jsa=8121&tk=1hd1fvosi2gtk000&from=jasx&wvign=1
127,kraken,"Building the Future of Crypto
  
  
  
    Our Krakenites are a world-class team with crypto conviction, united by our desire to discover and unlock the potential of crypto and blockchain technology.
  
  
  
    What makes us different? Kraken is a mission-focused company rooted in crypto values. As a Krakenite, you’ll join us on our mission to accelerate the global adoption of crypto, so that everyone can achieve financial freedom and inclusion. For over a decade, Kraken’s focus on our mission and crypto ethos has attracted many of the most talented crypto experts in the world.
  
  
  
    Before you apply, please read the Kraken Culture page to learn more about our internal culture, values, and mission.
  
  
  
    As a fully remote company, we have Krakenites in 60+ countries who speak over 50 languages. Krakenites are industry pioneers who develop premium crypto products for experienced traders, institutions, and newcomers to the space. Kraken is committed to industry-leading security, crypto education, and world-class client support through our products like Kraken Pro, Kraken NFT, and Kraken Futures.
  
  
  
    Become a Krakenite and build the future of crypto!
  
  
  
    Proof of work
  
  
  
    The team
  
  
  
    Join our Data Infrastructure team and play a pivotal role in upholding the reliability, scalability, and efficiency of our robust Data platform. As a Senior Site Reliability Engineer (SRE) specialized in Data Infrastructure, you will collaborate closely with diverse cross-functional teams to conceive, execute, and oversee the foundational data infrastructure that empowers our array of applications and services. As a key member of our Data Infrastructure team, you will be at the forefront of ensuring the unfaltering availability and performance of our platform. Your profound proficiency in cloud technologies, infrastructure as code, automation, monitoring/alerting, logging, user and machine AuthNZ, and certificate management will be instrumental in upholding the exceptional operational standards we set for our services.
  
  
  
    This role is destined to candidates based in the Americas.
  
  
 
  
   The Opportunity
   
    
      Architect and implement data infrastructure solutions (self service) that support the needs of 10+ business units and over 100 engineering and data analysts
      Utilize Infrastructure as Code (IaC) principles to design, provision, and manage both on-premises and cloud (AWS) infrastructure components using tools such as Terraform
      Collaborate with teams to ensure seamless integration of data-related services with existing systems.
      Develop and maintain automation scripts using bash/shell scripting and to automate operational tasks and deployments.
      Enhance and manage CI/CD pipelines to facilitate consistent software deployments across the data infrastructure.
      Enable engineering self-service under tight security requirements using ChatOps and GitOps methodologies
      Implement robust data monitoring and alerting solutions to proactively detect anomalies and performance issues.
      Manage user and machine authentication and authorization mechanisms to ensure secure access to data and resources.
      Evangelize and implement role-based access control (RBAC) and permissions for a multitude of user groups and machine workflows across different environments
      Design and deploy MLOps platforms, using AWS Sagemaker and GitOps methodologies.
      Manage and maintain real-time streaming data architecture using technologies like Kafka and Debezium Change Data Capture (CDC).
      Ensure the timely and accurate processing of streaming data, enabling data analysts and engineers to gain insights from up-to-date information.
      Utilize Kubernetes to manage containerized applications within the data infrastructure, ensuring efficient deployment, scaling, and orchestration.
      Implement effective incident response procedures and participate in on-call rotations.
      Troubleshoot and resolve incidents promptly to minimize downtime and impact.
      Collaborate with data analysts, engineers, and cross-functional teams to understand requirements and implement appropriate solutions.
      Document architecture, processes, and best practices to enable knowledge sharing and support continuous improvement.
      Enable environments for ML experimentation
      Create and manage MLOps flows for training, validation and deployment of models
      Implement efficient, reproducible production deployment of ML models for inference
    
   
  
  
 
  
   Skills you should HODL
   
    
      Bachelor’s degree in Computer Science, Engineering, or a related field (or equivalent experience).
      Proven experience (5+ years) working as a Site Reliability Engineer, Infrastructure Engineer, or similar roles, with a focus on data infrastructure and security.
      Experience with real-time data processing technologies, such as Kafka and Debezium
      Strong expertise in cloud technologies, particularly AWS and (HashiCorp nice to have).
      Proficiency in Infrastructure as Code tools such as Terraform and Atlantis.
      Experience with containerization and orchestration tools, particularly Kubernetes.
      Solid understanding of bash/shell scripting and proficiency in at least one programming language.
      Familiarity with CI/CD deployment pipelines and related tools.
      Knowledge of HashiCorp products like Vault, Nomad, and Consul is a plus.
      Strong problem-solving skills and the ability to troubleshoot complex systems.
      Expertise in zero-trust architecture and service meshes is a plus
      Experience with data-related technologies (databases, airflow, data warehousing, data lakes) is a plus.
    
   
  
  
 
  
   $117,000 - $176,000 a year
  
  
    This is the target annual salary range for this role. This range is not inclusive of other additional compensation elements, such as our Bonus program, Equity program, Wellness allowance, and other benefits [US Only] (including medical, dental, vision and 401(k)).
  
  
  
    The compensation range provided is influenced by various factors and represents the initial target range. Our salary offerings are dynamic and we strive to ensure that our base salary and total compensation package aligns and recognizes the top talent we aim to attract and retain. The compensation package of the successful candidate is based on various factors such as their skillset, experience, and job scope.
  
  
 
  
   Location Tagging: #US #LI-Remote
  
  
  
    Kraken is powered by people from around the world and we celebrate all Krakenites for their diverse talents, backgrounds, contributions and unique perspectives. We hire strictly based on merit, meaning we seek out the candidates with the right abilities, knowledge, and skills considered the most suitable for the job. We encourage you to apply for roles where you don't fully meet the listed requirements, especially if you're passionate or knowledgable about crypto!
  
  
  
    As an equal opportunity employer, we don’t tolerate discrimination or harassment of any kind. Whether that’s based on race, ethnicity, age, gender identity, citizenship, religion, sexual orientation, disability, pregnancy, veteran status or any other protected characteristic as outlined by federal, state or local laws.
  
  
  
    Stay in the know
  
  
  
    Follow us on Twitter
  
  
    Learn on the Kraken Blog
  
  
    Connect on LinkedIn","<div>
 <div>
  <div>
   <b>Building the Future of Crypto</b>
  </div>
  <div></div>
  <div>
   <br> Our Krakenites are a world-class team with crypto conviction, united by our desire to discover and unlock the potential of crypto and blockchain technology.
  </div>
  <div></div>
  <div>
   <br> What makes us different? Kraken is a mission-focused company rooted in crypto values. As a Krakenite, you&#x2019;ll join us on our mission to accelerate the global adoption of crypto, so that everyone can achieve financial freedom and inclusion. For over a decade, Kraken&#x2019;s focus on our mission and crypto ethos has attracted many of the most talented crypto experts in the world.
  </div>
  <div></div>
  <div>
   <br> Before you apply, please read the Kraken Culture page to learn more about our internal culture, values, and mission.
  </div>
  <div></div>
  <div>
   <br> As a fully remote company, we have Krakenites in 60+ countries who speak over 50 languages. Krakenites are industry pioneers who develop premium crypto products for experienced traders, institutions, and newcomers to the space. Kraken is committed to industry-leading security, crypto education, and world-class client support through our products like Kraken Pro, Kraken NFT, and Kraken Futures.
  </div>
  <div></div>
  <div>
   <br> Become a Krakenite and build the future of crypto!
  </div>
  <div></div>
  <div>
   <b><br> Proof of work</b>
  </div>
  <div></div>
  <div>
   <b><br> The team</b>
  </div>
  <div></div>
  <div>
   <br> Join our Data Infrastructure team and play a pivotal role in upholding the reliability, scalability, and efficiency of our robust Data platform. As a Senior Site Reliability Engineer (SRE) specialized in Data Infrastructure, you will collaborate closely with diverse cross-functional teams to conceive, execute, and oversee the foundational data infrastructure that empowers our array of applications and services. As a key member of our Data Infrastructure team, you will be at the forefront of ensuring the unfaltering availability and performance of our platform. Your profound proficiency in cloud technologies, infrastructure as code, automation, monitoring/alerting, logging, user and machine AuthNZ, and certificate management will be instrumental in upholding the exceptional operational standards we set for our services.
  </div>
  <div></div>
  <div>
   <br> This role is destined to candidates based in the Americas.
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>The Opportunity</b></h3>
   <ul>
    <ul>
     <li> Architect and implement data infrastructure solutions (self service) that support the needs of 10+ business units and over 100 engineering and data analysts</li>
     <li> Utilize Infrastructure as Code (IaC) principles to design, provision, and manage both on-premises and cloud (AWS) infrastructure components using tools such as Terraform</li>
     <li> Collaborate with teams to ensure seamless integration of data-related services with existing systems.</li>
     <li> Develop and maintain automation scripts using bash/shell scripting and to automate operational tasks and deployments.</li>
     <li> Enhance and manage CI/CD pipelines to facilitate consistent software deployments across the data infrastructure.</li>
     <li> Enable engineering self-service under tight security requirements using ChatOps and GitOps methodologies</li>
     <li> Implement robust data monitoring and alerting solutions to proactively detect anomalies and performance issues.</li>
     <li> Manage user and machine authentication and authorization mechanisms to ensure secure access to data and resources.</li>
     <li> Evangelize and implement role-based access control (RBAC) and permissions for a multitude of user groups and machine workflows across different environments</li>
     <li> Design and deploy MLOps platforms, using AWS Sagemaker and GitOps methodologies.</li>
     <li> Manage and maintain real-time streaming data architecture using technologies like Kafka and Debezium Change Data Capture (CDC).</li>
     <li> Ensure the timely and accurate processing of streaming data, enabling data analysts and engineers to gain insights from up-to-date information.</li>
     <li> Utilize Kubernetes to manage containerized applications within the data infrastructure, ensuring efficient deployment, scaling, and orchestration.</li>
     <li> Implement effective incident response procedures and participate in on-call rotations.</li>
     <li> Troubleshoot and resolve incidents promptly to minimize downtime and impact.</li>
     <li> Collaborate with data analysts, engineers, and cross-functional teams to understand requirements and implement appropriate solutions.</li>
     <li> Document architecture, processes, and best practices to enable knowledge sharing and support continuous improvement.</li>
     <li> Enable environments for ML experimentation</li>
     <li> Create and manage MLOps flows for training, validation and deployment of models</li>
     <li> Implement efficient, reproducible production deployment of ML models for inference</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>Skills you should HODL</b></h3>
   <ul>
    <ul>
     <li> Bachelor&#x2019;s degree in Computer Science, Engineering, or a related field (or equivalent experience).</li>
     <li> Proven experience (5+ years) working as a Site Reliability Engineer, Infrastructure Engineer, or similar roles, with a focus on data infrastructure and security.</li>
     <li> Experience with real-time data processing technologies, such as Kafka and Debezium</li>
     <li> Strong expertise in cloud technologies, particularly AWS and (HashiCorp nice to have).</li>
     <li> Proficiency in Infrastructure as Code tools such as Terraform and Atlantis.</li>
     <li> Experience with containerization and orchestration tools, particularly Kubernetes.</li>
     <li> Solid understanding of bash/shell scripting and proficiency in at least one programming language.</li>
     <li> Familiarity with CI/CD deployment pipelines and related tools.</li>
     <li> Knowledge of HashiCorp products like Vault, Nomad, and Consul is a plus.</li>
     <li> Strong problem-solving skills and the ability to troubleshoot complex systems.</li>
     <li> Expertise in zero-trust architecture and service meshes is a plus</li>
     <li> Experience with data-related technologies (databases, airflow, data warehousing, data lakes) is a plus.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   &#x24;117,000 - &#x24;176,000 a year
  </div>
  <div>
    This is the target annual salary range for this role. This range is not inclusive of other additional compensation elements, such as our Bonus program, Equity program, Wellness allowance, and other benefits [US Only] (including medical, dental, vision and 401(k)).
  </div>
  <div></div>
  <div>
   <br> The compensation range provided is influenced by various factors and represents the initial target range. Our salary offerings are dynamic and we strive to ensure that our base salary and total compensation package aligns and recognizes the top talent we aim to attract and retain. The compensation package of the successful candidate is based on various factors such as their skillset, experience, and job scope.
  </div>
 </div> 
 <div>
  <div>
   Location Tagging: #US #LI-Remote
  </div>
  <div></div>
  <div>
   <br> Kraken is powered by people from around the world and we celebrate all Krakenites for their diverse talents, backgrounds, contributions and unique perspectives. We hire strictly based on merit, meaning we seek out the candidates with the right abilities, knowledge, and skills considered the most suitable for the job. We encourage you to apply for roles where you don&apos;t fully meet the listed requirements, especially if you&apos;re passionate or knowledgable about crypto!
  </div>
  <div></div>
  <div>
   <br> As an equal opportunity employer, we don&#x2019;t tolerate discrimination or harassment of any kind. Whether that&#x2019;s based on race, ethnicity, age, gender identity, citizenship, religion, sexual orientation, disability, pregnancy, veteran status or any other protected characteristic as outlined by federal, state or local laws.
  </div>
  <div></div>
  <div>
   <b><br> Stay in the know</b>
  </div>
  <div></div>
  <div>
   <br> Follow us on Twitter
  </div>
  <div>
    Learn on the Kraken Blog
  </div>
  <div>
    Connect on LinkedIn
  </div>
 </div>
</div>",https://jobs.lever.co/kraken/4bc1b010-f1b4-4a64-ae6a-956e2be4ddf3?lever-source=Indeed,d2f7d960ce9f68e6,,Full-time,,,Remote,Site Reliability Engineer - Data Platform,30+ days ago,2023-09-18T13:36:03.509Z,3.6,35.0,"$117,000 - $176,000 a year",2023-10-18T13:36:03.513Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d2f7d960ce9f68e6&from=jasx&tk=1hd1fvosi2gtk000&vjs=3
132,Dollar Tree,"We are seeking a highly skilled and experienced HR Data Engineer to join our team and lead the data migration process to Workday, a leading cloud-based human capital management system. Additionally, you will be responsible for establishing an HR Data Mart, to enhance our People Analytics teams data analytics capabilities. This is an exciting opportunity to be at the forefront of our digital transformation journey and contribute to the success of our HR operations.
  
  
 Responsibilities:
  
  1. Data Migration Strategy: Collaborate with cross-functional teams, including HR, IT, and business stakeholders, to understand data requirements, source systems, and mapping rules to ultimately help develop a comprehensive data migration strategy and plan for the transition from our existing HR systems to Workday
  
  2. HR Data Mart Implementation: Establish an HR Data Mart to centralize HR data from multiple sources and enable enhanced data analytics capabilities. Design and implement the data architecture, including data models, dimensional structures, and reporting frameworks, to support the HR Data Mart. Currently we are using Power BI with Microsoft Fabric and Google Bigquery.
  
  3. ETL Development: Design, develop, and maintain ETL (Extract, Transform, Load) processes and workflows to extract data from source systems, transform it based on mapping rules, and load it into Workday. Optimize ETL processes for efficiency and scalability, considering factors like data volume, complexity, and system performance.
  
  4. Data Validation and Testing: Implement rigorous data validation and testing methodologies to ensure the accuracy, completeness, and consistency of migrated data. Identify and resolve data discrepancies or issues, collaborating with relevant stakeholders as necessary.
  
  5. Data Mapping and Transformation: Analyze the structure and quality of HR data in the current systems and define mapping rules to transform and load the data into the Workday platform. Ensure data accuracy, integrity, and adherence to data governance standards during the migration process.
  
  6. Data Governance and Security: Ensure compliance with data governance policies, security protocols, and regulatory requirements throughout the migration process and HR Data Mart implementation. Implement appropriate data access controls, encryption, and anonymization techniques to protect sensitive HR data.
  
  7. Documentation and Training: Document data migration processes, workflows, mapping rules, and HR Data Mart architecture for future reference. Conduct training sessions and provide support to HR and IT teams on the usage and maintenance of migrated data and the HR Data Mart.
  
  8. Project Management: Lead the data migration project and HR Data Mart implementation, ensuring timely delivery of milestones, monitoring progress, and managing risks and issues. Collaborate with project managers and stakeholders to align data migration and HR Data Mart activities with overall project timelines and objectives.
  
  9. Continuous Improvement: Stay up to date with the latest industry trends, best practices, and technologies related to HR data management, data warehousing, and analytics. Proactively identify opportunities for process improvement and optimization within the HR data ecosystem, including the HR Data Mart.
  
  
 Qualifications:
  
  
 
  Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
  5+ years’ experience as a HR Data Engineer or similar role, with a focus on HR systems, implementing data warehousing solutions and data migration.
  Extensive knowledge of HR data management principles, data integration techniques, ETL processes, and data warehousing concepts.
  Strong expertise in working with HR systems, preferably with hands-on experience in implementing or migrating to Workday.
  Proficient in SQL and scripting languages (Python, R, etc.) for data manipulation, transformation, and validation.
  Familiarity with data governance practices, data privacy regulations, and security protocols.
  Experience in designing and implementing data warehousing solutions, including data modeling and dimensional structures.
  Excellent analytical and problem-solving skills, with the ability to handle complex data challenges.
  Effective communication and interpersonal skills, with the ability to collaborate with diverse stakeholders.
  Project management experience, including planning, coordinating, and monitoring project activities.
  Strong attention to detail and commitment to delivering high-quality results.
 
  If you are a driven and talented HR Data Engineer with a passion for data migration, HR systems, and data warehousing, we would love to hear from you. Join our team and play a key role in transforming our HR data ecosystem by leading the migration to Workday and establishing an HR Data Mart for advanced analytics capabilities.
  
  Compensation range: $120,000 - $133,000 based on experience + bonus + RSUs
  Dollar Tree offers Health, Dental, & Vision, flexible spending account, life and disability insurance benefits, 401k plan, 12 days of PTO & 7 paid holidays annually, and an employee stock purchase plan.","<div>
 We are seeking a highly skilled and experienced HR Data Engineer to join our team and lead the data migration process to Workday, a leading cloud-based human capital management system. Additionally, you will be responsible for establishing an HR Data Mart, to enhance our People Analytics teams data analytics capabilities. This is an exciting opportunity to be at the forefront of our digital transformation journey and contribute to the success of our HR operations.
 <br> 
 <br> 
 <b>Responsibilities:</b>
 <br> 
 <br> 1. Data Migration Strategy: Collaborate with cross-functional teams, including HR, IT, and business stakeholders, to understand data requirements, source systems, and mapping rules to ultimately help develop a comprehensive data migration strategy and plan for the transition from our existing HR systems to Workday
 <br> 
 <br> 2. HR Data Mart Implementation: Establish an HR Data Mart to centralize HR data from multiple sources and enable enhanced data analytics capabilities. Design and implement the data architecture, including data models, dimensional structures, and reporting frameworks, to support the HR Data Mart. Currently we are using Power BI with Microsoft Fabric and Google Bigquery.
 <br> 
 <br> 3. ETL Development: Design, develop, and maintain ETL (Extract, Transform, Load) processes and workflows to extract data from source systems, transform it based on mapping rules, and load it into Workday. Optimize ETL processes for efficiency and scalability, considering factors like data volume, complexity, and system performance.
 <br> 
 <br> 4. Data Validation and Testing: Implement rigorous data validation and testing methodologies to ensure the accuracy, completeness, and consistency of migrated data. Identify and resolve data discrepancies or issues, collaborating with relevant stakeholders as necessary.
 <br> 
 <br> 5. Data Mapping and Transformation: Analyze the structure and quality of HR data in the current systems and define mapping rules to transform and load the data into the Workday platform. Ensure data accuracy, integrity, and adherence to data governance standards during the migration process.
 <br> 
 <br> 6. Data Governance and Security: Ensure compliance with data governance policies, security protocols, and regulatory requirements throughout the migration process and HR Data Mart implementation. Implement appropriate data access controls, encryption, and anonymization techniques to protect sensitive HR data.
 <br> 
 <br> 7. Documentation and Training: Document data migration processes, workflows, mapping rules, and HR Data Mart architecture for future reference. Conduct training sessions and provide support to HR and IT teams on the usage and maintenance of migrated data and the HR Data Mart.
 <br> 
 <br> 8. Project Management: Lead the data migration project and HR Data Mart implementation, ensuring timely delivery of milestones, monitoring progress, and managing risks and issues. Collaborate with project managers and stakeholders to align data migration and HR Data Mart activities with overall project timelines and objectives.
 <br> 
 <br> 9. Continuous Improvement: Stay up to date with the latest industry trends, best practices, and technologies related to HR data management, data warehousing, and analytics. Proactively identify opportunities for process improvement and optimization within the HR data ecosystem, including the HR Data Mart.
 <br> 
 <br> 
 <b>Qualifications:</b>
 <br> 
 <br> 
 <ul>
  <li>Bachelor&apos;s or Master&apos;s degree in Computer Science, Information Systems, or a related field.</li>
  <li>5+ years&#x2019; experience as a HR Data Engineer or similar role, with a focus on HR systems, implementing data warehousing solutions and data migration.</li>
  <li>Extensive knowledge of HR data management principles, data integration techniques, ETL processes, and data warehousing concepts.</li>
  <li>Strong expertise in working with HR systems, preferably with hands-on experience in implementing or migrating to Workday.</li>
  <li>Proficient in SQL and scripting languages (Python, R, etc.) for data manipulation, transformation, and validation.</li>
  <li>Familiarity with data governance practices, data privacy regulations, and security protocols.</li>
  <li>Experience in designing and implementing data warehousing solutions, including data modeling and dimensional structures.</li>
  <li>Excellent analytical and problem-solving skills, with the ability to handle complex data challenges.</li>
  <li>Effective communication and interpersonal skills, with the ability to collaborate with diverse stakeholders.</li>
  <li>Project management experience, including planning, coordinating, and monitoring project activities.</li>
  <li>Strong attention to detail and commitment to delivering high-quality results.</li>
 </ul>
 <br> If you are a driven and talented HR Data Engineer with a passion for data migration, HR systems, and data warehousing, we would love to hear from you. Join our team and play a key role in transforming our HR data ecosystem by leading the migration to Workday and establishing an HR Data Mart for advanced analytics capabilities.
 <br> 
 <br> Compensation range: &#x24;120,000 - &#x24;133,000 based on experience + bonus + RSUs
 <br> Dollar Tree offers Health, Dental, &amp; Vision, flexible spending account, life and disability insurance benefits, 401k plan, 12 days of PTO &amp; 7 paid holidays annually, and an employee stock purchase plan.
</div>",https://careers.dollartree.com/us/en/job/DTYDTJUS544004BREXTERNALENUS/Senior-HR-Data-Engineer-Remote-Eligible?utm_source=indeed&utm_medium=phenom-feeds&Codes=Indeed,2dbc3a3fcff92774,,,,,"500 Volvo Pkwy, Chesapeake, VA 23320",Senior HR Data Engineer - Remote Eligible,30+ days ago,2023-09-18T13:36:14.926Z,3.3,24770.0,"$120,000 - $133,000 a year",2023-10-18T13:36:14.928Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2dbc3a3fcff92774&from=jasx&tk=1hd1fvosi2gtk000&vjs=3
134,ShipHero,"Hello. We are ShipHero (https://shiphero.com). We have built a software platform entrusted by hundreds of eCommerce companies, large and small to run their operations and we continue to grow. About US$5 billion of eCommerce orders are shipped a year via ShipHero. Our customers sell on Shopify, Amazon, Etsy, eBay, WooCommerce, BigCommerce, and many other platforms. We're driven to help our customers grow their businesses by providing a platform that solves complex problems, and is engineered to be reliable and fast. We are obsessed with building great technology that is beautiful, easy to use, and is loved by our customers. Our culture also reflects our ethos and belief that by bringing passionate, talented, and great people together – you can do great things. 
 Our team is fully remote, the company has always been remote. We communicate regularly using video chat and Slack and put a strong emphasis on asynchronous work so people have large chunks of uninterrupted time to focus and do deep work. 
 About You 
 
  You understand that great things are accomplished when teams work together.
   You've made a lot of mistakes, and most importantly, have learned from them.
   You are experienced in operating and improving the reliability of data storage and processing systems (relational databases, data warehouses, data lakes and distributed processing systems), including operational optimization (e.g. indexes, query tuning, monitoring).
   You have a solid understanding of stream processing and operating streaming solutions (using Kafka/Kinesis or some other solution) and/or CDC workloads.
   You have experience in planning, provisioning, scaling and maintaining reliable data processing systems in AWS or GCP (using Terraform/Ansible).
   You are comfortable with Python and are familiar with maintaining ETL jobs using Airflow or some other solution.
   You are always eager to learn more and love to try out new solutions on your own.
   Experience with the AWS data ecosystem is highly appreciated (Amazon Aurora MySQL, DocumentDB/MongoDB, OpenSearch/ElasticSearch, Redshift, Glue/Spark, MSK/Kafka, Kinesis, Debezium, S3/Apache Hudi, MWAA/Airflow)
  
 The Role
 
   Operate, improve and extend ShipHero's existing data infrastructure.
   Support the teams working on improving the performance of reports, listings, searches and other data and analytics services.
   Collaborate with our DevOps team to identify and remediate current and future data infrastructure performance or reliability issues.
   Work with ETL and streaming solutions.
   Review features and requirements, design and implement solutions together with our data engineers, data scientists, developers and designers.
 
  
 The Perks 
 
  $2.500 so you can buy any equipment you need to be happy at your job
   20 days paid vacation + new year & Christmas
   Conference days don't count against your vacation days, we want you to stay up-to-date
   We will pay for courses & conferences, if you learn we all learn","<div>
 <p>Hello. We are ShipHero (https://shiphero.com). We have built a software platform entrusted by hundreds of eCommerce companies, large and small to run their operations and we continue to grow. About US&#x24;5 billion of eCommerce orders are shipped a year via ShipHero. Our customers sell on Shopify, Amazon, Etsy, eBay, WooCommerce, BigCommerce, and many other platforms. We&apos;re driven to help our customers grow their businesses by providing a platform that solves complex problems, and is engineered to be reliable and fast. We are obsessed with building great technology that is beautiful, easy to use, and is loved by our customers. Our culture also reflects our ethos and belief that by bringing passionate, talented, and great people together &#x2013; you can do great things.</p> 
 <p>Our team is fully remote, the company has always been remote. We communicate regularly using video chat and Slack and put a strong emphasis on asynchronous work so people have large chunks of uninterrupted time to focus and do deep work.</p> 
 <h2 class=""jobSectionHeader""><b>About You</b></h2> 
 <ul>
  <li>You understand that great things are accomplished when teams work together.</li>
  <li> You&apos;ve made a lot of mistakes, and most importantly, have learned from them.</li>
  <li> You are experienced in operating and improving the reliability of data storage and processing systems (relational databases, data warehouses, data lakes and distributed processing systems), including operational optimization (e.g. indexes, query tuning, monitoring).</li>
  <li> You have a solid understanding of stream processing and operating streaming solutions (using Kafka/Kinesis or some other solution) and/or CDC workloads.</li>
  <li> You have experience in planning, provisioning, scaling and maintaining reliable data processing systems in AWS or GCP (using Terraform/Ansible).</li>
  <li> You are comfortable with Python and are familiar with maintaining ETL jobs using Airflow or some other solution.</li>
  <li> You are always eager to learn more and love to try out new solutions on your own.</li>
  <li> Experience with the AWS data ecosystem is highly appreciated (Amazon Aurora MySQL, DocumentDB/MongoDB, OpenSearch/ElasticSearch, Redshift, Glue/Spark, MSK/Kafka, Kinesis, Debezium, S3/Apache Hudi, MWAA/Airflow)</li>
 </ul> 
 <h2 class=""jobSectionHeader""><b>The Role</b></h2>
 <ul>
  <li> Operate, improve and extend ShipHero&apos;s existing data infrastructure.</li>
  <li> Support the teams working on improving the performance of reports, listings, searches and other data and analytics services.</li>
  <li> Collaborate with our DevOps team to identify and remediate current and future data infrastructure performance or reliability issues.</li>
  <li> Work with ETL and streaming solutions.</li>
  <li> Review features and requirements, design and implement solutions together with our data engineers, data scientists, developers and designers.</li>
 </ul>
 <p></p> 
 <h2 class=""jobSectionHeader""><b>The Perks</b></h2> 
 <ul>
  <li>&#x24;2.500 so you can buy any equipment you need to be happy at your job</li>
  <li> 20 days paid vacation + new year &amp; Christmas</li>
  <li> Conference days don&apos;t count against your vacation days, we want you to stay up-to-date</li>
  <li> We will pay for courses &amp; conferences, if you learn we all learn</li>
 </ul>
</div>",https://shiphero.breezy.hr/p/41dafea81338-senior-data-platform-reliability-engineer?source=indeed&ittk=AITEBWA4YD,c7bdf04e7b7bd148,,Full-time,,,Remote,Senior Data Platform Reliability Engineer,30+ days ago,2023-09-18T13:36:00.732Z,3.4,25.0,"$90,000 - $120,000 a year",2023-10-18T13:36:00.735Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=c7bdf04e7b7bd148&from=jasx&tk=1hd1fvosi2gtk000&vjs=3
136,ManTech International Corporation,"Secure our Nation, Ignite your Future 
 
 Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement. 
 
 
  Currently, ManTech is seeking a motivated, career and customer-oriented Data Services Lead Engineer to join our team! 
  
  This is a remote, technical position with an Agile Scrum team that will require a broad scope of skills and knowledge of DoD systems. 
  
  Responsibilities include but are not limited to : 
  
   Designs, develops, tests, debugs, and implements operating systems components, software tools and utilities. 
   Ensures that system improvements are successfully implemented and monitored to increase efficiency. 
   Generates systems software engineering policies, standards, and procedures. 
   Demonstrates proficiency in Java, Python, JavaScript, and HTML5 for software development and integration within the MIP. 
   Operating Systems: Experience with both Windows and RedHat Linux operating systems for deploying and managing data services and integrations. 
   JavaScript and Web Development: Proficiency in JavaScript, JSON, HTML5, CSS, and AJAX, along with experience in jQuery for web interface development and integration. 
   Development Tools: Familiarity with Integrated Development Environments (IDEs) such as Eclipse and IntelliJ for efficient software development. 
   Uses debugging tools and methodologies to troubleshoot and debug UI and software components. 
   Demonstrates ability to think critically and creatively, contributing to the development of innovative solutions to software problems within the MIP. 
   Writes automation test cases to validate system requirements, ensuring the reliability and performance of data services and integrations. 
  
  
  Basic Qualifications: 
  
   A minimum of 4 years of professional experience in software development. 
   Expertise in Java: Strong expertise in Java, including Java Web Containers (Tomcat/JBoss) and utilization of technologies like JDBC for database connectivity. 
   System Integration: Extensive experience in project, product, tools, and system integration, with a track record of successfully integrating components within the MIP. 
   Team Collaboration: Proven ability to work effectively in a team environment, fostering collaboration and achieving project goals collectively. 
   Experience with Operating Systems like Windows and RedHat Linux 
   Strong knowledge of interface development and experience with HTML 5, Java, JavaScript, CSS, and AJAX, JQuery, JSON, Python. 
   Experience utilizing debugging tools and methodologies for debugging UI and components. 
   Excellent oral and written communication skills, and ability to facilitate effective team discussions. 
   Demonstrates the ability to think critically and creatively to develop innovative solutions to software problems. 
   Familiarity with Agile methodologies and frameworks. 
   Solid understanding of software development life cycle (SDLC), iterative development, and release management processes. 
   Excellent oral and written communication skills, ability to collaborate with diverse stakeholders, and facilitate effective team discussions. 
  
  
  Preferred Qualifications: 
  
   Bachelor’s degree in Information Technology, or a related field. 
   Working knowledge of military health systems, along with the positive track record in the DHA (Defense Health Agency) customer space. 
  
  
  Security Clearance Requirement: 
  
   US Citizenship. 
   Must have an active Public Trust clearance. 
   Hired candidate will be processed for a Secret clearance. 
  
  
  Physical Requirements: 
  
   Sedentary work that primarily involves sitting/standing/walking/Talking and must be able to remain in a stationary position 50%. 
   Moving about to accomplish tasks or moving from one work site to another. 
   The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc. 
   Requires frequently communicates with co-workers, management, and customers. 
   Communicating with others to exchange information. 
   Working with computers. 
   Must be able to lift and move hardware weighing up to 50 pounds. 
  
 
 The projected compensation range for this position is $82,400-$137,000. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it’s employees beyond just compensation. ManTech’s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections. 
 
 For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone. 
 
 
  
   
    
     
      
       
        
         
          
           
            
             
              
               
                
                 
                  
                   
                    
                     
                      
                       ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law. 
                       
                       If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services. 
                       
                       If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access","<div>
 <p><b>Secure our Nation, Ignite your Future </b></p>
 <p></p>
 <p>Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At <b>ManTech, </b>you&#x2019;ll help protect our national security while working on innovative projects that offer opportunities for advancement. </p>
 <p></p>
 <div>
  <p>Currently, <b>ManTech </b>is seeking a motivated, career and customer-oriented <b>Data Services </b><b>Lead Engineer </b>to join our team! </p>
  <p></p>
  <p><b>This is a remote, technical position with an Agile Scrum team that will require a broad scope of skills and knowledge of DoD systems. </b></p>
  <p></p>
  <p><b>Responsibilities include but are not limited to </b>: </p>
  <ul>
   <li><p>Designs, develops, tests, debugs, and implements operating systems components, software tools and utilities. </p></li>
   <li><p>Ensures that system improvements are successfully implemented and monitored to increase efficiency. </p></li>
   <li><p>Generates systems software engineering policies, standards, and procedures. </p></li>
   <li><p>Demonstrates proficiency in Java, Python, JavaScript, and HTML5 for software development and integration within the MIP. </p></li>
   <li><p>Operating Systems: Experience with both Windows and RedHat Linux operating systems for deploying and managing data services and integrations. </p></li>
   <li><p>JavaScript and Web Development: Proficiency in JavaScript, JSON, HTML5, CSS, and AJAX, along with experience in jQuery for web interface development and integration. </p></li>
   <li><p>Development Tools: Familiarity with Integrated Development Environments (IDEs) such as Eclipse and IntelliJ for efficient software development. </p></li>
   <li><p>Uses debugging tools and methodologies to troubleshoot and debug UI and software components. </p></li>
   <li><p>Demonstrates ability to think critically and creatively, contributing to the development of innovative solutions to software problems within the MIP. </p></li>
   <li><p>Writes automation test cases to validate system requirements, ensuring the reliability and performance of data services and integrations. </p></li>
  </ul>
  <p></p>
  <p><b>Basic Qualifications: </b></p>
  <ul>
   <li><p>A minimum of 4 years of professional experience in software development. </p></li>
   <li><p>Expertise in Java: Strong expertise in Java, including Java Web Containers (Tomcat/JBoss) and utilization of technologies like JDBC for database connectivity. </p></li>
   <li><p>System Integration: Extensive experience in project, product, tools, and system integration, with a track record of successfully integrating components within the MIP. </p></li>
   <li><p>Team Collaboration: Proven ability to work effectively in a team environment, fostering collaboration and achieving project goals collectively. </p></li>
   <li><p>Experience with Operating Systems like Windows and RedHat Linux </p></li>
   <li><p>Strong knowledge of interface development and experience with HTML 5, Java, JavaScript, CSS, and AJAX, JQuery, JSON, Python. </p></li>
   <li><p>Experience utilizing debugging tools and methodologies for debugging UI and components. </p></li>
   <li><p>Excellent oral and written communication skills, and ability to facilitate effective team discussions. </p></li>
   <li><p>Demonstrates the ability to think critically and creatively to develop innovative solutions to software problems. </p></li>
   <li><p>Familiarity with Agile methodologies and frameworks. </p></li>
   <li><p>Solid understanding of software development life cycle (SDLC), iterative development, and release management processes. </p></li>
   <li><p>Excellent oral and written communication skills, ability to collaborate with diverse stakeholders, and facilitate effective team discussions. </p></li>
  </ul>
  <p></p>
  <p><b>Preferred Qualifications: </b></p>
  <ul>
   <li><p>Bachelor&#x2019;s degree in Information Technology, or a related field. </p></li>
   <li><p>Working knowledge of military health systems, along with the positive track record in the DHA (Defense Health Agency) customer space.<br> </p></li>
  </ul>
  <p></p>
  <p><b>Security Clearance Requirement: </b></p>
  <ul>
   <li><p>US Citizenship. </p></li>
   <li><p>Must have an active Public Trust clearance. </p></li>
   <li><p>Hired candidate will be processed for a Secret clearance. </p></li>
  </ul>
  <p></p>
  <p><b>Physical Requirements: </b></p>
  <ul>
   <li><p>Sedentary work that primarily involves sitting/standing/walking/Talking and must be able to remain in a stationary position 50%. </p></li>
   <li><p>Moving about to accomplish tasks or moving from one work site to another. </p></li>
   <li><p>The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc. </p></li>
   <li><p>Requires frequently communicates with co-workers, management, and customers. </p></li>
   <li><p>Communicating with others to exchange information. </p></li>
   <li><p>Working with computers. </p></li>
   <li><p>Must be able to lift and move hardware weighing up to 50 pounds. </p></li>
  </ul>
 </div>
 <p></p>The projected compensation range for this position is &#x24;82,400-&#x24;137,000. There are differentiating factors that can impact a final salary/hourly rate, including, but not limited to, Contract Wage Determination, relevant work experience, skills and competencies that align to the specified role, geographic location (For Remote Opportunities), education and certifications as well as Federal Government Contract Labor categories. In addition, ManTech invests in it&#x2019;s employees beyond just compensation. ManTech&#x2019;s benefits offerings include, dependent upon position, Health Insurance, Life Insurance, Paid Time Off, Holiday Pay, Short Term and Long Term Disability, Retirement and Savings, Learning and Development opportunities, wellness programs as well as other optional benefit elections. 
 <p></p>
 <p>For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone. </p>
 <p></p>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                <div>
                 <div>
                  <div>
                   <div>
                    <div>
                     <div>
                      <div>
                       <p>ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law. </p>
                       <p></p>
                       <p>If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech&apos;s Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech&apos;s utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services. </p>
                       <p></p>
                       <p>If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access</p>
                      </div>
                     </div>
                    </div>
                   </div>
                  </div>
                 </div>
                </div>
               </div>
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>",https://mantech.wd1.myworkdayjobs.com/en-US/External/job/USA-Remote-Work/Data-Services-Lead-Engineer--Remote-_R42973?source=Indeed,8b7d96de56a29c5d,,Full-time,,,Remote,Data Services Lead Engineer (Remote),30+ days ago,2023-09-18T13:36:28.725Z,3.9,1701.0,"$82,400 - $137,000 a year",2023-10-18T13:36:28.729Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=8b7d96de56a29c5d&from=jasx&tk=1hd1fvosi2gtk000&vjs=3
137,GEICO,"Position Summary
 
 
   GEICO is seeking an experienced Principal Engineer with a passion for building high performance, low maintenance, zero-downtime platforms, and applications. You will help drive our insurance business transformation as we transition from a traditional IT model to a tech organization with engineering excellence as its mission, while co-creating the culture of psychological safety and continuous improvement.
 
 
 
   Position Description
 
 
   Our Principal Engineer works with our Distinguished Engineer and Sr. Engineers to innovate and build new systems, improve, and enhance existing systems as well as identify new opportunities to apply your knowledge to solve critical problems. You will lead the strategy and execution of a technical roadmap that will increase the velocity of delivering products and unlock new engineering capabilities. The ideal candidate has deep technical expertise in software engineering, cryptography, and open-source IaaS platform domain
 
 
 
   Position Responsibilities
 
 
   As a Principal Engineer, you will:
 
 
   Focus on multiple areas and provide technical and thought leadership to the enterprise
   Collaborate with product managers, team members, customers, and other engineering teams to solve our toughest problems
   Develop and execute technical software development strategy for the IaaS Engineering domain, while optimizing for performance and efficiency
   Own accountability for the quality, usability, and performance of the solutions
   Be a role model and mentor, helping to coach and strengthen the technical expertise and know-how of our engineering and product community. Influence and educate executives
   Consistently share best practices and improve processes within and across teams
   Analyze cost and forecast, incorporating them into business plans
   Determine and support resource requirements, evaluate operational processes, measure outcomes to ensure desired results, demonstrate adaptability and sponsor continuous learning
   Take on-call and operation support
 
 
 
   Qualifications
 
 
   Strong foundations in software engineering
   Deep hands-on experience in complex system design and data pipeline and architectures, scale and performance, tuning, with good knowledge on Docker and Kubernetes
   Professional experience in software development at least one modern programming language, including Go, Python, Java, or Rust
   Hands-on experience with public and/or private cloud environments (OpenShift, Kubernetes, Azure, AWS, GCP, etc.)
   Experience and technical knowledge of security engineering, system and network security, authentication and security protocols and cryptography
   Experience in CI/CD pipeline and related open-source tools like GIT/Jenkin/CircleCI/SonarQube and knowledge in Terraform will be big plus
   Demonstrated ability to design and implement resilient, scalable, and efficient solutions
   Strong problem-solving abilities and a proactive approach to identifying and mitigating risks
   Excellent communication skills, able to communicate complex technical concepts to technical and non-technical stakeholders
   Knowledge on Open-source monitoring software like Grafana and Prometheus
   One of more of the following certifications are highly desired:
   Certified Information Systems Security Professional (CISSP)
   Certified Information Security Manager (CISM)
   o Certified Information Systems Auditor (CISA)
 
 
 
   Experience
 
 
   6+ years of professional IaaS experience
   4+ years of experience in open-source frameworks
   3+ years of experience with architecture and design
   3+ years of experience with AWS, GCP, Azure, or another cloud service
   1+ years of people management experience
 
 
 
   Education
 
 
   Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience
 
 
   Benefits:
   As an Associate, you’ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including:
   
  
   Premier Medical, Dental and Vision Insurance with no waiting period**
   Paid Vacation, Sick and Parental Leave
   401(k) Plan
   Tuition Reimbursement
   Paid Training and Licensures
   Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
   
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
   
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
 
 
 
   #LI-RP2
 
 
   #DICE
 
 
 
   Annual Salary
  $100,000.00 - $204,500.00
 
   The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.
 
  
  At this time, GEICO will not sponsor a new applicant for employment authorization for this position.
 
 
   Benefits:
 
 
   As an Associate, you’ll enjoy our 
  
   Total Rewards Program
  
  
   to help secure your financial future and preserve your health and well-being, including:
  
 
 
   Premier Medical, Dental and Vision Insurance with no waiting period**
   Paid Vacation, Sick and Parental Leave
   401(k) Plan
   Tuition Reimbursement
   Paid Training and Licensures
 
 
 
  Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
 
 
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
 
 
 
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
 
 
 
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.","<div>
 <div>
  Position Summary
 </div>
 <div>
   GEICO is seeking an experienced Principal Engineer with a passion for building high performance, low maintenance, zero-downtime platforms, and applications. You will help drive our insurance business transformation as we transition from a traditional IT model to a tech organization with engineering excellence as its mission, while co-creating the culture of psychological safety and continuous improvement.
 </div>
 <div></div>
 <div>
   Position Description
 </div>
 <div>
   Our Principal Engineer works with our Distinguished Engineer and Sr. Engineers to innovate and build new systems, improve, and enhance existing systems as well as identify new opportunities to apply your knowledge to solve critical problems. You will lead the strategy and execution of a technical roadmap that will increase the velocity of delivering products and unlock new engineering capabilities. The ideal candidate has deep technical expertise in software engineering, cryptography, and open-source IaaS platform domain
 </div>
 <div></div>
 <div>
   Position Responsibilities
 </div>
 <div>
   As a Principal Engineer, you will:
 </div>
 <ul>
  <li> Focus on multiple areas and provide technical and thought leadership to the enterprise</li>
  <li> Collaborate with product managers, team members, customers, and other engineering teams to solve our toughest problems</li>
  <li> Develop and execute technical software development strategy for the IaaS Engineering domain, while optimizing for performance and efficiency</li>
  <li> Own accountability for the quality, usability, and performance of the solutions</li>
  <li> Be a role model and mentor, helping to coach and strengthen the technical expertise and know-how of our engineering and product community. Influence and educate executives</li>
  <li> Consistently share best practices and improve processes within and across teams</li>
  <li> Analyze cost and forecast, incorporating them into business plans</li>
  <li> Determine and support resource requirements, evaluate operational processes, measure outcomes to ensure desired results, demonstrate adaptability and sponsor continuous learning</li>
  <li> Take on-call and operation support</li>
 </ul>
 <div></div>
 <div>
   Qualifications
 </div>
 <ul>
  <li> Strong foundations in software engineering</li>
  <li> Deep hands-on experience in complex system design and data pipeline and architectures, scale and performance, tuning, with good knowledge on Docker and Kubernetes</li>
  <li> Professional experience in software development at least one modern programming language, including Go, Python, Java, or Rust</li>
  <li> Hands-on experience with public and/or private cloud environments (OpenShift, Kubernetes, Azure, AWS, GCP, etc.)</li>
  <li> Experience and technical knowledge of security engineering, system and network security, authentication and security protocols and cryptography</li>
  <li> Experience in CI/CD pipeline and related open-source tools like GIT/Jenkin/CircleCI/SonarQube and knowledge in Terraform will be big plus</li>
  <li> Demonstrated ability to design and implement resilient, scalable, and efficient solutions</li>
  <li> Strong problem-solving abilities and a proactive approach to identifying and mitigating risks</li>
  <li> Excellent communication skills, able to communicate complex technical concepts to technical and non-technical stakeholders</li>
  <li> Knowledge on Open-source monitoring software like Grafana and Prometheus</li>
  <li> One of more of the following certifications are highly desired:</li>
  <li> Certified Information Systems Security Professional (CISSP)</li>
  <li> Certified Information Security Manager (CISM)</li>
  <li> o Certified Information Systems Auditor (CISA)</li>
 </ul>
 <div></div>
 <div>
   Experience
 </div>
 <ul>
  <li> 6+ years of professional IaaS experience</li>
  <li> 4+ years of experience in open-source frameworks</li>
  <li> 3+ years of experience with architecture and design</li>
  <li> 3+ years of experience with AWS, GCP, Azure, or another cloud service</li>
  <li> 1+ years of people management experience</li>
 </ul>
 <div></div>
 <div>
   Education
 </div>
 <ul>
  <li> Bachelor&#x2019;s degree in Computer Science, Information Systems, or equivalent education or work experience</li>
 </ul>
 <div>
  <br> Benefits:
  <br> As an Associate, you&#x2019;ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including:
  <br> 
  <ul>
   <li>Premier Medical, Dental and Vision Insurance with no waiting period**</li>
   <li>Paid Vacation, Sick and Parental Leave</li>
   <li>401(k) Plan</li>
   <li>Tuition Reimbursement</li>
   <li>Paid Training and Licensures</li>
   <li>Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.</li>
  </ul> **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
  <br> 
  <br> The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
  <br> 
  <br> GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
 </div>
 <div></div>
 <div>
   #LI-RP2
 </div>
 <div>
   #DICE
 </div>
 <div></div>
 <div>
  <br> Annual Salary
 </div> &#x24;100,000.00 - &#x24;204,500.00
 <div>
   The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate&#x2019;s work experience, education and training, the work location as well as market and business considerations.
 </div>
 <br> 
 <div></div> At this time, GEICO will not sponsor a new applicant for employment authorization for this position.
 <div></div>
 <div>
  <br> Benefits:
 </div>
 <div>
   As an Associate, you&#x2019;ll enjoy our 
  <div>
   Total Rewards Program
  </div>
  <ul>
   <li>to help secure your financial future and preserve your health and well-being, including:</li>
  </ul>
 </div>
 <ul>
  <li> Premier Medical, Dental and Vision Insurance with no waiting period**</li>
  <li> Paid Vacation, Sick and Parental Leave</li>
  <li> 401(k) Plan</li>
  <li> Tuition Reimbursement</li>
  <li> Paid Training and Licensures</li>
 </ul>
 <div></div>
 <ul>
  <li>Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.</li>
 </ul>
 <div>
   **Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
 </div>
 <div></div>
 <div>
   The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
 </div>
 <div></div>
 <div>
   GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
 </div>
</div>",https://geico.wd1.myworkdayjobs.com/en-US/External/job/Chevy-Chase-MD/Principal-Engineer---IaaS--Cryptography-and-Data-Protection---REMOTE-_R0046548,9db0754fcf79928c,,Full-time,,,"Chevy Chase, MD",Principal Software Engineer – IaaS (Cryptography and Data Protection) (REMOTE),30+ days ago,2023-09-18T13:36:33.001Z,3.2,7497.0,"$100,000 - $204,500 a year",2023-10-18T13:36:33.022Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=9db0754fcf79928c&from=jasx&tk=1hd1fvosi2gtk000&vjs=3
140,Balsam Brands,"Job Description
  As Data Engineer, you will be responsible for designing and developing robust and scalable data warehousing solutions. The Data Engineer will be responsible for building data solutions based on the business requirements. Data solutions may involve retrieval, transformation, storage, and delivery of the data. The Data Engineer must follow standards and implement best practices while writing code and provide production support for the enterprise data warehouse. Our ideal candidate is a skillful data wrangler who enjoys building data solutions from the ground up and optimizing their performance.
  This full-time position reports to the Manager of Data Engineering and can work remote from any U.S. state where Balsam Brands is currently setup as an employer, which includes: CA, CO, FL, GA, ID, IL, IN, KS, KY, MD, MA, MO, NJ, NC, OH, OR, PA, TN, TX, VA, and WA. This role can also work locally in our Redwood City, CA or Boise, ID office location. Our local teams work in a hybrid model, which currently includes Tuesday and Wednesday in-office.
  To ensure sufficient overlap with functional and cross-functional team members globally, some flexibility with this role's regular work schedule will be required. Most of our teams have overlap with early morning and/or early evening PST. Specific scheduling needs for this role will be discussed in the initial interview. 
 What you’ll do: 
 
  Be accountable for building and maintaining the data infrastructure for the organization
   Collaborate with systems analysts and cross functional partners to understand data requirements
   Champion data warehouse, create denormalized data foundation layer and normalized data marts
   Define strategies to capture all data sources and impact of business process changes on data coming from those sources
   Work on all aspects of the data warehouse/BI environment including architecture, design, development, automation, caching and performance tuning
   Continually explore new technologies like Big Data, Artificial Intelligence, Generative AI, Machine Learning, and Predictive Data Modeling
 
  What you bring to the table: 
 
  5+ years of professional experience in the data engineering field
   Demonstrated history of designing and building schemas, tables, views, and data pipelines
   Experience in cloud technologies like Azure, AWS
   Experience in Azure Data Factory (ADF) or equivalent ETL tool
   Knowledge and experience of working with SQL and relational databases like SQL Server, Oracle, Postgres and MySQL
   Ability to understand and tell the story embedded in the data at the core of our business
   Ability to communicate with non-technical audience from a variety of business functions
   Strong knowledge of coding standards, best practices and data governance
 
  Travel for remote team members: At Balsam Brands, we believe that time spent together, in-person, collaborating and building relationships is important to who we are. For our newest remote Brandits, we will arrange travel to one of our local offices within your first three months of employment so you can meet and train with your new team in-person. You may also get to travel an additional 1 – 2 times a year for events such as team retreats, offsites, or learning and development opportunities.
  Notes: This is a full-time, permanent position with benefits. Please submit a cover letter and resume, and only apply if you are able to live and work full-time in one of the states listed in this posting. State locations and specifics are subject to change as our hiring requirements shift.
  About Us: Balsam Brands is a global, eCommerce retailer with roots in holiday and home décor. We strive for excellence in everything we do and present a unique opportunity for those seeking to have a meaningful impact in a people-first company that values relationship building, authenticity, and doing the right thing. We have steadily growing teams in Boise, the Bay Area, Dublin, the Philippines - and most recently, Windsor, Canada!
  The company's mission is to create joy together. We empower our team and partners to love what they do, provide products and experiences that inspire meaningful moments with family and friends, and give back to our families and communities in impactful ways. When you join Balsam Brands, you'll find a culture of caring people doing challenging work and building a welcoming workplace.
 
   Check out our flagship brand, Balsam Hill: www.balsamhill.com
   Balsam Brands in Forbes: https://bit.ly/balsambrandsforbes
   Balsam Brands on LinkedIn: http://www.linkedin.com/company/balsam-brands/
   Glassdoor: https://bit.ly/balsambrands-glassdoor
 
  At Balsam Brands, we strive to offer a competitive compensation and benefits package. For permanent, full-time team members, our current package includes:
 
   Competitive compensation, including a cash-based incentive plan; salary is reviewed yearly and may be adjusted as part of the normal compensation review process
   Comprehensive Medical, Dental, and Vision coverage, with 100% of monthly premiums covered for team members, and 85%+ employer-paid premiums for other coverage tiers that include dependents
   Up to $2,000 annual funding toward HSA accounts
   Medical, transit, dependent care FSA
   Infertility coverage offered on all medical plans
   Generous parental leave program and flexible return options
   Company-paid life and AD&D insurance
   Company-paid short and long-term disability insurance
   401(k) with dollar-for-dollar company match up to $4,000 per calendar year
   Employee Assistance Program (EAP) and other mental health and wellness perks
   Paid holidays, annual shutdown week, PTO, and volunteer time-off (VTO) packages
   Paid 5-week sabbatical leave after 10 years of employment
   Annual continuous learning benefit up to $1,000 per person, per fiscal year
   Up to $300 flexible reimbursement to support setup of new team member's work-from-home environment
   Generous team member merchandise discount
   Valuable extras: identity theft protection, subsidized parking, monthly wellness, pet insurance, accident & critical illness insurance
 
  The base pay range for this position is: $111,000 to $162,000. Where an individual falls within that range will vary based on several factors including geographic location and may vary depending on candidate qualifications and experience, applicable skills, and other job-related factors. We benchmark our pay ranges against current external data sources and regularly review compensation for our team members. Balsam Brands is committed to providing our team members with an internally fair, externally competitive, and fiscally prudent total compensation package administered in a simple and consistent manner.
  At Balsam Brands, we strive to build a diverse, equitable, and inclusive team to fulfill our purpose to create joy together. Balsam Brands is proud to be an equal opportunity employer. We encourage people from all backgrounds, ages, abilities, and experiences to apply. We do not discriminate on the basis of race, ethnicity, religion, national origin, citizenship, marital or family status, disability, sexual orientation, gender identity or expression, pregnancy or caregiver status, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application and interview process, to perform essential job functions, and to receive other benefits and privileges of employment.
  #DICE
 
 

 Additional Information
  All your information will be kept confidential according to EEO guidelines.","<div>
 Job Description
 <p><br> As Data Engineer, you will be responsible for designing and developing robust and scalable data warehousing solutions. The Data Engineer will be responsible for building data solutions based on the business requirements. Data solutions may involve retrieval, transformation, storage, and delivery of the data. The Data Engineer must follow standards and implement best practices while writing code and provide production support for the enterprise data warehouse. Our ideal candidate is a skillful data wrangler who enjoys building data solutions from the ground up and optimizing their performance.</p>
 <p> This full-time position reports to the Manager of Data Engineering and can work remote from any U.S. state where Balsam Brands is currently setup as an employer, which includes: CA, CO, FL, GA, ID, IL, IN, KS, KY, MD, MA, MO, NJ, NC, OH, OR, PA, TN, TX, VA, and WA. This role can also work locally in our Redwood City, CA or Boise, ID office location. Our local teams work in a hybrid model, which currently includes Tuesday and Wednesday in-office.</p>
 <p> To ensure sufficient overlap with functional and cross-functional team members globally, some flexibility with this role&apos;s regular work schedule will be required. Most of our teams have overlap with early morning and/or early evening PST. Specific scheduling needs for this role will be discussed in the initial interview.<i> </i></p>
 <p><b>What you&#x2019;ll do: </b></p>
 <ul>
  <li>Be accountable for building and maintaining the data infrastructure for the organization</li>
  <li> Collaborate with systems analysts and cross functional partners to understand data requirements</li>
  <li> Champion data warehouse, create denormalized data foundation layer and normalized data marts</li>
  <li> Define strategies to capture all data sources and impact of business process changes on data coming from those sources</li>
  <li> Work on all aspects of the data warehouse/BI environment including architecture, design, development, automation, caching and performance tuning</li>
  <li> Continually explore new technologies like Big Data, Artificial Intelligence, Generative AI, Machine Learning, and Predictive Data Modeling</li>
 </ul>
 <p><b> What you bring to the table: </b></p>
 <ul>
  <li>5+ years of professional experience in the data engineering field</li>
  <li> Demonstrated history of designing and building schemas, tables, views, and data pipelines</li>
  <li> Experience in cloud technologies like Azure, AWS</li>
  <li> Experience in Azure Data Factory (ADF) or equivalent ETL tool</li>
  <li> Knowledge and experience of working with SQL and relational databases like SQL Server, Oracle, Postgres and MySQL</li>
  <li> Ability to understand and tell the story embedded in the data at the core of our business</li>
  <li> Ability to communicate with non-technical audience from a variety of business functions</li>
  <li> Strong knowledge of coding standards, best practices and data governance</li>
 </ul>
 <p><b> Travel for remote team members: </b>At Balsam Brands, we believe that time spent together, in-person, collaborating and building relationships is important to who we are. For our newest remote Brandits, we will arrange travel to one of our local offices within your first three months of employment so you can meet and train with your new team in-person. You may also get to travel an additional 1 &#x2013; 2 times a year for events such as team retreats, offsites, or learning and development opportunities.</p>
 <p><b> Notes: </b>This is a full-time, permanent position with benefits. Please submit a cover letter and resume, and only apply if you are able to live and work full-time in one of the states listed in this posting. State locations and specifics are subject to change as our hiring requirements shift.</p>
 <p><b> About Us: </b>Balsam Brands is a global, eCommerce retailer with roots in holiday and home d&#xe9;cor. We strive for excellence in everything we do and present a unique opportunity for those seeking to have a meaningful impact in a people-first company that values relationship building, authenticity, and doing the right thing. We have steadily growing teams in Boise, the Bay Area, Dublin, the Philippines - and most recently, Windsor, Canada!</p>
 <p> The company&apos;s mission is to create joy together. We empower our team and partners to love what they do, provide products and experiences that inspire meaningful moments with family and friends, and give back to our families and communities in impactful ways. When you join Balsam Brands, you&apos;ll find a culture of caring people doing challenging work and building a welcoming workplace.</p>
 <ul>
  <li> Check out our flagship brand, Balsam Hill: www.balsamhill.com</li>
  <li> Balsam Brands in Forbes: https://bit.ly/balsambrandsforbes</li>
  <li> Balsam Brands on LinkedIn: http://www.linkedin.com/company/balsam-brands/</li>
  <li> Glassdoor: https://bit.ly/balsambrands-glassdoor</li>
 </ul>
 <p> At Balsam Brands, we strive to offer a competitive compensation and benefits package. For permanent, full-time team members, our current package includes:</p>
 <ul>
  <li> Competitive compensation, including a cash-based incentive plan; salary is reviewed yearly and may be adjusted as part of the normal compensation review process</li>
  <li> Comprehensive Medical, Dental, and Vision coverage, with 100% of monthly premiums covered for team members, and 85%+ employer-paid premiums for other coverage tiers that include dependents</li>
  <li> Up to &#x24;2,000 annual funding toward HSA accounts</li>
  <li> Medical, transit, dependent care FSA</li>
  <li> Infertility coverage offered on all medical plans</li>
  <li> Generous parental leave program and flexible return options</li>
  <li> Company-paid life and AD&amp;D insurance</li>
  <li> Company-paid short and long-term disability insurance</li>
  <li> 401(k) with dollar-for-dollar company match up to &#x24;4,000 per calendar year</li>
  <li> Employee Assistance Program (EAP) and other mental health and wellness perks</li>
  <li> Paid holidays, annual shutdown week, PTO, and volunteer time-off (VTO) packages</li>
  <li> Paid 5-week sabbatical leave after 10 years of employment</li>
  <li> Annual continuous learning benefit up to &#x24;1,000 per person, per fiscal year</li>
  <li> Up to &#x24;300 flexible reimbursement to support setup of new team member&apos;s work-from-home environment</li>
  <li> Generous team member merchandise discount</li>
  <li> Valuable extras: identity theft protection, subsidized parking, monthly wellness, pet insurance, accident &amp; critical illness insurance</li>
 </ul>
 <p> The base pay range for this position is: &#x24;111,000 to &#x24;162,000. Where an individual falls within that range will vary based on several factors including geographic location and may vary depending on candidate qualifications and experience, applicable skills, and other job-related factors. We benchmark our pay ranges against current external data sources and regularly review compensation for our team members. Balsam Brands is committed to providing our team members with an internally fair, externally competitive, and fiscally prudent total compensation package administered in a simple and consistent manner.</p>
 <p><i> At Balsam Brands, we strive to build a diverse, equitable, and inclusive team to fulfill our purpose to create joy together. Balsam Brands is proud to be an equal opportunity employer. We encourage people from all backgrounds, ages, abilities, and experiences to apply. We do not discriminate on the basis of race, ethnicity, religion, national origin, citizenship, marital or family status, disability, sexual orientation, gender identity or expression, pregnancy or caregiver status, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application and interview process, to perform essential job functions, and to receive other benefits and privileges of employment.</i></p>
 <p> #DICE</p>
</div> 
<br> 
<div>
 Additional Information
 <p><br> All your information will be kept confidential according to EEO guidelines.</p>
</div>",https://jobs.smartrecruiters.com/BalsamBrands/743999935137570-data-engineer-remote-option-,07b20c6371c1a9da,,Full-time,,,"Boise, ID",Data Engineer (Remote Option),12 days ago,2023-10-06T13:36:49.456Z,3.9,14.0,"$111,000 - $162,000 a year",2023-10-18T13:36:49.458Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=07b20c6371c1a9da&from=jasx&tk=1hd1g1f082gvk000&vjs=3
142,Fusemachines,"About Fusemachines
  Fusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer Maskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI. With a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time employees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.
  About the role:
  This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and maintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and Advanced Analytics).
  Salary Range: US$7000/month
  Qualification / Skill Set Requirement:
 
   3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)
   Proven experience as a Snowflake Developer, with a strong understanding of Snowflake architecture and concepts.
   Proficient in snowflake services such as snowpipe, stages, stored procedures, views, materialized views, tasks and streams.
   Strong programming skills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and manipulation.
   Robust understanding of data partitioning and other optimization techniques in Snowflake.
   Knowledge of data security measures in Snowflake, including role-based access control (RBAC) and data encryption.
   Highly skilled in one or more languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration, storage, processing and manipulation.
   Strong knowledge of SDLC tools and technologies, including project management software (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or similar) and binary repository manager (AWS CodeArtifact or similar).
   Skilled in Data Integration from different sources such as APIs, databases, flat files, event streaming.
   Good understanding of Data Modeling and Database Design Principles. Being able to design and implement efficient database schemas that meet the requirements of the data architecture to support data solutions.
   Strong experience in working with ELT and ETL tools and being able to develop custom integration solutions as needed.
   Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and Kafka, to be able to handle large volumes of data.
   Strong experience in designing and implementing Data Warehousing solutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that extract data from source systems, transform it (DBT), and load it into the data warehouse.
   Strong experience in Orchestration using Apache Airflow.
   Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS services like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etc
   Good understanding of Data Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that data is accurate, complete, and consistent.
   Good Problem-Solving skills: being able to troubleshoot data processing pipelines and identify performance bottlenecks and other issues.
 
  Responsibilities:
 
   Follow established design, constructed data architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination. Handle ELT processes, including data extraction, loading, transformation and load data from various sources into Snowflake.
   Ensure the reliability, scalability, and efficiency of data systems are maintained at all times
   Assist in the configuration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior team members.
   Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and Analysts to thoroughly understand data requirements and provide data engineering support.
   Contribute to data quality assurance efforts, such as implementing data validation checks and tests.
   Evaluate and implement cutting-edge technologies and continue learning and expanding skills in data engineering and cloud platforms.
   Develop, design, and execute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance frameworks that align with current analytics demands and industry best practices
   Document data engineering processes and data flows.
   Care about architecture, observability, testing, and building reliable infrastructure and data pipelines.
   Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and performance tuning.
   Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL queries and database operations.
   Assess best practices and design schemas that matches business needs for delivering a modern analytics solution (descriptive, diagnostic, predictive, prescriptive)
   Be an active member of our Agile team, participating in all ceremonies and continuous improvement activities.
 
  Equal Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.
  
 1ukLvPmZsT","<div>
 <p><b>About Fusemachines</b></p>
 <p> Fusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer Maskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI. With a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time employees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.</p>
 <p><b> About the role:</b></p>
 <p> This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and maintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and Advanced Analytics).</p>
 <p> Salary Range: US&#x24;7000/month</p>
 <p><b> Qualification / Skill Set Requirement:</b></p>
 <ul>
  <li> 3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)</li>
  <li> Proven experience as a Snowflake Developer, with a strong understanding of Snowflake architecture and concepts.</li>
  <li> Proficient in snowflake services such as snowpipe, stages, stored procedures, views, materialized views, tasks and streams.</li>
  <li> Strong programming skills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and manipulation.</li>
  <li> Robust understanding of data partitioning and other optimization techniques in Snowflake.</li>
  <li> Knowledge of data security measures in Snowflake, including role-based access control (RBAC) and data encryption.</li>
  <li> Highly skilled in one or more languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration, storage, processing and manipulation.</li>
  <li> Strong knowledge of SDLC tools and technologies, including project management software (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or similar) and binary repository manager (AWS CodeArtifact or similar).</li>
  <li> Skilled in Data Integration from different sources such as APIs, databases, flat files, event streaming.</li>
  <li> Good understanding of Data Modeling and Database Design Principles. Being able to design and implement efficient database schemas that meet the requirements of the data architecture to support data solutions.</li>
  <li> Strong experience in working with ELT and ETL tools and being able to develop custom integration solutions as needed.</li>
  <li> Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and Kafka, to be able to handle large volumes of data.</li>
  <li> Strong experience in designing and implementing Data Warehousing solutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that extract data from source systems, transform it (DBT), and load it into the data warehouse.</li>
  <li> Strong experience in Orchestration using Apache Airflow.</li>
  <li> Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS services like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etc</li>
  <li> Good understanding of Data Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that data is accurate, complete, and consistent.</li>
  <li> Good Problem-Solving skills: being able to troubleshoot data processing pipelines and identify performance bottlenecks and other issues.</li>
 </ul>
 <p><b> Responsibilities:</b></p>
 <ul>
  <li> Follow established design, constructed data architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination. Handle ELT processes, including data extraction, loading, transformation and load data from various sources into Snowflake.</li>
  <li> Ensure the reliability, scalability, and efficiency of data systems are maintained at all times</li>
  <li> Assist in the configuration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior team members.</li>
  <li> Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and Analysts to thoroughly understand data requirements and provide data engineering support.</li>
  <li> Contribute to data quality assurance efforts, such as implementing data validation checks and tests.</li>
  <li> Evaluate and implement cutting-edge technologies and continue learning and expanding skills in data engineering and cloud platforms.</li>
  <li> Develop, design, and execute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance frameworks that align with current analytics demands and industry best practices</li>
  <li> Document data engineering processes and data flows.</li>
  <li> Care about architecture, observability, testing, and building reliable infrastructure and data pipelines.</li>
  <li> Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and performance tuning.</li>
  <li> Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL queries and database operations.</li>
  <li> Assess best practices and design schemas that matches business needs for delivering a modern analytics solution (descriptive, diagnostic, predictive, prescriptive)</li>
  <li> Be an active member of our Agile team, participating in all ceremonies and continuous improvement activities.</li>
 </ul>
 <p><i> Equal Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.</i></p>
 <p> </p>
 <p>1ukLvPmZsT</p>
</div>",https://fusemachines.applytojob.com/apply/1ukLvPmZsT/Data-Engineer?source=INDE,b7211874c261a672,,Full-time,Contract,,Remote,Data Engineer,13 days ago,2023-10-05T13:36:54.738Z,,,"$7,000 a month",2023-10-18T13:36:54.741Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=b7211874c261a672&from=jasx&tk=1hd1g1f082gvk000&vjs=3
143,Healthe systems,"Healthesystems offers workplace flexibility with our Work-From-Home model, and a competitive compensation and benefits package including healthcare coverage, PTO, paid holidays, 401(k), company-provided life insurance/disability coverage, wellness options, and more.
   
   
  Note: we are unable to hire in every state 
   
   
    
     
      Summary: Responsible for the analysis, design, documentation, development, unit testing, and support of Data Integration and database objects development for software applications. Provides support and guidance regarding Data Integration and T-SQL best practices and development standards. Promotes approved agile methodologies, leading the design and development efforts for the agile team. Actively coaches, guides, and mentors team members in providing valuable solutions to our customer. 
       Key Responsibilities: “To simplify complexities for each customer.” 
       
       
        Collaborates with stakeholders and development team members to achieve business results.
        
       
        Work closely with other engineers to integrate databases with other applications.
        
       
        Leads the design, development, and implementation of database applications and solutions for managing and integrating data between operational systems, data repositories, and reporting and analytical applications. This includes but is not limited to ETL, stored procedures, views, and functions.
        
       
        Recommends and provide guidance regarding Data Integration and database development, T-SQL best practices, and standards to the development team members as needed.
        
       
        Create and propose technical design documentation which includes current and future functionality, database objects affected, specifications, and flows/diagrams to detail the proposed database and/or Data Integration implementation.
        
       
        Has a deep understanding of the business processes and the technology platform that enables it.
        
       
        Translates stakeholder’s requirements into common language that can be adopted for the use with Behavior Driven Development (BDD) or Test Driven Development (TDD).
        
       
        Participates in industry and other professional networks to ensure awareness of industry standards, trends and best practices in order to strengthen organizational and technical knowledge.
        
       
        Provides support for investigating and troubleshooting production issues.
        
       
        Promotes the establishment of group standards and processes. Participates in the Communities of Practice.
        
       
        Works continually on improving performance of source code using industry standard methodologies.
        
       
        Helps drive technology direction and choices of technologies by making recommendations based on experience and research.
       
      
       
       
       Qualifications/Education/Certifications: 
       Bachelor's degree from four-year college or university (in Information Technology or Computer Science preferred), plus five to eight years related experience and/or training; or equivalent combination of education and experience. 
       Knowledge, Skills and Abilities: 
       Prefer experience in Healthcare, PBM and/or ABM, workers’ compensation and/or insurance industry. 
       
       Required experience: 
        
         5+ years SQL Server 2008/2014 
        
       
        5+ years Data Integration technologies and principles 
        Advanced knowledge of T-SQL including complex SQL queries (ex: using various joins and sub-queries) and best practices 
        Advanced knowledge of index design and T-SQL performance tuning techniques 
        Advanced experience integrating data from structured and unstructured formats: flat files, XML, EDI, JSON, EXCEL 
        Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases and schemas 
        Advanced knowledge of Data Warehousing methodologies and concepts 
        Experience with TDD / BDD 
       
       The following knowledge is not required, but is preferred: 
       
        Experience with BI Tools is a plus 
        Basic understanding of object oriented programming 
        Experience in distributed architectures such as Microservices, SOA, and RESTful APIs 
        Continuous Integration 
        Cucumber, Gherkin 
        Jira 
       
       Agile Competency Requirements: 
       
        Requires an understanding of the application of Agile development methodology. 
        Must be comfortable with change, close collaboration, and have conflict resolution skills. 
        Knowledge of or willingness to learn Agile / DevOps values. 
        Takes initiative and are passionate about what they do. 
        Adaption, Ability & Desire to Learn, Team Oriented - tolerance & helpful, and Quality Focus 
        
      
      Physical Demands/Working Conditions: 
       Duties are performed primarily in a home office setting utilizing computer equipment. Travel to attend meetings and visit locations throughout the country may be required. While performing the duties of this job, the employee is regularly required to sit and talk or hear. The employee is frequently required to use hands. The employee is occasionally required to stand and walk.*** Job descriptions will be reviewed and are subject to changes of business necessity. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. 
      
    
   
  
  
   
    
     Pay is based on several factors including but not limited to education, work experience, certifications, geographical cost of labor, etc. In addition to base pay, Healthesystems offers a comprehensive benefits package including, health, dental, vision, disability and life insurance, wellness resources, recognition programs, 401k contribution, and PTO & Holiday pay (all subject to eligibility requirements). Applicable statutory benefits also provided. https://healthesystems.com/careers/
    
    
      Anticipated Starting Pay Range
    
    
      $98,100—$135,000 USD
    
   
   
  
   To facilitate working from home, and as a requirement for this role, candidates must provide their own reliable, high speed internet access with sufficient bandwidth to execute all job functions. Company laptop will be provided.","<p></p>
<div>
 <div>
  <div>
   <div>
    <b>Healthesystems offers workplace flexibility with our Work-From-Home model, and a competitive compensation and benefits package including healthcare coverage, PTO, paid holidays, 401(k), company-provided life insurance/disability coverage, wellness options, and more.</b>
   </div>
  </div> 
  <p><i>Note: we are unable to hire in every state</i></p> 
  <div> 
   <div>
    <div>
     <div>
      <p><b>Summary:</b> Responsible for the analysis, design, documentation, development, unit testing, and support of Data Integration and database objects development for software applications. Provides support and guidance regarding Data Integration and T-SQL best practices and development standards. Promotes approved agile methodologies, leading the design and development efforts for the agile team. Actively coaches, guides, and mentors team members in providing valuable solutions to our customer.</p> 
      <p><b> </b><b>Key Responsibilities: &#x201c;To simplify complexities for each customer.&#x201d;</b></p> 
      <div> 
       <ul>
        <li>Collaborates with stakeholders and development team members to achieve business results.</li>
       </ul> 
       <ul>
        <li>Work closely with other engineers to integrate databases with other applications.</li>
       </ul> 
       <ul>
        <li>Leads the design, development, and implementation of database applications and solutions for managing and integrating data between operational systems, data repositories, and reporting and analytical applications. This includes but is not limited to ETL, stored procedures, views, and functions.</li>
       </ul> 
       <ul>
        <li>Recommends and provide guidance regarding Data Integration and database development, T-SQL best practices, and standards to the development team members as needed.</li>
       </ul> 
       <ul>
        <li>Create and propose technical design documentation which includes current and future functionality, database objects affected, specifications, and flows/diagrams to detail the proposed database and/or Data Integration implementation.</li>
       </ul> 
       <ul>
        <li>Has a deep understanding of the business processes and the technology platform that enables it.</li>
       </ul> 
       <ul>
        <li>Translates stakeholder&#x2019;s requirements into common language that can be adopted for the use with Behavior Driven Development (BDD) or Test Driven Development (TDD).</li>
       </ul> 
       <ul>
        <li>Participates in industry and other professional networks to ensure awareness of industry standards, trends and best practices in order to strengthen organizational and technical knowledge.</li>
       </ul> 
       <ul>
        <li>Provides support for investigating and troubleshooting production issues.</li>
       </ul> 
       <ul>
        <li>Promotes the establishment of group standards and processes. Participates in the Communities of Practice.</li>
       </ul> 
       <ul>
        <li>Works continually on improving performance of source code using industry standard methodologies.</li>
       </ul> 
       <ul>
        <li>Helps drive technology direction and choices of technologies by making recommendations based on experience and research.</li>
       </ul>
      </div>
      <br> 
      <p></p> 
      <p><b> Qualifications/Education/Certifications:</b></p> 
      <p> Bachelor&apos;s degree from four-year college or university (in Information Technology or Computer Science preferred), plus five to eight years related experience and/or training; or equivalent combination of education and experience.</p> 
      <p><b> Knowledge, Skills and Abilities:</b></p> 
      <p> Prefer experience in Healthcare, PBM and/or ABM, workers&#x2019; compensation and/or insurance industry.</p> 
      <ul> 
       <li>Required experience: 
        <ul>
         <li>5+ years SQL Server 2008/2014</li> 
        </ul></li>
       <ul>
        <li>5+ years Data Integration technologies and principles</li> 
        <li>Advanced knowledge of T-SQL including complex SQL queries (ex: using various joins and sub-queries) and best practices</li> 
        <li>Advanced knowledge of index design and T-SQL performance tuning techniques</li> 
        <li>Advanced experience integrating data from structured and unstructured formats: flat files, XML, EDI, JSON, EXCEL</li> 
        <li>Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases and schemas</li> 
        <li>Advanced knowledge of Data Warehousing methodologies and concepts</li> 
        <li>Experience with TDD / BDD</li> 
       </ul>
       <li>The following knowledge is not required, but is preferred:</li> 
       <ul>
        <li>Experience with BI Tools is a plus</li> 
        <li>Basic understanding of object oriented programming</li> 
        <li>Experience in distributed architectures such as Microservices, SOA, and RESTful APIs</li> 
        <li>Continuous Integration</li> 
        <li>Cucumber, Gherkin</li> 
        <li>Jira</li> 
       </ul>
       <li>Agile Competency Requirements:</li> 
       <ul>
        <li>Requires an understanding of the application of Agile development methodology.</li> 
        <li>Must be comfortable with change, close collaboration, and have conflict resolution skills.</li> 
        <li>Knowledge of or willingness to learn Agile / DevOps values.</li> 
        <li>Takes initiative and are passionate about what they do.</li> 
        <li>Adaption, Ability &amp; Desire to Learn, Team Oriented - tolerance &amp; helpful, and Quality Focus</li> 
       </ul> 
      </ul>
      <p><b>Physical Demands/Working Conditions:</b></p> 
      <p> Duties are performed primarily in a home office setting utilizing computer equipment. Travel to attend meetings and visit locations throughout the country may be required. While performing the duties of this job, the employee is regularly required to sit and talk or hear. The employee is frequently required to use hands. The employee is occasionally required to stand and walk.<i>*** Job descriptions will be reviewed and are subject to changes of business necessity. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.</i></p> 
     </div> 
    </div>
   </div>
  </div>
  <div>
   <div>
    <div>
     <p>Pay is based on several factors including but not limited to education, work experience, certifications, geographical cost of labor, etc. In addition to base pay, Healthesystems offers a comprehensive benefits package including, health, dental, vision, disability and life insurance, wellness resources, recognition programs, 401k contribution, and PTO &amp; Holiday pay (all subject to eligibility requirements). Applicable statutory benefits also provided. https://healthesystems.com/careers/</p>
    </div>
    <div>
      Anticipated Starting Pay Range
    </div>
    <div>
      &#x24;98,100&#x2014;&#x24;135,000 USD
    </div>
   </div>
  </div> 
  <div>
   <p>To facilitate working from home, and as a requirement for this role, candidates must provide their own reliable, high speed internet access with sufficient bandwidth to execute all job functions. Company laptop will be provided.</p>
  </div>
 </div>
</div>",https://healthesystems.com/careers-list/?gh_jid=5763661003,d4ed26d290fe9912,,,,,"Tampa, FL",Senior Data Integration Engineer - Remote,7 days ago,2023-10-11T13:36:50.893Z,3.7,57.0,"$98,100 - $135,000 a year",2023-10-18T13:36:50.896Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d4ed26d290fe9912&from=jasx&tk=1hd1g1f082gvk000&vjs=3
147,GTECH LLC,"Role- Data EngineerLocation- RemoteJob Description:
Undergraduate degree or equivalent work experience
8+ years’ experience in Development, design, test and implementation of complex database programs using Oracle and third-party tools. within a distributed, service-based enterprise environment
4+ years Hands-on development using Oracle PL/SQL.
Demonstrates expertise in a variety of data warehousing and business intelligence concepts, practices, and procedures.
Strong experience with oracle functions, procedures, triggers, packages & performance tuning,
Significant experience and comfortable with production support (and willing to take on slots within our 24/7 support rotation).Providing technical assistance, problem resolution and troubleshooting support issues.
Analytical approach to problem solving
At least 2+ year practical experience of developing solutions hosting within key major cloud providers such as OpenShift, Kubernetes, AWS, Google Cloud and Azure
Experience in using modern software engineering and product development tools including Agile / SAFE, Continuous Integration, Continuous Delivery, DevOps etc.
Demonstrate being an avid supporter of the Open-Source software community
Excellent time management, communication, decision making, and presentation skills
Display a strong desire to achieve and attain high levels of both internal and external customer satisfaction
Strong experience of operating in a quickly changing environment and driving technological innovation to meet business requirement
Proven track record of building relationships across cross-functional teams
Positive attitude and easy to work with
Initiative-taker. Has grit and can solve problems without management oversight.
Takes ownership for work. When something goes wrong, stance is introspective rather than blaming.
Engineering mindset(automate manual process) when it comes to ETL processes.
Accustomed to developing code in Git and using CI/CD practices.
Preferred: development experience using Java or open-source technologies, developing Restful APIs
Job Type: Full-time
Salary: $83,521.75 - $130,759.23 per year
Benefits:

 Health insurance

Experience level:

 8 years
 9 years

Schedule:

 Monday to Friday

Work Location: Remote","<p>Role- Data Engineer<br>Location- Remote<br><b>Job Description:</b></p>
<p>Undergraduate degree or equivalent work experience</p>
<p>8+ years&#x2019; experience in Development, design, test and implementation of complex database programs using Oracle and third-party tools. within a distributed, service-based enterprise environment</p>
<p>4+ years Hands-on development using Oracle PL/SQL.</p>
<p>Demonstrates expertise in a variety of data warehousing and business intelligence concepts, practices, and procedures.</p>
<p>Strong experience with oracle functions, procedures, triggers, packages &amp; performance tuning,</p>
<p>Significant experience and comfortable with production support (and willing to take on slots within our 24/7 support rotation).Providing technical assistance, problem resolution and troubleshooting support issues.</p>
<p>Analytical approach to problem solving</p>
<p>At least 2+ year practical experience of developing solutions hosting within key major cloud providers such as OpenShift, Kubernetes, AWS, Google Cloud and Azure</p>
<p>Experience in using modern software engineering and product development tools including Agile / SAFE, Continuous Integration, Continuous Delivery, DevOps etc.</p>
<p>Demonstrate being an avid supporter of the Open-Source software community</p>
<p>Excellent time management, communication, decision making, and presentation skills</p>
<p>Display a strong desire to achieve and attain high levels of both internal and external customer satisfaction</p>
<p>Strong experience of operating in a quickly changing environment and driving technological innovation to meet business requirement</p>
<p>Proven track record of building relationships across cross-functional teams</p>
<p>Positive attitude and easy to work with</p>
<p>Initiative-taker. Has grit and can solve problems without management oversight.</p>
<p>Takes ownership for work. When something goes wrong, stance is introspective rather than blaming.</p>
<p>Engineering mindset(automate manual process) when it comes to ETL processes.</p>
<p>Accustomed to developing code in Git and using CI/CD practices.</p>
<p>Preferred: development experience using Java or open-source technologies, developing Restful APIs</p>
<p>Job Type: Full-time</p>
<p>Salary: &#x24;83,521.75 - &#x24;130,759.23 per year</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,6ca598d4286ea230,,Full-time,,,Remote,Sr. Data Engineer,7 days ago,2023-10-11T13:37:14.443Z,,,"$83,522 - $130,759 a year",2023-10-18T13:37:14.444Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=6ca598d4286ea230&from=jasx&tk=1hd1g273r2gvn004&vjs=3
148,Talent Group,"Must Have: Azure Data Engineering, RDBMS, Airflow, ADF, API, ETL Knowledge, Python, Communication, L1 Support
 Should have 8+ Years of Experience
 Provide L1 support - job monitoring, re-run failed jobs, analyze reasons for failures, bug fixes.
 Strong Expertise in Azure Cloud.

Job Types: Permanent, Full-time
Salary: Up to $110,000.00 per year
Benefits:

 401(k)

Experience level:

 8 years

Schedule:

 8 hour shift

Work Location: Remote","<ul>
 <li><b>Must Have:</b> Azure Data Engineering, RDBMS, Airflow, ADF, API, ETL Knowledge, Python, Communication, L1 Support</li>
 <li>Should have 8+ Years of Experience</li>
 <li>Provide L1 support - job monitoring, re-run failed jobs, analyze reasons for failures, bug fixes.</li>
 <li>Strong Expertise in Azure Cloud.</li>
</ul>
<p>Job Types: Permanent, Full-time</p>
<p>Salary: Up to &#x24;110,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,e7aca82a03dbb9b5,,Full-time,Permanent,,Remote,Azure Data Engineer - Fulltime,8 days ago,2023-10-10T13:37:13.561Z,4.2,20.0,"Up to $110,000 a year",2023-10-18T13:37:13.563Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e7aca82a03dbb9b5&from=jasx&tk=1hd1g273r2gvn004&vjs=3
149,Novul Solutions,"Location: Ashburn, VA
Work Schedule: Fully remote with occasional onsite meetings
Clearance: DoD Top Secret or CBP Background Investigation
As a Data Engineer, you will play a crucial role in managing and optimizing data. Your responsibilities will involve handling database systems, data warehousing, ETL processing, machine learning support, and programming to ensure efficient data operations.
Responsibilities:

 Utilize SQL and NoSQL to build and manage relational database systems, organizing data in tables with rows and columns.
 Maintain data warehouses that store extensive historical and current data, sourced from various systems like CRM, accounting, and ERP.
 Extract, transform, and load data (ETL) from different sources into data warehouses, ensuring data is in a suitable format for analysis.
 Collaborate with data scientists to implement machine learning algorithms/models for making predictions based on historical data.
 Work with data APIs, enabling software applications to access and retrieve data efficiently.
 Use programming languages such as Python, Java, and Scala to develop data-related solutions.
 While the primary focus is data processing and optimization, having a basic understanding of algorithms and data structures is beneficial for aligning data tasks with overall business goals.

Qualifications:

 Bachelor's degree in a relevant field or equivalent work experience.
 Proven experience in managing database systems (SQL and NoSQL).
 Familiarity with data warehousing solutions and ETL tools.
 Proficiency in Python and at least one other programming language (Java or Scala).
 Experience with data APIs and integrating data into software applications.
 Basic knowledge of machine learning concepts is a plus.
 Strong analytical and problem-solving skills.
 Excellent communication and teamwork abilities.

Benefits
​
Core Benefits:

 Paid Time Off (PTO): TEN (10) Paid days off & FIVE (5) Floating days off.
 Holidays: 11 Paid Holidays. Flex time can be utilized in lieu of holiday time usage.
 Payroll: Paid Bi-Monthly.
 401(k): Partnered with the SECOND LARGEST Retirement plan provider in the U.S. Guaranteed 3% match. Eligibility – 21 years of age or older, after 3 months of employment
 Individual or company-wide performance and recognition awards (Quarterly)

Health Benefits:

 UNITED HEALTHCARE PPO, extensive national coverage.
 INCLUDES: Medical/Dental/Vision/HSA.
 Eligible on the first of the month, immediately after the start date.
 Submit the enrollment form within 30 days
 of your start date otherwise, you will have to wait until October for the new year enrollment.

Quality of Life Benefits:

 Training & Career Development Reimbursement of Tuition and training needed to support career development.
 $150 monthly reimbursement contribution paid monthly towards parking expenses.
 Receipts must be submitted by the close of business on the 25th of each month.
 Reimbursements will be paid on the first payroll AFTER reimbursements are submitted each month.

Special Benefits:

 Performance bonus – Project-based
 Yearly bonus – Company based

​
Job Type: Full-time
Pay: $150,000.00 - $160,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Vision insurance

Experience level:

 3 years

Schedule:

 8 hour shift

Experience:

 Python, Java, and Scala programming language: 3 years (Required)
 ETL: 3 years (Required)
 SQL and noSQL: 3 years (Required)

Security clearance:

 Top Secret (Required)

Work Location: Remote","<p>Location: Ashburn, VA</p>
<p>Work Schedule: Fully remote with occasional onsite meetings</p>
<p>Clearance: DoD Top Secret or CBP Background Investigation</p>
<p>As a Data Engineer, you will play a crucial role in managing and optimizing data. Your responsibilities will involve handling database systems, data warehousing, ETL processing, machine learning support, and programming to ensure efficient data operations.</p>
<p>Responsibilities:</p>
<ul>
 <li>Utilize SQL and NoSQL to build and manage relational database systems, organizing data in tables with rows and columns.</li>
 <li>Maintain data warehouses that store extensive historical and current data, sourced from various systems like CRM, accounting, and ERP.</li>
 <li>Extract, transform, and load data (ETL) from different sources into data warehouses, ensuring data is in a suitable format for analysis.</li>
 <li>Collaborate with data scientists to implement machine learning algorithms/models for making predictions based on historical data.</li>
 <li>Work with data APIs, enabling software applications to access and retrieve data efficiently.</li>
 <li>Use programming languages such as Python, Java, and Scala to develop data-related solutions.</li>
 <li>While the primary focus is data processing and optimization, having a basic understanding of algorithms and data structures is beneficial for aligning data tasks with overall business goals.</li>
</ul>
<p>Qualifications:</p>
<ul>
 <li>Bachelor&apos;s degree in a relevant field or equivalent work experience.</li>
 <li>Proven experience in managing database systems (SQL and NoSQL).</li>
 <li>Familiarity with data warehousing solutions and ETL tools.</li>
 <li>Proficiency in Python and at least one other programming language (Java or Scala).</li>
 <li>Experience with data APIs and integrating data into software applications.</li>
 <li>Basic knowledge of machine learning concepts is a plus.</li>
 <li>Strong analytical and problem-solving skills.</li>
 <li>Excellent communication and teamwork abilities.</li>
</ul>
<p><b>Benefits</b></p>
<p>&#x200b;</p>
<p><b>Core Benefits:</b></p>
<ul>
 <li>Paid Time Off (PTO): TEN (10) Paid days off &amp; FIVE (5) Floating days off.</li>
 <li>Holidays: 11 Paid Holidays. Flex time can be utilized in lieu of holiday time usage.</li>
 <li>Payroll: Paid Bi-Monthly.</li>
 <li>401(k): Partnered with the SECOND LARGEST Retirement plan provider in the U.S. Guaranteed 3% match. Eligibility &#x2013; 21 years of age or older, after 3 months of employment</li>
 <li>Individual or company-wide performance and recognition awards (Quarterly)</li>
</ul>
<p><b>Health Benefits:</b></p>
<ul>
 <li>UNITED HEALTHCARE PPO, extensive national coverage.</li>
 <li>INCLUDES: Medical/Dental/Vision/HSA.</li>
 <li>Eligible on the first of the month, immediately after the start date.</li>
 <li>Submit the enrollment form within 30 days</li>
 <li>of your start date otherwise, you will have to wait until October for the new year enrollment.</li>
</ul>
<p><b>Quality of Life Benefits:</b></p>
<ul>
 <li>Training &amp; Career Development Reimbursement of Tuition and training needed to support career development.</li>
 <li>&#x24;150 monthly reimbursement contribution paid monthly towards parking expenses.</li>
 <li>Receipts must be submitted by the close of business on the 25th of each month.</li>
 <li>Reimbursements will be paid on the first payroll AFTER reimbursements are submitted each month.</li>
</ul>
<p><b>Special Benefits:</b></p>
<ul>
 <li>Performance bonus &#x2013; Project-based</li>
 <li>Yearly bonus &#x2013; Company based</li>
</ul>
<p>&#x200b;</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;150,000.00 - &#x24;160,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Python, Java, and Scala programming language: 3 years (Required)</li>
 <li>ETL: 3 years (Required)</li>
 <li>SQL and noSQL: 3 years (Required)</li>
</ul>
<p>Security clearance:</p>
<ul>
 <li>Top Secret (Required)</li>
</ul>
<p>Work Location: Remote</p>",,73eb2333d67e7087,,Full-time,,,"Ashburn, VA",Data Engineer,11 days ago,2023-10-07T13:37:17.986Z,,,"$150,000 - $160,000 a year",2023-10-18T13:37:18.004Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=73eb2333d67e7087&from=jasx&tk=1hd1g27112f34000&vjs=3
150,Hope.Tech,"About the Company:
Wellplaece is a technology company building a B2B, SaaS-enabled marketplace for dental practices to centralize and consolidate dental supply purchasing with best pricing on quality dental products. We are an early-stage company that recently closed a $3 million seed round and are currently working with beta-phase customers.
Wellplaece is an equal opportunity company.
About the Role:
As the first dedicated Staff Data Engineer at Wellplaece, you will work closely with the CTO and play a critical role in building, maintaining, and optimizing our data infrastructure. This is initially an IC role, with the opportunity to to build and lead a team as the company grows.
Responsibilities:

 Design, develop, and maintain scalable, reliable, and performant web scrapers and data pipelines to ingest, process, and store large datasets.
 Maximize data quality and freshness across multiple sources, to ensure our product catalogs are always accurate and up to date.
 Optimize data architectures and database queries for performance and scalability.
 Stay updated with the latest technologies and trends in the data engineering field.

Requirements:

 Bachelor’s or Master’s degree in Computer Science or related field.
 Strong CS fundamentals, ability to reverse engineer websites and APIs.
 Strong experience with relational databases (Postgres preferred) and SQL.
 Some professional programming experience in NodeJS with TypeScript.
 Experience with deployments on cloud platforms (AWS preferred).
 Functionally Fluent in English.

Nice to Have:

 Experience working at early stage startups and building from scratch. Bonus points if able to show meaningful side projects (e.g. open source contributions, bootstrapped a product/company).
 Experience working in a remote setting and following best practices such as CI/CD, automated testing (both frontend and backend), version control (GitHub), code reviews, etc.

Job Type: Full-time
Pay: $135,000.00 - $150,000.00 per year
Benefits:

 Flexible schedule
 Paid time off

Experience level:

 5 years

Schedule:

 Monday to Friday

Work Location: Remote","<p><b>About the Company:</b></p>
<p>Wellplaece is a technology company building a B2B, SaaS-enabled marketplace for dental practices to centralize and consolidate dental supply purchasing with best pricing on quality dental products. We are an early-stage company that recently closed a &#x24;3 million seed round and are currently working with beta-phase customers.</p>
<p>Wellplaece is an equal opportunity company.</p>
<p><b>About the Role:</b></p>
<p>As the first dedicated Staff Data Engineer at Wellplaece, you will work closely with the CTO and play a critical role in building, maintaining, and optimizing our data infrastructure. This is initially an IC role, with the opportunity to to build and lead a team as the company grows.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Design, develop, and maintain scalable, reliable, and performant web scrapers and data pipelines to ingest, process, and store large datasets.</li>
 <li>Maximize data quality and freshness across multiple sources, to ensure our product catalogs are always accurate and up to date.</li>
 <li>Optimize data architectures and database queries for performance and scalability.</li>
 <li>Stay updated with the latest technologies and trends in the data engineering field.</li>
</ul>
<p><b>Requirements:</b></p>
<ul>
 <li>Bachelor&#x2019;s or Master&#x2019;s degree in Computer Science or related field.</li>
 <li>Strong CS fundamentals, ability to reverse engineer websites and APIs.</li>
 <li>Strong experience with relational databases (Postgres preferred) and SQL.</li>
 <li>Some professional programming experience in NodeJS with TypeScript.</li>
 <li>Experience with deployments on cloud platforms (AWS preferred).</li>
 <li>Functionally Fluent in English.</li>
</ul>
<p><b>Nice to Have:</b></p>
<ul>
 <li>Experience working at early stage startups and building from scratch. Bonus points if able to show meaningful side projects (e.g. open source contributions, bootstrapped a product/company).</li>
 <li>Experience working in a remote setting and following best practices such as CI/CD, automated testing (both frontend and backend), version control (GitHub), code reviews, etc.</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;135,000.00 - &#x24;150,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>Flexible schedule</li>
 <li>Paid time off</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,cebc0b8cc9a9f1a0,,Full-time,,,Remote,Senior Data Engineer,12 days ago,2023-10-06T13:37:19.529Z,,,"$135,000 - $150,000 a year",2023-10-18T13:37:19.555Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=cebc0b8cc9a9f1a0&from=jasx&tk=1hd1g26uejoov800&vjs=3
152,Clairvoyant Inc.,"Location: EST Preferred
Type: FTE
Min requirements 

 4+ years of experience in data extraction and creating data pipeline workflows on
 Bigdata (Hive, HQL/PySpark) with knowledge of Data Engineering concepts.
 Experience in analyzing large data sets from multiple data sources, perform validation of data.
 Knowledge of Hadoop eco-system components like HDFS, Spark, Hive, Sqoop.
 Experience writing codes in Python.
 Knowledge of SQL/HQL to write optimized queries.
 Hands on with GCP Cloud Services such as Big Query, Airflow DAG, Dataflow, Beam etc.
 Ability to build a migration plan in collaboration with various stakeholders.
 Analytical, problem-solving and excellent comm skills.
 Must have US Citizenship/Green Card

Job Type: Full-time
Salary: $130,000.00 - $140,000.00 per year
Benefits:

 Health insurance

Experience level:

 10 years

Schedule:

 Monday to Friday

Experience:

 Hive, HQL/PySpark: 4 years (Required)
 Python: 1 year (Required)
 Hadoop: 1 year (Required)
 GCP: 1 year (Required)
 Big Query: 1 year (Required)

Work Location: Remote","<p><b>Location: EST Preferred</b></p>
<p><b>Type: FTE</b></p>
<p><b>Min requirements </b></p>
<ul>
 <li>4+ years of experience in data extraction and creating data pipeline workflows on</li>
 <li>Bigdata (Hive, HQL/PySpark) with knowledge of Data Engineering concepts.</li>
 <li>Experience in analyzing large data sets from multiple data sources, perform validation of data.</li>
 <li>Knowledge of Hadoop eco-system components like HDFS, Spark, Hive, Sqoop.</li>
 <li>Experience writing codes in Python.</li>
 <li>Knowledge of SQL/HQL to write optimized queries.</li>
 <li>Hands on with GCP Cloud Services such as Big Query, Airflow DAG, Dataflow, Beam etc.</li>
 <li>Ability to build a migration plan in collaboration with various stakeholders.</li>
 <li>Analytical, problem-solving and excellent comm skills.</li>
 <li>Must have US Citizenship/Green Card</li>
</ul>
<p>Job Type: Full-time</p>
<p>Salary: &#x24;130,000.00 - &#x24;140,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Hive, HQL/PySpark: 4 years (Required)</li>
 <li>Python: 1 year (Required)</li>
 <li>Hadoop: 1 year (Required)</li>
 <li>GCP: 1 year (Required)</li>
 <li>Big Query: 1 year (Required)</li>
</ul>
<p>Work Location: Remote</p>",,989549b8e8d9773e,,Full-time,,,Remote,Data Engineer,8 days ago,2023-10-10T13:37:20.022Z,,,"$130,000 - $140,000 a year",2023-10-18T13:37:20.024Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=989549b8e8d9773e&from=jasx&tk=1hd1g27112f34000&vjs=3
154,"Source Select Group, LLC","PLEASE NO VENDORS OR CANDIDATES THAT REQUIRE SPONSORSHIP NOW OR IN THE FUTURE.
Sr. Azure Data Engineer
Type: Contract to Hire
Location: Florida, Georgia, Hawaii, Virginia, Texas, Colorado, North Carolina, TN
The ideal candidate will have 10+ years of hands-on experience in designing, implementing, and delivering into production, large-scale, multi-tenant, and near real-time data warehouses.
Required Skills 

 10 years of hands-on experience designing complex data models for both OLTP and OLAP systems
 10 years of hands-on experience designing and implementing large-scale data warehouses
 5 years of experience in designing, developing, and tuning Azure Data Lake including in-depth knowledge of performance tuning techniques
 5 years of experience working with multi-tenant data warehouses
 5 years of experience working with data governance
 Well-versed in Change Data Capture (CDC) solutions for structured, semi-structured, and unstructured data
 Well-versed with Azure Databricks, Databricks SQL, Unity Catalog, and Delta Lake and desire to become SME with Databricks technologies
 Well-versed in data lake storage formats
 Well-versed in techniques for handling slowly changing dimensions (SCDs)
 Familiar with data orchestration tools such as Airflow
 Broad experience in Microsoft SQL technologies including Power BI
 Experience with data integration through APIs, Web Services, and/or REST services


 Bachelor's degree in CS or related field, master’s degree preferred
 10+ years of experience with designing and developing complex data architecture solutions
 5+ years of design and development experience with Microsoft Azure data architecture and related solutions
 Working knowledge of Azure Databricks, Databricks SQL, Unity Catalog, and Delta Lake

Job Types: Contract, Full-time
Pay: $80.00 - $90.00 per hour
Expected hours: 40 per week
Benefits:

 401(k)
 Dental insurance
 Health insurance

Schedule:

 Monday to Friday

Application Question(s):

 We are unable to work with candidates that work thru a Vendor or 3rd Party. 

All ELIGIBLE candidates must work on our W2 and must be authorized to work in the USA without sponsorship now or in the future.
Do you meet this requirement?
Experience:

 data architecture: 10 years (Required)
 Azure databricks: 3 years (Required)
 SQL: 10 years (Required)
 ETL: 5 years (Required)
 Azure Data architecture: 4 years (Required)

Work Location: Remote","<p><b>PLEASE NO VENDORS OR CANDIDATES THAT REQUIRE SPONSORSHIP NOW OR IN THE FUTURE.</b></p>
<p><b>Sr. Azure Data Engineer</b></p>
<p><b>Type: Contract to Hire</b></p>
<p><b>Location: </b>Florida, Georgia, Hawaii, Virginia, Texas, Colorado, North Carolina, TN</p>
<p>The ideal candidate will have 10+ years of hands-on experience in designing, implementing, and delivering into production, large-scale, multi-tenant, and near real-time data warehouses.</p>
<p><b>Required Skills </b></p>
<ul>
 <li>10 years of hands-on experience designing complex data models for both OLTP and OLAP systems</li>
 <li>10 years of hands-on experience designing and implementing large-scale data warehouses</li>
 <li>5 years of experience in designing, developing, and tuning Azure Data Lake including in-depth knowledge of performance tuning techniques</li>
 <li>5 years of experience working with multi-tenant data warehouses</li>
 <li>5 years of experience working with data governance</li>
 <li><b>Well-versed in Change Data Capture (CDC) solutions for structured, semi-structured, and unstructured data</b></li>
 <li><b>Well-versed with Azure Databricks, Databricks SQL, Unity Catalog, and Delta Lake and desire to become SME with Databricks technologies</b></li>
 <li>Well-versed in data lake storage formats</li>
 <li>Well-versed in techniques for handling slowly changing dimensions (SCDs)</li>
 <li>Familiar with data orchestration tools such as Airflow</li>
 <li>Broad experience in Microsoft SQL technologies including Power BI</li>
 <li>Experience with data integration through APIs, Web Services, and/or REST services</li>
</ul>
<ul>
 <li>Bachelor&apos;s degree in CS or related field, master&#x2019;s degree preferred</li>
 <li>10+ years of experience with designing and developing complex data architecture solutions</li>
 <li>5+ years of design and development experience with Microsoft Azure data architecture and related solutions</li>
 <li>Working knowledge of Azure Databricks, Databricks SQL, Unity Catalog, and Delta Lake</li>
</ul>
<p>Job Types: Contract, Full-time</p>
<p>Pay: &#x24;80.00 - &#x24;90.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>We are unable to work with candidates that work thru a Vendor or 3rd Party. </li>
</ul>
<p>All ELIGIBLE candidates must work on our W2 and must be authorized to work in the USA without sponsorship now or in the future.</p>
<p>Do you meet this requirement?</p>
<p>Experience:</p>
<ul>
 <li>data architecture: 10 years (Required)</li>
 <li>Azure databricks: 3 years (Required)</li>
 <li>SQL: 10 years (Required)</li>
 <li>ETL: 5 years (Required)</li>
 <li>Azure Data architecture: 4 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,33b4ed0d98ba78e0,,Full-time,Contract,,Texas,Sr. Azure Data Engineer,7 days ago,2023-10-11T13:37:34.862Z,,,$80 - $90 an hour,2023-10-18T13:37:34.965Z,US,remote,data engineer,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Dknu-XJx1lvG7TapgMlWnDguf9J9bebwcn7i5H53jr-drpQ5Li0Kh0ocmNMFc5deE_9wtv9DXwuMqE2mt0p40WQPf2PCXMnynHuk9iib6LchjbRvPjkkC4egmG7QqF7fSptQJF1ivoDgybiTo63AqUuZ1PIr0vteL-AJdzEYaN8RgV2NXCRr0c0KDg3H-vbXWLgH-TMvKCw9ljydPqnptMvZGW_ZmGI6FZvOWoQm4T6_uEpQu0Es2bHuALkGKzPTv_RQNPb0iF8Uo35n-NkCtlP-1nRHp7VuMhDK62sntBPLt_mbiJRDPed-i2RlMHhjZVqSc5lUbhkCer2QUhAnooErSx1SU31qIKR1jPoprU_SKH2xzWpBBiWqwYNEMwnhb2IvG5K_UNr4nvPAxctvBqr-wmNLZj8GNE25iim4ycJ__6_9LIUEibVaXp8lHpV1bsR4kPIqwmAkM7WLtC8GEH2_HgCheJrMNtFOD-kLj5blINUWPyKS361fRlDw4Hf5FwQNegF_3tRAN-4hl_rrGj7RLFgA5a5-qMQyuZJYzMVItfRW7HhKSJzsLIadWE-lPkVs4lw3kWne3ZjUVZLKaJvANSw3XwsetZYnm6N4D3AQPR_qDzHb8g&xkcb=SoDr-_M3JhoFdXxiNZ0cbzkdCdPP&p=8&fvj=1&vjs=3&jsa=2704&tk=1hd1g2l89joov801&from=jasx&wvign=1
155,Recruiting From Scratch,"This is for a client of Recruiting from Scratch. 
  
  
  
   Who is Recruiting from Scratch: 
  
  
   Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more. 
  
  
  
   https://www.recruitingfromscratch.com/ 
  
  
   Our Client: 
  
  
   
    
     
      
       Our client is looking for a senior data engineer to help revolutionize healthcare supply chain data. This role will be responsible for architecting, designing, and implementing both improvements to our existing data processes and new processes going forward. 
       This role is responsible for: 
       
        Designing, implementing, deploying, and operating data pipelines and platforms 
        Writing the code for our pipelines, and reviewing the code of others 
        Prototyping and iterating tools for data review and validation by our domain experts 
        Determining our data roadmap and prioritizing tasks to create the best value/cost ratio possible 
        Analyzing our current and future data sets to aid the above prioritization 
        Being a leader on our data team as the company grows 
       
       Skills Required 
       
        5+ years experience as a software engineer, of which at least 2+ years experience in data-based software engineering role 
        Ability to architect solutions on AWS and design for appropriate scales 
        Strong experience with Python and SQL 
        Ability to add tests/validations for your data pipelining code 
        DevOps experience and the ability to operationalize the code you write (creating the developer experience, CI/CD, managing infrastructure to run the code, etc.) 
        The drive to take over project timelines and deliverables 
       
       Skills that would be nice to have 
       
        Experience with natural language processing 
        Experience with web-scraping 
        Experience working with any healthcare IT Infrastructure 
        Experience with supply chain IT or management 
       
       Additional requirements 
       
        Bachelor’s degree in a technical field (sciences, engineering, computer science, etc.) 
        Authorized to work in the US 
       
       We offer 
       
        Competitive base compensation 
        Profit-sharing bonuses 
        401k Match 
        Company-provided health insurance plans 
        US based Remote role with scheduled team in-person meet-ups 
        Close collaboration with our healthcare data management team 
        A collaborative team focused on making healthcare better 
        Immediate impact on the roadmap and priorities so you can help define your own agenda 
        Opportunity to grow and take ownership of various business initiatives 
       
      
     
     
     
      
       Salary Range: $125,000-$150,000 base. Equity. Medical, Dental, Vision. 
      
     
    
   
   
   
    
     https://www.recruitingfromscratch.com/","<div>
 <div>
  <div>
   <b>This is for a client of Recruiting from Scratch. </b>
  </div>
  <div></div>
  <div>
   <b>Who is Recruiting from Scratch: </b>
  </div>
  <div>
   Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more. 
  </div>
  <div></div>
  <div>
   https://www.recruitingfromscratch.com/ 
  </div>
  <div>
   <b>Our Client: </b>
  </div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <p>Our client is looking for a senior data engineer to help revolutionize healthcare supply chain data. This role will be responsible for architecting, designing, and implementing both improvements to our existing data processes and new processes going forward. </p>
       <h3 class=""jobSectionHeader""><b>This role is responsible for: </b></h3>
       <ul>
        <li>Designing, implementing, deploying, and operating data pipelines and platforms </li>
        <li>Writing the code for our pipelines, and reviewing the code of others </li>
        <li>Prototyping and iterating tools for data review and validation by our domain experts </li>
        <li>Determining our data roadmap and prioritizing tasks to create the best value/cost ratio possible </li>
        <li>Analyzing our current and future data sets to aid the above prioritization </li>
        <li>Being a leader on our data team as the company grows </li>
       </ul>
       <h3 class=""jobSectionHeader""><b>Skills Required </b></h3>
       <ul>
        <li>5+ years experience as a software engineer, of which at least 2+ years experience in data-based software engineering role </li>
        <li>Ability to architect solutions on AWS and design for appropriate scales </li>
        <li>Strong experience with Python and SQL </li>
        <li>Ability to add tests/validations for your data pipelining code </li>
        <li>DevOps experience and the ability to operationalize the code you write (creating the developer experience, CI/CD, managing infrastructure to run the code, etc.) </li>
        <li>The drive to take over project timelines and deliverables </li>
       </ul>
       <h3 class=""jobSectionHeader""><b>Skills that would be nice to have </b></h3>
       <ul>
        <li>Experience with natural language processing </li>
        <li>Experience with web-scraping </li>
        <li>Experience working with any healthcare IT Infrastructure </li>
        <li>Experience with supply chain IT or management </li>
       </ul>
       <h3 class=""jobSectionHeader""><b>Additional requirements </b></h3>
       <ul>
        <li>Bachelor&#x2019;s degree in a technical field (sciences, engineering, computer science, etc.) </li>
        <li>Authorized to work in the US </li>
       </ul>
       <h3 class=""jobSectionHeader""><b>We offer </b></h3>
       <ul>
        <li>Competitive base compensation </li>
        <li>Profit-sharing bonuses </li>
        <li>401k Match </li>
        <li>Company-provided health insurance plans </li>
        <li>US based Remote role with scheduled team in-person meet-ups </li>
        <li>Close collaboration with our healthcare data management team </li>
        <li>A collaborative team focused on making healthcare better </li>
        <li>Immediate impact on the roadmap and priorities so you can help define your own agenda </li>
        <li>Opportunity to grow and take ownership of various business initiatives </li>
       </ul>
      </div>
     </div>
     <div></div>
     <div>
      <div>
       <b>Salary Range: &#x24;125,000-&#x24;150,000 base. </b>Equity. Medical, Dental, Vision. 
      </div>
     </div>
    </div>
   </div>
   <div></div>
   <div>
    <div>
     https://www.recruitingfromscratch.com/
    </div>
   </div>
  </div>
 </div>
</div>",https://recruiterflow.com/recruitingfromscratch/jobs/1996?source=indeed&utm_channel=recruiterflow-posting&location=1,f8bf5f4c80df4dc1,,Full-time,,,Remote,Senior Data Engineer,12 days ago,2023-10-06T13:37:31.818Z,,,"$130,000 - $170,000 a year",2023-10-18T13:37:31.848Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=f8bf5f4c80df4dc1&from=jasx&tk=1hd1g2n5fjm7b800&vjs=3
158,YCHARTS INC,"Description: 
  YCharts enables its customers to make smarter investment decisions. Our cloud-based financial data and investment research platform provides investment advisors, wealth managers, and institutional and retail investors with comprehensive data, powerful visualization tools, and advanced analytics. YCharts has transitioned from a fintech startup to a thriving growth company (7x Inc. 5000 Fastest Growing Company). YCharts proudly services industry-leading names such as WealthManagement.com, Seeking Alpha, TD Ameritrade, Fidelity, Charles Schwab, The Wall Street Journal, Morgan Stanley, and Wells Fargo.
  The Position:
 
  As a software engineer on the Data team, you can expect to work with technologies that ensure our data pipeline efficiently maintains and calculates data for the 200,000+ securities in our universe. Engineers on the Data team typically work on features that ensure financial data is consistent, reliable, timely and accurate by importing data from third party data providers, performing custom calculations on this imported data, and efficiently storing this data.
  Being a Senior Engineer you will have the autonomy to design and prioritize projects independently, demonstrating your expertise. Writing efficient, well-designed, and thoroughly tested code will be a key responsibility, while also actively participating in code reviews to ensure code quality across the team.
  As an engineering organization, we put a lot of focus on collaboration, building new products, and refactoring our codebase which has allowed us to scale and grow with as little pain as possible. We look for engineers who have an attention to detail and have a strong desire to do things the right way even if it requires taking a step back rather than the first thing that comes to mind.
  You will be working alongside other high-performing engineers and there is never a shortage of challenging and interesting projects that will keep you busy and keep you constantly learning new things both from a technical and domain perspective.
  Job Responsibilities:
  
 
 
   Write code in Python and the Django framework to implement complex backend features and ensure that your code is well-tested
   Use tools like Airflow to build / enhance financial data pipelines for over 200,000 securities
   Make use of pandas and asynchronous task tools like celery to efficiently calculate 4,000+ unique data points for each security
   Design robust data models that account for the structure and type of data to store
   Design, plan, estimate, and ticket features that are scoped by our product team
   Work directly with our product team to clarify feature requests and negotiate solutions
   Take ownership of projects and be responsible for the entire lifecycle of your code: Development, test, production, and subsequent fixes and improvements
   Perform code reviews for other engineers on the team
   Document the work you have done both in repo as well as outside of the repo for future engineers
   Collaborate with other engineers both verbally and in writing to plan, design, and build a world-class financial research platform
  Requirements: 
  About You:
  
 
 
   5+ years of relevant industry or academic experience
   Experience developing in Python (knowledge of the Django framework a plus)
   Experience designing, building and maintaining application features on the backend
   Experience participating in multi-month projects from conception to maintenance with multiple team members
   Desire to take ownership and responsibility when a problem or opportunity arises
   Experience working with non-technical teams (product, business, etc) where explaining technical concepts is needed
   Self-organized and able to work independently or within a team
 
  Compensation:
 
   Base salary range: $150,000 - $180,000 annually
   This position is also eligible for an annual bonus based on individual and team goals and company performance.
 
  Why YCharts?:
 
   Opportunity to work in a fast-growing fintech company that is shaping the future of investment research and data analytics.
   Collaborative and inclusive work environment that encourages creativity and innovation.
 
  Awards and Accolades:
 
   7x Inc. 5000 “Fastest Growing Companies”
   American Banker's ""Best Fintechs to Work For""
   Built in Chicago’s “Best Places to Work” and “Best Small Company to Work For”
   Inc.’s “Best Places to Work”
   Inc.’s “Top Regionals: Midwest”
   Crain's ""Best Places to Work in Chicago""
   InvestmentNews' ""Biggest Fintech Innovations""
   Technology Tools for Today & Inside Information’s “Top Tool Advisors Are Thinking About Adding”
   Kitces Report’s “Investment Data” market leader, #1 most-adopted in the last year, #2 in market share
   Business Intelligent Group’s “Best Places to Work”
   Hired’s “Top Employers Winning Tech Talent”
 
  Perks & Rec:
 
   Chicago (River North) & NY (Chelsea) offices with flexible remote options
   100% Employer-covered medical, dental & vision insurance
   Flexible Spending Accounts (Healthcare and Dependent Care)
   401(k) match
   Paid parental leave
   Discounted Pet Insurance
   Great Work/Life Balance: Generous PTO including Vacation Days, Paid Holidays, Sick Days, Professional Education Days, and “Celebration Days”
   DEI commitment
   Continued education via “Starbucks and Study”
   Opportunity to join committees (Educational committee, DEI committee, Social Committee, Women at YCharts, and more!)
   Summer hours— we head out early during the warm months!
 
  In-Office Perks:
 
   Weekly Grubhub credits for in-office lunches
   Rotating selection of high-quality coffees
   Craft beer, kombucha, and cold brew on tap
   Snacks and drinks to get you through the day
   Opportunity to join team leagues like kickball
   Fun company outings including an annual celebration in Chicago, Whirlyball, community service, baseball games and happy hours!
 
 
  YCharts provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
  At YCharts, we feel strongly that respect and inclusion are essential ingredients for creativity, innovation, and success. While we know there’s more work to be done to advance diversity and inclusion, we’re proud of our success thus far and excited for the journey to come.","<div>
 Description: 
 <p> YCharts enables its customers to make smarter investment decisions. Our cloud-based financial data and investment research platform provides investment advisors, wealth managers, and institutional and retail investors with comprehensive data, powerful visualization tools, and advanced analytics. YCharts has transitioned from a fintech startup to a thriving growth company (7x Inc. 5000 Fastest Growing Company). YCharts proudly services industry-leading names such as WealthManagement.com, Seeking Alpha, TD Ameritrade, Fidelity, Charles Schwab, The Wall Street Journal, Morgan Stanley, and Wells Fargo.</p>
 <p> The Position:</p>
 <p></p>
 <p><br> As a software engineer on the Data team, you can expect to work with technologies that ensure our data pipeline efficiently maintains and calculates data for the 200,000+ securities in our universe. Engineers on the Data team typically work on features that ensure financial data is consistent, reliable, timely and accurate by importing data from third party data providers, performing custom calculations on this imported data, and efficiently storing this data.</p>
 <p> Being a Senior Engineer you will have the autonomy to design and prioritize projects independently, demonstrating your expertise. Writing efficient, well-designed, and thoroughly tested code will be a key responsibility, while also actively participating in code reviews to ensure code quality across the team.</p>
 <p> As an engineering organization, we put a lot of focus on collaboration, building new products, and refactoring our codebase which has allowed us to scale and grow with as little pain as possible. We look for engineers who have an attention to detail and have a strong desire to do things the right way even if it requires taking a step back rather than the first thing that comes to mind.</p>
 <p> You will be working alongside other high-performing engineers and there is never a shortage of challenging and interesting projects that will keep you busy and keep you constantly learning new things both from a technical and domain perspective.</p>
 <p> Job Responsibilities:</p>
 <br> 
 <p></p>
 <ul>
  <li> Write code in Python and the Django framework to implement complex backend features and ensure that your code is well-tested</li>
  <li> Use tools like Airflow to build / enhance financial data pipelines for over 200,000 securities</li>
  <li> Make use of pandas and asynchronous task tools like celery to efficiently calculate 4,000+ unique data points for each security</li>
  <li> Design robust data models that account for the structure and type of data to store</li>
  <li> Design, plan, estimate, and ticket features that are scoped by our product team</li>
  <li> Work directly with our product team to clarify feature requests and negotiate solutions</li>
  <li> Take ownership of projects and be responsible for the entire lifecycle of your code: Development, test, production, and subsequent fixes and improvements</li>
  <li> Perform code reviews for other engineers on the team</li>
  <li> Document the work you have done both in repo as well as outside of the repo for future engineers</li>
  <li> Collaborate with other engineers both verbally and in writing to plan, design, and build a world-class financial research platform</li>
 </ul> Requirements: 
 <p> About You:</p>
 <br> 
 <p></p>
 <ul>
  <li> 5+ years of relevant industry or academic experience</li>
  <li> Experience developing in Python (knowledge of the Django framework a plus)</li>
  <li> Experience designing, building and maintaining application features on the backend</li>
  <li> Experience participating in multi-month projects from conception to maintenance with multiple team members</li>
  <li> Desire to take ownership and responsibility when a problem or opportunity arises</li>
  <li> Experience working with non-technical teams (product, business, etc) where explaining technical concepts is needed</li>
  <li> Self-organized and able to work independently or within a team</li>
 </ul>
 <p> Compensation:</p>
 <ul>
  <li> Base salary range: &#x24;150,000 - &#x24;180,000 annually</li>
  <li> This position is also eligible for an annual bonus based on individual and team goals and company performance.</li>
 </ul>
 <p> Why YCharts?:</p>
 <ul>
  <li> Opportunity to work in a fast-growing fintech company that is shaping the future of investment research and data analytics.</li>
  <li> Collaborative and inclusive work environment that encourages creativity and innovation.</li>
 </ul>
 <p> Awards and Accolades:</p>
 <ul>
  <li> 7x Inc. 5000 &#x201c;Fastest Growing Companies&#x201d;</li>
  <li> American Banker&apos;s &quot;Best Fintechs to Work For&quot;</li>
  <li> Built in Chicago&#x2019;s &#x201c;Best Places to Work&#x201d; and &#x201c;Best Small Company to Work For&#x201d;</li>
  <li> Inc.&#x2019;s &#x201c;Best Places to Work&#x201d;</li>
  <li> Inc.&#x2019;s &#x201c;Top Regionals: Midwest&#x201d;</li>
  <li> Crain&apos;s &quot;Best Places to Work in Chicago&quot;</li>
  <li> InvestmentNews&apos; &quot;Biggest Fintech Innovations&quot;</li>
  <li> Technology Tools for Today &amp; Inside Information&#x2019;s &#x201c;Top Tool Advisors Are Thinking About Adding&#x201d;</li>
  <li> Kitces Report&#x2019;s &#x201c;Investment Data&#x201d; market leader, #1 most-adopted in the last year, #2 in market share</li>
  <li> Business Intelligent Group&#x2019;s &#x201c;Best Places to Work&#x201d;</li>
  <li> Hired&#x2019;s &#x201c;Top Employers Winning Tech Talent&#x201d;</li>
 </ul>
 <p> Perks &amp; Rec:</p>
 <ul>
  <li> Chicago (River North) &amp; NY (Chelsea) offices with flexible remote options</li>
  <li> 100% Employer-covered medical, dental &amp; vision insurance</li>
  <li> Flexible Spending Accounts (Healthcare and Dependent Care)</li>
  <li> 401(k) match</li>
  <li> Paid parental leave</li>
  <li> Discounted Pet Insurance</li>
  <li> Great Work/Life Balance: Generous PTO including Vacation Days, Paid Holidays, Sick Days, Professional Education Days, and &#x201c;Celebration Days&#x201d;</li>
  <li> DEI commitment</li>
  <li> Continued education via &#x201c;Starbucks and Study&#x201d;</li>
  <li> Opportunity to join committees (Educational committee, DEI committee, Social Committee, Women at YCharts, and more!)</li>
  <li> Summer hours&#x2014; we head out early during the warm months!</li>
 </ul>
 <p> In-Office Perks:</p>
 <ul>
  <li> Weekly Grubhub credits for in-office lunches</li>
  <li> Rotating selection of high-quality coffees</li>
  <li> Craft beer, kombucha, and cold brew on tap</li>
  <li> Snacks and drinks to get you through the day</li>
  <li> Opportunity to join team leagues like kickball</li>
  <li> Fun company outings including an annual celebration in Chicago, Whirlyball, community service, baseball games and happy hours!</li>
 </ul>
 <p></p>
 <p><br> YCharts provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.</p>
 <p> At YCharts, we feel strongly that respect and inclusion are essential ingredients for creativity, innovation, and success. While we know there&#x2019;s more work to be done to advance diversity and inclusion, we&#x2019;re proud of our success thus far and excited for the journey to come.</p>
</div>",https://recruiting.paylocity.com/recruiting/jobs/Details/2001320/YCHARTS-INC/Senior-Software-Engineer--Data-Team?source=Indeed_Feed,fd53daa023410125,,Full-time,,,Remote,Senior Software Engineer- Data Team,8 days ago,2023-10-10T13:37:36.986Z,4.0,3.0,"$150,000 - $180,000 a year",2023-10-18T13:37:36.989Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=fd53daa023410125&from=jasx&tk=1hd1g2n5fjm7b800&vjs=3
160,Healthesystems,"Healthesystems offers workplace flexibility with our Work-From-Home model, and a competitive compensation and benefits package including healthcare coverage, PTO, paid holidays, 401(k), company-provided life insurance/disability coverage, wellness options, and more.
  
 
  Note: we are unable to hire in every state 
  
  
   
    
     Summary: Responsible for the analysis, design, documentation, development, unit testing, and support of Data Integration and database objects development for software applications. Provides support and guidance regarding Data Integration and T-SQL best practices and development standards. Promotes approved agile methodologies, leading the design and development efforts for the agile team. Actively coaches, guides, and mentors team members in providing valuable solutions to our customer. 
      Key Responsibilities: ""To simplify complexities for each customer."" 
      
      
       Collaborates with stakeholders and development team members to achieve business results.
       
      
       Work closely with other engineers to integrate databases with other applications.
       
      
       Leads the design, development, and implementation of database applications and solutions for managing and integrating data between operational systems, data repositories, and reporting and analytical applications. This includes but is not limited to ETL, stored procedures, views, and functions.
       
      
       Recommends and provide guidance regarding Data Integration and database development, T-SQL best practices, and standards to the development team members as needed.
       
      
       Create and propose technical design documentation which includes current and future functionality, database objects affected, specifications, and flows/diagrams to detail the proposed database and/or Data Integration implementation.
       
      
       Has a deep understanding of the business processes and the technology platform that enables it.
       
      
       Translates stakeholder's requirements into common language that can be adopted for the use with Behavior Driven Development (BDD) or Test Driven Development (TDD).
       
      
       Participates in industry and other professional networks to ensure awareness of industry standards, trends and best practices in order to strengthen organizational and technical knowledge.
       
      
       Provides support for investigating and troubleshooting production issues.
       
      
       Promotes the establishment of group standards and processes. Participates in the Communities of Practice.
       
      
       Works continually on improving performance of source code using industry standard methodologies.
       
      
       Helps drive technology direction and choices of technologies by making recommendations based on experience and research.
      
     
      
      
      Qualifications/Education/Certifications: 
      Bachelor's degree from four-year college or university (in Information Technology or Computer Science preferred), plus five to eight years related experience and/or training; or equivalent combination of education and experience. 
      Knowledge, Skills and Abilities: 
      Prefer experience in Healthcare, PBM and/or ABM, workers' compensation and/or insurance industry. 
      
      Required experience: 
       
        5+ years SQL Server 2008/2014 
       
      
       5+ years Data Integration technologies and principles 
       Advanced knowledge of T-SQL including complex SQL queries (ex: using various joins and sub-queries) and best practices 
       Advanced knowledge of index design and T-SQL performance tuning techniques 
       Advanced experience integrating data from structured and unstructured formats: flat files, XML, EDI, JSON, EXCEL 
       Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases and schemas 
       Advanced knowledge of Data Warehousing methodologies and concepts 
       Experience with TDD / BDD 
      
      The following knowledge is not required, but is preferred: 
      
       Experience with BI Tools is a plus 
       Basic understanding of object oriented programming 
       Experience in distributed architectures such as Microservices, SOA, and RESTful APIs 
       Continuous Integration 
       Cucumber, Gherkin 
       Jira 
      
      Agile Competency Requirements: 
      
       Requires an understanding of the application of Agile development methodology. 
       Must be comfortable with change, close collaboration, and have conflict resolution skills. 
       Knowledge of or willingness to learn Agile / DevOps values. 
       Takes initiative and are passionate about what they do. 
       Adaption, Ability & Desire to Learn, Team Oriented - tolerance & helpful, and Quality Focus 
       
     
     Physical Demands/Working Conditions: 
      Duties are performed primarily in a home office setting utilizing computer equipment. Travel to attend meetings and visit locations throughout the country may be required. While performing the duties of this job, the employee is regularly required to sit and talk or hear. The employee is frequently required to use hands. The employee is occasionally required to stand and walk.*** Job descriptions will be reviewed and are subject to changes of business necessity. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. 
     
   
  
 
 
  
   Pay is based on several factors including but not limited to education, work experience, certifications, geographical cost of labor, etc. In addition to base pay, Healthesystems offers a comprehensive benefits package including, health, dental, vision, disability and life insurance, wellness resources, recognition programs, 401k contribution, and PTO & Holiday pay (all subject to eligibility requirements). Applicable statutory benefits also provided. https://healthesystems.com/careers/
  
   Anticipated Starting Pay Range
  
    $98,100—$135,000 USD
  
 
 
   To facilitate working from home, and as a requirement for this role, candidates must provide their own reliable, high speed internet access with sufficient bandwidth to execute all job functions. Company laptop will be provided.","<p></p>
<div>
 <div>
  <div>
   <b>Healthesystems offers workplace flexibility with our Work-From-Home model, and a competitive compensation and benefits package including healthcare coverage, PTO, paid holidays, 401(k), company-provided life insurance/disability coverage, wellness options, and more.</b>
  </div>
 </div>
 <p><i> Note: we are unable to hire in every state</i></p> 
 <div> 
  <div>
   <div>
    <div>
     <p><b>Summary:</b> Responsible for the analysis, design, documentation, development, unit testing, and support of Data Integration and database objects development for software applications. Provides support and guidance regarding Data Integration and T-SQL best practices and development standards. Promotes approved agile methodologies, leading the design and development efforts for the agile team. Actively coaches, guides, and mentors team members in providing valuable solutions to our customer.</p> 
     <p><b> </b><b>Key Responsibilities: &quot;To simplify complexities for each customer.&quot;</b></p> 
     <div> 
      <ul>
       <li>Collaborates with stakeholders and development team members to achieve business results.</li>
      </ul> 
      <ul>
       <li>Work closely with other engineers to integrate databases with other applications.</li>
      </ul> 
      <ul>
       <li>Leads the design, development, and implementation of database applications and solutions for managing and integrating data between operational systems, data repositories, and reporting and analytical applications. This includes but is not limited to ETL, stored procedures, views, and functions.</li>
      </ul> 
      <ul>
       <li>Recommends and provide guidance regarding Data Integration and database development, T-SQL best practices, and standards to the development team members as needed.</li>
      </ul> 
      <ul>
       <li>Create and propose technical design documentation which includes current and future functionality, database objects affected, specifications, and flows/diagrams to detail the proposed database and/or Data Integration implementation.</li>
      </ul> 
      <ul>
       <li>Has a deep understanding of the business processes and the technology platform that enables it.</li>
      </ul> 
      <ul>
       <li>Translates stakeholder&apos;s requirements into common language that can be adopted for the use with Behavior Driven Development (BDD) or Test Driven Development (TDD).</li>
      </ul> 
      <ul>
       <li>Participates in industry and other professional networks to ensure awareness of industry standards, trends and best practices in order to strengthen organizational and technical knowledge.</li>
      </ul> 
      <ul>
       <li>Provides support for investigating and troubleshooting production issues.</li>
      </ul> 
      <ul>
       <li>Promotes the establishment of group standards and processes. Participates in the Communities of Practice.</li>
      </ul> 
      <ul>
       <li>Works continually on improving performance of source code using industry standard methodologies.</li>
      </ul> 
      <ul>
       <li>Helps drive technology direction and choices of technologies by making recommendations based on experience and research.</li>
      </ul>
     </div>
     <br> 
     <p></p> 
     <p><b> Qualifications/Education/Certifications:</b></p> 
     <p> Bachelor&apos;s degree from four-year college or university (in Information Technology or Computer Science preferred), plus five to eight years related experience and/or training; or equivalent combination of education and experience.</p> 
     <p><b> Knowledge, Skills and Abilities:</b></p> 
     <p> Prefer experience in Healthcare, PBM and/or ABM, workers&apos; compensation and/or insurance industry.</p> 
     <ul> 
      <li>Required experience: 
       <ul>
        <li>5+ years SQL Server 2008/2014</li> 
       </ul></li>
      <ul>
       <li>5+ years Data Integration technologies and principles</li> 
       <li>Advanced knowledge of T-SQL including complex SQL queries (ex: using various joins and sub-queries) and best practices</li> 
       <li>Advanced knowledge of index design and T-SQL performance tuning techniques</li> 
       <li>Advanced experience integrating data from structured and unstructured formats: flat files, XML, EDI, JSON, EXCEL</li> 
       <li>Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases and schemas</li> 
       <li>Advanced knowledge of Data Warehousing methodologies and concepts</li> 
       <li>Experience with TDD / BDD</li> 
      </ul>
      <li>The following knowledge is not required, but is preferred:</li> 
      <ul>
       <li>Experience with BI Tools is a plus</li> 
       <li>Basic understanding of object oriented programming</li> 
       <li>Experience in distributed architectures such as Microservices, SOA, and RESTful APIs</li> 
       <li>Continuous Integration</li> 
       <li>Cucumber, Gherkin</li> 
       <li>Jira</li> 
      </ul>
      <li>Agile Competency Requirements:</li> 
      <ul>
       <li>Requires an understanding of the application of Agile development methodology.</li> 
       <li>Must be comfortable with change, close collaboration, and have conflict resolution skills.</li> 
       <li>Knowledge of or willingness to learn Agile / DevOps values.</li> 
       <li>Takes initiative and are passionate about what they do.</li> 
       <li>Adaption, Ability &amp; Desire to Learn, Team Oriented - tolerance &amp; helpful, and Quality Focus</li> 
      </ul> 
     </ul>
     <p><b>Physical Demands/Working Conditions:</b></p> 
     <p> Duties are performed primarily in a home office setting utilizing computer equipment. Travel to attend meetings and visit locations throughout the country may be required. While performing the duties of this job, the employee is regularly required to sit and talk or hear. The employee is frequently required to use hands. The employee is occasionally required to stand and walk.<i>*** Job descriptions will be reviewed and are subject to changes of business necessity. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.</i></p> 
    </div> 
   </div>
  </div>
 </div>
 <div>
  <div>
   <p>Pay is based on several factors including but not limited to education, work experience, certifications, geographical cost of labor, etc. In addition to base pay, Healthesystems offers a comprehensive benefits package including, health, dental, vision, disability and life insurance, wellness resources, recognition programs, 401k contribution, and PTO &amp; Holiday pay (all subject to eligibility requirements). Applicable statutory benefits also provided. https://healthesystems.com/careers/</p>
  </div>
  <p><b> Anticipated Starting Pay Range</b></p>
  <div>
    &#x24;98,100&#x2014;&#x24;135,000 USD
  </div>
 </div>
 <div>
  <p> To facilitate working from home, and as a requirement for this role, candidates must provide their own reliable, high speed internet access with sufficient bandwidth to execute all job functions. Company laptop will be provided.</p>
 </div>
</div>",https://healthesystems.com/careers-list/?gh_jid=5763661003&gh_src=8466226f3us,2ca88fbc827420a3,,,,,Remote,Senior Data Integration Engineer - Remote,7 days ago,2023-10-11T13:37:39.602Z,3.7,57.0,"$98,100 - $135,000 a year",2023-10-18T13:37:39.606Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2ca88fbc827420a3&from=jasx&tk=1hd1g2npm2f32000&vjs=3
161,DIGITAL ENVOY,"Description: 
  Digital Envoy (DE) is the leader in geolocation and VPN data for over 23 years for customers in various verticals - Streaming Media, Ad Tech, Cybersecurity, E-commerce, and Data Analytics. Our core product IP address geolocation powers customers like Netflix, Hulu, Trade Desk, Paypal, Adobe, ebay, and many others. Our VPN and Proxy data coupled with accurate IP address geolocation data helps companies determine malicious activity for threat intelligence, authentication, and digital piracy.
  Digital Envoy is looking for a full-time Senior Data Engineer who will support the development and maintenance of sustainable data integration pipelines that enable our team to perform timely analyses. The Data Engineer will also build scalable tools and work with data warehousing systems needed to support customers.
  This person can work remotely from any of the following states: California (CA), Colorado (CO), Connecticut (CT), Florida (FL), Georgia (GA), Hawaii (HI), Louisiana (LA), Massachusetts (MA), Michigan (MI), New York (NY), New Jersey (NJ), Ohio (OH), Pennsylvania (PA), Tennessee (TN), Texas (TX), Virginia (VA), Washington (WA)
  Georgia residents are encouraged and welcomed to join us a couple days/week in our Peachtree Corners, Georgia office.
  Please note Digital Envoy is not sponsoring visas for this position.
  WHAT YOU'LL DO:
 
   Write Spark, Python, and SQL to perform ETL on billions of location records per day
   Implement ETL pipelines in AWS EMR + Airflow to support feature stores for customer exports, internal analysis and machine learning use cases.
   Write complex SQL, including geospatial, to fulfill customer requests for analysis
   Build dashboards in Tableau to surface data-driven insights
   Develop scripts in Python, Spark and Postgis to acquire and curate spatial data
 
  WHAT WE OFFER:
 
   Competitive Salary & Bonus program
   Medical, Dental and Vision
   Paid Holidays & Unlimited PTO policy
   401(k) with employer contribution match
   We value your input: make a real impact in a growing company!
 
  At Digital Envoy, we’re excited about building a diverse team and creating an inclusive environment where everyone can thrive, and we encourage all applicants of any educational background, gender identity and expression, sexual orientation, religion, ethnicity, age, citizenship, socioeconomic status, disability, and veteran status to apply. Requirements: 
  WHO YOU ARE:
 
   4+ years of data engineering or relevant industry experience
   2+ years experience and working proficiency with Spark
   Bachelor’s Degree in Computer Science or related technical areas like Math, Statistics, and/or other Engineering degrees
   Strong proficiency with Python
   Advanced proficiency with SQL, comfortable with complex joins
   Experience with AWS data engineering products (S3, RDS, EMR, Glue, Athena...) or similar tools from other cloud providers
   Familiarity with orchestration systems, preferably Airflow
   Self-initiative and an entrepreneurial mindset
   Strong communication skills
   Passion for data
 
  Nice-To-Haves:
 
   Experience with spatial data, joins and operations
   Working knowledge of Scala
   Proficiency with building Tableau dashboards
   Familiarity with Snowflake
   Experience with Machine Learning","<div>
 Description: 
 <p> Digital Envoy (DE) is the leader in geolocation and VPN data for over 23 years for customers in various verticals - Streaming Media, Ad Tech, Cybersecurity, E-commerce, and Data Analytics. Our core product IP address geolocation powers customers like Netflix, Hulu, Trade Desk, Paypal, Adobe, ebay, and many others. Our VPN and Proxy data coupled with accurate IP address geolocation data helps companies determine malicious activity for threat intelligence, authentication, and digital piracy.</p>
 <p> Digital Envoy is looking for a full-time Senior Data Engineer who will support the development and maintenance of sustainable data integration pipelines that enable our team to perform timely analyses. The Data Engineer will also build scalable tools and work with data warehousing systems needed to support customers.</p>
 <p><b> This person can work remotely from any of the following states: California (CA), Colorado (CO), Connecticut (CT), Florida (FL), Georgia (GA), Hawaii (HI), Louisiana (LA), Massachusetts (MA), Michigan (MI), New York (NY), New Jersey (NJ), Ohio (OH), Pennsylvania (PA), Tennessee (TN), Texas (TX), Virginia (VA), Washington (WA)</b></p>
 <p><b> Georgia residents are encouraged and welcomed to join us a couple days/week in our Peachtree Corners, Georgia office.</b></p>
 <p><b> Please note Digital Envoy is not sponsoring visas for this position.</b></p>
 <p><b> WHAT YOU&apos;LL DO:</b></p>
 <ul>
  <li> Write Spark, Python, and SQL to perform ETL on billions of location records per day</li>
  <li> Implement ETL pipelines in AWS EMR + Airflow to support feature stores for customer exports, internal analysis and machine learning use cases.</li>
  <li> Write complex SQL, including geospatial, to fulfill customer requests for analysis</li>
  <li> Build dashboards in Tableau to surface data-driven insights</li>
  <li> Develop scripts in Python, Spark and Postgis to acquire and curate spatial data</li>
 </ul>
 <p><b> WHAT WE OFFER:</b></p>
 <ul>
  <li> Competitive Salary &amp; Bonus program</li>
  <li> Medical, Dental and Vision</li>
  <li> Paid Holidays &amp; Unlimited PTO policy</li>
  <li> 401(k) with employer contribution match</li>
  <li> We value your input: make a real impact in a growing company!</li>
 </ul>
 <p> At Digital Envoy, we&#x2019;re excited about building a diverse team and creating an inclusive environment where everyone can thrive, and we encourage all applicants of any educational background, gender identity and expression, sexual orientation, religion, ethnicity, age, citizenship, socioeconomic status, disability, and veteran status to apply.</p> Requirements: 
 <p><b> WHO YOU ARE:</b></p>
 <ul>
  <li> 4+ years of data engineering or relevant industry experience</li>
  <li> 2+ years experience and working proficiency with Spark</li>
  <li> Bachelor&#x2019;s Degree in Computer Science or related technical areas like Math, Statistics, and/or other Engineering degrees</li>
  <li> Strong proficiency with Python</li>
  <li> Advanced proficiency with SQL, comfortable with complex joins</li>
  <li> Experience with AWS data engineering products (S3, RDS, EMR, Glue, Athena...) or similar tools from other cloud providers</li>
  <li> Familiarity with orchestration systems, preferably Airflow</li>
  <li> Self-initiative and an entrepreneurial mindset</li>
  <li> Strong communication skills</li>
  <li> Passion for data</li>
 </ul>
 <h4 class=""jobSectionHeader""><b><i> Nice-To-Haves:</i></b></h4>
 <ul>
  <li> Experience with spatial data, joins and operations</li>
  <li> Working knowledge of Scala</li>
  <li> Proficiency with building Tableau dashboards</li>
  <li> Familiarity with Snowflake</li>
  <li> Experience with Machine Learning</li>
 </ul>
</div>",https://recruiting.paylocity.com/recruiting/jobs/Details/1996356/DIGITAL-ENVOY/Senior-Data-Engineer?source=Indeed_Feed,00fe614f1245546c,,Full-time,,,Remote,Senior Data Engineer,12 days ago,2023-10-06T13:37:47.270Z,,,"$140,000 - $160,000 a year",2023-10-18T13:37:47.273Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=00fe614f1245546c&from=jasx&tk=1hd1g2npm2f32000&vjs=3
162,RVO Health,"AT A GLANCE 
   RVO Health is looking to grow our Talent Analytics team by adding a Senior Data Engineer. In this role, you'll be challenged to help shape our Talent Analytics strategy while working on high-priority data efforts – all in line with our broader mission of attracting diverse talent and giving them an experience that will bring out their very best. You will be responsible for scoping, executing, and delivering technical projects to stakeholders across the Human Capital organization, and producing data engineering & analytical solutions that connect them to the data they need. 
  
 
  What You'll Do 
  
   Develop/maintain data pipelines from various data sources (ADP WFN, Greenhouse Recruiting/Onboarding, CultureAmp, Docebo, etc) to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies. 
   Conduct hands-on, advanced data engineering & analytics using multiple data sources originating from different applications and systems. 
   Collaborate with the data science team to identify new opportunities for deep analytics within the Human Capital organization. 
   Provide input into strategies as they drive the team forward with delivery of business value and technical acumen. 
   Execute on proof of concepts, where appropriate, to help improve our technical processes. 
   Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations. 
  
  What We're Looking For 
  
   5+ years of Data Engineering experience 
   3+ years of writing SQL experience against complex databases for data extraction using AWS Athena (Presto), Databricks Delta Lake along with Data Modeling & Data warehousing experience. 
   3+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines and experience data processing using Parquet and Avro 
   3+ years of Python coding experience, familiar with utilizing packages such as pandas, boto3, requests, json, csv, os 
   3+ years of experience working on AWS services including Glue, Athena, Lambda, S3, SNS, SQS, Cloud formation, Step Functions, Serverless architecture. 
   Experience with GitHub, Code check-in, versioning, Git commands 
   Introduce and drive adoption of CI/CD framework within the team and build/deploy CI/CD Pipelines using Terraform or AWS Cloud Formation 
   Experience with visualization tools such as Tableau, Looker or PowerBI to build dynamic/scalable dashboards and reports. 
   Strong analytical and interpersonal skills 
   Knowledge or experience within Talent/People analytics is a plus 
   Enthusiastic, highly motivated and ability to learn quickly. 
   Able to work through ambiguity in a fast-paced, dynamically changing business environment. 
   Ability to manage multiple tasks at the same time with minimal supervision. 
   
  Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements. 
   
   Starting Salary: $100,000 - $170,000 
    
     Note actual salary is based on geographic location, qualifications and experience
     
   Access to a Free Udemy for Business subscription—thousands of hours of learning content on hundreds of different subjects at your fingertips 
   Health Insurance Coverage (medical, dental, and vision) 
   Life Insurance 
   Short and Long-Term Disability Insurance 
   Flexible Spending Accounts 
   Paid Time Off 
   Holiday Pay 
   401(k) with match 
   Employee Assistance Program 
   Paid Parental Bonding Benefit Program 
   
  This position may occasionally require travel for training and other work-related duties. 
   Who We Are: 
   Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health & wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group's Optum Health. Together we're focused on delivering on our vision of a stronger and healthier world. 
   RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife. 
   We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program. 
   RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person's merit and qualifications. 
   We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com. 
   #LI-REMOTE 
 
 
   RVO Health Privacy Policy: https://rvohealth.com/legal/privacy","<div>
 <div>
  <h2 class=""jobSectionHeader""><b>AT A GLANCE</b></h2> 
  <p> RVO Health is looking to grow our Talent Analytics team by adding a Senior Data Engineer. In this role, you&apos;ll be challenged to help shape our Talent Analytics strategy while working on high-priority data efforts &#x2013; all in line with our broader mission of attracting diverse talent and giving them an experience that will bring out their very best. You will be responsible for scoping, executing, and delivering technical projects to stakeholders across the Human Capital organization, and producing data engineering &amp; analytical solutions that connect them to the data they need.</p> 
 </div> 
 <div>
  <h2 class=""jobSectionHeader""><b>What You&apos;ll Do</b></h2> 
  <ul>
   <li>Develop/maintain data pipelines from various data sources (ADP WFN, Greenhouse Recruiting/Onboarding, CultureAmp, Docebo, etc) to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies.</li> 
   <li>Conduct hands-on, advanced data engineering &amp; analytics using multiple data sources originating from different applications and systems.</li> 
   <li>Collaborate with the data science team to identify new opportunities for deep analytics within the Human Capital organization.</li> 
   <li>Provide input into strategies as they drive the team forward with delivery of business value and technical acumen.</li> 
   <li>Execute on proof of concepts, where appropriate, to help improve our technical processes.</li> 
   <li>Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations.</li> 
  </ul>
  <h2 class=""jobSectionHeader""><b>What We&apos;re Looking For</b></h2> 
  <ul>
   <li>5+ years of Data Engineering experience</li> 
   <li>3+ years of writing SQL experience against complex databases for data extraction using AWS Athena (Presto), Databricks Delta Lake along with Data Modeling &amp; Data warehousing experience.</li> 
   <li>3+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines and experience data processing using Parquet and Avro</li> 
   <li>3+ years of Python coding experience, familiar with utilizing packages such as pandas, boto3, requests, json, csv, os</li> 
   <li>3+ years of experience working on AWS services including Glue, Athena, Lambda, S3, SNS, SQS, Cloud formation, Step Functions, Serverless architecture.</li> 
   <li>Experience with GitHub, Code check-in, versioning, Git commands</li> 
   <li>Introduce and drive adoption of CI/CD framework within the team and build/deploy CI/CD Pipelines using Terraform or AWS Cloud Formation</li> 
   <li>Experience with visualization tools such as Tableau, Looker or PowerBI to build dynamic/scalable dashboards and reports.</li> 
   <li>Strong analytical and interpersonal skills</li> 
   <li>Knowledge or experience within Talent/People analytics is a plus</li> 
   <li>Enthusiastic, highly motivated and ability to learn quickly.</li> 
   <li>Able to work through ambiguity in a fast-paced, dynamically changing business environment.</li> 
   <li>Ability to manage multiple tasks at the same time with minimal supervision.</li> 
  </ul> 
  <p><b>Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements.</b></p> 
  <ul> 
   <li>Starting Salary: &#x24;100,000 - &#x24;170,000<br> 
    <ul>
     <li><i>Note actual salary is based on geographic location, qualifications and experience</i></li>
    </ul></li> 
   <li>Access to a Free Udemy for Business subscription&#x2014;thousands of hours of learning content on hundreds of different subjects at your fingertips</li> 
   <li>Health Insurance Coverage (medical, dental, and vision)</li> 
   <li>Life Insurance</li> 
   <li>Short and Long-Term Disability Insurance</li> 
   <li>Flexible Spending Accounts</li> 
   <li>Paid Time Off</li> 
   <li>Holiday Pay</li> 
   <li>401(k) with match</li> 
   <li>Employee Assistance Program</li> 
   <li>Paid Parental Bonding Benefit Program</li> 
  </ul> 
  <p><b>This position may occasionally require travel for training and other work-related duties.</b></p> 
  <p><b> Who We Are:</b></p> 
  <p> Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health &amp; wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group&apos;s Optum Health. Together we&apos;re focused on delivering on our vision of a stronger and healthier world.</p> 
  <p> RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife.</p> 
  <p> We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.</p> 
  <p> RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person&apos;s merit and qualifications.</p> 
  <p> We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com.</p> 
  <p> #LI-REMOTE</p> 
 </div>
 <div>
  <p> RVO Health Privacy Policy: https://rvohealth.com/legal/privacy</p>
 </div>
</div>",https://boards.greenhouse.io/rvohealth/jobs/4326840005?gh_src=4ad89c055us,623c55b56f304bb5,,,,,"Charlotte, NC","Senior Data Engineer, Talent Analytics",8 days ago,2023-10-10T13:37:44.564Z,,,"$100,000 - $170,000 a year",2023-10-18T13:37:44.566Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=623c55b56f304bb5&from=jasx&tk=1hd1g2npm2f32000&vjs=3
163,Novanta,"Build a career powered by innovations that matter! At Novanta, our innovations power technology products that are transforming healthcare and advanced manufacturing—improving productivity, enhancing people’s lives and redefining what’s possible. We create for our global customers engineered components and sub-systems that deliver extreme precision and performance for a range of mission-critical applications—from minimally invasive surgery to robotics to 3D metal printing.
 
 
 
   Novanta is one global team with over 26 offices located in The Americas, Europe and Asia-Pacific. Looking for a great place to work? You have found it with a culture that embraces teamwork, collaboration and empowerment. Come explore Novanta.
 
 
 
   This position is part of Novanta’s Corporate and Shared Services global teams. Novanta’s Corporate and Shared Services teams play an important role in executing the company’s strategic mission and operations. Included in Corporate and Shared Services are the business functions including Finance, Accounting, Human Resources, Information Technology, Legal, Compliance, Corporate Development and Corporate Marketing. The Corporate and Shared Services teams work closely with all Novanta business units to support operating initiatives contributing to the organization’s financial success.
 
 
 
   Job Summary
 
 
 
   As a Data Analytics Engineer, you will be responsible for building and maintaining the data layer for our analytics stack, top to bottom. Your role will span multiple disciplines from data engineering to data analytics and visualization across all stages of data maturity for the purpose of delivering robust Business Intelligence solutions. You will consider software engineering best practices including version control, automated testing, documentation, code review and continuous integration, as essential to any data stack.
 
 
 
   Primary Responsibilities
 
 
   Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.
   Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)
   Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.
   Automate standard report creation and sharing using tools or scripts
   Convert raw data into consumable information applying business logic and utilizing clean engineering workflows
   Ensure that data, systems, architecture, business logic, and metrics are well-documented
   Support the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics database
   Serve as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence
   Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challenges
 
 
   Draw insights from data and clearly communicate findings to stakeholders and external customers
   Provide exceptional customer service to stakeholders through project execution and timely delivery of solutions
 
 
 
   Required Experience, Education, Skills and Competencies
 
 
   Bachelor’s degree in Information Technology preferred or relevant experience.
   3+ years - Experience with MS SQL Server and Snowflake
   3+ years - Experience with ETL/ELT Tools (ex. Mulesoft, API, Informatica)
   3+ years - Experience using Power BI, Tableau, or similar data visualization tool
   Expert SQL Fluency (Well versed in CTEs and window functions)
   Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDW
   Experienced building data warehouse infrastructure and BI tables
   Motivated individual with strong analytic, problem solving, and troubleshooting skills
 
 
 
   Travel Requirements
 
 
   Less than 20%
 
 
 
   Compensation and Benefits
 
 
   The base pay for this position ranges from $90,000 to $120,000 depending on the geographic market
   Dependent on the position offered, annual bonuses and other forms of compensation may be provided as part of the compensation package.
   Novanta supports all aspects of your life. This position provides a full range of benefits including paid parental and family leave.
 
 
 
   Novanta is proud to be an equal employment opportunity and affirmative action workplace. We consider all qualified applicants without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender identity or expression, national origin, military and veteran status, disability, genetics, or any other category protected by federal law or Novanta policy.
 
 
 
   Please call +1 781-266-5700 if you need a disability accommodation for any part of the employment process.","<div>
 <div>
  Build a career powered by innovations that matter! At Novanta, our innovations power technology products that are transforming healthcare and advanced manufacturing&#x2014;improving productivity, enhancing people&#x2019;s lives and redefining what&#x2019;s possible. We create for our global customers engineered components and sub-systems that deliver extreme precision and performance for a range of mission-critical applications&#x2014;from minimally invasive surgery to robotics to 3D metal printing.
 </div>
 <div></div>
 <div>
   Novanta is one global team with over 26 offices located in The Americas, Europe and Asia-Pacific. Looking for a great place to work? You have found it with a culture that embraces teamwork, collaboration and empowerment. Come explore Novanta.
 </div>
 <div></div>
 <div>
   This position is part of Novanta&#x2019;s Corporate and Shared Services global teams. Novanta&#x2019;s Corporate and Shared Services teams play an important role in executing the company&#x2019;s strategic mission and operations. Included in Corporate and Shared Services are the business functions including Finance, Accounting, Human Resources, Information Technology, Legal, Compliance, Corporate Development and Corporate Marketing. The Corporate and Shared Services teams work closely with all Novanta business units to support operating initiatives contributing to the organization&#x2019;s financial success.
 </div>
 <div></div>
 <div>
   Job Summary
 </div>
 <div></div>
 <div>
   As a Data Analytics Engineer, you will be responsible for building and maintaining the data layer for our analytics stack, top to bottom. Your role will span multiple disciplines from data engineering to data analytics and visualization across all stages of data maturity for the purpose of delivering robust Business Intelligence solutions. You will consider software engineering best practices including version control, automated testing, documentation, code review and continuous integration, as essential to any data stack.
 </div>
 <div></div>
 <div>
   Primary Responsibilities
 </div>
 <ul>
  <li> Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.</li>
  <li> Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)</li>
  <li> Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.</li>
  <li> Automate standard report creation and sharing using tools or scripts</li>
  <li> Convert raw data into consumable information applying business logic and utilizing clean engineering workflows</li>
  <li> Ensure that data, systems, architecture, business logic, and metrics are well-documented</li>
  <li> Support the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics database</li>
  <li> Serve as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence</li>
  <li> Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challenges</li>
 </ul>
 <ul>
  <li> Draw insights from data and clearly communicate findings to stakeholders and external customers</li>
  <li> Provide exceptional customer service to stakeholders through project execution and timely delivery of solutions</li>
 </ul>
 <div></div>
 <div>
   Required Experience, Education, Skills and Competencies
 </div>
 <ul>
  <li> Bachelor&#x2019;s degree in Information Technology preferred or relevant experience.</li>
  <li> 3+ years - Experience with MS SQL Server and Snowflake</li>
  <li> 3+ years - Experience with ETL/ELT Tools (ex. Mulesoft, API, Informatica)</li>
  <li> 3+ years - Experience using Power BI, Tableau, or similar data visualization tool</li>
  <li> Expert SQL Fluency (Well versed in CTEs and window functions)</li>
  <li> Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDW</li>
  <li> Experienced building data warehouse infrastructure and BI tables</li>
  <li> Motivated individual with strong analytic, problem solving, and troubleshooting skills</li>
 </ul>
 <div></div>
 <div>
   Travel Requirements
 </div>
 <ul>
  <li> Less than 20%</li>
 </ul>
 <div></div>
 <div>
   Compensation and Benefits
 </div>
 <ul>
  <li> The base pay for this position ranges from &#x24;90,000 to &#x24;120,000 depending on the geographic market</li>
  <li> Dependent on the position offered, annual bonuses and other forms of compensation may be provided as part of the compensation package.</li>
  <li> Novanta supports all aspects of your life. This position provides a full range of benefits including paid parental and family leave.</li>
 </ul>
 <div></div>
 <div>
   Novanta is proud to be an equal employment opportunity and affirmative action workplace. We consider all qualified applicants without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender identity or expression, national origin, military and veteran status, disability, genetics, or any other category protected by federal law or Novanta policy.
 </div>
 <div></div>
 <div>
   Please call +1 781-266-5700 if you need a disability accommodation for any part of the employment process.
 </div>
</div>",https://novanta.wd5.myworkdayjobs.com/en-US/Novanta-Careers/job/Remote---US/Data-Analytics-Engineer_R006258,7a36e7ca655affdd,,Full-time,,,Remote,Data Analytics Engineer,11 days ago,2023-10-07T13:37:48.908Z,2.9,31.0,"$90,000 - $120,000 a year",2023-10-18T13:37:48.911Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=7a36e7ca655affdd&from=jasx&tk=1hd1g2npm2f32000&vjs=3
164,Proactive Logic Consulting Inc,"Company: Proactive Logic Consulting INC
Location: Remote
Contract Type: Corp to Corp
Duration: 3 Months
About Us:
Proactive Logic Consulting INC is a boutique technology consulting firm specializing in assessments and roadmaps, Azure cloud migrations, and app modernization. We are committed to delivering excellence and empowering businesses to achieve their full potential.
Job Description:
We are seeking a highly skilled Data Factory Engineer with experience in Microsoft Dataverse and the healthcare sector. This is a remote, 3-month Corp to Corp contract position. The ideal candidate will have a strong background in data integration, ETL processes, and cloud-based solutions.
Responsibilities:

 Design, develop, and deploy data pipelines using Azure Data Factory
 Integrate Microsoft Dataverse to streamline data flow and improve operational efficiency
 Work closely with healthcare clients to understand their data requirements and provide solutions
 Ensure data quality and compliance with healthcare regulations
 Collaborate with cross-functional teams to deliver on project milestones

Requirements:

 Minimum 3 years of experience with Azure Data Factory
 Hands-on experience with Microsoft Dataverse
 Prior experience in the healthcare sector is a strong plus
 Strong understanding of ETL processes and data warehousing concepts
 Excellent communication skills

Job Type: Contract
Pay: $75.00 - $100.00 per hour
Expected hours: 40 per week
Experience:

 Azure Data Factory: 5 years (Required)
 Microsoft Dynamics 365: 3 years (Preferred)
 Power BI: 5 years (Required)
 ETL: 10 years (Required)

Work Location: Remote","<p><b>Company: Proactive Logic Consulting INC</b></p>
<p><b>Location: Remote</b></p>
<p><b>Contract Type: Corp to Corp</b></p>
<p><b>Duration: 3 Months</b></p>
<p><b>About Us:</b></p>
<p>Proactive Logic Consulting INC is a boutique technology consulting firm specializing in assessments and roadmaps, Azure cloud migrations, and app modernization. We are committed to delivering excellence and empowering businesses to achieve their full potential.</p>
<p><b>Job Description:</b></p>
<p>We are seeking a highly skilled Data Factory Engineer with experience in Microsoft Dataverse and the healthcare sector. This is a remote, 3-month Corp to Corp contract position. The ideal candidate will have a strong background in data integration, ETL processes, and cloud-based solutions.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Design, develop, and deploy data pipelines using Azure Data Factory</li>
 <li>Integrate Microsoft Dataverse to streamline data flow and improve operational efficiency</li>
 <li>Work closely with healthcare clients to understand their data requirements and provide solutions</li>
 <li>Ensure data quality and compliance with healthcare regulations</li>
 <li>Collaborate with cross-functional teams to deliver on project milestones</li>
</ul>
<p><b>Requirements:</b></p>
<ul>
 <li>Minimum 3 years of experience with Azure Data Factory</li>
 <li>Hands-on experience with Microsoft Dataverse</li>
 <li>Prior experience in the healthcare sector is a strong plus</li>
 <li>Strong understanding of ETL processes and data warehousing concepts</li>
 <li>Excellent communication skills</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;75.00 - &#x24;100.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Experience:</p>
<ul>
 <li>Azure Data Factory: 5 years (Required)</li>
 <li>Microsoft Dynamics 365: 3 years (Preferred)</li>
 <li>Power BI: 5 years (Required)</li>
 <li>ETL: 10 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,2d5efc02e5091f2d,,Contract,,,Remote,"Azure Data Factory Engineer (Remote, Contract)",12 days ago,2023-10-06T13:37:58.579Z,,,$75 - $100 an hour,2023-10-18T13:37:58.585Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=2d5efc02e5091f2d&from=jasx&tk=1hd1g2n5fjm7b800&vjs=3
165,Zscaler,"About Zscaler 
   Zscaler (NASDAQ: ZS) accelerates digital transformation so that customers can be more agile, efficient, resilient, and secure. The Zscaler Zero Trust Exchange is the company's cloud-native platform that protects thousands of customers from cyberattacks and data loss by securely connecting users, devices, and applications in any location. 
   With more than 10 years of experience developing, operating, and scaling the cloud, Zscaler serves thousands of enterprise customers around the world, including 450 of the Forbes Global 2000 organizations. In addition to protecting customers from damaging threats, such as ransomware and data exfiltration, it helps them slash costs, reduce complexity, and improve the user experience by eliminating stacks of latency-creating gateway appliances. 
   Zscaler was founded in 2007 with a mission to make the cloud a safe place to do business and a more enjoyable experience for enterprise users. Zscaler's purpose-built security platform puts a company's defenses and controls where the connections occur—the internet—so that every connection is fast and secure, no matter how or where users connect or where their applications and workloads reside.
 
  Position: Data Engineer 
  Location: Remote within United States 
  Responsibilities/What You'll Do 
  
  Collaborate with Data & Technical architects, integration and engineering teams to capture inbound/outbound data pipeline requirements, conceptualize and develop solutions. 
  Support the evaluation and implementation of the current and future data applications/technologies to support the evolving Zscaler business needs. 
  Collaborate with IT business engagement & applications engineer teams, enterprise data engineering and business data partner teams to identify data source requirements. 
  Profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating into Zscaler's data warehouse in Snowflake. 
  Continuously optimize existing data integrations, data models and views while developing new features and capabilities to meet our business partners needs. 
  Work with Data Platform Lead to design and implement data management standards and best practices. 
  Continue to learn and develop next generation technology/ data capabilities that enhance our data engineering solutions. 
  Develop large scale and mission-critical data pipelines using modern cloud and big data architectures. 
 
 Qualifications/Your Background: 
 
  
   3 - 5 years of experience in data warehouse design & development. 
   Proficiency in building data pipelines to integrate business applications (salesforce, Netsuite, Google Analytics etc) with Snowflake 
   Must have proficiency in data modeling techniques (Dimensional) – able to write structured and efficient queries on large data sets 
   Must have hands-on experience in Python to extract data from APIs, build data pipelines. 
   Completely proficient in advanced SQL, Python/Snowpark(PySpark)/Scala (any Object Oriented language Concepts), ML libraries. 
   Strong hands-on experience in ELT Tools like Matillion, Fivetran, Talend, IDMC (Matillion preferred) , data transformational tool – DBT and in using AWS services like EC2, s3, lambda, glue. 
   Solid understanding of CI/CD process, git versioning, & advanced snowflake concepts like warehouse optimizations, SQL tuning/pruning 
   Experience in using data orchestration workflows using open-source tools like Apache Airflow, Prefect 
   Knowledge of data visualization tools such as Tableau, and/or Power BI 
   Must demonstrate good analytical skills, should be detail-oriented, team-player and must have ability to manage multiple projects simultaneously. 
   
  #LI-YC2 
   #LI-Remote 
 
 
  
    Zscaler’s salary ranges are benchmarked and are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training. 
    The base salary range listed for this full-time position excludes commission/ bonus/ equity (if applicable) + benefits.
  
   Base Pay Range
  
    $110,000—$135,000 USD
  
 
 
   Zscaler is proud to be an equal opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy or related medical conditions), age, national origin, sexual orientation, gender identity or expression, genetic information, disability status, protected veteran status or any other characteristics protected by federal, state, or local laws. 
   See more information by clicking on the Know Your Rights: Workplace Discrimination is Illegal link. 
   Pay Transparency 
   Zscaler complies with all applicable federal, state, and local pay transparency rules. For additional information about the federal requirements, click here. 
   Zscaler is committed to providing reasonable support (called accommodations or adjustments) in our recruiting processes for candidates who are differently abled, have long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support. If you need support, please contact us by sending an email to accommodations@zscaler.com. This email address is used specifically for accommodation requests only, and resumes, CV's, or questions other than accommodations will not be replied to or accepted.","<div>
 <div>
  <p><b>About Zscaler</b></p> 
  <p> Zscaler (NASDAQ: ZS) accelerates digital transformation so that customers can be more agile, efficient, resilient, and secure. The Zscaler Zero Trust Exchange is the company&apos;s cloud-native platform that protects thousands of customers from cyberattacks and data loss by securely connecting users, devices, and applications in any location.</p> 
  <p> With more than 10 years of experience developing, operating, and scaling the cloud, Zscaler serves thousands of enterprise customers around the world, including 450 of the Forbes Global 2000 organizations. In addition to protecting customers from damaging threats, such as ransomware and data exfiltration, it helps them slash costs, reduce complexity, and improve the user experience by eliminating stacks of latency-creating gateway appliances.</p> 
  <p> Zscaler was founded in 2007 with a mission to make the cloud a safe place to do business and a more enjoyable experience for enterprise users. Zscaler&apos;s purpose-built security platform puts a company&apos;s defenses and controls where the connections occur&#x2014;the internet&#x2014;so that every connection is fast and secure, no matter how or where users connect or where their applications and workloads reside.</p>
 </div>
 <p><b> Position: Data Engineer</b></p> 
 <p><b> Location: Remote within United States</b></p> 
 <p><b> Responsibilities/What You&apos;ll Do</b></p> 
 <ul> 
  <li>Collaborate with Data &amp; Technical architects, integration and engineering teams to capture inbound/outbound data pipeline requirements, conceptualize and develop solutions.</li> 
  <li>Support the evaluation and implementation of the current and future data applications/technologies to support the evolving Zscaler business needs.</li> 
  <li>Collaborate with IT business engagement &amp; applications engineer teams, enterprise data engineering and business data partner teams to identify data source requirements.</li> 
  <li>Profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating into Zscaler&apos;s data warehouse in Snowflake.</li> 
  <li>Continuously optimize existing data integrations, data models and views while developing new features and capabilities to meet our business partners needs.</li> 
  <li>Work with Data Platform Lead to design and implement data management standards and best practices.</li> 
  <li>Continue to learn and develop next generation technology/ data capabilities that enhance our data engineering solutions.</li> 
  <li>Develop large scale and mission-critical data pipelines using modern cloud and big data architectures.</li> 
 </ul>
 <h3 class=""jobSectionHeader""><b>Qualifications/Your Background:</b></h3> 
 <div>
  <ul>
   <li>3 - 5 years of experience in data warehouse design &amp; development.</li> 
   <li>Proficiency in building data pipelines to integrate business applications (salesforce, Netsuite, Google Analytics etc) with Snowflake</li> 
   <li>Must have proficiency in data modeling techniques (Dimensional) &#x2013; able to write structured and efficient queries on large data sets</li> 
   <li>Must have hands-on experience in Python to extract data from APIs, build data pipelines.</li> 
   <li>Completely proficient in advanced SQL, Python/Snowpark(PySpark)/Scala (any Object Oriented language Concepts), ML libraries.</li> 
   <li>Strong hands-on experience in ELT Tools like Matillion, Fivetran, Talend, IDMC (Matillion preferred) , data transformational tool &#x2013; DBT and in using AWS services like EC2, s3, lambda, glue.</li> 
   <li>Solid understanding of CI/CD process, git versioning, &amp; advanced snowflake concepts like warehouse optimizations, SQL tuning/pruning</li> 
   <li>Experience in using data orchestration workflows using open-source tools like Apache Airflow, Prefect</li> 
   <li>Knowledge of data visualization tools such as Tableau, and/or Power BI</li> 
   <li>Must demonstrate good analytical skills, should be detail-oriented, team-player and must have ability to manage multiple projects simultaneously.</li> 
  </ul> 
  <p>#LI-YC2</p> 
  <p> #LI-Remote</p> 
 </div>
 <div>
  <div>
   <p> Zscaler&#x2019;s salary ranges are benchmarked and are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.</p> 
   <p> The base salary range listed for this full-time position excludes commission/ bonus/ equity (if applicable) + benefits.</p>
  </div>
  <p><b> Base Pay Range</b></p>
  <div>
    &#x24;110,000&#x2014;&#x24;135,000 USD
  </div>
 </div>
 <div>
  <p> Zscaler is proud to be an equal opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy or related medical conditions), age, national origin, sexual orientation, gender identity or expression, genetic information, disability status, protected veteran status or any other characteristics protected by federal, state, or local laws.</p> 
  <p><i> See more information by clicking on the </i><i>Know Your Rights: Workplace Discrimination is Illegal </i><i>link.</i></p> 
  <p><i> Pay Transparency</i></p> 
  <p><i> Zscaler complies with all applicable federal, state, and local pay transparency rules. For additional information about the federal requirements, </i><i>click here</i><i>.</i></p> 
  <p> Zscaler is committed to providing reasonable support (called accommodations or adjustments) in our recruiting processes for candidates who are differently abled, have long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support. If you need support, please contact us by sending an email to accommodations@zscaler.com. This email address is used specifically for accommodation requests only, and resumes, CV&apos;s, or questions other than accommodations will not be replied to or accepted.</p>
 </div>
</div>",https://boards.greenhouse.io/zscaler/jobs/4101969007?gh_src=29836c077us,211c5cd9297af263,,Full-time,,,"San Jose, CA",Data Engineer,7 days ago,2023-10-11T13:37:53.645Z,3.6,42.0,"$110,000 - $135,000 a year",2023-10-18T13:37:53.659Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=211c5cd9297af263&from=jasx&tk=1hd1g2npm2f32000&vjs=3
166,TIAG,"TIAG is now hiring a
   Senior Data Engineer to join our team supporting the Federal Emergency Management Agency (FEMA). To address FEMA’s data and analytics gaps, the Office of Policy and Program Analysis (OPPA) established the Enterprise Data and Analytics Modernization Initiative (EDAMI) Program to create an enterprise analytics capability by making improvements in people, process, and technology. While this role is primarily remote, to meet the requirements of our FEMA customer, this role must have the ability to travel to FEMA HQ in Washington DC as needed.
  
 
 
  The Senior Data Engineer will be a key technical leader in supporting TIAG's growing team in the Washington DC Metro area, Charleston, SC, and other approved areas. The Senior Data Engineer is responsible for coordinating, communicating, and executing to provide comprehensive data migration, engineering solutions, and source system integration support to deliver mission needs through a focused effort on creating a strong cadre of subject matter experts, streamlined and re-engineered business processes, and development and delivery of a new IT system supporting a greater enterprise data analytics business capability.
  
 
 
  The ideal candidate for this position will possess a wide range of data engineering skills including technical, analytical, and communication skills that can support FEMA in its effort to develop, design, and implement innovative data strategies and solutions. The candidate must possess a strong understanding of data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. In this position, you will work with data (raw, structured, unstructured) to identify opportunities to make improvements, identify correlations, patterns or trends that support data driven decision making.
  
 
 
  The Senior Data Engineer will oversee the department's
  
 
 
  Additional responsibilities include:
  
 
  Creating effective technological solutions for working with and improving processes and systems with big data, using automation where possible. 
  Data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. Conduct complex data analysis and report on results. 
  Analyze and organize raw data, build data systems and pipelines, build algorithms and prototypes. Prepare data for prescriptive and predictive modeling. 
  Evaluate business needs and objectives, interpret trends and patterns. 
  Supports the development of tools, workflows, or other analytical products that support data management initiatives and/or increase program efficiencies. 
  Work with business lines to identify patterns and relationships across a variety of data sets and communicate the technical data assessment to non-technical individuals, communicate and work with individuals and groups in a constructive and collaborative manner. 
  Providing relevant data-related reports to leadership and key stakeholders for decision-making, action planning, and continuous improvement. 
  Providing technical assistance and building understanding among partners about the effective use of data engineering. 
  Work with the team to identify and resolve technical debt to improve the team’s throughput. 
  Identifies and resolves problems and/or issues. 
  Assures quality of task products, services, and deliverables, including participating in reviews, audits, and site visits. 
  Prepare, analyze, and brief recommendations for new technical approaches and technologies to sustain mission success. 
  Perform job responsibilities under Agile project methodologies. 
  Documents technical deliverables, provide training and complete/review other technical documents as required. 
  Support new business proposals and contract performance details by preparing technical past performance references, approach-based solutions and supporting color team reviews. 
 
 
  Required Experience:
  
 
  Bachelor’s degree in computer science or similar technical area desired. 
  Eight (8) years of technical experience in support of data engineering and/or data warehouse programs. 
  Three (3) years executing Agile IT execution methodologies and implementation to include sprint planning, retrospectives and agile tools such as Azure Dev Ops, Jira, etc. 
  Direct experience supporting FEMA is a strong preference. 
  Experience with cloud technologies (Azure), data science, machine learning, decision support tools and programming technologies such as Python, Spark and SQL Scala. 
  Databricks Certified Data Engineer Associate, equivalent certification, or equivalent experience to qualify as a contractor capable of migrating source system data into a Databricks Serverless SQL environment highly desired. 
  Growth minded individual with strong desire for supporting a diverse variety of efforts. 
  Experience supporting proposal development activities desired. 
  Must have the ability to manage and ensure the successful completion of multiple technical tasks in assigned project(s). 
  Maturity, high judgment, negotiation skills, ability to influence, analytical talent and leadership are essential to success in this role. 
  Position requires an active Public Trust at minimum to be considered. 
  
 
   TIAG is an equal opportunity and affirmative action employer that does not discriminate on the basis of race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations. TIAG's policy applies to all terms and conditions of employment. To achieve our goal of equal opportunity, TIAG maintains an affirmative action plan through which it makes good faith efforts to recruit, hire, and advance in employment qualified minorities, women, individuals with disabilities, and protected veterans.","<div>
 <div>
  TIAG is now hiring a
  <b> Senior Data Engineer</b> to join our team supporting the Federal Emergency Management Agency (FEMA). To address FEMA&#x2019;s data and analytics gaps, the Office of Policy and Program Analysis (OPPA) established the Enterprise Data and Analytics Modernization Initiative (EDAMI) Program to create an enterprise analytics capability by making improvements in people, process, and technology. While this role is primarily remote, to meet the requirements of our FEMA customer, this role must have the ability to travel to FEMA HQ in Washington DC as needed.
 </div> 
 <div></div>
 <div>
  The Senior Data Engineer will be a key technical leader in supporting TIAG&apos;s growing team in the Washington DC Metro area, Charleston, SC, and other approved areas. The Senior Data Engineer is responsible for coordinating, communicating, and executing to provide comprehensive data migration, engineering solutions, and source system integration support to deliver mission needs through a focused effort on creating a strong cadre of subject matter experts, streamlined and re-engineered business processes, and development and delivery of a new IT system supporting a greater enterprise data analytics business capability.
 </div> 
 <div></div>
 <div>
  The ideal candidate for this position will possess a wide range of data engineering skills including technical, analytical, and communication skills that can support FEMA in its effort to develop, design, and implement innovative data strategies and solutions. The candidate must possess a strong understanding of data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. In this position, you will work with data (raw, structured, unstructured) to identify opportunities to make improvements, identify correlations, patterns or trends that support data driven decision making.
 </div> 
 <div></div>
 <div>
  The Senior Data Engineer will oversee the department&apos;s
 </div> 
 <div></div>
 <div>
  Additional responsibilities include:
 </div> 
 <ul>
  <li>Creating effective technological solutions for working with and improving processes and systems with big data, using automation where possible.</li> 
  <li>Data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. Conduct complex data analysis and report on results.</li> 
  <li>Analyze and organize raw data, build data systems and pipelines, build algorithms and prototypes. Prepare data for prescriptive and predictive modeling.</li> 
  <li>Evaluate business needs and objectives, interpret trends and patterns.</li> 
  <li>Supports the development of tools, workflows, or other analytical products that support data management initiatives and/or increase program efficiencies.</li> 
  <li>Work with business lines to identify patterns and relationships across a variety of data sets and communicate the technical data assessment to non-technical individuals, communicate and work with individuals and groups in a constructive and collaborative manner.</li> 
  <li>Providing relevant data-related reports to leadership and key stakeholders for decision-making, action planning, and continuous improvement.</li> 
  <li>Providing technical assistance and building understanding among partners about the effective use of data engineering.</li> 
  <li>Work with the team to identify and resolve technical debt to improve the team&#x2019;s throughput.</li> 
  <li>Identifies and resolves problems and/or issues.</li> 
  <li>Assures quality of task products, services, and deliverables, including participating in reviews, audits, and site visits.</li> 
  <li>Prepare, analyze, and brief recommendations for new technical approaches and technologies to sustain mission success.</li> 
  <li>Perform job responsibilities under Agile project methodologies.</li> 
  <li>Documents technical deliverables, provide training and complete/review other technical documents as required.</li> 
  <li>Support new business proposals and contract performance details by preparing technical past performance references, approach-based solutions and supporting color team reviews.</li> 
 </ul>
 <div>
  Required Experience:
 </div> 
 <ul>
  <li>Bachelor&#x2019;s degree in computer science or similar technical area desired.</li> 
  <li>Eight (8) years of technical experience in support of data engineering and/or data warehouse programs.</li> 
  <li>Three (3) years executing Agile IT execution methodologies and implementation to include sprint planning, retrospectives and agile tools such as Azure Dev Ops, Jira, etc.</li> 
  <li>Direct experience supporting FEMA is a strong preference.</li> 
  <li>Experience with cloud technologies (Azure), data science, machine learning, decision support tools and programming technologies such as Python, Spark and SQL Scala.</li> 
  <li>Databricks Certified Data Engineer Associate, equivalent certification, or equivalent experience to qualify as a contractor capable of migrating source system data into a Databricks Serverless SQL environment highly desired.</li> 
  <li>Growth minded individual with strong desire for supporting a diverse variety of efforts.</li> 
  <li>Experience supporting proposal development activities desired.</li> 
  <li>Must have the ability to manage and ensure the successful completion of multiple technical tasks in assigned project(s).</li> 
  <li>Maturity, high judgment, negotiation skills, ability to influence, analytical talent and leadership are essential to success in this role.</li> 
  <li>Position requires an active Public Trust at minimum to be considered.</li> 
 </ul> 
 <div>
  <br> TIAG is an equal opportunity and affirmative action employer that does not discriminate on the basis of race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations. TIAG&apos;s policy applies to all terms and conditions of employment. To achieve our goal of equal opportunity, TIAG maintains an affirmative action plan through which it makes good faith efforts to recruit, hire, and advance in employment qualified minorities, women, individuals with disabilities, and protected veterans.
 </div>
</div>",https://tiag.net/careers/career-opportunities/?gnk=job&gni=8a7883ac8afcf316018b0136b3fb463e&gns=Indeed+Free,90d55aeca2904404,,,,,Remote,Senior Data Engineer,12 days ago,2023-10-06T13:38:09.552Z,4.2,16.0,"$100,000 - $140,000 a year",2023-10-18T13:38:09.562Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=90d55aeca2904404&from=jasx&tk=1hd1g3ucrjm7b801&vjs=3
170,NTT DATA,"NTT DATA Services strives to hire exceptional, innovative, and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now!
  
  We are currently seeking a 
 Data Engineer with Java Background to join our team HYBRID in Jersey City, New Jersey.
  
  
 Job Title: Data Engineer - Java 
 Location: Jersey City, NJ 
 Job Type: Hybrid 
 Duration: 12+ months
  
  
 Responsibilities:
  
 
  Develop, maintain, and optimize data pipelines to extract, transform, and load large datasets from diverse sources into our data ecosystem.
  Design and implement efficient and scalable data models that align with business requirements, ensuring data integrity and performance.
  Collaborate with cross-functional teams to understand data needs and deliver solutions that meet those requirements.
  Work closely with data scientists, analysts, and software engineers to ensure seamless integration of data solutions into larger systems.
  Identify and resolve data quality issues, ensuring accuracy, reliability, and consistency of the data infrastructure.
  Continuously monitor and improve data pipelines and processes, identifying opportunities for automation and optimization.
  Stay updated with emerging trends, technologies, and best practices in data engineering, data modeling, and backend Java engineering.
  Provide technical guidance and mentorship to junior team members, fostering their growth and development.
 
  
  Requirements:
  
 
  Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
  5+years of hands-on experience as a Data Engineer, working on complex data projects and implementing data modeling solutions.
 
  
  Data Engineering
  
  
 Must have
  
 
  Solid understanding of SQL and expertise in working with relational databases (e.g., PostgreSQL, MySQL).
  In-depth knowledge of data modeling techniques and experience with data modeling tools.
  Proficiency in designing and optimizing data pipelines using ETL/ELT frameworks and tools (e.g., Informatica, Apache Spark, Airflow, AWS Glue).
  Working knowledge on Data warehousing
  Familiarity with cloud-based data platforms and services (e.g., Snowflake, AWS, Google Cloud, Azure).
  Experience with version control systems (e.g., Git) and agile software development methodologies.
  Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders.
  Excellent problem-solving skills and the ability to work independently and collaboratively in a fast-paced environment.
 
  
  Good to Have
  
 
  JAVA 8, REST APIs, and microservices, Spring Boot framework
  Alteryx
  UNIX scripting
 
  
  About NTT DATA Services: NTT DATA Services is a recognized leader in IT and business services, including cloud, data, and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients' long-term success. Visit nttdata.com or LinkedIn to learn more.
  
  NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
  
  This position is eligible for company benefits including participation in medical, dental, and vision insurance, flexible spending or health savings account, and AD&D insurance, employee assistance, participation in a 401k program, and additional voluntary or legally required benefits.
  
  Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting pay range for this remote role is [$50.00 - $62.00/hour W2 only]. This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on several factors, including the candidate's actual work location, relevant experience, technical skills, and other qualifications. This position may also be eligible for incentive compensation based on individual and/or company performance.
  
  #LI-IST","<div>
 NTT DATA Services strives to hire exceptional, innovative, and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now!
 <br> 
 <br> We are currently seeking a 
 <b>Data Engineer with Java Background to join our team HYBRID in Jersey City, New Jersey.</b>
 <br> 
 <br> 
 <b>Job Title: Data Engineer - Java </b>
 <b>Location: Jersey City, NJ</b> 
 <b>Job Type: Hybrid</b> 
 <b>Duration: 12+ months</b>
 <br> 
 <br> 
 <b>Responsibilities:</b>
 <br> 
 <ul>
  <li>Develop, maintain, and optimize data pipelines to extract, transform, and load large datasets from diverse sources into our data ecosystem.</li>
  <li>Design and implement efficient and scalable data models that align with business requirements, ensuring data integrity and performance.</li>
  <li>Collaborate with cross-functional teams to understand data needs and deliver solutions that meet those requirements.</li>
  <li>Work closely with data scientists, analysts, and software engineers to ensure seamless integration of data solutions into larger systems.</li>
  <li>Identify and resolve data quality issues, ensuring accuracy, reliability, and consistency of the data infrastructure.</li>
  <li>Continuously monitor and improve data pipelines and processes, identifying opportunities for automation and optimization.</li>
  <li>Stay updated with emerging trends, technologies, and best practices in data engineering, data modeling, and backend Java engineering.</li>
  <li>Provide technical guidance and mentorship to junior team members, fostering their growth and development.</li>
 </ul>
 <br> 
 <b> Requirements:</b>
 <br> 
 <ul>
  <li>Bachelor&apos;s or Master&apos;s degree in Computer Science, Engineering, or a related field.</li>
  <li>5+years of hands-on experience as a Data Engineer, working on complex data projects and implementing data modeling solutions.</li>
 </ul>
 <br> 
 <b> Data Engineering</b>
 <br> 
 <br> 
 <b>Must have</b>
 <br> 
 <ul>
  <li>Solid understanding of <b>SQL</b> and expertise in working with <b>relational databases</b> (e.g., PostgreSQL, MySQL).</li>
  <li>In-depth knowledge of data modeling techniques and experience with <b>data modeling tools</b>.</li>
  <li>Proficiency in designing and optimizing data pipelines using <b>ETL/ELT</b> frameworks and tools (e.g., Informatica, Apache Spark, Airflow, AWS Glue).</li>
  <li>Working knowledge on <b>Data warehousing</b></li>
  <li>Familiarity with <b>cloud-based data platforms</b> and services (e.g., <b>Snowflake</b>, AWS, Google Cloud, Azure).</li>
  <li>Experience with version control systems (e.g., <b>Git</b>) and agile software development methodologies.</li>
  <li>Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders.</li>
  <li>Excellent problem-solving skills and the ability to work independently and collaboratively in a fast-paced environment.</li>
 </ul>
 <br> 
 <b> Good to Have</b>
 <br> 
 <ul>
  <li>JAVA 8, <b>REST</b> <b>APIs</b>, and <b>microservices</b>, <b>Spring Boot</b> framework</li>
  <li>Alteryx</li>
  <li>UNIX scripting</li>
 </ul>
 <br> 
 <b> About NTT DATA Services:</b> NTT DATA Services is a recognized leader in IT and business services, including cloud, data, and applications, headquartered in Texas. As part of NTT DATA, a &#x24;30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients&apos; long-term success. Visit nttdata.com or LinkedIn to learn more.
 <br> 
 <br> NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
 <br> 
 <br> This position is eligible for company benefits including participation in medical, dental, and vision insurance, flexible spending or health savings account, and AD&amp;D insurance, employee assistance, participation in a 401k program, and additional voluntary or legally required benefits.
 <br> 
 <br> Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting pay range for this remote role is [&#x24;50.00 - &#x24;62.00/hour W2 only]. This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on several factors, including the candidate&apos;s actual work location, relevant experience, technical skills, and other qualifications. This position may also be eligible for incentive compensation based on individual and/or company performance.
 <br> 
 <br> #LI-IST
</div>",https://click.appcast.io/track/hnnmduu-org?cs=fzm&jg=2guz&bid=lUf2CslKyPxm6i440ZgUYA==&jobPipeline=Indeed&ittk=8S9CDO3UTW,46368a3a77ba0041,,,,,"Jersey City, NJ 07302",Data Engineer - Java,13 days ago,2023-10-05T13:38:17.802Z,3.5,3971.0,$50 - $62 an hour,2023-10-18T13:38:17.805Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=46368a3a77ba0041&from=jasx&tk=1hd1g40sjjm7l801&vjs=3
174,Abercrombie and Fitch Co.,"Company Description
  Job Description
  The primary responsibility of the Senior Engineer, Global Data & Insights - Data Management is to build data pipelines, model and prepare data, perform complex data analysis to answer Business questions, build and automate data pipeline and quality framework to enable and promote self service data pipelines, assist in operationalizing the AI / ML Engineering solutions. This role is expected to lead and guide other team members and evangelize the design patterns as well as coding standards.
  This role plays an active part in our Data Modernization project to migrate the from on-prem platforms such as IBM Netezza to cloud project
  What Will You Be Doing?
 
   Team up with the engineering teams and enterprise architecture (EA) to define standards, design patterns, accelerators, development practices, DevOps and CI/CD automation
   Create and maintain the data ingestion, quality testing and audit framework
   Conduct complex data analysis to answer the queries from Business Users or Technology team partners either directly from Analysts or stemmed from one of the Reporting tools suchs PowerBI, Tableau, OBIEE.
   Build and automate the data ingestion, transformation and aggregation pipelines using Azure Data Factory, Databricks / Spark, Snowflake, Kafka as well as Enterprise Scheduler tools such as CA Workload automation or Control M
   Setup and evangelize the metadata driven approach to data pipelines to promote self service
   Setup and continuously improve the data quality and audit monitoring as well as alerting
   Constantly evaluate the process automation options and collaborate with engineering as well as architecture to review the proposed design.
   Demonstrate mastery of build and release engineering principles and methodologies including source control, branch management, build and smoke testing, archiving and retention practices
   Adhere to and enhance and document the design principles, best practices by collaborating with Solution and in some cases Enterprise Architects
   Participate in and support the Data Academy and Data Literacy program to train the Business Users and Technology teams on Data
   Respond SLA driven production data quality or pipeline issues
   Work in a fast-paced Agile/Scrum environment
   Identify and assist with implementation of DevOps practices in support of fully automated deployments
   Document the Data Flow Diagrams, Data Models, Technical Data Mapping and Production Support Information for Data Pipelines
   Follow the Industry standard data security practices and evangelize the same across the team.
 
  What Do You Need To Bring?
 
   Bachelor’s degree in Computer Science or Engineering or Mathematics or related field and 5+ years of experience in various cloud technologies within a large-scale organization
   Personal Attributes: Self-starter, Collaborative, Curious, Strong work ethic, highly motivated, Team oriented
   Experience designing and building complex data pipelines in an agile environment
   Expertise on data analysis and wrangling using sql, python, databricks
   Experience with modern cloud development and design concepts; software development lifecycle; multi-developer code versioning and conflict resolution; planning, design, and problem resolution enterprise data applications / solutions
   Demonstrated ability in developing a culture that embraces innovation, and challenges existing paradigms
   5+ years of experience in an Enterprise Data Management or Data Engineering role
   3+ of hands on experience in building metadata driven data pipelines using Azure Data Factory, Databricks / Spark for Cloud Datalake
   5+ years hands on experience with using one or more of the following for data analysis and wrangling Databricks, Python / PySpark, Jupyter Notebooks
   Expert level SQL knowledge on databases such as but not limited to Snowflake, Netezza, Oracle, Sql Server, MySQL, Teradata
   3+ years of hands on experience on one or more of big data technologies such as Cloudera Hadoop, Pivotal, Vertica, MapR is a plus
   Experience working in a multi developer environment and hands on experience in using either azure devops or gitlab
   Preferably experienced in SLA driven Production Data Pipeline or Quality support
   Experience or strong understanding of the traditional enterprise ETL platforms such as IBM Datastage, Informatica, Pentaho, Ab Initio etc.
   Functional knowledge of some of the following technologies - Terraform, Azure CLI, PowerShell, Containerization (Kubernetes, Docker)
   Functional knowledge of one or more Reporting tools such as PowerBI, Tableau, OBIEE
   Team player with excellent communication skills, ability to communicate with the customer directly and able to explain the status of the deliverables in scrum calls
   Ability to implement Agile methodologies and work in an Agile DevOps environment
 
  Our Company
  Abercrombie & Fitch Co. (A&F Co.) is a global retailer of five iconic, omnichannel lifestyle brands catering to the kid through millennial customer: Abercrombie & Fitch, abercrombie kids, Hollister, Gilly Hicks and Social Tourist. At A&F Co., we’re here for our associates, customers and communities on the journey to being and becoming who they are – and because no journey is the same, we strive to create an inclusive culture, where everyone is free to share ideas.
  Our Values
  We lead with purpose and always put our people first, which is evidenced by our Great Place to Work™ Certification, as well as being a 2021 recipient of Fortune’s Best Workplaces in Retail, and named a Best Place to Work for LGBTQ+ Equality by the Human Rights Campaign for 16 consecutive years. We’re proud to offer equitable compensation and benefits, including flexibility and competitive Paid Time Off, as well as education and engagement events, including various Associate Resource Groups, volunteer opportunities and additional time off to give back to our global communities.
  What You'll Get
  As an Abercrombie & Fitch Co. (A&F Co.) associate, you’ll be eligible to participate in a variety of benefit programs designed to fit you and your lifestyle. A&F is committed to providing simple, competitive, and comprehensive benefits that align with our Company’s culture and values, but most importantly – with you! We also provide competitive incentives to reward the commitment our associates have for moving our global business forward:
 
   Incentive Bonus Program
   Paid Time Off and Work From Anywhere Flexibility
   Paid Volunteer Day per Year, allowing you to give back to your community
   Merchandise Discount
   Medical, Dental and Vision Insurance Available
   Life and Disability Insurance
   Associate Assistance Program
   Paid Parental and Adoption Leave
   Access to Carrot to support your unique parenthood journey
   Access to Headspace dedicated to creating healthier, happier lives from the inside out
   401(K) Savings Plan with Company Match
   Opportunities for Career Advancement, we believe in promoting from within
   A Global Team of People Who'll Celebrate you for Being YOU
 
 
 

 Additional Information
  ABERCROMBIE & FITCH CO. IS AN EQUAL OPPORTUNITY EMPLOYER
  Notice (For Colorado, New York, California and Washington): The recruiting pay range for this position is $110,000 - $136,000. Factors that may be used to determine your actual salary may include your specific skills, your years of experience, your work location, comparison to other employees in similar or related roles, or market demands. The range may be modified in the future.","<div>
 Company Description
 <p><b><br> Job Description</b></p>
 <p> The primary responsibility of the <i>Senior Engineer, Global Data &amp; Insights - Data Management</i> is to build data pipelines, model and prepare data, perform complex data analysis to answer Business questions, build and automate data pipeline and quality framework to enable and promote self service data pipelines, assist in operationalizing the AI / ML Engineering solutions. This role is expected to lead and guide other team members and evangelize the design patterns as well as coding standards.</p>
 <p> This role plays an active part in our Data Modernization project to migrate the from on-prem platforms such as IBM Netezza to cloud project</p>
 <p><b> What Will You Be Doing?</b></p>
 <ul>
  <li><p> Team up with the engineering teams and enterprise architecture (EA) to define standards, design patterns, accelerators, development practices, DevOps and CI/CD automation</p></li>
  <li><p> Create and maintain the data ingestion, quality testing and audit framework</p></li>
  <li><p> Conduct complex data analysis to answer the queries from Business Users or Technology team partners either directly from Analysts or stemmed from one of the Reporting tools suchs PowerBI, Tableau, OBIEE.</p></li>
  <li><p> Build and automate the data ingestion, transformation and aggregation pipelines using Azure Data Factory, Databricks / Spark, Snowflake, Kafka as well as Enterprise Scheduler tools such as CA Workload automation or Control M</p></li>
  <li><p> Setup and evangelize the metadata driven approach to data pipelines to promote self service</p></li>
  <li><p> Setup and continuously improve the data quality and audit monitoring as well as alerting</p></li>
  <li><p> Constantly evaluate the process automation options and collaborate with engineering as well as architecture to review the proposed design.</p></li>
  <li><p> Demonstrate mastery of build and release engineering principles and methodologies including source control, branch management, build and smoke testing, archiving and retention practices</p></li>
  <li><p> Adhere to and enhance and document the design principles, best practices by collaborating with Solution and in some cases Enterprise Architects</p></li>
  <li><p> Participate in and support the Data Academy and Data Literacy program to train the Business Users and Technology teams on Data</p></li>
  <li><p> Respond SLA driven production data quality or pipeline issues</p></li>
  <li><p> Work in a fast-paced Agile/Scrum environment</p></li>
  <li><p> Identify and assist with implementation of DevOps practices in support of fully automated deployments</p></li>
  <li><p> Document the Data Flow Diagrams, Data Models, Technical Data Mapping and Production Support Information for Data Pipelines</p></li>
  <li><p> Follow the Industry standard data security practices and evangelize the same across the team.</p></li>
 </ul>
 <p><b> What Do You Need To Bring?</b></p>
 <ul>
  <li><p> Bachelor&#x2019;s degree in Computer Science or Engineering or Mathematics or related field and 5+ years of experience in various cloud technologies within a large-scale organization</p></li>
  <li><p> Personal Attributes: Self-starter, Collaborative, Curious, Strong work ethic, highly motivated, Team oriented</p></li>
  <li><p> Experience designing and building complex data pipelines in an agile environment</p></li>
  <li><p> Expertise on data analysis and wrangling using sql, python, databricks</p></li>
  <li><p> Experience with modern cloud development and design concepts; software development lifecycle; multi-developer code versioning and conflict resolution; planning, design, and problem resolution enterprise data applications / solutions</p></li>
  <li><p> Demonstrated ability in developing a culture that embraces innovation, and challenges existing paradigms</p></li>
  <li><p> 5+ years of experience in an Enterprise Data Management or Data Engineering role</p></li>
  <li><p> 3+ of hands on experience in building metadata driven data pipelines using Azure Data Factory, Databricks / Spark for Cloud Datalake</p></li>
  <li><p> 5+ years hands on experience with using one or more of the following for data analysis and wrangling Databricks, Python / PySpark, Jupyter Notebooks</p></li>
  <li><p> Expert level SQL knowledge on databases such as but not limited to Snowflake, Netezza, Oracle, Sql Server, MySQL, Teradata</p></li>
  <li><p> 3+ years of hands on experience on one or more of big data technologies such as Cloudera Hadoop, Pivotal, Vertica, MapR is a plus</p></li>
  <li><p> Experience working in a multi developer environment and hands on experience in using either azure devops or gitlab</p></li>
  <li><p> Preferably experienced in SLA driven Production Data Pipeline or Quality support</p></li>
  <li><p> Experience or strong understanding of the traditional enterprise ETL platforms such as IBM Datastage, Informatica, Pentaho, Ab Initio etc.</p></li>
  <li><p> Functional knowledge of some of the following technologies - Terraform, Azure CLI, PowerShell, Containerization (Kubernetes, Docker)</p></li>
  <li><p> Functional knowledge of one or more Reporting tools such as PowerBI, Tableau, OBIEE</p></li>
  <li><p> Team player with excellent communication skills, ability to communicate with the customer directly and able to explain the status of the deliverables in scrum calls</p></li>
  <li><p> Ability to implement Agile methodologies and work in an Agile DevOps environment</p></li>
 </ul>
 <p><b> Our Company</b></p>
 <p> Abercrombie &amp; Fitch Co. (A&amp;F Co.) is a global retailer of five iconic, omnichannel lifestyle brands catering to the kid through millennial customer: Abercrombie &amp; Fitch, abercrombie kids, Hollister, Gilly Hicks and Social Tourist. At A&amp;F Co., we&#x2019;re here for our associates, customers and communities on the journey to being and becoming who they are &#x2013; and because no journey is the same, we strive to create an inclusive culture, where everyone is free to share ideas.</p>
 <p><b> Our Values</b></p>
 <p> We lead with purpose and always put our people first, which is evidenced by our Great Place to Work&#x2122; Certification, as well as being a 2021 recipient of Fortune&#x2019;s Best Workplaces in Retail, and named a Best Place to Work for LGBTQ+ Equality by the Human Rights Campaign for 16 consecutive years. We&#x2019;re proud to offer equitable compensation and benefits, including flexibility and competitive Paid Time Off, as well as education and engagement events, including various Associate Resource Groups, volunteer opportunities and additional time off to give back to our global communities.</p>
 <p><b> What You&apos;ll Get</b></p>
 <p> As an Abercrombie &amp; Fitch Co. (A&amp;F Co.) associate, you&#x2019;ll be eligible to participate in a variety of benefit programs designed to fit you and your lifestyle. A&amp;F is committed to providing simple, competitive, and comprehensive benefits that align with our Company&#x2019;s culture and values, but most importantly &#x2013; with you! We also provide competitive incentives to reward the commitment our associates have for moving our global business forward:</p>
 <ul>
  <li> Incentive Bonus Program</li>
  <li> Paid Time Off and Work From Anywhere Flexibility</li>
  <li> Paid Volunteer Day per Year, allowing you to give back to your community</li>
  <li> Merchandise Discount</li>
  <li> Medical, Dental and Vision Insurance Available</li>
  <li> Life and Disability Insurance</li>
  <li> Associate Assistance Program</li>
  <li> Paid Parental and Adoption Leave</li>
  <li> Access to Carrot to support your unique parenthood journey</li>
  <li> Access to Headspace dedicated to creating healthier, happier lives from the inside out</li>
  <li> 401(K) Savings Plan with Company Match</li>
  <li> Opportunities for Career Advancement, we believe in promoting from within</li>
  <li> A Global Team of People Who&apos;ll Celebrate you for Being YOU</li>
 </ul>
</div> 
<br> 
<div>
 Additional Information
 <p><br> ABERCROMBIE &amp; FITCH CO. IS AN EQUAL OPPORTUNITY EMPLOYER</p>
 <p><i> Notice (For Colorado, New York, California and Washington): The recruiting pay range for this position is &#x24;110,000 - &#x24;136,000. Factors that may be used to determine your actual salary may include your specific skills, your years of experience, your work location, comparison to other employees in similar or related roles, or market demands. The range may be modified in the future.</i></p>
</div>",https://jobs.smartrecruiters.com/AbercrombieAndFitchCo/743999936426242-senior-engineer-data-insights-remote-,651209e06d4f7358,,Full-time,,,"Columbus, OH","Senior Engineer, Data Insights (Remote)",7 days ago,2023-10-11T13:38:28.113Z,3.5,5399.0,"$110,000 - $136,000 a year",2023-10-18T13:38:28.122Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=651209e06d4f7358&from=jasx&tk=1hd1g3s5pjfnq801&vjs=3
175,Lincoln Financial,"Date: Oct 5, 2023 
 Primary Location: Radnor, PA, US 
 Company: Lincoln Financial 
 
  
   Alternate Locations: Work from Home
  
   
   
  
   Work Arrangement:
   
  
   Hybrid/Flexible : Work at home and use the office as appropriate for in-person collaboration.
  
   
   
  
   Relocation assistance: is not available for this opportunity.
  
   
   
  
   Requisition #: 72284
  
   
   
  
   
    
     
       The Role at a Glance
     
    
    
    
      
     
      This is a great opportunity to join the growing Life IT organization at Lincoln Financial Group to build next generation application solutions to help our customers achieve their financial goals and objectives. This position will consult/analyze and deliver on more complex assignments/projects for your assigned area(s) of application development responsibility to build out a new data and analytics platform on AWS to integrate with a number of back-end applications. You will also act as a resource and develop more complex innovative business solutions by creating new applications, modifying existing applications and providing post production support (as necessary). You will deliver more complex business application software solutions following the system development life cycle process.
     
      
      
    
   
   
    
     
       What you'll be doing
     
    
    
    
      
     
      Participates in analysis, design, and ETL development as part of Agile / Scrum Develop Team 
      Architect, implement and support big data initiatives for the enterprise (using AWS, Informatica and Python to further these objectives) 
      Lead other developers, solution designers and quality analysts on project led efforts to add new data sources to the Life data platform 
      Understands data mapping and data modeling methodologies including normal form, star, and snowflake to reduce data redundancy and improve data integrity. 
      Liaise with internal Lincoln business partners on requirements, design, testing and production topics in order to create solution proposals and develop code. 
      Performs technical tasks including estimating, analysis, technical requirements, design, construction and unit & integration testing following SDLC. 
      Assist analytical teams with the design and implementation of Data solutions and systems, including integration with Operational Datastores and Data Warehouses, both on-premise and in the Cloud. 
      Develop, enhance, and support Informatica workflows and processes using Informatica Power Center for the extraction and transformation of data in UNIX/Oracle, Windows/MS SQL Server and Aurora/postgres database environments. 
      Maintains knowledge on current and emerging developments/trends for assigned area(s) of responsibility, assesses the impact, and collaborates with Scrum Team and Leadership to incorporate new trends and developments in current and future solutions 
      Participates and enhances organizational initiatives by positively influencing and supporting change management and/or departmental/enterprise initiatives within assigned area(s) of responsibility 
      Identifies and directs the implementation of process improvements that significantly improve quality across the team, department and/or business unit for his/her assigned area(s) of responsibility 
      Provides expertise to team members and applicable internal/external stakeholders on complex assignments/projects for his/her assigned area(s) of responsibility 
      Provides direction on complex assignments, projects, and/or initiatives to build and enhance the capability of his/her assigned area(s) of responsibility 
     
    
   
   
    
     
      What we’re looking for
     
    
    
    
      
     
      4 Year/Bachelor's degree or equivalent work experience (4 years of experience in lieu of Bachelors)_Minimum Required in Computer Science, Computer Information Systems, Information Systems, Information Technology or Computer Engineering or equivalent work experience 
      3+ years developing data movement and engineering applications and worked on integrating disparate systems using ETL following SDLC and/or Agile methodologies 
      3+ years of experience in ETL application development that directly aligns with the specific responsibilities for this position 
      3+ years utilizing Structured Query Language optimization on platforms including Redshift, Oracle, Postgres in order to build and manage large scale data warehouses 
      3+ years utilizing programming languages such as Python in order to program and develop software 
      3+ years of experience in Informatica Big Data Management Solution on AWS Cloud 
      3+ years of experience working on Elastic Map Reduce using AWS Cloud services to process massive amount using Informatica Big Data Management solution 
      3+ years of experience in Informatica PowerCenter (or comparable tool) developing ETL mappings using Designer, Workflow Manager, Workflow Monitor, and Repository Manager 
      3+ years of experience with Architecture Design and Data Modeling 
      3+ years of experience writing Unix/Linux or Windows Scripts in tools such as PERL, Shell script, Python, etc. 
      2+ years of experience in scheduling jobs using Autosys (or comparable distributed scheduler) 
      3+ years of experience in creating complex technical specifications from business requirements/specifications 
      3+ years of experience as a Sr Developer leading/mentoring other Developers on the team 
      Big Data – NO SQL Modeling, Hive, HBase, Pig, Cassandra, MongoDB, Redshift utilizing AWS Services and integrating data on cloud is a plus
     
     
       
     
     
      #DICE
     
    
   
  
   
   
  
   What’s it like to work here?
   
  
   At Lincoln Financial Group, we love what we do. We make meaningful contributions each and every day to empower our customers to take charge of their lives. Working alongside dedicated and talented colleagues, we build fulfilling careers and stronger communities through a company that values our unique perspectives, insights and contributions and invests in programs that empower each of us to take charge of our own future.
  
   
   
  
   What’s in it for YOU:
   
  
   
    
     A clearly defined career framework to help you successfully manage your career
     
   
    
     Leadership development and virtual training opportunities
     
   
    
     PTO/parental leave
     
   
    
     Competitive 401K and employee benefits
     
   
    
     Free financial counseling, health coaching and employee assistance program
     
   
    
     Tuition assistance program
     
   
    
     A leadership team that prioritizes your health and well-being; offering a remote work environment and flexible work hybrid situations
     
   
    
     Effective productivity/technology tools and training
     
  
  
  
   Pay Range: $102,301 - $140,000
  
   
   
  
   Actual base pay could vary based on non-discriminatory factors including but not limited to work experience, education, location, licensure requirements, proficiency and qualifications required for the role. The base pay is just one component of Lincoln’s total rewards package for employees. In addition, the role may be eligible for the Annual Incentive Program, which is discretionary and based on the performance of the company, business unit and individual. Other rewards may include long-term incentives, sales incentives and Lincoln’s standard benefits package.
  
   
   
  
   About The Company
   
  
   Lincoln Financial Group provides advice and solutions that help people take charge of their financial lives with confidence and optimism. Today, approximately 16 million customers trust our retirement, insurance and wealth protection expertise to help address their lifestyle, savings and income goals, and guard against long-term care expenses.
  
   
  
    Headquartered in Radnor, Pennsylvania, Lincoln Financial Group is the marketing name for Lincoln National Corporation (NYSE:LNC) and its affiliates. The company had $290 billion in end-of-period account balances net of reinsurance as of March 31, 2023.
  
   
   
  
   Lincoln Financial Group is a committed corporate citizen included on major sustainability indices including the Dow Jones Sustainability Index North America and ranks among Newsweek’s Most Responsible Companies. Dedicated to diversity, equity and inclusion, we are included on transparency benchmarking tools such as the Corporate Equality Index, the Disability Equality Index and the Bloomberg Gender-Equality Index. Committed to providing our employees with flexible work arrangements, we were named to FlexJobs’ list of the Top 100 Companies to Watch for Remote Jobs in 2022. With a long and rich legacy of acting ethically, telling the truth and speaking up for what is right, Lincoln was recognized as one of Ethisphere’s 2022 World’s Most Ethical Companies®. We create opportunities for early career talent through our intern development program, which ranks among WayUp and Yello’s annual list of Top 100 Internship Programs.
  
   
   
  
   Lincoln is committed to creating a diverse and inclusive environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
  
   
   
  
   Follow us on Facebook, Twitter, LinkedIn, and Instagram.
  
   
   
  
   Be Aware of Fraudulent Recruiting Activities
   
  
   If you are interested in a career at Lincoln, we encourage you to review our current openings and apply on our website. Lincoln values the privacy and security of every applicant and urges all applicants to diligently protect their sensitive personal information from scams targeting job seekers. These scams can take many forms including fake employment applications, bogus interviews and falsified offer letters.
   
  
   Lincoln will not ask applicants to provide their social security numbers, date of birth, bank account information or other sensitive information in job applications. Additionally, our recruiters do not communicate with applicants through free e-mail accounts (Gmail, Yahoo, Hotmail) or conduct interviews utilizing video chat rooms. We will never ask applicants to provide payment during the hiring process or extend an offer without conducting a phone, live video or in-person interview. Please contact Lincoln's fraud team at fraudhotline@lfg.com if you encounter a recruiter or see a job opportunity that seems suspicious.
  
   
   
  
   Additional Information
   
  
   This position may be subject to Lincoln’s Political Contribution Policy. An offer of employment may be contingent upon disclosing to Lincoln the details of certain political contributions. Lincoln may decline to extend an offer or terminate employment for this role if it determines political contributions made could have an adverse impact on Lincoln’s current or future business interests, misrepresentations were made, or for failure to fully disclose applicable political contributions and or fundraising activities.
  
   
   
  
   Any unsolicited resumes/candidate profiles submitted through our web site or to personal e-mail accounts of employees of Lincoln Financial Group are considered property of Lincoln Financial Group and are not subject to payment of agency fees.
  
   
   
  
   Lincoln Financial Group (“LFG”) is an Equal Opportunity employer and, as such, is committed in policy and practice to recruit, hire, compensate, train and promote, in all job classifications, without regard to race, color, religion, sex (including pregnancy), age, national origin, disability, sexual orientation, gender identity and expression, Veteran status, or genetic information. Applicants are evaluated on the basis of job qualifications. If you are a person with a disability that impedes your ability to express your interest for a position through our online application process, or require TTY/TDD assistance, contact us by calling 260-455-2558.","<p></p>
<div>
 <p><b>Date: </b>Oct 5, 2023 </p>
 <p><b>Primary Location:</b> Radnor, PA, US </p>
 <p><b>Company: </b>Lincoln Financial </p>
 <div>
  <div>
   <b>Alternate Locations:</b> Work from Home
  </div>
  <br> 
  <div></div> 
  <div>
   <b>Work Arrangement:</b>
  </div> 
  <div>
   Hybrid/Flexible : Work at home and use the office as appropriate for in-person collaboration.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>Relocation assistance:</b> is not available for this opportunity.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>Requisition #:</b> 72284
  </div>
  <br> 
  <div></div> 
  <div>
   <div>
    <div>
     <div>
      <b> The Role at a Glance</b>
     </div>
    </div>
    <div></div>
    <div>
     <br> 
     <div>
      This is a great opportunity to join the growing Life IT organization at Lincoln Financial Group to build next generation application solutions to help our customers achieve their financial goals and objectives. This position will consult/analyze and deliver on more complex assignments/projects for your assigned area(s) of application development responsibility to build out a new data and analytics platform on AWS to integrate with a number of back-end applications. You will also act as a resource and develop more complex innovative business solutions by creating new applications, modifying existing applications and providing post production support (as necessary). You will deliver more complex business application software solutions following the system development life cycle process.
     </div>
     <br> 
     <div></div> 
    </div>
   </div>
   <div>
    <div>
     <div>
      <b> What you&apos;ll be doing</b>
     </div>
    </div>
    <div></div>
    <div>
     <br> 
     <ul>
      <li>Participates in analysis, design, and ETL development as part of Agile / Scrum Develop Team</li> 
      <li>Architect, implement and support big data initiatives for the enterprise (using AWS, Informatica and Python to further these objectives)</li> 
      <li>Lead other developers, solution designers and quality analysts on project led efforts to add new data sources to the Life data platform</li> 
      <li>Understands data mapping and data modeling methodologies including normal form, star, and snowflake to reduce data redundancy and improve data integrity.</li> 
      <li>Liaise with internal Lincoln business partners on requirements, design, testing and production topics in order to create solution proposals and develop code.</li> 
      <li>Performs technical tasks including estimating, analysis, technical requirements, design, construction and unit &amp; integration testing following SDLC.</li> 
      <li>Assist analytical teams with the design and implementation of Data solutions and systems, including integration with Operational Datastores and Data Warehouses, both on-premise and in the Cloud.</li> 
      <li>Develop, enhance, and support Informatica workflows and processes using Informatica Power Center for the extraction and transformation of data in UNIX/Oracle, Windows/MS SQL Server and Aurora/postgres database environments.</li> 
      <li>Maintains knowledge on current and emerging developments/trends for assigned area(s) of responsibility, assesses the impact, and collaborates with Scrum Team and Leadership to incorporate new trends and developments in current and future solutions</li> 
      <li>Participates and enhances organizational initiatives by positively influencing and supporting change management and/or departmental/enterprise initiatives within assigned area(s) of responsibility</li> 
      <li>Identifies and directs the implementation of process improvements that significantly improve quality across the team, department and/or business unit for his/her assigned area(s) of responsibility</li> 
      <li>Provides expertise to team members and applicable internal/external stakeholders on complex assignments/projects for his/her assigned area(s) of responsibility</li> 
      <li>Provides direction on complex assignments, projects, and/or initiatives to build and enhance the capability of his/her assigned area(s) of responsibility<br> </li>
     </ul>
    </div>
   </div>
   <div>
    <div>
     <div>
      <b>What we&#x2019;re looking for</b>
     </div>
    </div>
    <div></div>
    <div>
     <br> 
     <ul>
      <li>4 Year/Bachelor&apos;s degree or equivalent work experience (4 years of experience in lieu of Bachelors)_Minimum Required in Computer Science, Computer Information Systems, Information Systems, Information Technology or Computer Engineering or equivalent work experience</li> 
      <li>3+ years developing data movement and engineering applications and worked on integrating disparate systems using ETL following SDLC and/or Agile methodologies</li> 
      <li>3+ years of experience in ETL application development that directly aligns with the specific responsibilities for this position</li> 
      <li>3+ years utilizing Structured Query Language optimization on platforms including Redshift, Oracle, Postgres in order to build and manage large scale data warehouses</li> 
      <li>3+ years utilizing programming languages such as Python in order to program and develop software</li> 
      <li>3+ years of experience in Informatica Big Data Management Solution on AWS Cloud</li> 
      <li>3+ years of experience working on Elastic Map Reduce using AWS Cloud services to process massive amount using Informatica Big Data Management solution</li> 
      <li>3+ years of experience in Informatica PowerCenter (or comparable tool) developing ETL mappings using Designer, Workflow Manager, Workflow Monitor, and Repository Manager</li> 
      <li>3+ years of experience with Architecture Design and Data Modeling</li> 
      <li>3+ years of experience writing Unix/Linux or Windows Scripts in tools such as PERL, Shell script, Python, etc.</li> 
      <li>2+ years of experience in scheduling jobs using Autosys (or comparable distributed scheduler)</li> 
      <li>3+ years of experience in creating complex technical specifications from business requirements/specifications</li> 
      <li>3+ years of experience as a Sr Developer leading/mentoring other Developers on the team</li> 
      <li>Big Data &#x2013; NO SQL Modeling, Hive, HBase, Pig, Cassandra, MongoDB, Redshift utilizing AWS Services and integrating data on cloud is a plus</li>
     </ul>
     <div>
      <br> 
     </div>
     <div>
      #DICE
     </div>
    </div>
   </div>
  </div>
  <br> 
  <div></div> 
  <div>
   <b>What&#x2019;s it like to work here?</b>
  </div> 
  <div>
   At Lincoln Financial Group, we love what we do. We make meaningful contributions each and every day to empower our customers to take charge of their lives. Working alongside dedicated and talented colleagues, we build fulfilling careers and stronger communities through a company that values our unique perspectives, insights and contributions and invests in programs that empower each of us to take charge of our own future.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>What&#x2019;s in it for YOU:</b>
  </div> 
  <ul>
   <li>
    <div>
     A clearly defined career framework to help you successfully manage your career
    </div> </li>
   <li>
    <div>
     Leadership development and virtual training opportunities
    </div> </li>
   <li>
    <div>
     PTO/parental leave
    </div> </li>
   <li>
    <div>
     Competitive 401K and employee benefits
    </div> </li>
   <li>
    <div>
     Free financial counseling, health coaching and employee assistance program
    </div> </li>
   <li>
    <div>
     Tuition assistance program
    </div> </li>
   <li>
    <div>
     A leadership team that prioritizes your health and well-being; offering a remote work environment and flexible work hybrid situations
    </div> </li>
   <li>
    <div>
     Effective productivity/technology tools and training
    </div><br> </li>
  </ul>
  <div></div>
  <div>
   <b>Pay Range:</b> &#x24;102,301 - &#x24;140,000
  </div>
  <br> 
  <div></div> 
  <div>
   Actual base pay could vary based on non-discriminatory factors including but not limited to work experience, education, location, licensure requirements, proficiency and qualifications required for the role. The base pay is just one component of Lincoln&#x2019;s total rewards package for employees. In addition, the role may be eligible for the Annual Incentive Program, which is discretionary and based on the performance of the company, business unit and individual. Other rewards may include long-term incentives, sales incentives and Lincoln&#x2019;s standard benefits package.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>About The Company</b>
  </div> 
  <div>
   Lincoln Financial Group provides advice and solutions that help people take charge of their financial lives with confidence and optimism. Today, approximately 16 million customers trust our retirement, insurance and wealth protection expertise to help address their lifestyle, savings and income goals, and guard against long-term care expenses.
  </div>
  <br> 
  <div>
   <br> Headquartered in Radnor, Pennsylvania, Lincoln Financial Group is the marketing name for Lincoln National Corporation (NYSE:LNC) and its affiliates. The company had &#x24;290 billion in end-of-period account balances net of reinsurance as of March 31, 2023.
  </div>
  <br> 
  <div></div> 
  <div>
   Lincoln Financial Group is a committed corporate citizen included on major sustainability indices including the Dow Jones Sustainability Index North America and ranks among Newsweek&#x2019;s Most Responsible Companies. Dedicated to diversity, equity and inclusion, we are included on transparency benchmarking tools such as the Corporate Equality Index, the Disability Equality Index and the Bloomberg Gender-Equality Index. Committed to providing our employees with flexible work arrangements, we were named to FlexJobs&#x2019; list of the Top 100 Companies to Watch for Remote Jobs in 2022. With a long and rich legacy of acting ethically, telling the truth and speaking up for what is right, Lincoln was recognized as one of Ethisphere&#x2019;s 2022 World&#x2019;s Most Ethical Companies&#xae;. We create opportunities for early career talent through our intern development program, which ranks among WayUp and Yello&#x2019;s annual list of Top 100 Internship Programs.
  </div>
  <br> 
  <div></div> 
  <div>
   Lincoln is committed to creating a diverse and inclusive environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
  </div>
  <br> 
  <div></div> 
  <div>
   Follow us on Facebook, Twitter, LinkedIn, and Instagram.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>Be Aware of Fraudulent Recruiting Activities</b>
  </div> 
  <div>
   If you are interested in a career at Lincoln, we encourage you to review our current openings and apply on our website. Lincoln values the privacy and security of every applicant and urges all applicants to diligently protect their sensitive personal information from scams targeting job seekers. These scams can take many forms including fake employment applications, bogus interviews and falsified offer letters.
  </div> 
  <div>
   Lincoln will not ask applicants to provide their social security numbers, date of birth, bank account information or other sensitive information in job applications. Additionally, our recruiters do not communicate with applicants through free e-mail accounts (Gmail, Yahoo, Hotmail) or conduct interviews utilizing video chat rooms. We will never ask applicants to provide payment during the hiring process or extend an offer without conducting a phone, live video or in-person interview. Please contact Lincoln&apos;s fraud team at fraudhotline@lfg.com if you encounter a recruiter or see a job opportunity that seems suspicious.
  </div>
  <br> 
  <div></div> 
  <div>
   <b>Additional Information</b>
  </div> 
  <div>
   This position may be subject to Lincoln&#x2019;s Political Contribution Policy. An offer of employment may be contingent upon disclosing to Lincoln the details of certain political contributions. Lincoln may decline to extend an offer or terminate employment for this role if it determines political contributions made could have an adverse impact on Lincoln&#x2019;s current or future business interests, misrepresentations were made, or for failure to fully disclose applicable political contributions and or fundraising activities.
  </div>
  <br> 
  <div></div> 
  <div>
   Any unsolicited resumes/candidate profiles submitted through our web site or to personal e-mail accounts of employees of Lincoln Financial Group are considered property of Lincoln Financial Group and are not subject to payment of agency fees.
  </div>
  <br> 
  <div></div> 
  <div>
   Lincoln Financial Group (&#x201c;LFG&#x201d;) is an Equal Opportunity employer and, as such, is committed in policy and practice to recruit, hire, compensate, train and promote, in all job classifications, without regard to race, color, religion, sex (including pregnancy), age, national origin, disability, sexual orientation, gender identity and expression, Veteran status, or genetic information. Applicants are evaluated on the basis of job qualifications. If you are a person with a disability that impedes your ability to express your interest for a position through our online application process, or require TTY/TDD assistance, contact us by calling 260-455-2558.
  </div>
 </div>
</div>
<p></p>",https://jobs.lincolnfinancial.com/job/Radnor-Sr_-ETL-Data-Software-Engineer-%28Remote%29-PA/1073013100/?feedId=176100&utm_source=Indeed&utm_campaign=LFG_Indeed,7dba33f9aebd660e,,Internship,,,"Radnor, PA",Sr. ETL Data Software Engineer (Remote),11 days ago,2023-10-07T13:38:26.138Z,3.5,1273.0,"$102,301 - $140,000 a year",2023-10-18T13:38:26.141Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=7dba33f9aebd660e&from=jasx&tk=1hd1g40sjjm7l801&vjs=3
176,Compact Information Systems LLC,"Description: 
  About Deep Sync
  Our parent company, Compact Information Systems LLC, is considered a pioneer of the data industry and was originally founded in 1988 as a mailing list company for direct marketers and print shops. Thirty-five years later, and combining the strength of our sister brands – AccuData Integrated Marketing, AlumniFinder, ASL Marketing, College Bound Selection Service (CBSS), Deep Sync Labs and HomeData – we have grown to become some of the foremost data suppliers in the U.S.
  Today, we are Deep Sync. A company that powers agencies and brands with unmatched audience insights, unsurpassed reach, and unrivaled expertise by combining the industry’s most comprehensive data with easy-to-activate solutions. We provide billions of privacy-first data connections annually to thousands of customers. Learn more about us here.
  Position Overview
  Position Overview: We are looking for a senior level Data Engineer with a strong background in data engineering and a solid understanding of data science principles. The ideal candidate will play a critical role in designing, developing, and maintaining our data infrastructure, while also adding expertise to enable advanced analytics and machine learning initiatives.
  Key Responsibilities:
 
   Data Pipeline Development: 
   
    Design, implement, and maintain scalable data pipelines to collect, process, and store data from various sources. 
    Ensure data quality, accuracy, and consistency throughout the pipeline. 
   
  Data Modeling: 
   
    Design and implement data models for predictive analytics, machine learning, and data exploration. 
    Optimize data structures and storage to support efficient querying and analysis. 
   
  Data Integration: 
   
    Work closely with cross-functional teams to integrate data from diverse sources, including databases, APIs, and external data providers. 
    Develop and maintain ETL processes to transform and enrich raw data into actionable insights. 
   
  Performance Tuning: 
   
    Monitor and optimize the performance of data pipelines and databases to meet business requirements. 
    Identify and resolve bottlenecks and performance issues. 
   
  Continuous Learning and mentoring: 
   
    Stay up-to-date with the latest advancements in data engineering and data science technologies. 
    Share knowledge and mentor junior team members. 
   
 Requirements: 
  Requirements :
 
   5+ years experience in SQL Query Design, SQL Performance Tuning and Query Optimization
   5+ years of relevant experience in Data Warehouse Design,Data Warehouse Technical Architectures, Development and Implementation
   5+ years of relevant experience in ETL Development, ETL Implementation, Unit Testing, Troubleshooting and Support of ETL Processes
   2+ years of relevant experience with the application of Data Science principles and data modeling.
 
  Knowledge and Skills:
 
   Proficiency in SQL Query Design and Implementation
   Strong Experience with Relational Data Warehouse Systems 
   
    Data Warehouse Management Systems 
    Optimization by Indexing, Partitioning and Denormalization 
   
  Strong Ability to build and optimize data sets, ‘big data’ data pipelines and architecture
   Knowledge of data science concepts, machine learning algorithms, and statistical analysis.
   Programming skills in languages such as Python, Java, or C# required.
   Strong analytical and problem-solving skills
 
  Location:
 
   Position may be located in Redmond, Washington, we will consider remote candidates.
 
  Salary:
 
   The annualized salary range for this senior role is $130,000 - $150,000, commensurate with experience and expertise.","<div>
 Description: 
 <p><b> About Deep Sync</b></p>
 <p> Our parent company, Compact Information Systems LLC, is considered a pioneer of the data industry and was originally founded in 1988 as a mailing list company for direct marketers and print shops. Thirty-five years later, and combining the strength of our sister brands &#x2013; AccuData Integrated Marketing, AlumniFinder, ASL Marketing, College Bound Selection Service (CBSS), Deep Sync Labs and HomeData &#x2013; we have grown to become some of the foremost data suppliers in the U.S.</p>
 <p> Today, we are Deep Sync. A company that powers agencies and brands with unmatched audience insights, unsurpassed reach, and unrivaled expertise by combining the industry&#x2019;s most comprehensive data with easy-to-activate solutions. We provide billions of privacy-first data connections annually to thousands of customers. Learn more about us here.</p>
 <p><b> Position Overview</b></p>
 <p> Position Overview: We are looking for a senior level Data Engineer with a strong background in data engineering and a solid understanding of data science principles. The ideal candidate will play a critical role in designing, developing, and maintaining our data infrastructure, while also adding expertise to enable advanced analytics and machine learning initiatives.</p>
 <p><b> Key Responsibilities:</b></p>
 <ul>
  <li> Data Pipeline Development: 
   <ul>
    <li>Design, implement, and maintain scalable data pipelines to collect, process, and store data from various sources.</li> 
    <li>Ensure data quality, accuracy, and consistency throughout the pipeline.</li> 
   </ul></li>
  <li>Data Modeling: 
   <ul>
    <li>Design and implement data models for predictive analytics, machine learning, and data exploration.</li> 
    <li>Optimize data structures and storage to support efficient querying and analysis.</li> 
   </ul></li>
  <li>Data Integration: 
   <ul>
    <li>Work closely with cross-functional teams to integrate data from diverse sources, including databases, APIs, and external data providers.</li> 
    <li>Develop and maintain ETL processes to transform and enrich raw data into actionable insights.</li> 
   </ul></li>
  <li>Performance Tuning: 
   <ul>
    <li>Monitor and optimize the performance of data pipelines and databases to meet business requirements.</li> 
    <li>Identify and resolve bottlenecks and performance issues.</li> 
   </ul></li>
  <li>Continuous Learning and mentoring: 
   <ul>
    <li>Stay up-to-date with the latest advancements in data engineering and data science technologies.</li> 
    <li>Share knowledge and mentor junior team members.</li> 
   </ul></li>
 </ul>Requirements: 
 <p><b> Requirements :</b></p>
 <ul>
  <li> 5+ years experience in SQL Query Design, SQL Performance Tuning and Query Optimization</li>
  <li> 5+ years of relevant experience in Data Warehouse Design,Data Warehouse Technical Architectures, Development and Implementation</li>
  <li> 5+ years of relevant experience in ETL Development, ETL Implementation, Unit Testing, Troubleshooting and Support of ETL Processes</li>
  <li> 2+ years of relevant experience with the application of Data Science principles and data modeling.</li>
 </ul>
 <p><b> Knowledge and Skills:</b></p>
 <ul>
  <li> Proficiency in SQL Query Design and Implementation</li>
  <li> Strong Experience with Relational Data Warehouse Systems 
   <ul>
    <li>Data Warehouse Management Systems</li> 
    <li>Optimization by Indexing, Partitioning and Denormalization</li> 
   </ul></li>
  <li>Strong Ability to build and optimize data sets, &#x2018;big data&#x2019; data pipelines and architecture</li>
  <li> Knowledge of data science concepts, machine learning algorithms, and statistical analysis.</li>
  <li> Programming skills in languages such as Python, Java, or C# required.</li>
  <li> Strong analytical and problem-solving skills</li>
 </ul>
 <p><b> Location:</b></p>
 <ul>
  <li> Position may be located in Redmond, Washington, we will consider remote candidates.</li>
 </ul>
 <p><b> Salary:</b></p>
 <ul>
  <li> The annualized salary range for this senior role is &#x24;130,000 - &#x24;150,000, commensurate with experience and expertise.</li>
 </ul>
</div>",https://recruiting.paylocity.com/recruiting/jobs/Details/2002466/Compact-Information-Systems-LLC/Data-EngineerData-Scientist?source=Indeed_Feed,e6243931b07ddf9f,,,,,"7120 185th Ave NE, Redmond, WA 98052",Data Engineer/Data Scientist,8 days ago,2023-10-10T13:38:31.763Z,,,"$130,000 - $150,000 a year",2023-10-18T13:38:31.841Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=e6243931b07ddf9f&from=jasx&tk=1hd1g3s5pjfnq801&vjs=3
182,Murmuration,"Who We Are
  Murmuration is a nonprofit organization focused on leveraging civic engagement to drive greater equity. We provide sophisticated tools, data, strategic guidance, and programmatic support to help our partner organizations increase civic engagement and marshal support to drive change at the community level. Our best-in-class data and easy-to-use tools have been used by hundreds of organizations to make informed decisions about who they need to reach and how to achieve and sustain impact – and to put those decisions into action. 
 Note: At Murmuration, we are committed to becoming an even more diverse, equitable, and inclusive workplace. To this end, all staff members are expected to actively participate in DEI (diversity, equity, inclusion) programming. 
 About the Position
  We are looking for an innovative Data Engineer who will build and support key components within our data infrastructure with a specific focus on the data pipelines that power our products. This individual will work within our Data Engineering team, partnering with Data Managers and Data Scientists to manage the ongoing delivery of our key data sets for our analytical and product use cases. This individual must be able to understand data requirements and will also be responsible for providing continuous refinement and improvements to our data pipelines. The Data Team is a highly collaborative, friendly, and hard-working group, and we are looking for team members who embody those values. 
 The Data Engineer will report to our Senior Data Engineer. 
 What You’ll Do:
  
  Design, develop, and maintain data pipelines using tools and technologies, such as Dagster and Airflow for orchestration, and Snowflake, AWS, and MongoDB for datastores; 
  Ensure pipelines are scalable, reliable, and fault-tolerant; 
  Be responsible for managing data from various sources, such as third party data providers, data collected, or data created internally; 
  Ensure data is ingested in a timely and efficient manner, with processes to manage data quality and integrity; 
  Transform and cleanse raw data into a structured and usable format; 
  Implement monitoring and alerting processes to detect, communicate, and address issues in data pipelines; 
  Implement data quality checks and validation processes to ensure data accuracy, completeness, and consistency; 
  Continuously optimize data pipelines for better performance and cost efficiency; 
  Maintain comprehensive and up-to-date documentation for data pipelines, including data lineage, dependencies, and configurations; 
  Ensure documentation is up-to-date and accessible to team members; 
  Provide support for data-related issues, including investigating and resolving pipeline failures; 
  Respond to ad-hoc data requests and troubleshoot data-related problems; 
  Collaborate with data scientists, analysts, and other stakeholders to understand their data requirements and deliver data in a usable format; and 
  Work closely with other data engineers to align data pipelines with overall data architecture strategies. 
 
 Requirements
  What You Should Have:
  
  Education and/or experience in Computer Science, Computer Engineering, or relevant field; 
  A minimum of 3 years’ experience working with large scale databases/cloud databases using SQL and Python; 
  Strong organizational and analytical abilities; 
  Strong problem-solving skills; 
  Strong written and verbal communication skills; 
  Familiarity with Data Orchestration Tools (Dagster, Airflow); 
  Familiarity with Snowflake and AWS (primarily S3, EC2, ECS); 
  Experience working flexibly within smaller teams; and 
  Practical knowledge of software development lifecycle (SDLC). 
 
 What You Could Have:
  
  Familiarity with Voter File Data; 
  Experience with or interest in political data; and 
  Experience within a support team providing technical support to other data functions (e.g., Data Scientists, Data Managers, etc.) 
 
 Talented Data Engineers come from all walks of life and careers. If you are passionate about civic engagement and technology, please apply, even if you do not check every box!
  Benefits
  Location and Compensation
  The Data Engineer is a full-time, salaried position with a comprehensive benefits package. It is based anywhere in the U.S. The salary range for this position is $100,000 - $130,000 and is commensurate with experience. 
 Our Culture of Care
  We work hard to create a culture of care to ensure that our staff are best equipped to lead happy, healthy, and balanced lives. To that end, we offer a comprehensive benefits package which includes:
  
  Health, vision, and dental insurance with 100% of premiums covered for you and qualifying family members; 
  Retirement benefits with a 4% employer match; 
  A flexible unlimited PTO plan; 
  Generous paid parental leave; 
  Pre-tax commuter benefits; 
  A company laptop; 
  A flexible remote work environment; 
  A home office setup stipend for all new employees; 
  Monthly reimbursement for remote work expenses; 
  A yearly professional development fund; 
  Mental health and wellness benefits through Calm and Better Help; and 
  Yearly in-person staff retreats; and 
  A welcoming culture that celebrates diversity, equity, and inclusion. 
 
 An Equal-Opportunity Employer with a Commitment to Diversity
  Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.","<div>
 <p><b>Who We Are</b></p>
 <p> Murmuration is a nonprofit organization focused on leveraging civic engagement to drive greater equity. We provide sophisticated tools, data, strategic guidance, and programmatic support to help our partner organizations increase civic engagement and marshal support to drive change at the community level. Our best-in-class data and easy-to-use tools have been used by hundreds of organizations to make informed decisions about who they need to reach and how to achieve and sustain impact &#x2013; and to put those decisions into action.</p> 
 <p>Note: At Murmuration, we are committed to becoming an even more diverse, equitable, and inclusive workplace. To this end, all staff members are expected to actively participate in DEI (diversity, equity, inclusion) programming.</p> 
 <p><b>About the Position</b></p>
 <p> We are looking for an innovative Data Engineer who will build and support key components within our data infrastructure with a specific focus on the data pipelines that power our products. This individual will work within our Data Engineering team, partnering with Data Managers and Data Scientists to manage the ongoing delivery of our key data sets for our analytical and product use cases. This individual must be able to understand data requirements and will also be responsible for providing continuous refinement and improvements to our data pipelines. The Data Team is a highly collaborative, friendly, and hard-working group, and we are looking for team members who embody those values. </p>
 <p>The Data Engineer will report to our Senior Data Engineer.</p> 
 <p><b>What You&#x2019;ll Do:</b></p>
 <ul> 
  <li>Design, develop, and maintain data pipelines using tools and technologies, such as Dagster and Airflow for orchestration, and Snowflake, AWS, and MongoDB for datastores;</li> 
  <li>Ensure pipelines are scalable, reliable, and fault-tolerant;</li> 
  <li>Be responsible for managing data from various sources, such as third party data providers, data collected, or data created internally;</li> 
  <li>Ensure data is ingested in a timely and efficient manner, with processes to manage data quality and integrity;</li> 
  <li>Transform and cleanse raw data into a structured and usable format;</li> 
  <li>Implement monitoring and alerting processes to detect, communicate, and address issues in data pipelines;</li> 
  <li>Implement data quality checks and validation processes to ensure data accuracy, completeness, and consistency;</li> 
  <li>Continuously optimize data pipelines for better performance and cost efficiency;</li> 
  <li>Maintain comprehensive and up-to-date documentation for data pipelines, including data lineage, dependencies, and configurations;</li> 
  <li>Ensure documentation is up-to-date and accessible to team members;</li> 
  <li>Provide support for data-related issues, including investigating and resolving pipeline failures;</li> 
  <li>Respond to ad-hoc data requests and troubleshoot data-related problems;</li> 
  <li>Collaborate with data scientists, analysts, and other stakeholders to understand their data requirements and deliver data in a usable format; and</li> 
  <li>Work closely with other data engineers to align data pipelines with overall data architecture strategies.</li> 
 </ul>
 <p><b>Requirements</b></p>
 <p><b> What You </b><b><i>Should</i></b><b> Have:</b></p>
 <ul> 
  <li>Education and/or experience in Computer Science, Computer Engineering, or relevant field;</li> 
  <li>A minimum of 3 years&#x2019; experience working with large scale databases/cloud databases using SQL and Python;</li> 
  <li>Strong organizational and analytical abilities;</li> 
  <li>Strong problem-solving skills;</li> 
  <li>Strong written and verbal communication skills;</li> 
  <li>Familiarity with Data Orchestration Tools (Dagster, Airflow);</li> 
  <li>Familiarity with Snowflake and AWS (primarily S3, EC2, ECS);</li> 
  <li>Experience working flexibly within smaller teams; and</li> 
  <li>Practical knowledge of software development lifecycle (SDLC).</li> 
 </ul>
 <p><b>What You </b><b><i>Could</i></b><b> Have:</b></p>
 <ul> 
  <li>Familiarity with Voter File Data;</li> 
  <li>Experience with or interest in political data; and</li> 
  <li>Experience within a support team providing technical support to other data functions (e.g., Data Scientists, Data Managers, etc.)</li> 
 </ul>
 <p>Talented Data Engineers come from all walks of life and careers. If you are passionate about civic engagement and technology, please apply, even if you do not check every box!</p>
 <p><b> Benefits</b></p>
 <p><b> Location and Compensation</b></p>
 <p> The Data Engineer is a full-time, salaried position with a comprehensive benefits package. It is based anywhere in the U.S. The salary range for this position is &#x24;100,000 - &#x24;130,000 and is commensurate with experience.</p> 
 <p><b>Our Culture of Care</b></p>
 <p> We work hard to create a culture of care to ensure that our staff are best equipped to lead happy, healthy, and balanced lives. To that end, we offer a comprehensive benefits package which includes:</p>
 <ul> 
  <li>Health, vision, and dental insurance with 100% of premiums covered for you and qualifying family members;</li> 
  <li>Retirement benefits with a 4% employer match;</li> 
  <li>A flexible unlimited PTO plan;</li> 
  <li>Generous paid parental leave;</li> 
  <li>Pre-tax commuter benefits;</li> 
  <li>A company laptop;</li> 
  <li>A flexible remote work environment;</li> 
  <li>A home office setup stipend for all new employees;</li> 
  <li>Monthly reimbursement for remote work expenses;</li> 
  <li>A yearly professional development fund;</li> 
  <li>Mental health and wellness benefits through Calm and Better Help; and</li> 
  <li>Yearly in-person staff retreats; and</li> 
  <li>A welcoming culture that celebrates diversity, equity, and inclusion.</li> 
 </ul>
 <p><b>An Equal-Opportunity Employer with a Commitment to Diversity</b></p>
 <p> Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.</p>
</div>",https://apply.workable.com/murmuration/j/1CEFDBC383,1f74a01d7a395286,,Full-time,,,Remote,Data Engineer,11 days ago,2023-10-07T13:38:43.155Z,,,"$100,000 - $130,000 a year",2023-10-18T13:38:43.160Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=1f74a01d7a395286&from=jasx&tk=1hd1g3s5pjfnq801&vjs=3
185,Ascension,"Details
 
   
 
 
  Department: Data Delivery Governance
   Schedule: Full time
   Location: Remote
 
  Benefits 
 
  Paid time off (PTO)
   Various health insurance options & wellness plans
   Retirement benefits including employer match plans
   Long-term & short-term disability
   Employee assistance programs (EAP)
   Parental leave & adoption assistance
   Tuition reimbursement
   Ways to give back to your community
 
 
 
   As a military friendly organization, Ascension promotes career flexibility and offers many benefits to help support the well-being of our military families, spouses, veterans and reservists. Our associates are empowered to apply their military experience and unique perspective to their civilian career with Ascension.
 
 
  
    Please note, benefits and benefits eligibility can vary by position, exclusions may apply for some roles (for example: PRN, Short-Term Option, etc.). Connect with your Talent Advisor today for additional specifics.
  
 
  Responsibilities 
 
  Responsible for construction and development of ""large-scale cloud data processing systems"" The Data Engineer must have considerable expertise in data warehousing and the job requires proven coding expertise with Python, Java, SQL, and Spark languages. Must be able to implement enterprise cloud data architecture designs, and will work closely with the rest of the scrum team and internal business partners to identify, evaluate, design, and implement large scale data solutions, structured and unstructured, public and proprietary data. The Data Engineer will work iteratively on the cloud platform to design, develop and implement scalable, high performance solutions that offer measurable business value to customers.
 
  
 
 
   Proficient in multiple programming languages, frameworks, domains, and tools.
   Coding skills in Scala
   Experience with gcp platform development tools Pub/sub, cloud storage, big table, big query, data flow, data proc, and composer desired.
   Strong Linux/Unix background and hands on knowledge.
   Knowledge in Hadoop and cloud platforms and surrounding ecosystems.
   Experience with web services and APIs as in RESTful and SOAP.
   Ability to document designs and concepts
   API Orchestration and Choreography for consumer apps
   Well rounded technical expertise in Apache packages and Hybrid cloud architectures
   Pipeline creation and automation for Data Acquisition
   Metadata extraction pipeline design and creation between raw and finally transformed datasets
   Quality control metrics data collection on data acquisition pipelines
   Able to collaborate with scrum team including scrum master, product owner, data analysts, Quality Assurance, business owners, and data architecture to produce the best possible end products
   Experience contributing to and leveraging jira and confluence.
   Strong experience working with real time streaming applications and batch style large scale distributed computing applications using tools like Spark, Kafka, Flume, pubsub, and airflow.
   Ability to work with different file formats like Avro, Parquet, and JSON.
   Managing and scheduling batch jobs.
   Hands on experience in Analysis, Design, Coding and Testing phases of Software Development Life Cycle (SDLC).
 
  Requirements 
 
  Education:
 
 
   High School diploma equivalency with 2 years of cumulative experience OR Associate's degree/Bachelor's degree with 1 year of experience OR 5 years of applicable cumulative job specific experience required. 2 years of leadership or management experience preferred.
 
  Additional Preferences 
 
  5+ years of experience in an engineering role using Python, Java, Spark, and SQL.
   Some of the minimum experience requirement may be met with Masters or other advanced degree
   Cloud Experience Required
   Coding experience with Python, Java, Spark, and SQL
   Strong Linux/Unix background and hands on knowledge.
   Past experience with big data technologies including HDFS, Spark, Impala, Hive,
   Experience with Shell scripting and bash.
   Experience with version control platform github
   Experience unit testing code.
   Experience with development ecosystem including Jenkins, Artifactory, CI/CD, and Terraform.
   Works on problems of diverse scope and complexity ranging from moderate to substantial
   Assists senior professionals in determining methods and procedures for new tasks
   Leads basic or moderately complex projects/activities on semi-regular basis
   Must possess excellent written and verbal communication skills
   Ability to understand and analyze complex data sets
   Exercises independent judgment on basic or moderately complex issues regarding job and related tasks
   Makes recommendations to management on new processes, tools and techniques, or development of new products and services
   Makes decisions regarding daily priorities for a work group; provides guidance to and/or assists staff on non-routine or escalated issues
   Decisions have a moderate impact on operations within a department
   Works under minimal supervision, uses independent judgment requiring analysis of variable factors
   Requires little instruction on day-to-day work and general direction on more complex tasks and projects
   Collaborates with senior professionals in the development of methods, techniques and analytical approach
   Ability to advise management on approaches to optimize for data platform success.
   Able to effectively communicate highly technical information to numerous audiences, including management, the user community, and less-experienced staff.
   Consistently communicate on status of project deliverables
   Consistently provide work effort estimates to management to assist in setting priorities
   Deliver timely work in accordance with estimates
   Solve problems as they arise and communicate potential roadblocks to manage expectations
   Adhere strictly to all security policies
 
 
 
   #LI-Remote #AscensionTechnologies
 
  Why Join Our Team 
 
  When you join Ascension, you join a team of over 150,000 individuals across the country committed to a Mission of serving others and providing compassionate, personalized care to all. Our inclusive culture, continuing education programs, career coaches and benefit offerings are just a few of the resources and tools that team members can use to create a rewarding career path. In fact, Ascension spent nearly $46 million in tuition assistance alone to support associate growth and development. If you are looking for a career where you can grow and make a difference in your community, we invite you to join our team today.
 
  Equal Employment Opportunity Employer 
 
  Ascension will provide equal employment opportunities (EEO) to all associates and applicants for employment regardless of race, color, religion, national origin, citizenship, gender, sexual orientation, gender identification or expression, age, disability, marital status, amnesty, genetic information, carrier status or any other legally protected status or status as a covered veteran in accordance with applicable federal, state and local laws.
 
 
 
   For further information, view the EEO Know Your Rights (English) poster or EEO Know Your Rights (Spanish) poster.
 
 
 
   Pay Non-Discrimination Notice
 
 
 
   Please note that Ascension will make an offer of employment only to individuals who have applied for a position using our official application. Be on alert for possible fraudulent offers of employment. Ascension will not solicit money or banking information from applicants.
 
  E-Verify Statement 
 
  This employer participates in the Electronic Employment Verification Program. Please click the E-Verify link below for more information.
   
   E-Verify","<div>
 <h3 class=""jobSectionHeader""><b>Details</b></h3>
 <div>
  <br> 
 </div>
 <ul>
  <li><b>Department: </b>Data Delivery Governance</li>
  <li><b> Schedule: </b>Full time</li>
  <li><b> Location: </b>Remote</li>
 </ul>
 <h3 class=""jobSectionHeader""><b><br> Benefits </b></h3>
 <div>
  Paid time off (PTO)
  <br> Various health insurance options &amp; wellness plans
  <br> Retirement benefits including employer match plans
  <br> Long-term &amp; short-term disability
  <br> Employee assistance programs (EAP)
  <br> Parental leave &amp; adoption assistance
  <br> Tuition reimbursement
  <br> Ways to give back to your community
 </div>
 <div></div>
 <div>
  <br> As a military friendly organization, Ascension promotes career flexibility and offers many benefits to help support the well-being of our military families, spouses, veterans and reservists. Our associates are empowered to apply their military experience and unique perspective to their civilian career with Ascension.
 </div>
 <div>
  <ul>
   <li><br> <i>Please note, benefits and benefits eligibility can vary by position, exclusions may apply for some roles (for example: PRN, Short-Term Option, etc.). Connect with your Talent Advisor today for additional specifics.</i></li>
  </ul>
 </div>
 <h3 class=""jobSectionHeader""><b><br> Responsibilities </b></h3>
 <div>
  Responsible for construction and development of &quot;large-scale cloud data processing systems&quot; The Data Engineer must have considerable expertise in data warehousing and the job requires proven coding expertise with Python, Java, SQL, and Spark languages. Must be able to implement enterprise cloud data architecture designs, and will work closely with the rest of the scrum team and internal business partners to identify, evaluate, design, and implement large scale data solutions, structured and unstructured, public and proprietary data. The Data Engineer will work iteratively on the cloud platform to design, develop and implement scalable, high performance solutions that offer measurable business value to customers.
 </div>
 <br> 
 <div></div>
 <ul>
  <li> Proficient in multiple programming languages, frameworks, domains, and tools.</li>
  <li> Coding skills in Scala</li>
  <li> Experience with gcp platform development tools Pub/sub, cloud storage, big table, big query, data flow, data proc, and composer desired.</li>
  <li> Strong Linux/Unix background and hands on knowledge.</li>
  <li> Knowledge in Hadoop and cloud platforms and surrounding ecosystems.</li>
  <li> Experience with web services and APIs as in RESTful and SOAP.</li>
  <li> Ability to document designs and concepts</li>
  <li> API Orchestration and Choreography for consumer apps</li>
  <li> Well rounded technical expertise in Apache packages and Hybrid cloud architectures</li>
  <li> Pipeline creation and automation for Data Acquisition</li>
  <li> Metadata extraction pipeline design and creation between raw and finally transformed datasets</li>
  <li> Quality control metrics data collection on data acquisition pipelines</li>
  <li> Able to collaborate with scrum team including scrum master, product owner, data analysts, Quality Assurance, business owners, and data architecture to produce the best possible end products</li>
  <li> Experience contributing to and leveraging jira and confluence.</li>
  <li> Strong experience working with real time streaming applications and batch style large scale distributed computing applications using tools like Spark, Kafka, Flume, pubsub, and airflow.</li>
  <li> Ability to work with different file formats like Avro, Parquet, and JSON.</li>
  <li> Managing and scheduling batch jobs.</li>
  <li> Hands on experience in Analysis, Design, Coding and Testing phases of Software Development Life Cycle (SDLC).</li>
 </ul>
 <h3 class=""jobSectionHeader""><b><br> Requirements </b></h3>
 <div>
  Education:
 </div>
 <ul>
  <li> High School diploma equivalency with 2 years of cumulative experience OR Associate&apos;s degree/Bachelor&apos;s degree with 1 year of experience OR 5 years of applicable cumulative job specific experience required. 2 years of leadership or management experience preferred.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b><br> Additional Preferences </b></h3>
 <ul>
  <li>5+ years of experience in an engineering role using Python, Java, Spark, and SQL.</li>
  <li> Some of the minimum experience requirement may be met with Masters or other advanced degree</li>
  <li> Cloud Experience Required</li>
  <li> Coding experience with Python, Java, Spark, and SQL</li>
  <li> Strong Linux/Unix background and hands on knowledge.</li>
  <li> Past experience with big data technologies including HDFS, Spark, Impala, Hive,</li>
  <li> Experience with Shell scripting and bash.</li>
  <li> Experience with version control platform github</li>
  <li> Experience unit testing code.</li>
  <li> Experience with development ecosystem including Jenkins, Artifactory, CI/CD, and Terraform.</li>
  <li> Works on problems of diverse scope and complexity ranging from moderate to substantial</li>
  <li> Assists senior professionals in determining methods and procedures for new tasks</li>
  <li> Leads basic or moderately complex projects/activities on semi-regular basis</li>
  <li> Must possess excellent written and verbal communication skills</li>
  <li> Ability to understand and analyze complex data sets</li>
  <li> Exercises independent judgment on basic or moderately complex issues regarding job and related tasks</li>
  <li> Makes recommendations to management on new processes, tools and techniques, or development of new products and services</li>
  <li> Makes decisions regarding daily priorities for a work group; provides guidance to and/or assists staff on non-routine or escalated issues</li>
  <li> Decisions have a moderate impact on operations within a department</li>
  <li> Works under minimal supervision, uses independent judgment requiring analysis of variable factors</li>
  <li> Requires little instruction on day-to-day work and general direction on more complex tasks and projects</li>
  <li> Collaborates with senior professionals in the development of methods, techniques and analytical approach</li>
  <li> Ability to advise management on approaches to optimize for data platform success.</li>
  <li> Able to effectively communicate highly technical information to numerous audiences, including management, the user community, and less-experienced staff.</li>
  <li> Consistently communicate on status of project deliverables</li>
  <li> Consistently provide work effort estimates to management to assist in setting priorities</li>
  <li> Deliver timely work in accordance with estimates</li>
  <li> Solve problems as they arise and communicate potential roadblocks to manage expectations</li>
  <li> Adhere strictly to all security policies</li>
 </ul>
 <div></div>
 <div>
  <br> #LI-Remote #AscensionTechnologies
 </div>
 <h3 class=""jobSectionHeader""><b><br> Why Join Our Team </b></h3>
 <div>
  When you join Ascension, you join a team of over 150,000 individuals across the country committed to a Mission of serving others and providing compassionate, personalized care to all. Our inclusive culture, continuing education programs, career coaches and benefit offerings are just a few of the resources and tools that team members can use to create a rewarding career path. In fact, Ascension spent nearly &#x24;46 million in tuition assistance alone to support associate growth and development. If you are looking for a career where you can grow and make a difference in your community, we invite you to join our team today.
 </div>
 <h3 class=""jobSectionHeader""><b><br> Equal Employment Opportunity Employer </b></h3>
 <div>
  Ascension will provide equal employment opportunities (EEO) to all associates and applicants for employment regardless of race, color, religion, national origin, citizenship, gender, sexual orientation, gender identification or expression, age, disability, marital status, amnesty, genetic information, carrier status or any other legally protected status or status as a covered veteran in accordance with applicable federal, state and local laws.
 </div>
 <div></div>
 <div>
  <br> For further information, view the EEO Know Your Rights (English) poster or EEO Know Your Rights (Spanish) poster.
 </div>
 <div></div>
 <div>
  <br> Pay Non-Discrimination Notice
 </div>
 <div></div>
 <div>
  <br> Please note that Ascension will make an offer of employment only to individuals who have applied for a position using our official application. Be on alert for possible fraudulent offers of employment. Ascension will not solicit money or banking information from applicants.
 </div>
 <h3 class=""jobSectionHeader""><b><br> E-Verify Statement </b></h3>
 <div>
  This employer participates in the Electronic Employment Verification Program. Please click the E-Verify link below for more information.
  <br> 
  <br> E-Verify
 </div>
</div>",https://www.indeed.com/rc/clk?jk=3203987de4a37c72&atk=&xpse=SoBU67I3JhoYWyzCmh0LbzkdCdPP,3203987de4a37c72,,Full-time,,,"Irving, TX 75063",Data Engineer - Sr. Specialist,12 days ago,2023-10-06T13:39:05.134Z,3.6,8458.0,"From $97,677 a year",2023-10-18T13:39:05.135Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=3203987de4a37c72&from=jasx&tk=1hd1g5e7qk7bn800&vjs=3
187,"Stark Dev, LLC","This position is open for United States Citizens/ GC/GC EAD/H4 EAD/TN VISA HOLDERS.
Must be a TEKsystems former.
Duration
12
Duration Unit
Month(s)
W2 Contract
We are looking for TEKsystems formers here with 15+ years of experience. Azure Datafactory, Azure Databricks, PySpark, Python are must haves.
Req Description
Job Title
Sr. Data Engineer
Top Skills Details
1) 8+ years of Data Engineering, must be strong with Enterprise Data Warehousing Concepts2) Pyspark, Python3) Azure Data Factory (ADF) and Databricks4) Agile/Scrum5) Must have a consulting mindset, be intellectually curious, ask the right questions, proactive in their approach and communication
DescriptionPosition: Senior Data EngineerOpen Positions: 1Placement Type: 12 month contract to startWorksite Location: 100% Remote
Job Type: Contract
Pay: From $75.00 per hour
Experience level:

 10 years

Schedule:

 Monday to Friday

Experience:

 Data Engineering: 10 years (Required)
 Enterprise Data Warehousing Concepts: 8 years (Required)
 Pyspark, Python: 8 years (Required)
 Azure Data Factory (ADF) and Databricks: 8 years (Required)
 Agile/Scrum: 8 years (Required)
 Snowflake: 3 years (Preferred)
 Kafka: 2 years (Preferred)
 complex SQL queries: 5 years (Required)
 Elastic Search: 2 years (Preferred)
 TEKsystems: 2 years (Required)
 Azure Data factory/ Azure Databricks: 8 years (Required)

Work Location: Remote","<p>This position is open for <b>United States Citizens/ GC/GC EAD/H4 EAD/TN VISA HOLDERS.</b></p>
<p><b>Must be a TEKsystems former.</b></p>
<p><b>Duration</b></p>
<p><b>12</b></p>
<p><b>Duration Unit</b></p>
<p><b>Month(s)</b></p>
<p><b>W2 Contract</b></p>
<p><b>We are looking for TEKsystems formers here with 15+ years of experience. Azure Datafactory, Azure Databricks, PySpark, Python are must haves.</b></p>
<p><b>Req Description</b></p>
<p><b>Job Title</b></p>
<p><b>Sr. Data Engineer</b></p>
<p><b>Top Skills Details</b></p>
<p><b>1) 8+ years of Data Engineering, must be strong with Enterprise Data Warehousing Concepts</b><br><b>2) Pyspark, Python</b><br><b>3) Azure Data Factory (ADF) and Databricks</b><br><b>4) Agile/Scrum</b><br><b>5) Must have a consulting mindset, be intellectually curious, ask the right questions, proactive in their approach and communication</b></p>
<p><b>Description</b><br><b>Position: Senior Data Engineer</b><br><b>Open Positions: 1</b><br><b>Placement Type: 12 month contract to start</b><br><b>Worksite Location: 100% Remote</b></p>
<p>Job Type: Contract</p>
<p>Pay: From &#x24;75.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Engineering: 10 years (Required)</li>
 <li>Enterprise Data Warehousing Concepts: 8 years (Required)</li>
 <li>Pyspark, Python: 8 years (Required)</li>
 <li>Azure Data Factory (ADF) and Databricks: 8 years (Required)</li>
 <li>Agile/Scrum: 8 years (Required)</li>
 <li>Snowflake: 3 years (Preferred)</li>
 <li>Kafka: 2 years (Preferred)</li>
 <li>complex SQL queries: 5 years (Required)</li>
 <li>Elastic Search: 2 years (Preferred)</li>
 <li>TEKsystems: 2 years (Required)</li>
 <li>Azure Data factory/ Azure Databricks: 8 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,117f6275ba0a1edd,,Contract,,,Remote,Sr. Data Engineer / W2,18 days ago,2023-09-30T13:39:16.161Z,,,From $75 an hour,2023-10-18T13:39:16.164Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=117f6275ba0a1edd&from=jasx&tk=1hd1g5htglell801&vjs=3
190,CareFirst BlueCross BlueShield,"Resp & Qualifications 
 Purpose: The Lead Data Engineer/Analyst is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on leading the development of solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company.  Essential Functions:
 
   Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability.
   Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform.
   Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, De-normalized Data Structures, OLAP, and Data Warehousing concepts.
   Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ELT/ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Enforces the implementation of best practices for data auditing, scalability, reliability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
   Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
 
  Qualifications  Education: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field. In lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.
  Experience: 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance.
  Preferred Qualifications: Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.
  Salary Range: $105,408 - $209,352
  Salary Range Disclaimer 
 The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate's work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case's facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).
  Department 
 Department: Finance Data Systems & Decision)
  Equal Employment Opportunity 
 CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
  Where To Apply 
 Please visit our website to apply: www.carefirst.com/careers
  Federal Disc/Physical Demand 
 Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
  PHYSICAL DEMANDS:
  The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
  Sponsorship in US 
 Must be eligible to work in the U.S. without Sponsorship
  #LI-CB1","<div>
 <p><b>Resp &amp; Qualifications</b> </p>
 <p><b>Purpose</b>:<br> The Lead Data Engineer/Analyst is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on leading the development of solutions and helping transform the company&apos;s platforms deliver data-driven, meaningful insights and value to company.<br> <br> <b>Essential Functions</b>:</p>
 <ul>
  <li> Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability.</li>
  <li> Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent MapReduce platform.</li>
  <li> Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, De-normalized Data Structures, OLAP, and Data Warehousing concepts.</li>
  <li> Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ELT/ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.</li>
  <li> Enforces the implementation of best practices for data auditing, scalability, reliability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.</li>
  <li> Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.</li>
  <li> Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.</li>
 </ul>
 <p><b><br> Qualifications</b><br> <br> <b>Education:</b> Bachelor&apos;s Degree in Computer Science, Information Technology or Engineering or related field. In lieu of a Bachelor&apos;s degree, an additional 4 years of relevant work experience is required in addition to the required work experience.</p>
 <p><b> Experience:</b> 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance.</p>
 <p><b><br> Preferred Qualifications: </b>Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.</p>
 <p><br> Salary Range: &#x24;105,408 - &#x24;209,352</p>
 <p><b> Salary Range Disclaimer</b> </p>
 <p>The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate&apos;s work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case&apos;s facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).</p>
 <p><b> Department</b> </p>
 <p><b>Department: </b>Finance Data Systems &amp; Decision)</p>
 <p><b> Equal Employment Opportunity</b> </p>
 <p>CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.</p>
 <p><b> Where To Apply</b> </p>
 <p>Please visit our website to apply: www.carefirst.com/careers</p>
 <p><b> Federal Disc/Physical Demand</b> </p>
 <p>Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.</p>
 <p><b> PHYSICAL DEMANDS:</b></p>
 <p> The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.</p>
 <p><b> Sponsorship in US</b> </p>
 <p>Must be eligible to work in the U.S. without Sponsorship</p>
 <p> #LI-CB1</p>
</div>",https://carefirstcareers.ttcportals.com/jobs/13427494-lead-data-engineer-slash-analyst-remote?tm_job=18889-1A&tm_event=view&tm_company=2380,fe0fc5669a01f689,,Full-time,,,"1501 South Clinton Street, Baltimore, MD 21224",Lead Data Engineer / Analyst (Remote),15 days ago,2023-10-03T13:39:18.780Z,3.8,732.0,"$105,408 - $209,352 a year",2023-10-18T13:39:18.786Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=fe0fc5669a01f689&from=jasx&tk=1hd1g5htglell801&vjs=3
191,OCHIN,"Description: 
  Make a difference at OCHIN
  OCHIN provides leading-edge technology, data analytics, research, and support services to nearly 1,000 community health care sites, reaching nearly 6 million patients nationally. We believe that every individual, no matter their race, ethnicity, background, or zip code, should have fair opportunity to achieve their full health potential. Our work addresses differences in health that are systemic, avoidable, and unjust. We partner, learn, innovate, and advocate, in order to close the gap in health for individuals and communities negatively impacted by racism or other structural inequities.
  At OCHIN, we value the unique perspectives and experiences of every individual and work hard to maintain a culture of belonging.
  Founded in Oregon in 2000, OCHIN employs a growing virtual workforce of more than 1,000 diverse professionals, working remotely across 48 states. We offer a generous compensation package and are committed to supporting our employees’ entire well-being by fostering a healthy work-life balance and equitable opportunity for professional advancement. We are curious, collaborative learners who strive to live our values everyday: learning, heart, belonging and impact. OCHIN is excited to support our continued national expansion and the increasing demand for our innovative tools and services by welcoming new talent to our growing team.
  Position Overview
  The Research Data Engineer will provide high-level professional and technical skills in support of designing, building, and maintaining data pipelines, databases, and cloud platforms to support the needs of the OCHIN Research team.
  In this role, you will be collaborating with an innovative, collaborative team of people moving exciting projects forward and working to improve systems and processes along the way.
 
  Essential Duties
 
   Performing day to day management of on-premises, cloud, and hybrid research databases and database platforms including the Research Data Warehouse
   Integrating and transforming health-related data from a variety of sources and formats such as EHRs, geospatial, claims, and census into analyzable formats for research
   Building and maintaining datasets and data marts
   Monitor and maintain data pipelines proactively to ensure high service availability
   In partnership with Research Data Science staff and leadership, assist with scoping and designing new research data pipelines and platforms to optimize research data solutions
   Create scripts and programs to automate data operations
   Preparing and maintaining technical documentation and metadata
   Providing technical/consultative services to internal and external research partners, investigators, and other research personnel
   Performing other duties as requested by the research team
  Requirements: 
 
  A Master’s level degree in Informatics, Computer Science or related discipline. Equivalent knowledge and skills obtained through a combination of education, training, and experience may meet this requirement.
   At least 5 years of experience in database development and administration in a healthcare and/or health research setting
   At least 3 years’ experience with data warehousing, including ETL techniques
   Strong technical proficiency with SQL required
   High technical proficiency with Microsoft SQL Server, including the ability to create and edit complex queries and T-SQL scripts including dynamic SQL, required; experience with SSIS
   Strong working knowledge of standard desktop computing software packages (word processing, spreadsheets, presentation software, Internet browsers, etc.).
   Strong analytical and problem-solving skills
   Experience with cloud and/or hybrid cloud/on-premises database architectures preferred
   Knowledge of specialized and complex statistical modeling and/or machine learning techniques preferred
 
  Base Pay Overview
  The typical offer range for this role is minimum to midpoint, ($98,819 - $128,465) with the midpoint representing the average pay in a national market scope for this position. Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.
  Work Location and Travel Requirements
  The typical offer range for this role is minimum to midpoint, with the midpoint representing the average pay in a national market scope for this position. Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.
 
   Ability to work independently and efficiently from a home office environment
   High Speed Internet Service
   It is a requirement that employees work in a distraction free workplace
 
  We offer a comprehensive range of benefits. See our website for details: https://ochin.org/employment-openings
  Equal Opportunity Statement
  OCHIN is proud to be an equal opportunity employer. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills for the benefit of our staff, our mission, and the communities we serve.
  As an Equal Opportunity and Affirmative Action employer, OCHIN, Inc. does not discriminate on the basis of race, ethnicity, sex, gender identity, sexual orientation, religion, marital or civil union status, age, disability status, veteran status, or any other protected characteristics. All aspects of employment are based on merit, performance, and business needs.
  COVID-19 Vaccination Requirement
  To keep our colleagues, members, and communities safe, OCHIN requires all employees—including remote employees, contractors, interns, and new hires—to be vaccinated with a COVID-19 vaccine, as supported by state and federal public health officials, as a condition of employment. All new hires are required to provide proof of full vaccination or receive approval for a medical or religious exemption before their hire date.","<div>
 Description: 
 <p><b> Make a difference at OCHIN</b></p>
 <p> OCHIN provides leading-edge technology, data analytics, research, and support services to nearly 1,000 community health care sites, reaching nearly 6 million patients nationally. We believe that every individual, no matter their race, ethnicity, background, or zip code, should have fair opportunity to achieve their full health potential. Our work addresses differences in health that are systemic, avoidable, and unjust. We partner, learn, innovate, and advocate, in order to close the gap in health for individuals and communities negatively impacted by racism or other structural inequities.</p>
 <p><b><br> At OCHIN, we value the unique perspectives and experiences of every individual and work hard to maintain a culture of belonging.</b></p>
 <p> Founded in Oregon in 2000, OCHIN employs a growing virtual workforce of more than 1,000 diverse professionals, working remotely across 48 states. We offer a generous compensation package and are committed to supporting our employees&#x2019; entire well-being by fostering a healthy work-life balance and equitable opportunity for professional advancement. We are curious, collaborative learners who strive to live our values everyday: learning, heart, belonging and impact. OCHIN is excited to support our continued national expansion and the increasing demand for our innovative tools and services by welcoming new talent to our growing team.</p>
 <p><b> Position Overview</b></p>
 <p> The <b>Research Data Engineer </b>will provide high-level professional and technical skills in support of designing, building, and maintaining data pipelines, databases, and cloud platforms to support the needs of the OCHIN Research team.</p>
 <p> In this role, you will be collaborating with an innovative, collaborative team of people moving exciting projects forward and working to improve systems and processes along the way.</p>
 <p></p>
 <p><b><br> Essential Duties</b></p>
 <ul>
  <li> Performing day to day management of on-premises, cloud, and hybrid research databases and database platforms including the Research Data Warehouse</li>
  <li> Integrating and transforming health-related data from a variety of sources and formats such as EHRs, geospatial, claims, and census into analyzable formats for research</li>
  <li> Building and maintaining datasets and data marts</li>
  <li> Monitor and maintain data pipelines proactively to ensure high service availability</li>
  <li> In partnership with Research Data Science staff and leadership, assist with scoping and designing new research data pipelines and platforms to optimize research data solutions</li>
  <li> Create scripts and programs to automate data operations</li>
  <li> Preparing and maintaining technical documentation and metadata</li>
  <li> Providing technical/consultative services to internal and external research partners, investigators, and other research personnel</li>
  <li> Performing other duties as requested by the research team</li>
 </ul> Requirements: 
 <ul>
  <li>A Master&#x2019;s level degree in Informatics, Computer Science or related discipline. Equivalent knowledge and skills obtained through a combination of education, training, and experience may meet this requirement.</li>
  <li> At least 5 years of experience in database development and administration in a healthcare and/or health research setting</li>
  <li> At least 3 years&#x2019; experience with data warehousing, including ETL techniques</li>
  <li> Strong technical proficiency with SQL required</li>
  <li> High technical proficiency with Microsoft SQL Server, including the ability to create and edit complex queries and T-SQL scripts including dynamic SQL, required; experience with SSIS</li>
  <li> Strong working knowledge of standard desktop computing software packages (word processing, spreadsheets, presentation software, Internet browsers, etc.).</li>
  <li> Strong analytical and problem-solving skills</li>
  <li> Experience with cloud and/or hybrid cloud/on-premises database architectures preferred</li>
  <li> Knowledge of specialized and complex statistical modeling and/or machine learning techniques preferred</li>
 </ul>
 <p><b> Base Pay Overview</b></p>
 <p><b> The typical offer range for this role is minimum to midpoint, (&#x24;98,819 - &#x24;128,465) with the midpoint representing the average pay in a national market scope for this position.</b> Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.</p>
 <p><b> Work Location and Travel Requirements</b></p>
 <p> The typical offer range for this role is minimum to midpoint, with the midpoint representing the average pay in a national market scope for this position. Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.</p>
 <ul>
  <li> Ability to work independently and efficiently from a home office environment</li>
  <li> High Speed Internet Service</li>
  <li> It is a requirement that employees work in a distraction free workplace</li>
 </ul>
 <p> We offer a comprehensive range of benefits. See our website for details: https://ochin.org/employment-openings</p>
 <p><b> Equal Opportunity Statement</b></p>
 <p><i> OCHIN is proud to be an equal opportunity employer. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills for the benefit of our staff, our mission, and the communities we serve.</i></p>
 <p><i> As an Equal Opportunity and Affirmative Action employer, OCHIN, Inc. does not discriminate on the basis of race, ethnicity, sex, gender identity, sexual orientation, religion, marital or civil union status, age, disability status, veteran status, or any other protected characteristics. All aspects of employment are based on merit, performance, and business needs.</i></p>
 <p><b> COVID-19 Vaccination Requirement</b></p>
 <p><i> To keep our colleagues, members, and communities safe, OCHIN requires all employees&#x2014;including remote employees, contractors, interns, and new hires&#x2014;to be vaccinated with a COVID-19 vaccine, as supported by state and federal public health officials, as a condition of employment. All new hires are required to provide proof of full vaccination or receive approval for a medical or religious exemption before their hire date.</i></p>
</div>",https://recruiting.paylocity.com/recruiting/jobs/Details/1974445/OCHIN/RESEARCH-DATA-ENGINEER-REMOTE?source=Indeed_Feed&source=Indeed,d2dcb51303834420,,Full-time,,,Remote,RESEARCH DATA ENGINEER (REMOTE),19 days ago,2023-09-29T13:39:25.661Z,3.4,31.0,"$98,819 - $158,111 a year",2023-10-18T13:39:25.676Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=d2dcb51303834420&from=jasx&tk=1hd1g5htglell801&vjs=3
192,Phantom,"Our crypto wallet is used by millions of people and provides a single, convenient solution for managing all your accounts and tokens across Solana, Ethereum, and Polygon. As a data engineer at Phantom you will see a direct correlation between your work, company growth, and our users' satisfaction. Beyond this, you will work with some of the brightest minds in the web3 space, and you'll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few web3 companies can match. 
  
  Snowflake / dbt / Rudderstack / Real-time Data Streaming / Python / SQL 
 
 Responsibilities 
 
  Architect: Design, build and launch extremely efficient and reliable data pipelines (ETL) to move data across a number of platforms including third party analytics, frontend & backend systems. 
  Educate teams: Use your data and analytics experience to see what's missing, and identify and address gaps in their existing systems and processes. 
  Partnership: Partner with stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions 
  Data: Manage the delivery of high-impact dashboards, tools, and data visualizations 
 
 Qualifications 
 
  5+ years of experience in SQL or similar languages, and development experience in at least one language (Python, Javascript, etc.) 
  5+ years of experience in the data warehouse space, custom ETL design, implementation, and maintenance. 
  Experience in leading data-driven projects from definition through interpretation and execution 
  Experience with data architecture, data modeling, schema design, and software development 
  Experience working with cloud analytics platforms and tools, specifically Snowflake, dbt, and Rudderstack 
  Bonus: Experience with blockchain or cryptocurrencies, real-time data streaming, and startup environments. 
  This role is fully remote; however, we're only open to candidates based in US and EU time zones. 
  
 The target base salary for this role will range between $150,000 to $250,000 with the addition of equity and benefits. This is determined by a few factors including your skillset, prior relevant experience, quality of interviews and market factors at the point in time of offer.","<div>
 <p>Our crypto wallet is used by millions of people and provides a single, convenient solution for managing all your accounts and tokens across Solana, Ethereum, and Polygon. As a data engineer at Phantom you will see a direct correlation between your work, company growth, and our users&apos; satisfaction. Beyond this, you will work with some of the brightest minds in the web3 space, and you&apos;ll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few web3 companies can match.</p> 
 <ul> 
  <li>Snowflake / dbt / Rudderstack / Real-time Data Streaming / Python / SQL</li> 
 </ul>
 <h1 class=""jobSectionHeader""><b>Responsibilities</b></h1> 
 <ul>
  <li><b>Architect:</b> Design, build and launch extremely efficient and reliable data pipelines (ETL) to move data across a number of platforms including third party analytics, frontend &amp; backend systems.</li> 
  <li><b>Educate teams:</b> Use your data and analytics experience to see what&apos;s missing, and identify and address gaps in their existing systems and processes.</li> 
  <li><b>Partnership:</b> Partner with stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions</li> 
  <li><b>Data:</b> Manage the delivery of high-impact dashboards, tools, and data visualizations</li> 
 </ul>
 <h1 class=""jobSectionHeader""><b>Qualifications</b></h1> 
 <ul>
  <li>5+ years of experience in SQL or similar languages, and development experience in at least one language (Python, Javascript, etc.)</li> 
  <li>5+ years of experience in the data warehouse space, custom ETL design, implementation, and maintenance.</li> 
  <li>Experience in leading data-driven projects from definition through interpretation and execution</li> 
  <li>Experience with data architecture, data modeling, schema design, and software development</li> 
  <li>Experience working with cloud analytics platforms and tools, specifically Snowflake, dbt, and Rudderstack</li> 
  <li>Bonus: Experience with blockchain or cryptocurrencies, real-time data streaming, and startup environments.</li> 
  <li>This role is fully remote; however, we&apos;re only open to candidates based in US and EU time zones.</li> 
 </ul> 
 <p>The target base salary for this role will range between &#x24;150,000 to &#x24;250,000 with the addition of equity and benefits. This is determined by a few factors including your skillset, prior relevant experience, quality of interviews and market factors at the point in time of offer.</p>
</div>",https://boards.greenhouse.io/phantom45/jobs/4047954005?gh_jid=4047954005&gh_src=5698beae5us,26945eb7798bb1ff,,,,,"518 Castro Street, San Francisco, CA 94114",Senior Data Engineer,15 days ago,2023-10-03T13:39:18.914Z,,,"$150,000 - $250,000 a year",2023-10-18T13:39:18.922Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=26945eb7798bb1ff&from=jasx&tk=1hd1g5htglell801&vjs=3
197,Mission Cloud,"As a Data Engineer, Planning & Analysis, you will report to the Vice President, Planning & Analysis. You will build and maintain Mission Cloud’s internal data lake and its data connections to Mission Cloud’s Business Intelligence Platform. You will collaborate across multiple departments to understand data sources and reporting needs, and business problems. You will use your technical expertise to automate and simplify reporting and data analysis requirements for teams at Mission Cloud.
 
 
 
   This position is 100% remote.
  
 Responsibilities 
 
  Build and maintain Mission Cloud’s internal data lake
   Establish connections from the data lake to business intelligence tools, including Amazon Quicksight
   Configure AWS services to support ETL and data warehousing
   Build and maintain the process for syncing the data lake with Amazon Quicksight
   Create operational efficiencies and improve data governance by establishing a singular data access point and source of truth for Mission Cloud
   Build repeatable, efficient processes to help teams at Mission Cloud become self-sufficient and able to accurately measure, monitor, and forecast their key performance indicators (KPIs)
   Creation of data visualizations and dashboards that effectively communicate key business insights and tell a compelling story
   Exercises judgment in selecting methods, evaluating, adapting of complex techniques and evaluation criteria for obtaining results pertaining to data warehousing.
 
  Requirements
 
   Ability to work in a business intelligence / data engineering role, preferably in the technology industry
   Design & implementation experience with distributed applications
   Exhibit advanced wide-ranging experience, using in-depth professional knowledge of database architectures and data pipeline development 
  Ability to handle unstructured, and semi-structured data, working in a data lake environment
   Experience building data lakes in Redshift
   Ability to translate data into insightful dashboards in Amazon Quicksight or a similar tool
   Expertise in designing and implementing AWS-based solutions, including services like Amazon Redshift, AWS Glue, Lambda, AWS Athena, AWS QuickSight
   Demonstrated proficiency in data analytics, data transformation (ETL/ELT), data integration, data warehousing, data lake architecture, and relevant AWS services.
   Experience working with extracting and ingesting data from various data sources using AWS Glue, AWS AppFlow, Lambda, Step Functions, Event Bridge
   Experience working with technologies like Amazon CDK, Cloud Formation, Terraform or equivalent
   Experience working with AWS Cloud Watch, Cloud Trail, Secrets Manager, KMS
   Ability to work across departments and manage multiple stakeholder needs and requests at one time
   Ability to manage and execute overlapping projects of varying durations and complexities
   Knowledge of Finance, Mathematics, Statistics or related field
   AWS Certification (required within 6 months of hire)
 
  Benefits
 
   Access to health, vision and dental insurance with options 100% covered by Mission Cloud for employee and their dependents
   Flexible Spending Accounts (Healthcare & Dependent Care)
   Generous Paid Time Off (FlexPTO, parental leave, volunteering time off)
   Reproductive health benefits
   Pet insurance
   401k matching program
   Life insurance paid by Mission Cloud
   Monthly flex stipend
   Monthly cell phone stipend
   Home office expense benefit
   An internal department dedicated to helping team members on their career path
   Inclusive work environment with several Employee Resource Groups
 
 
   Placement within the range is determined by a variety of factors, including but not limited to knowledge, skills, and ability as evaluated during the interview process. Range: $110000-$139,435
 
 
   Commitment to Diversity and Inclusion
 
 
 
   We are committed to diversity and inclusion. We value every individual’s unique story, experience, and perspective. We aim to amplify the voices of our team members and our community to create a safe, empathetic, and inclusive environment where everyone can contribute to one’s authentic self. Mission Cloud makes every effort to ensure that all employees are compensated fairly regardless of gender, ethnicity, race, or past salary history. We understand that fair compensation practices establish that diversity, fair hiring processes, and fair pay are part of who we are as a company and maintain positive employee morale. We use market data to define salary ranges for each role and regularly review compensation adjustments as needed based on salary range updates.
 
 
 
   Mission Cloud is an Equal Opportunity Employer and participant in the U.S. Federal E-Verify program. Mission Cloud will consider qualified applicants with criminal histories in a manner consistent with The Los Angeles Fair Chance Initiative for Hiring Ordinance.
 
 
 
   About Mission Cloud
 
 
 
   Mission Cloud is an Amazon Web Services (AWS) Premier Consulting Partner and MSP. Clients depend on us to expertly and securely architect, migrate, manage, and optimize their cloud environments.
 
 
   Mission Cloud’s team of AWS Certified Solutions Architects and DevOps Engineers are ready to help you harness the full power of the AWS cloud to transform your business and operations.","<div>
 <div>
  As a Data Engineer, Planning &amp; Analysis, you will report to the Vice President, Planning &amp; Analysis. You will build and maintain Mission Cloud&#x2019;s internal data lake and its data connections to Mission Cloud&#x2019;s Business Intelligence Platform. You will collaborate across multiple departments to understand data sources and reporting needs, and business problems. You will use your technical expertise to automate and simplify reporting and data analysis requirements for teams at Mission Cloud.
 </div>
 <div></div>
 <div>
  <br> This position is 100% remote.
 </div> 
 <h3 class=""jobSectionHeader""><b>Responsibilities </b></h3>
 <ul>
  <li>Build and maintain Mission Cloud&#x2019;s internal data lake</li>
  <li> Establish connections from the data lake to business intelligence tools, including Amazon Quicksight</li>
  <li> Configure AWS services to support ETL and data warehousing</li>
  <li> Build and maintain the process for syncing the data lake with Amazon Quicksight</li>
  <li> Create operational efficiencies and improve data governance by establishing a singular data access point and source of truth for Mission Cloud</li>
  <li> Build repeatable, efficient processes to help teams at Mission Cloud become self-sufficient and able to accurately measure, monitor, and forecast their key performance indicators (KPIs)</li>
  <li> Creation of data visualizations and dashboards that effectively communicate key business insights and tell a compelling story</li>
  <li> Exercises judgment in selecting methods, evaluating, adapting of complex techniques and evaluation criteria for obtaining results pertaining to data warehousing.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Requirements</b></h3>
 <ul>
  <li> Ability to work in a business intelligence / data engineering role, preferably in the technology industry</li>
  <li> Design &amp; implementation experience with distributed applications</li>
  <li> Exhibit advanced wide-ranging experience, using in-depth professional knowledge of database architectures and data pipeline development </li>
  <li>Ability to handle unstructured, and semi-structured data, working in a data lake environment</li>
  <li> Experience building data lakes in Redshift</li>
  <li> Ability to translate data into insightful dashboards in Amazon Quicksight or a similar tool</li>
  <li> Expertise in designing and implementing AWS-based solutions, including services like Amazon Redshift, AWS Glue, Lambda, AWS Athena, AWS QuickSight</li>
  <li> Demonstrated proficiency in data analytics, data transformation (ETL/ELT), data integration, data warehousing, data lake architecture, and relevant AWS services.</li>
  <li> Experience working with extracting and ingesting data from various data sources using AWS Glue, AWS AppFlow, Lambda, Step Functions, Event Bridge</li>
  <li> Experience working with technologies like Amazon CDK, Cloud Formation, Terraform or equivalent</li>
  <li> Experience working with AWS Cloud Watch, Cloud Trail, Secrets Manager, KMS</li>
  <li> Ability to work across departments and manage multiple stakeholder needs and requests at one time</li>
  <li> Ability to manage and execute overlapping projects of varying durations and complexities</li>
  <li> Knowledge of Finance, Mathematics, Statistics or related field</li>
  <li> AWS Certification (required within 6 months of hire)</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Benefits</b></h3>
 <ul>
  <li> Access to health, vision and dental insurance with options 100% covered by Mission Cloud for employee and their dependents</li>
  <li> Flexible Spending Accounts (Healthcare &amp; Dependent Care)</li>
  <li> Generous Paid Time Off (FlexPTO, parental leave, volunteering time off)</li>
  <li> Reproductive health benefits</li>
  <li> Pet insurance</li>
  <li> 401k matching program</li>
  <li> Life insurance paid by Mission Cloud</li>
  <li> Monthly flex stipend</li>
  <li> Monthly cell phone stipend</li>
  <li> Home office expense benefit</li>
  <li> An internal department dedicated to helping team members on their career path</li>
  <li> Inclusive work environment with several Employee Resource Groups</li>
 </ul>
 <div>
   Placement within the range is determined by a variety of factors, including but not limited to knowledge, skills, and ability as evaluated during the interview process. Range: &#x24;110000-&#x24;139,435
 </div>
 <div>
  <b> Commitment to Diversity and Inclusion</b>
 </div>
 <div></div>
 <div>
  <br> We are committed to diversity and inclusion. We value every individual&#x2019;s unique story, experience, and perspective. We aim to amplify the voices of our team members and our community to create a safe, empathetic, and inclusive environment where everyone can contribute to one&#x2019;s authentic self. Mission Cloud makes every effort to ensure that all employees are compensated fairly regardless of gender, ethnicity, race, or past salary history. We understand that fair compensation practices establish that diversity, fair hiring processes, and fair pay are part of who we are as a company and maintain positive employee morale. We use market data to define salary ranges for each role and regularly review compensation adjustments as needed based on salary range updates.
 </div>
 <div></div>
 <div>
  <br> Mission Cloud is an Equal Opportunity Employer and participant in the U.S. Federal E-Verify program. Mission Cloud will consider qualified applicants with criminal histories in a manner consistent with The Los Angeles Fair Chance Initiative for Hiring Ordinance.
 </div>
 <div></div>
 <div>
  <b><br> About Mission Cloud</b>
 </div>
 <div></div>
 <div>
  <br> Mission Cloud is an Amazon Web Services (AWS) Premier Consulting Partner and MSP. Clients depend on us to expertly and securely architect, migrate, manage, and optimize their cloud environments.
 </div>
 <div>
   Mission Cloud&#x2019;s team of AWS Certified Solutions Architects and DevOps Engineers are ready to help you harness the full power of the AWS cloud to transform your business and operations.
 </div>
</div>",https://jobs.lever.co/missioncloud/5d3925fe-0239-4f4e-b728-0f20b27f004f?lever-source=Indeed,60ee2be8d11b8703,,Full-time,,,Remote,"Senior Data Engineer, Planning & Analysis",18 days ago,2023-09-30T13:39:38.973Z,4.0,2.0,"$130,000 - $156,958 a year",2023-10-18T13:39:38.987Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=60ee2be8d11b8703&from=jasx&tk=1hd1g5htglell801&vjs=3
199,Chartboost,"Who we are: 
  Chartboost is the leading in-app monetization and programmatic advertising platform. We reach a global audience of over 700 million monthly active users and process over 2.7 trillion monthly advertising auctions. The Chartboost SDK is one the most widely integrated mobile ad SDKs and through the Chartboost Exchange, Ad Network, DSP and other services, we empower mobile app developers to build businesses, while connecting advertisers to highly engaged audiences. 
  
 Chartboost is hiring accomplished data engineers to join our team, helping us build and maintain a data platform that supports diverse uses. This includes data analysis, exploration, aggregation, user modeling, and scalable training systems. As part of the Data Team you will be a key player in our small, nimble, internationally-distributed team and drive significant impact all across the company's data technology and business. 
  We currently use Python, Java, and Scala to develop tools with Spark, Kafka, Airflow, MySQL, Druid, Spinnaker, and Kubernetes. We run in both GCP and AWS, but primarily work with Dataproc, Dataflow, BigQuery and Big Table. You will have the opportunity to join us in exploring new technologies and use them to design, deploy and operate highly performant systems. 
  In this role you are expected to be comfortable working to high standards as a professional data engineer, dealing with huge amounts of business-critical data (PBs), and to contribute across a full spectrum of responsibilities from architecture to ops. 
  Impact you will make: 
  
  Develop high-quality reliable data pipelines that convert data streams into valuable information 
  Design, implement and deploy both real time and batch data processing pipelines for internal and external customers 
  Develop tools to monitor, debug, analyze and operate our data infrastructure 
  Design and implement data technologies that can scale for hundreds of millions of users 
  Collaborate with our product and business teams to deliver valuable new features and functions 
  
 Who you are: 
  
  BS in Computer Science or related technical discipline or equivalent experience 
  2+ years of professional experience in data engineering environments 
  2+ years of experience with SQL and programming in any of Python/Java/Scala or similar HLL 
  Experience with data pipelines processing larger than 10TB of data is a plus 
  Experience working in cloud environments, ideally with GCP or AWS 
  Strong experience in improving performance of queries and data jobs and scaling the system for exponential growth in data volumes and traffic 
  Expert debugging skills and enthusiasm for automation to deliver high-quality reliable systems 
  Comfortable with modern development tools such as Git and Confluence and working in a distributed agile team environment with both high autonomy and regular collaboration 
  
 
 Perks: 
  
  Comprehensive medical, dental and vision insurance 
  Restricted Stock Units (RSUs) - you will have the potential of RSUs depending on the level/role 
  401(k) plan with match through Fidelity 
  Catered lunches and fully stocked kitchens 
  Commuter Program 
  Flex Vacation – personal time to refresh your mind/body/soul, spend time with loved ones and celebrate life events. There is no accrual or specific limit to the amount of time an employee may use 
  
 
 More about us: 
  We are proud of the product we've built and appreciate the impact it has on other people's businesses and lives. We want to be surrounded by people who are always finding opportunities to try something new and grow. We love data and anything that helps drive intelligent decisions and always design with the user in mind. Sounds like a fit? Join us, and be part of the team that will change the future of mobile gaming! 
  We are an equal opportunity employer — we celebrate diversity and are committed to creating an inclusive environment for all employees and make our hiring decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. 
  California Residents, please review the Chartboost California Employment Candidate Privacy Notice before submitting any personal information. 
  
 The pay range for this position in California at the start of employment is expected to be between $108,720.00 and $135,900.00 per year. For applicants based in New York City and New Jersey at the start of employment is expected to be between $108,720.00 and $135,900.00 per year. Furthermore, the pay range for this position for applicants based in Washington at the start of employment is expected to be between $96,640.00 and $120,800.00 per year. 
  However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations.  Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards and eligibility to participate in our 401(K) plan, in addition to a full range of medical, dental, vision, and basic life insurance. Employees will also receive 13 paid holidays per calendar year, unlimited discretionary time off, and will receive 10 sick days per calendar year. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an ""at-will position"" and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors","<div>
 <p><b>Who we are:</b></p> 
 <p> Chartboost is the leading in-app monetization and programmatic advertising platform. We reach a global audience of over 700 million monthly active users and process over 2.7 trillion monthly advertising auctions. The Chartboost SDK is one the most widely integrated mobile ad SDKs and through the Chartboost Exchange, Ad Network, DSP and other services, we empower mobile app developers to build businesses, while connecting advertisers to highly engaged audiences.</p> 
 <p></p> 
 <p>Chartboost is hiring accomplished data engineers to join our team, helping us build and maintain a data platform that supports diverse uses. This includes data analysis, exploration, aggregation, user modeling, and scalable training systems. As part of the Data Team you will be a key player in our small, nimble, internationally-distributed team and drive significant impact all across the company&apos;s data technology and business.</p> 
 <p> We currently use Python, Java, and Scala to develop tools with Spark, Kafka, Airflow, MySQL, Druid, Spinnaker, and Kubernetes. We run in both GCP and AWS, but primarily work with Dataproc, Dataflow, BigQuery and Big Table. You will have the opportunity to join us in exploring new technologies and use them to design, deploy and operate highly performant systems.</p> 
 <p> In this role you are expected to be comfortable working to high standards as a professional data engineer, dealing with huge amounts of business-critical data (PBs), and to contribute across a full spectrum of responsibilities from architecture to ops.</p> 
 <p><b> Impact you will make:</b></p> 
 <ul> 
  <li>Develop high-quality reliable data pipelines that convert data streams into valuable information</li> 
  <li>Design, implement and deploy both real time and batch data processing pipelines for internal and external customers</li> 
  <li>Develop tools to monitor, debug, analyze and operate our data infrastructure</li> 
  <li>Design and implement data technologies that can scale for hundreds of millions of users</li> 
  <li>Collaborate with our product and business teams to deliver valuable new features and functions</li> 
 </ul> 
 <p><b>Who you are:</b></p> 
 <ul> 
  <li>BS in Computer Science or related technical discipline or equivalent experience</li> 
  <li>2+ years of professional experience in data engineering environments</li> 
  <li>2+ years of experience with SQL and programming in any of Python/Java/Scala or similar HLL</li> 
  <li>Experience with data pipelines processing larger than 10TB of data is a plus</li> 
  <li>Experience working in cloud environments, ideally with GCP or AWS</li> 
  <li>Strong experience in improving performance of queries and data jobs and scaling the system for exponential growth in data volumes and traffic</li> 
  <li>Expert debugging skills and enthusiasm for automation to deliver high-quality reliable systems</li> 
  <li>Comfortable with modern development tools such as Git and Confluence and working in a distributed agile team environment with both high autonomy and regular collaboration</li> 
 </ul> 
 <p></p>
 <p><b>Perks:</b></p> 
 <ul> 
  <li>Comprehensive medical, dental and vision insurance</li> 
  <li>Restricted Stock Units (RSUs) - you will have the potential of RSUs depending on the level/role</li> 
  <li>401(k) plan with match through Fidelity</li> 
  <li>Catered lunches and fully stocked kitchens</li> 
  <li>Commuter Program</li> 
  <li>Flex Vacation &#x2013; personal time to refresh your mind/body/soul, spend time with loved ones and celebrate life events. There is no accrual or specific limit to the amount of time an employee may use</li> 
 </ul> 
 <p></p>
 <p><b>More about us:</b></p> 
 <p> We are proud of the product we&apos;ve built and appreciate the impact it has on other people&apos;s businesses and lives. We want to be surrounded by people who are always finding opportunities to try something new and grow. We love data and anything that helps drive intelligent decisions and always design with the user in mind. Sounds like a fit? Join us, and be part of the team that will change the future of mobile gaming!</p> 
 <p> We are an equal opportunity employer &#x2014; we celebrate diversity and are committed to creating an inclusive environment for all employees and make our hiring decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.</p> 
 <p> California Residents, please review the Chartboost California Employment Candidate Privacy Notice before submitting any personal information.</p> 
 <p></p> 
 <p><i>The pay range for this position in </i><b><i>California</i></b><i> at the start of employment is expected to be between </i><i>&#x24;108,720.00</i><i> and </i><i>&#x24;135,900.00 </i><i>per year. </i><i>For applicants based in </i><b><i>New York City </i></b><i>and </i><b><i>New Jersey</i></b><i> at the start of employment is expected to be between </i><i>&#x24;108,720.00</i><i> and </i><i>&#x24;135,900.00</i><i> per year. Furthermore, the pay range for this position for applicants based in </i><b><i>Washington</i></b><i> at the start of employment is expected to be between </i><i>&#x24;96,640.00</i><i> and </i><i>&#x24;120,800.00</i><i> per year.</i></p> 
 <p><i> However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations.</i><br> <br> <i>Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards and eligibility to participate in our 401(K) plan, in addition to a full range of medical, dental, vision, and basic life insurance. Employees will also receive 13 paid holidays per calendar year, unlimited discretionary time off, and will receive 10 sick days per calendar year. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an &quot;at-will position&quot; and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors</i></p>
</div>
<p></p>",https://boards.greenhouse.io/chartboost/jobs/5404685?gh_src=fc304f791us,afe7bc4e3d93a349,,,,,"San Francisco, CA",Data Engineer,18 days ago,2023-09-30T13:39:31.718Z,4.3,3.0,"$96,640 - $120,800 a year",2023-10-18T13:39:31.738Z,US,remote,data engineer,https://www.indeed.com/rc/clk?jk=afe7bc4e3d93a349&from=jasx&tk=1hd1g5htglell801&vjs=3
