{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 pandas s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-apify-datasets-lake\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "AWS_S3_BUCKET = os.getenv('AWS_S3_BUCKET')\n",
    "s3_client = boto3.client(\"s3\")\n",
    "print(AWS_S3_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze/raw_indeed_jobs/dataset_indeed-scraper_2023-10-18_13-39-59-697.csv\n",
      "bronze/raw_indeed_jobs/dataset_indeed-scraper_2023-10-19_14-08-42-423.csv\n",
      "bronze/raw_indeed_jobs/dataset_indeed-scraper_2023-10-20_12-42-20-486.csv\n",
      "bronze/raw_indeed_jobs/dataset_indeed-scraper_2023-10-21_13-12-16-991.csv\n",
      "['bronze/raw_indeed_jobs/dataset_indeed-scraper_2023-10-18_13-39-59-697.csv', 'bronze/raw_indeed_jobs/dataset_indeed-scraper_2023-10-19_14-08-42-423.csv', 'bronze/raw_indeed_jobs/dataset_indeed-scraper_2023-10-20_12-42-20-486.csv', 'bronze/raw_indeed_jobs/dataset_indeed-scraper_2023-10-21_13-12-16-991.csv']\n"
     ]
    }
   ],
   "source": [
    "response = s3_client.list_objects(Bucket=AWS_S3_BUCKET, Prefix=\"bronze/raw_indeed_jobs/dataset\")\n",
    "contents = response[\"Contents\"]\n",
    "datasets = []\n",
    "for item in contents:\n",
    "    datasets.append(item[\"Key\"])\n",
    "    print(item[\"Key\"])\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully got desired object\n",
      "Successfully got desired object\n",
      "Successfully got desired object\n",
      "Successfully got desired object\n"
     ]
    }
   ],
   "source": [
    "# Get the data from bronze bucket and store it in array to be worked on\n",
    "dataframes = []\n",
    "for data in datasets:\n",
    "    job_object = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=data)\n",
    "    status = job_object.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "    jobs_df = pd.core.frame.DataFrame\n",
    "    if status == 200:\n",
    "        print(\"Successfully got desired object\")\n",
    "        jobs_df = pd.read_csv(job_object.get(\"Body\"))\n",
    "        dataframes.append(jobs_df)\n",
    "    else:\n",
    "        print(\"Unable to get object, something wrong\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_filtered_frames = []\n",
    "for frame in dataframes:\n",
    "    filtered_salary = frame['salary'].notnull()\n",
    "    filtered_curr = frame[filtered_salary]\n",
    "    salary_filtered_frames.append(filtered_curr)\n",
    "len(salary_filtered_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes\n",
    "# Final frame should represent correct number of uniques\n",
    "merged_frame = pd.DataFrame\n",
    "for index, frame in enumerate(salary_filtered_frames):\n",
    "    print(len(frame))\n",
    "    if index == 0:\n",
    "        merged_frame = frame\n",
    "    else:   \n",
    "        new_frame = merged_frame.merge(frame, how=\"outer\")\n",
    "        merged_frame = new_frame\n",
    "merged_frame.reset_index()\n",
    "final_frame = merged_frame.drop_duplicates(subset=['id'])\n",
    "final_frame.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "Index(['company', 'description', 'descriptionHTML', 'externalApplyLink', 'id',\n",
      "       'jobType', 'jobType/0', 'jobType/1', 'jobType/2', 'location',\n",
      "       'positionName', 'postedAt', 'postingDateParsed', 'rating',\n",
      "       'reviewsCount', 'salary', 'scrapedAt', 'searchInput/country',\n",
      "       'searchInput/location', 'searchInput/position', 'url'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(len(final_frame))\n",
    "print(final_frame.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "claude_v1_model_id = 'anthropic.claude-instant-v1'\n",
    "claude_v2_model_id = 'anthropic.claude-v2'\n",
    "llm = Bedrock(model_id=claude_v1_model_id, model_kwargs={'max_tokens_to_sample':8000})\n",
    "#output = llm.predict(\"How are you today?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate.from_template(\"\"\"\n",
    "Extract specific skills and responsibilities from the following job description: {job}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are the top 5 skills and top technologies extracted from the job description:\n",
      "\n",
      "<topskills>\n",
      "1. Databricks/PySpark\n",
      "2. AWS\n",
      "3. Cloud experience  \n",
      "4. Software engineering \n",
      "5. Typescript\n",
      "</topskills>\n",
      "\n",
      "<toptech>\n",
      "1. AWS\n",
      "2. Cloud \n",
      "3. Databricks\n",
      "4. PySpark\n",
      "5. Typescript\n",
      "</toptech>\n"
     ]
    }
   ],
   "source": [
    "huge_file_read = open(\"job_summaries_massive.txt\", \"r\", encoding=\"utf-8\")\n",
    "huge_file_text = huge_file_read.read()\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "Act as an expert formatter. You format based on the given format. Skip the preamble.\n",
    "I will provide you with job skills and responsibilities text for Data Engineering, output the top 5 skills AND technologies in the following format: <topskills>1. Skill 2. Skill ...</topskills> <toptech>1. Tech 2. Tech .... </toptech>.\n",
    "<dataengineeringtext>{text}</dataengineeringtext>\n",
    "\"\"\")\n",
    "prompt = template.format(text=huge_file_text)\n",
    "output = llm.predict(prompt)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactored example, with uniqued dataset filtered on JobID\n",
    "path = 'skills-res-v4-unique-day-3/'\n",
    "file_name = 'claude_v1_df_final_row_'\n",
    "for index, row in final_frame.iterrows():\n",
    "    description = row['description']\n",
    "    file = open(f\"{path}{file_name}{index}.txt\", \"w\", encoding=\"utf-8\")\n",
    "    prompt = template.format(job=description)\n",
    "    output = llm.predict(prompt)\n",
    "    file.write(output)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the link to the job posting \n",
    "path = 'skills-res-v4-unique-day-3/'\n",
    "file_name = 'claude_v1_df_final_row_'\n",
    "for index, row in final_frame.iterrows():\n",
    "    url = row['url']\n",
    "    file = open(f\"{path}{file_name}{index}.txt\", \"a\", encoding=\"utf-8\")\n",
    "    #prompt = template.format(job=description)\n",
    "    #output = llm.predict(prompt)\n",
    "    file.write(f\"\\n\\n{url}\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of the documents\n",
    "docs = os.listdir(path)\n",
    "combined_day_3 = \"\"\n",
    "for doc in docs:\n",
    "    doc_file = open(path+doc, \"r\", encoding=\"utf-8\")\n",
    "    doc_text = doc_file.read()\n",
    "    combined_day_3 += f\"\\n\\n{doc_text}\"\n",
    "    doc_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to gain some insight from AI\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "Act as an expert formatter. You format based on the given format. Skip the preamble.\n",
    "I will provide you with a combined list of job skills and responsibilities text for Data Engineering taken from multiple job postings, output the top 5 skills AND technologies that appear most often across the entire text, do not include skills such as Data Engineering or ETL processes, be specific in the following format: <topskills>1. Skill 2. Skill ...</topskills> <toptech>1. Tech 2. Tech .... </toptech>.\n",
    "<dataengineeringtext>{text}</dataengineeringtext>\n",
    "\"\"\")\n",
    "prompt = template.format(text=combined_day_3)\n",
    "llm.model_id = claude_v2_model_id\n",
    "output = llm.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are the top 5 skills and technologies extracted from the data engineering job description text:\n",
      "\n",
      "<topskills>\n",
      "1. SQL\n",
      "2. Python\n",
      "3. ETL processes\n",
      "4. Data modeling \n",
      "5. Data warehousing\n",
      "</topskills>\n",
      "\n",
      "<toptech>  \n",
      "1. AWS services (S3, Redshift, Glue, etc)\n",
      "2. Databricks\n",
      "3. Snowflake\n",
      "4. Azure services (Azure Data Factory, Azure Databricks, etc)\n",
      "5. Apache Spark\n",
      "</toptech>\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_final_frame = final_frame.to_csv(\"combined_10_21.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
