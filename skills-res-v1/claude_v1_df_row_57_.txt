 Here are the key skills and responsibilities extracted from the job description:

Skills:
- Proficiency in Python
- Familiarity with Postgres, Elasticsearch, HTML/CSS/JS
- Strong understanding of web services and distributed systems
- Problem-solving skills and attention to detail 
- Experience with web crawling, Django, workflow systems, distributed databases, Hadoop, Ansible, GitLab, GitHub, Sentry, Grafana, JIRA
- Linux experience

Responsibilities:
- Collaborate with team to understand user needs, design features, support web crawling/preservation, improve performance/reliability
- Implement, test, and maintain software across the stack  
- Develop, monitor, and maintain the partner application for web crawls  
- Improve the distributed system for web crawls, post-processing, indexing, deduplication, and reporting
- Participate in code reviews to ensure quality and knowledge diffusion
- Document architecture, software, and features